Darknet V5 "Moonlit" v5.0-158-gb0e55463 [feature/map-reporting-fixes-superclean]
CUDA runtime version 12080 (v12.8), driver version 12080 (v12.8)
cuDNN version 12080 (v9.7.0), use of half-size floats is ENABLED
=> 0: NVIDIA B200 [#10.0], 178.4 GiB
OpenCV 4.6.0, Ubuntu 24.04, docker
0
Prepare additional network for mAP calculation...
0: compute_capability=1000, cudnn_half=1, GPU=NVIDIA B200
Allocating workspace to transfer between CPU and GPU:  64.7 MiB
0: compute_capability=1000, cudnn_half=1, GPU=NVIDIA B200
Allocating workspace to transfer between CPU and GPU:  4.3 GiB
Learning Rate: 0.001300, Momentum: 0.949000, Decay: 0.000500
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
mAP calculations will be every 406 iterations
weights will be saved every 1000 iterations
Using 112 threads to prime loading 25984 images.
-> loading image 1795/25984 (7%) in 1.057 seconds         -> loading image 2353/25984 (9%) in 1.390 seconds         -> loading image 2928/25984 (11%) in 1.724 seconds         -> loading image 3495/25984 (13%) in 2.057 seconds         -> loading image 4054/25984 (16%) in 2.390 seconds         -> loading image 4599/25984 (18%) in 2.723 seconds         -> loading image 5161/25984 (20%) in 3.056 seconds         -> loading image 5735/25984 (22%) in 3.389 seconds         -> loading image 6274/25984 (24%) in 3.722 seconds         -> loading image 6828/25984 (26%) in 4.055 seconds         -> loading image 7437/25984 (29%) in 4.388 seconds         -> loading image 7988/25984 (31%) in 4.721 seconds         -> loading image 8552/25984 (33%) in 5.054 seconds         -> loading image 9096/25984 (35%) in 5.387 seconds         -> loading image 9685/25984 (37%) in 5.720 seconds         -> loading image 10215/25984 (39%) in 6.053 seconds         -> loading image 10774/25984 (41%) in 6.387 seconds         -> loading image 11346/25984 (44%) in 6.720 seconds         -> loading image 11928/25984 (46%) in 7.053 seconds         -> loading image 12493/25984 (48%) in 7.386 seconds         -> loading image 13065/25984 (50%) in 7.721 seconds         -> loading image 13624/25984 (52%) in 8.054 seconds         -> loading image 14195/25984 (55%) in 8.387 seconds         -> loading image 14752/25984 (57%) in 8.720 seconds         -> loading image 15311/25984 (59%) in 9.053 seconds         -> loading image 15857/25984 (61%) in 9.387 seconds         -> loading image 16451/25984 (63%) in 9.720 seconds         -> loading image 16978/25984 (65%) in 10.053 seconds         -> loading image 17527/25984 (67%) in 10.386 seconds         -> loading image 18128/25984 (70%) in 10.719 seconds         -> loading image 18682/25984 (72%) in 11.052 seconds         -> loading image 19249/25984 (74%) in 11.385 seconds         -> loading image 19810/25984 (76%) in 11.718 seconds         -> loading image 20375/25984 (78%) in 12.051 seconds         -> loading image 20922/25984 (81%) in 12.384 seconds         -> loading image 21494/25984 (83%) in 12.717 seconds         -> loading image 22059/25984 (85%) in 13.050 seconds         -> loading image 22617/25984 (87%) in 13.383 seconds         -> loading image 23180/25984 (89%) in 13.716 seconds         -> loading image 23731/25984 (91%) in 14.050 seconds         -> loading image 24283/25984 (93%) in 14.383 seconds         -> loading image 24852/25984 (96%) in 14.716 seconds         -> loading image 25412/25984 (98%) in 15.049 seconds         -> loading image 25951/25984 (100%) in 15.382 seconds         -> loading image 25984/25984 (100%) in 15.715 seconds         
-> loaded 25984 images in 15.715 seconds
-> done with the 112 image prime threads
Resizing, random_coef=1.400000, batch=4, 1376x1056
Creating 6 permanent CPU threads to load images and bounding boxes.
GPU #0: allocating workspace: 16.6 GiB begins at 0x144d48000000
1: loss=9278.097, avg loss=9278.097, last=none, best=none, next=1000, rate=0.00000000, load 64=1.1 seconds, train=5.5 seconds, 64 images, time remaining=26.7 hours
2: loss=9291.402, avg loss=9279.427, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=5.1 seconds, 128 images, time remaining=18.9 hours
3: loss=9284.670, avg loss=9279.951, last=none, best=none, next=1000, rate=0.00000000, load 64=902.9 milliseconds, train=5.1 seconds, 192 images, time remaining=16.3 hours
4: loss=9267.385, avg loss=9278.694, last=none, best=none, next=1000, rate=0.00000000, load 64=985.5 milliseconds, train=5.1 seconds, 256 images, time remaining=15 hours
5: loss=9303.682, avg loss=9281.192, last=none, best=none, next=1000, rate=0.00000000, load 64=932.9 milliseconds, train=5.1 seconds, 320 images, time remaining=14.2 hours
6: loss=9303.542, avg loss=9283.427, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=5.1 seconds, 384 images, time remaining=14.1 hours
7: loss=9279.008, avg loss=9282.984, last=none, best=none, next=1000, rate=0.00000000, load 64=1.3 seconds, train=5.1 seconds, 448 images, time remaining=13.6 hours
8: loss=9290.479, avg loss=9283.733, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=5.1 seconds, 512 images, time remaining=13.3 hours
9: loss=9254.341, avg loss=9280.794, last=none, best=none, next=1000, rate=0.00000000, load 64=1.3 seconds, train=5.1 seconds, 576 images, time remaining=13.1 hours
10: loss=9293.810, avg loss=9282.096, last=none, best=none, next=1000, rate=0.00000000, load 64=1.1 seconds, train=5.1 seconds, 640 images, time remaining=12.9 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144d48000000
11: loss=5844.826, avg loss=8938.368, last=none, best=none, next=1000, rate=0.00000000, load 64=936.0 milliseconds, train=3.4 seconds, 704 images, time remaining=12.5 hours
12: loss=5805.678, avg loss=8625.099, last=none, best=none, next=1000, rate=0.00000000, load 64=1.3 seconds, train=3.3 seconds, 768 images, time remaining=12.2 hours
13: loss=5804.603, avg loss=8343.049, last=none, best=none, next=1000, rate=0.00000000, load 64=920.0 milliseconds, train=3.4 seconds, 832 images, time remaining=11.8 hours
14: loss=5819.513, avg loss=8090.695, last=none, best=none, next=1000, rate=0.00000000, load 64=1.9 seconds, train=3.4 seconds, 896 images, time remaining=11.4 hours
15: loss=5805.159, avg loss=7862.141, last=none, best=none, next=1000, rate=0.00000000, load 64=1.1 seconds, train=3.4 seconds, 960 images, time remaining=11.2 hours
16: loss=5802.305, avg loss=7656.157, last=none, best=none, next=1000, rate=0.00000000, load 64=782.1 milliseconds, train=3.3 seconds, 1024 images, time remaining=11 hours
17: loss=5810.442, avg loss=7471.585, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=3.4 seconds, 1088 images, time remaining=10.7 hours
18: loss=5811.948, avg loss=7305.622, last=none, best=none, next=1000, rate=0.00000000, load 64=896.9 milliseconds, train=3.4 seconds, 1152 images, time remaining=10.6 hours
19: loss=5815.712, avg loss=7156.630, last=none, best=none, next=1000, rate=0.00000000, load 64=846.9 milliseconds, train=3.4 seconds, 1216 images, time remaining=10.4 hours
20: loss=5822.659, avg loss=7023.233, last=none, best=none, next=1000, rate=0.00000000, load 64=1.1 seconds, train=3.4 seconds, 1280 images, time remaining=10.2 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
21: loss=8784.143, avg loss=7199.324, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=4.9 seconds, 1344 images, time remaining=10.7 hours
22: loss=8784.203, avg loss=7357.812, last=none, best=none, next=1000, rate=0.00000000, load 64=990.1 milliseconds, train=4.9 seconds, 1408 images, time remaining=10.7 hours
23: loss=8768.319, avg loss=7498.862, last=none, best=none, next=1000, rate=0.00000000, load 64=986.5 milliseconds, train=4.9 seconds, 1472 images, time remaining=10.7 hours
24: loss=8750.047, avg loss=7623.980, last=none, best=none, next=1000, rate=0.00000000, load 64=1.2 seconds, train=4.9 seconds, 1536 images, time remaining=10.7 hours
25: loss=8750.405, avg loss=7736.623, last=none, best=none, next=1000, rate=0.00000000, load 64=822.4 milliseconds, train=4.9 seconds, 1600 images, time remaining=10.7 hours
26: loss=8734.550, avg loss=7836.415, last=none, best=none, next=1000, rate=0.00000000, load 64=1.1 seconds, train=5.0 seconds, 1664 images, time remaining=10.6 hours
27: loss=8712.625, avg loss=7924.036, last=none, best=none, next=1000, rate=0.00000000, load 64=852.4 milliseconds, train=4.9 seconds, 1728 images, time remaining=10.7 hours
28: loss=8708.856, avg loss=8002.518, last=none, best=none, next=1000, rate=0.00000000, load 64=942.1 milliseconds, train=4.9 seconds, 1792 images, time remaining=10.7 hours
29: loss=8694.021, avg loss=8071.668, last=none, best=none, next=1000, rate=0.00000000, load 64=740.6 milliseconds, train=4.9 seconds, 1856 images, time remaining=10.7 hours
30: loss=8663.355, avg loss=8130.836, last=none, best=none, next=1000, rate=0.00000000, load 64=940.3 milliseconds, train=4.9 seconds, 1920 images, time remaining=10.7 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x146016000000
31: loss=3255.004, avg loss=7643.253, last=none, best=none, next=1000, rate=0.00000000, load 64=1.3 seconds, train=1.5 seconds, 1984 images, time remaining=10.6 hours
32: loss=3238.526, avg loss=7202.780, last=none, best=none, next=1000, rate=0.00000000, load 64=803.1 milliseconds, train=1.5 seconds, 2048 images, time remaining=10.3 hours
33: loss=3237.164, avg loss=6806.218, last=none, best=none, next=1000, rate=0.00000000, load 64=657.4 milliseconds, train=1.5 seconds, 2112 images, time remaining=10.1 hours
34: loss=3214.695, avg loss=6447.066, last=none, best=none, next=1000, rate=0.00000000, load 64=894.0 milliseconds, train=1.5 seconds, 2176 images, time remaining=9.9 hours
35: loss=3195.918, avg loss=6121.951, last=none, best=none, next=1000, rate=0.00000000, load 64=626.0 milliseconds, train=1.5 seconds, 2240 images, time remaining=9.7 hours
36: loss=3184.035, avg loss=5828.159, last=none, best=none, next=1000, rate=0.00000000, load 64=1.0 seconds, train=1.5 seconds, 2304 images, time remaining=9.5 hours
37: loss=3153.404, avg loss=5560.684, last=none, best=none, next=1000, rate=0.00000000, load 64=494.3 milliseconds, train=1.5 seconds, 2368 images, time remaining=9.4 hours
38: loss=3131.525, avg loss=5317.768, last=none, best=none, next=1000, rate=0.00000000, load 64=679.3 milliseconds, train=1.5 seconds, 2432 images, time remaining=9.2 hours
39: loss=3114.986, avg loss=5097.489, last=none, best=none, next=1000, rate=0.00000000, load 64=661.3 milliseconds, train=1.5 seconds, 2496 images, time remaining=9.1 hours
40: loss=3088.300, avg loss=4896.570, last=none, best=none, next=1000, rate=0.00000000, load 64=858.4 milliseconds, train=1.5 seconds, 2560 images, time remaining=8.9 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b22000000
41: loss=2545.757, avg loss=4661.489, last=none, best=none, next=1000, rate=0.00000000, load 64=659.0 milliseconds, train=1.2 seconds, 2624 images, time remaining=8.8 hours
42: loss=2521.934, avg loss=4447.533, last=none, best=none, next=1000, rate=0.00000000, load 64=621.5 milliseconds, train=1.2 seconds, 2688 images, time remaining=8.6 hours
43: loss=2490.841, avg loss=4251.864, last=none, best=none, next=1000, rate=0.00000000, load 64=781.7 milliseconds, train=1.2 seconds, 2752 images, time remaining=8.5 hours
44: loss=2459.837, avg loss=4072.661, last=none, best=none, next=1000, rate=0.00000000, load 64=564.9 milliseconds, train=1.2 seconds, 2816 images, time remaining=8.4 hours
45: loss=2435.047, avg loss=3908.899, last=none, best=none, next=1000, rate=0.00000001, load 64=805.8 milliseconds, train=1.2 seconds, 2880 images, time remaining=8.2 hours
46: loss=2404.369, avg loss=3758.446, last=none, best=none, next=1000, rate=0.00000001, load 64=968.4 milliseconds, train=1.2 seconds, 2944 images, time remaining=8.1 hours
47: loss=2375.583, avg loss=3620.160, last=none, best=none, next=1000, rate=0.00000001, load 64=646.8 milliseconds, train=1.2 seconds, 3008 images, time remaining=8 hours
48: loss=2330.945, avg loss=3491.238, last=none, best=none, next=1000, rate=0.00000001, load 64=695.5 milliseconds, train=1.2 seconds, 3072 images, time remaining=7.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
49: loss=2299.416, avg loss=3372.056, last=none, best=none, next=1000, rate=0.00000001, load 64=1.7 seconds, train=1.2 seconds, 3136 images, time remaining=7.8 hours
50: loss=2259.456, avg loss=3260.796, last=none, best=none, next=1000, rate=0.00000001, load 64=982.1 milliseconds, train=1.2 seconds, 3200 images, time remaining=7.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b22000000
51: loss=2215.751, avg loss=3156.291, last=none, best=none, next=1000, rate=0.00000001, load 64=633.3 milliseconds, train=1.2 seconds, 3264 images, time remaining=7.6 hours
52: loss=2177.229, avg loss=3058.385, last=none, best=none, next=1000, rate=0.00000001, load 64=652.4 milliseconds, train=1.2 seconds, 3328 images, time remaining=7.6 hours
53: loss=2134.684, avg loss=2966.015, last=none, best=none, next=1000, rate=0.00000001, load 64=711.6 milliseconds, train=1.2 seconds, 3392 images, time remaining=7.5 hours
54: loss=2090.027, avg loss=2878.416, last=none, best=none, next=1000, rate=0.00000001, load 64=523.9 milliseconds, train=1.2 seconds, 3456 images, time remaining=7.4 hours
55: loss=2039.419, avg loss=2794.516, last=none, best=none, next=1000, rate=0.00000001, load 64=559.0 milliseconds, train=1.2 seconds, 3520 images, time remaining=7.3 hours
56: loss=1993.703, avg loss=2714.435, last=none, best=none, next=1000, rate=0.00000001, load 64=670.3 milliseconds, train=1.2 seconds, 3584 images, time remaining=7.2 hours
57: loss=1939.213, avg loss=2636.913, last=none, best=none, next=1000, rate=0.00000001, load 64=576.6 milliseconds, train=1.2 seconds, 3648 images, time remaining=7.1 hours
58: loss=1892.868, avg loss=2562.508, last=none, best=none, next=1000, rate=0.00000001, load 64=804.0 milliseconds, train=1.2 seconds, 3712 images, time remaining=7 hours
59: loss=1834.686, avg loss=2489.726, last=none, best=none, next=1000, rate=0.00000002, load 64=513.4 milliseconds, train=1.2 seconds, 3776 images, time remaining=7 hours
60: loss=1776.362, avg loss=2418.390, last=none, best=none, next=1000, rate=0.00000002, load 64=924.6 milliseconds, train=1.2 seconds, 3840 images, time remaining=6.9 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145d00e00000
61: loss=1978.694, avg loss=2374.420, last=none, best=none, next=1000, rate=0.00000002, load 64=672.6 milliseconds, train=1.4 seconds, 3904 images, time remaining=6.9 hours
62: loss=1920.791, avg loss=2329.057, last=none, best=none, next=1000, rate=0.00000002, load 64=652.1 milliseconds, train=1.4 seconds, 3968 images, time remaining=6.8 hours
63: loss=1852.147, avg loss=2281.366, last=none, best=none, next=1000, rate=0.00000002, load 64=1.2 seconds, train=1.4 seconds, 4032 images, time remaining=6.8 hours
64: loss=1781.034, avg loss=2231.333, last=none, best=none, next=1000, rate=0.00000002, load 64=547.7 milliseconds, train=1.4 seconds, 4096 images, time remaining=6.7 hours
65: loss=1706.944, avg loss=2178.894, last=none, best=none, next=1000, rate=0.00000002, load 64=524.4 milliseconds, train=1.4 seconds, 4160 images, time remaining=6.6 hours
66: loss=1635.464, avg loss=2124.551, last=none, best=none, next=1000, rate=0.00000002, load 64=391.8 milliseconds, train=1.4 seconds, 4224 images, time remaining=6.6 hours
67: loss=1573.031, avg loss=2069.399, last=none, best=none, next=1000, rate=0.00000003, load 64=740.0 milliseconds, train=1.4 seconds, 4288 images, time remaining=6.5 hours
68: loss=1501.122, avg loss=2012.571, last=none, best=none, next=1000, rate=0.00000003, load 64=1.0 seconds, train=1.5 seconds, 4352 images, time remaining=6.5 hours
69: loss=1427.550, avg loss=1954.069, last=none, best=none, next=1000, rate=0.00000003, load 64=822.5 milliseconds, train=1.4 seconds, 4416 images, time remaining=6.4 hours
70: loss=1351.608, avg loss=1893.823, last=none, best=none, next=1000, rate=0.00000003, load 64=718.4 milliseconds, train=1.4 seconds, 4480 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146464000000
71: loss=1404.018, avg loss=1844.842, last=none, best=none, next=1000, rate=0.00000003, load 64=513.1 milliseconds, train=1.5 seconds, 4544 images, time remaining=6.4 hours
72: loss=1337.549, avg loss=1794.113, last=none, best=none, next=1000, rate=0.00000003, load 64=975.0 milliseconds, train=1.5 seconds, 4608 images, time remaining=6.4 hours
73: loss=1246.264, avg loss=1739.328, last=none, best=none, next=1000, rate=0.00000004, load 64=730.6 milliseconds, train=1.5 seconds, 4672 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
74: loss=1175.751, avg loss=1682.970, last=none, best=none, next=1000, rate=0.00000004, load 64=3.2 seconds, train=1.5 seconds, 4736 images, time remaining=6.3 hours
75: loss=1092.030, avg loss=1623.876, last=none, best=none, next=1000, rate=0.00000004, load 64=864.8 milliseconds, train=1.5 seconds, 4800 images, time remaining=6.3 hours
76: loss=1020.927, avg loss=1563.581, last=none, best=none, next=1000, rate=0.00000004, load 64=693.6 milliseconds, train=1.5 seconds, 4864 images, time remaining=6.2 hours
77: loss=949.488, avg loss=1502.172, last=none, best=none, next=1000, rate=0.00000005, load 64=559.8 milliseconds, train=1.5 seconds, 4928 images, time remaining=6.2 hours
78: loss=871.706, avg loss=1439.125, last=none, best=none, next=1000, rate=0.00000005, load 64=781.5 milliseconds, train=1.5 seconds, 4992 images, time remaining=6.2 hours
79: loss=778.401, avg loss=1373.053, last=none, best=none, next=1000, rate=0.00000005, load 64=551.6 milliseconds, train=1.5 seconds, 5056 images, time remaining=6.1 hours
80: loss=723.037, avg loss=1308.051, last=none, best=none, next=1000, rate=0.00000005, load 64=611.9 milliseconds, train=1.5 seconds, 5120 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
81: loss=1211.906, avg loss=1298.437, last=none, best=none, next=1000, rate=0.00000006, load 64=706.4 milliseconds, train=3.6 seconds, 5184 images, time remaining=6.2 hours
82: loss=1086.772, avg loss=1277.270, last=none, best=none, next=1000, rate=0.00000006, load 64=818.5 milliseconds, train=3.6 seconds, 5248 images, time remaining=6.2 hours
83: loss=960.973, avg loss=1245.640, last=none, best=none, next=1000, rate=0.00000006, load 64=641.6 milliseconds, train=3.6 seconds, 5312 images, time remaining=6.2 hours
84: loss=877.487, avg loss=1208.825, last=none, best=none, next=1000, rate=0.00000006, load 64=627.1 milliseconds, train=3.6 seconds, 5376 images, time remaining=6.3 hours
85: loss=759.606, avg loss=1163.903, last=none, best=none, next=1000, rate=0.00000007, load 64=518.6 milliseconds, train=3.6 seconds, 5440 images, time remaining=6.3 hours
86: loss=673.906, avg loss=1114.903, last=none, best=none, next=1000, rate=0.00000007, load 64=639.9 milliseconds, train=3.6 seconds, 5504 images, time remaining=6.3 hours
87: loss=594.034, avg loss=1062.816, last=none, best=none, next=1000, rate=0.00000007, load 64=1.2 seconds, train=3.6 seconds, 5568 images, time remaining=6.3 hours
88: loss=538.143, avg loss=1010.349, last=none, best=none, next=1000, rate=0.00000008, load 64=682.6 milliseconds, train=3.6 seconds, 5632 images, time remaining=6.3 hours
89: loss=475.017, avg loss=956.816, last=none, best=none, next=1000, rate=0.00000008, load 64=520.0 milliseconds, train=3.6 seconds, 5696 images, time remaining=6.3 hours
90: loss=421.509, avg loss=903.285, last=none, best=none, next=1000, rate=0.00000009, load 64=761.6 milliseconds, train=3.6 seconds, 5760 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x145002000000
91: loss=245.246, avg loss=837.481, last=none, best=none, next=1000, rate=0.00000009, load 64=526.7 milliseconds, train=1.8 seconds, 5824 images, time remaining=6.3 hours
92: loss=226.604, avg loss=776.393, last=none, best=none, next=1000, rate=0.00000009, load 64=677.5 milliseconds, train=1.8 seconds, 5888 images, time remaining=6.3 hours
93: loss=208.207, avg loss=719.575, last=none, best=none, next=1000, rate=0.00000010, load 64=1.3 seconds, train=1.8 seconds, 5952 images, time remaining=6.3 hours
94: loss=183.885, avg loss=666.006, last=none, best=none, next=1000, rate=0.00000010, load 64=956.0 milliseconds, train=1.8 seconds, 6016 images, time remaining=6.3 hours
95: loss=169.171, avg loss=616.322, last=none, best=none, next=1000, rate=0.00000011, load 64=499.5 milliseconds, train=1.8 seconds, 6080 images, time remaining=6.2 hours
96: loss=150.540, avg loss=569.744, last=none, best=none, next=1000, rate=0.00000011, load 64=465.2 milliseconds, train=1.8 seconds, 6144 images, time remaining=6.2 hours
97: loss=144.625, avg loss=527.232, last=none, best=none, next=1000, rate=0.00000012, load 64=689.6 milliseconds, train=1.8 seconds, 6208 images, time remaining=6.2 hours
98: loss=128.710, avg loss=487.380, last=none, best=none, next=1000, rate=0.00000012, load 64=671.8 milliseconds, train=1.8 seconds, 6272 images, time remaining=6.2 hours
99: loss=121.848, avg loss=450.827, last=none, best=none, next=1000, rate=0.00000012, load 64=710.4 milliseconds, train=1.8 seconds, 6336 images, time remaining=6.1 hours
100: loss=113.494, avg loss=417.093, last=none, best=none, next=1000, rate=0.00000013, load 64=823.3 milliseconds, train=1.8 seconds, 6400 images, time remaining=6.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145cf0000000
101: loss=97.945, avg loss=385.179, last=none, best=none, next=1000, rate=0.00000014, load 64=797.2 milliseconds, train=1.6 seconds, 6464 images, time remaining=6.1 hours
102: loss=96.641, avg loss=356.325, last=none, best=none, next=1000, rate=0.00000014, load 64=712.6 milliseconds, train=1.6 seconds, 6528 images, time remaining=6.1 hours
103: loss=89.894, avg loss=329.682, last=none, best=none, next=1000, rate=0.00000015, load 64=711.8 milliseconds, train=1.6 seconds, 6592 images, time remaining=6.1 hours
104: loss=83.102, avg loss=305.024, last=none, best=none, next=1000, rate=0.00000015, load 64=818.9 milliseconds, train=1.6 seconds, 6656 images, time remaining=6.1 hours
105: loss=84.781, avg loss=282.999, last=none, best=none, next=1000, rate=0.00000016, load 64=473.3 milliseconds, train=1.6 seconds, 6720 images, time remaining=6 hours
106: loss=77.212, avg loss=262.421, last=none, best=none, next=1000, rate=0.00000016, load 64=752.4 milliseconds, train=1.6 seconds, 6784 images, time remaining=6 hours
107: loss=72.927, avg loss=243.471, last=none, best=none, next=1000, rate=0.00000017, load 64=513.5 milliseconds, train=1.6 seconds, 6848 images, time remaining=6 hours
108: loss=75.128, avg loss=226.637, last=none, best=none, next=1000, rate=0.00000018, load 64=693.8 milliseconds, train=1.6 seconds, 6912 images, time remaining=6 hours
109: loss=71.532, avg loss=211.126, last=none, best=none, next=1000, rate=0.00000018, load 64=801.8 milliseconds, train=1.6 seconds, 6976 images, time remaining=5.9 hours
110: loss=68.970, avg loss=196.911, last=none, best=none, next=1000, rate=0.00000019, load 64=491.4 milliseconds, train=1.6 seconds, 7040 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145f92000000
111: loss=64.905, avg loss=183.710, last=none, best=none, next=1000, rate=0.00000020, load 64=656.3 milliseconds, train=1.5 seconds, 7104 images, time remaining=5.9 hours
112: loss=70.820, avg loss=172.421, last=none, best=none, next=1000, rate=0.00000020, load 64=557.2 milliseconds, train=1.5 seconds, 7168 images, time remaining=5.9 hours
113: loss=64.573, avg loss=161.636, last=none, best=none, next=1000, rate=0.00000021, load 64=683.7 milliseconds, train=1.5 seconds, 7232 images, time remaining=5.9 hours
114: loss=67.829, avg loss=152.256, last=none, best=none, next=1000, rate=0.00000022, load 64=580.3 milliseconds, train=1.5 seconds, 7296 images, time remaining=5.8 hours
115: loss=67.887, avg loss=143.819, last=none, best=none, next=1000, rate=0.00000023, load 64=653.7 milliseconds, train=1.5 seconds, 7360 images, time remaining=5.8 hours
116: loss=62.576, avg loss=135.694, last=none, best=none, next=1000, rate=0.00000024, load 64=571.6 milliseconds, train=1.5 seconds, 7424 images, time remaining=5.8 hours
117: loss=61.714, avg loss=128.296, last=none, best=none, next=1000, rate=0.00000024, load 64=486.4 milliseconds, train=1.5 seconds, 7488 images, time remaining=5.8 hours
118: loss=62.762, avg loss=121.743, last=none, best=none, next=1000, rate=0.00000025, load 64=557.7 milliseconds, train=1.5 seconds, 7552 images, time remaining=5.8 hours
119: loss=53.529, avg loss=114.922, last=none, best=none, next=1000, rate=0.00000026, load 64=781.1 milliseconds, train=1.5 seconds, 7616 images, time remaining=5.7 hours
120: loss=55.244, avg loss=108.954, last=none, best=none, next=1000, rate=0.00000027, load 64=1.4 seconds, train=1.5 seconds, 7680 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145f92000000
121: loss=52.346, avg loss=103.293, last=none, best=none, next=1000, rate=0.00000028, load 64=680.2 milliseconds, train=1.5 seconds, 7744 images, time remaining=5.7 hours
122: loss=57.293, avg loss=98.693, last=none, best=none, next=1000, rate=0.00000029, load 64=640.6 milliseconds, train=1.6 seconds, 7808 images, time remaining=5.7 hours
123: loss=61.890, avg loss=95.013, last=none, best=none, next=1000, rate=0.00000030, load 64=661.8 milliseconds, train=1.5 seconds, 7872 images, time remaining=5.7 hours
124: loss=62.016, avg loss=91.713, last=none, best=none, next=1000, rate=0.00000031, load 64=484.5 milliseconds, train=1.5 seconds, 7936 images, time remaining=5.6 hours
125: loss=61.547, avg loss=88.696, last=none, best=none, next=1000, rate=0.00000032, load 64=507.7 milliseconds, train=1.5 seconds, 8000 images, time remaining=5.6 hours
126: loss=60.801, avg loss=85.907, last=none, best=none, next=1000, rate=0.00000033, load 64=466.0 milliseconds, train=1.5 seconds, 8064 images, time remaining=5.6 hours
127: loss=49.141, avg loss=82.230, last=none, best=none, next=1000, rate=0.00000034, load 64=617.3 milliseconds, train=1.5 seconds, 8128 images, time remaining=5.6 hours
128: loss=57.998, avg loss=79.807, last=none, best=none, next=1000, rate=0.00000035, load 64=504.1 milliseconds, train=1.5 seconds, 8192 images, time remaining=5.6 hours
129: loss=56.604, avg loss=77.487, last=none, best=none, next=1000, rate=0.00000036, load 64=621.4 milliseconds, train=1.5 seconds, 8256 images, time remaining=5.5 hours
130: loss=52.750, avg loss=75.013, last=none, best=none, next=1000, rate=0.00000037, load 64=605.6 milliseconds, train=1.5 seconds, 8320 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x14500a000000
131: loss=56.869, avg loss=73.199, last=none, best=none, next=1000, rate=0.00000038, load 64=588.2 milliseconds, train=1.5 seconds, 8384 images, time remaining=5.5 hours
132: loss=55.902, avg loss=71.469, last=none, best=none, next=1000, rate=0.00000039, load 64=653.5 milliseconds, train=1.5 seconds, 8448 images, time remaining=5.5 hours
133: loss=52.309, avg loss=69.553, last=none, best=none, next=1000, rate=0.00000041, load 64=1.0 seconds, train=1.5 seconds, 8512 images, time remaining=5.5 hours
134: loss=58.931, avg loss=68.491, last=none, best=none, next=1000, rate=0.00000042, load 64=808.4 milliseconds, train=1.5 seconds, 8576 images, time remaining=5.5 hours
135: loss=55.748, avg loss=67.217, last=none, best=none, next=1000, rate=0.00000043, load 64=609.1 milliseconds, train=1.5 seconds, 8640 images, time remaining=5.5 hours
136: loss=54.337, avg loss=65.929, last=none, best=none, next=1000, rate=0.00000044, load 64=402.6 milliseconds, train=1.5 seconds, 8704 images, time remaining=5.4 hours
137: loss=52.760, avg loss=64.612, last=none, best=none, next=1000, rate=0.00000046, load 64=686.5 milliseconds, train=1.5 seconds, 8768 images, time remaining=5.4 hours
138: loss=57.752, avg loss=63.926, last=none, best=none, next=1000, rate=0.00000047, load 64=534.2 milliseconds, train=1.5 seconds, 8832 images, time remaining=5.4 hours
139: loss=49.457, avg loss=62.479, last=none, best=none, next=1000, rate=0.00000049, load 64=573.7 milliseconds, train=1.5 seconds, 8896 images, time remaining=5.4 hours
140: loss=48.489, avg loss=61.080, last=none, best=none, next=1000, rate=0.00000050, load 64=463.5 milliseconds, train=1.5 seconds, 8960 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145f92000000
141: loss=53.255, avg loss=60.297, last=none, best=none, next=1000, rate=0.00000051, load 64=522.1 milliseconds, train=1.5 seconds, 9024 images, time remaining=5.4 hours
142: loss=55.617, avg loss=59.829, last=none, best=none, next=1000, rate=0.00000053, load 64=730.4 milliseconds, train=1.5 seconds, 9088 images, time remaining=5.4 hours
143: loss=57.288, avg loss=59.575, last=none, best=none, next=1000, rate=0.00000054, load 64=542.6 milliseconds, train=1.5 seconds, 9152 images, time remaining=5.4 hours
144: loss=53.385, avg loss=58.956, last=none, best=none, next=1000, rate=0.00000056, load 64=514.2 milliseconds, train=1.5 seconds, 9216 images, time remaining=5.3 hours
145: loss=57.785, avg loss=58.839, last=none, best=none, next=1000, rate=0.00000057, load 64=870.5 milliseconds, train=1.5 seconds, 9280 images, time remaining=5.3 hours
146: loss=51.935, avg loss=58.149, last=none, best=none, next=1000, rate=0.00000059, load 64=681.8 milliseconds, train=1.5 seconds, 9344 images, time remaining=5.3 hours
147: loss=52.466, avg loss=57.580, last=none, best=none, next=1000, rate=0.00000061, load 64=564.8 milliseconds, train=1.5 seconds, 9408 images, time remaining=5.3 hours
148: loss=53.459, avg loss=57.168, last=none, best=none, next=1000, rate=0.00000062, load 64=566.4 milliseconds, train=1.5 seconds, 9472 images, time remaining=5.3 hours
149: loss=54.977, avg loss=56.949, last=none, best=none, next=1000, rate=0.00000064, load 64=498.9 milliseconds, train=1.5 seconds, 9536 images, time remaining=5.3 hours
150: loss=61.754, avg loss=57.430, last=none, best=none, next=1000, rate=0.00000066, load 64=407.1 milliseconds, train=1.5 seconds, 9600 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
151: loss=58.452, avg loss=57.532, last=none, best=none, next=1000, rate=0.00000068, load 64=819.3 milliseconds, train=4.5 seconds, 9664 images, time remaining=5.3 hours
152: loss=66.727, avg loss=58.451, last=none, best=none, next=1000, rate=0.00000069, load 64=777.6 milliseconds, train=4.5 seconds, 9728 images, time remaining=5.4 hours
153: loss=54.447, avg loss=58.051, last=none, best=none, next=1000, rate=0.00000071, load 64=676.0 milliseconds, train=4.5 seconds, 9792 images, time remaining=5.4 hours
154: loss=59.069, avg loss=58.153, last=none, best=none, next=1000, rate=0.00000073, load 64=448.6 milliseconds, train=4.5 seconds, 9856 images, time remaining=5.4 hours
155: loss=62.729, avg loss=58.610, last=none, best=none, next=1000, rate=0.00000075, load 64=703.9 milliseconds, train=4.5 seconds, 9920 images, time remaining=5.5 hours
156: loss=55.189, avg loss=58.268, last=none, best=none, next=1000, rate=0.00000077, load 64=851.8 milliseconds, train=4.5 seconds, 9984 images, time remaining=5.5 hours
157: loss=53.305, avg loss=57.772, last=none, best=none, next=1000, rate=0.00000079, load 64=736.7 milliseconds, train=4.5 seconds, 10048 images, time remaining=5.5 hours
158: loss=54.951, avg loss=57.490, last=none, best=none, next=1000, rate=0.00000081, load 64=791.7 milliseconds, train=4.5 seconds, 10112 images, time remaining=5.5 hours
159: loss=51.778, avg loss=56.919, last=none, best=none, next=1000, rate=0.00000083, load 64=723.2 milliseconds, train=4.5 seconds, 10176 images, time remaining=5.6 hours
160: loss=54.914, avg loss=56.718, last=none, best=none, next=1000, rate=0.00000085, load 64=459.1 milliseconds, train=4.5 seconds, 10240 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145130000000
161: loss=56.015, avg loss=56.648, last=none, best=none, next=1000, rate=0.00000087, load 64=929.9 milliseconds, train=1.6 seconds, 10304 images, time remaining=5.6 hours
162: loss=53.690, avg loss=56.352, last=none, best=none, next=1000, rate=0.00000090, load 64=621.1 milliseconds, train=1.6 seconds, 10368 images, time remaining=5.6 hours
163: loss=49.403, avg loss=55.657, last=none, best=none, next=1000, rate=0.00000092, load 64=441.7 milliseconds, train=1.6 seconds, 10432 images, time remaining=5.6 hours
164: loss=49.810, avg loss=55.072, last=none, best=none, next=1000, rate=0.00000094, load 64=531.1 milliseconds, train=1.6 seconds, 10496 images, time remaining=5.5 hours
165: loss=52.390, avg loss=54.804, last=none, best=none, next=1000, rate=0.00000096, load 64=581.2 milliseconds, train=1.6 seconds, 10560 images, time remaining=5.5 hours
166: loss=55.432, avg loss=54.867, last=none, best=none, next=1000, rate=0.00000099, load 64=545.1 milliseconds, train=1.6 seconds, 10624 images, time remaining=5.5 hours
167: loss=44.202, avg loss=53.800, last=none, best=none, next=1000, rate=0.00000101, load 64=727.7 milliseconds, train=1.6 seconds, 10688 images, time remaining=5.5 hours
168: loss=46.557, avg loss=53.076, last=none, best=none, next=1000, rate=0.00000104, load 64=733.4 milliseconds, train=1.6 seconds, 10752 images, time remaining=5.5 hours
169: loss=49.595, avg loss=52.728, last=none, best=none, next=1000, rate=0.00000106, load 64=711.8 milliseconds, train=1.6 seconds, 10816 images, time remaining=5.5 hours
170: loss=46.695, avg loss=52.125, last=none, best=none, next=1000, rate=0.00000109, load 64=665.7 milliseconds, train=1.6 seconds, 10880 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144fdc000000
171: loss=49.177, avg loss=51.830, last=none, best=none, next=1000, rate=0.00000111, load 64=950.6 milliseconds, train=2.0 seconds, 10944 images, time remaining=5.5 hours
172: loss=52.826, avg loss=51.929, last=none, best=none, next=1000, rate=0.00000114, load 64=882.8 milliseconds, train=2.0 seconds, 11008 images, time remaining=5.5 hours
173: loss=48.754, avg loss=51.612, last=none, best=none, next=1000, rate=0.00000116, load 64=866.3 milliseconds, train=2.0 seconds, 11072 images, time remaining=5.5 hours
174: loss=47.145, avg loss=51.165, last=none, best=none, next=1000, rate=0.00000119, load 64=621.3 milliseconds, train=2.0 seconds, 11136 images, time remaining=5.5 hours
175: loss=49.105, avg loss=50.959, last=none, best=none, next=1000, rate=0.00000122, load 64=688.1 milliseconds, train=2.0 seconds, 11200 images, time remaining=5.5 hours
176: loss=47.794, avg loss=50.643, last=none, best=none, next=1000, rate=0.00000125, load 64=916.6 milliseconds, train=2.0 seconds, 11264 images, time remaining=5.4 hours
177: loss=50.991, avg loss=50.678, last=none, best=none, next=1000, rate=0.00000128, load 64=489.3 milliseconds, train=2.0 seconds, 11328 images, time remaining=5.4 hours
178: loss=52.467, avg loss=50.856, last=none, best=none, next=1000, rate=0.00000131, load 64=2.0 seconds, train=2.0 seconds, 11392 images, time remaining=5.4 hours
179: loss=52.894, avg loss=51.060, last=none, best=none, next=1000, rate=0.00000133, load 64=1.6 seconds, train=2.0 seconds, 11456 images, time remaining=5.4 hours
180: loss=56.900, avg loss=51.644, last=none, best=none, next=1000, rate=0.00000136, load 64=732.8 milliseconds, train=2.0 seconds, 11520 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
181: loss=44.874, avg loss=50.967, last=none, best=none, next=1000, rate=0.00000140, load 64=636.0 milliseconds, train=4.8 seconds, 11584 images, time remaining=5.5 hours
182: loss=50.552, avg loss=50.926, last=none, best=none, next=1000, rate=0.00000143, load 64=587.0 milliseconds, train=4.8 seconds, 11648 images, time remaining=5.5 hours
183: loss=53.307, avg loss=51.164, last=none, best=none, next=1000, rate=0.00000146, load 64=593.3 milliseconds, train=4.8 seconds, 11712 images, time remaining=5.6 hours
184: loss=48.989, avg loss=50.946, last=none, best=none, next=1000, rate=0.00000149, load 64=690.8 milliseconds, train=4.8 seconds, 11776 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
185: loss=43.686, avg loss=50.220, last=none, best=none, next=1000, rate=0.00000152, load 64=6.3 seconds, train=4.8 seconds, 11840 images, time remaining=5.6 hours
186: loss=46.815, avg loss=49.880, last=none, best=none, next=1000, rate=0.00000156, load 64=3.9 seconds, train=4.8 seconds, 11904 images, time remaining=5.6 hours
187: loss=54.445, avg loss=50.336, last=none, best=none, next=1000, rate=0.00000159, load 64=1.1 seconds, train=4.9 seconds, 11968 images, time remaining=5.7 hours
188: loss=45.871, avg loss=49.890, last=none, best=none, next=1000, rate=0.00000162, load 64=733.5 milliseconds, train=4.8 seconds, 12032 images, time remaining=5.7 hours
189: loss=43.729, avg loss=49.274, last=none, best=none, next=1000, rate=0.00000166, load 64=857.4 milliseconds, train=4.8 seconds, 12096 images, time remaining=5.7 hours
190: loss=49.646, avg loss=49.311, last=none, best=none, next=1000, rate=0.00000169, load 64=606.9 milliseconds, train=4.8 seconds, 12160 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
191: loss=48.024, avg loss=49.182, last=none, best=none, next=1000, rate=0.00000173, load 64=712.9 milliseconds, train=3.4 seconds, 12224 images, time remaining=5.8 hours
192: loss=47.099, avg loss=48.974, last=none, best=none, next=1000, rate=0.00000177, load 64=558.4 milliseconds, train=3.4 seconds, 12288 images, time remaining=5.8 hours
193: loss=44.253, avg loss=48.502, last=none, best=none, next=1000, rate=0.00000180, load 64=532.4 milliseconds, train=3.4 seconds, 12352 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
194: loss=50.468, avg loss=48.699, last=none, best=none, next=1000, rate=0.00000184, load 64=4.7 seconds, train=3.4 seconds, 12416 images, time remaining=5.8 hours
195: loss=41.276, avg loss=47.956, last=none, best=none, next=1000, rate=0.00000188, load 64=968.6 milliseconds, train=3.4 seconds, 12480 images, time remaining=5.8 hours
196: loss=42.394, avg loss=47.400, last=none, best=none, next=1000, rate=0.00000192, load 64=629.7 milliseconds, train=3.4 seconds, 12544 images, time remaining=5.8 hours
197: loss=40.433, avg loss=46.703, last=none, best=none, next=1000, rate=0.00000196, load 64=642.9 milliseconds, train=3.4 seconds, 12608 images, time remaining=5.8 hours
198: loss=45.210, avg loss=46.554, last=none, best=none, next=1000, rate=0.00000200, load 64=460.8 milliseconds, train=3.4 seconds, 12672 images, time remaining=5.9 hours
199: loss=44.592, avg loss=46.358, last=none, best=none, next=1000, rate=0.00000204, load 64=723.8 milliseconds, train=3.4 seconds, 12736 images, time remaining=5.9 hours
200: loss=39.663, avg loss=45.688, last=none, best=none, next=1000, rate=0.00000208, load 64=619.2 milliseconds, train=3.4 seconds, 12800 images, time remaining=5.9 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145151200000
201: loss=38.252, avg loss=44.945, last=none, best=none, next=1000, rate=0.00000212, load 64=522.7 milliseconds, train=1.4 seconds, 12864 images, time remaining=5.9 hours
202: loss=41.557, avg loss=44.606, last=none, best=none, next=1000, rate=0.00000216, load 64=708.6 milliseconds, train=1.4 seconds, 12928 images, time remaining=5.8 hours
203: loss=38.009, avg loss=43.946, last=none, best=none, next=1000, rate=0.00000221, load 64=621.0 milliseconds, train=1.4 seconds, 12992 images, time remaining=5.8 hours
204: loss=40.193, avg loss=43.571, last=none, best=none, next=1000, rate=0.00000225, load 64=448.0 milliseconds, train=1.4 seconds, 13056 images, time remaining=5.8 hours
205: loss=37.292, avg loss=42.943, last=none, best=none, next=1000, rate=0.00000230, load 64=488.3 milliseconds, train=1.4 seconds, 13120 images, time remaining=5.8 hours
206: loss=38.733, avg loss=42.522, last=none, best=none, next=1000, rate=0.00000234, load 64=543.3 milliseconds, train=1.4 seconds, 13184 images, time remaining=5.8 hours
207: loss=35.316, avg loss=41.801, last=none, best=none, next=1000, rate=0.00000239, load 64=524.5 milliseconds, train=1.4 seconds, 13248 images, time remaining=5.8 hours
208: loss=38.689, avg loss=41.490, last=none, best=none, next=1000, rate=0.00000243, load 64=572.7 milliseconds, train=1.5 seconds, 13312 images, time remaining=5.8 hours
209: loss=39.938, avg loss=41.335, last=none, best=none, next=1000, rate=0.00000248, load 64=642.3 milliseconds, train=1.4 seconds, 13376 images, time remaining=5.8 hours
210: loss=37.895, avg loss=40.991, last=none, best=none, next=1000, rate=0.00000253, load 64=675.3 milliseconds, train=1.4 seconds, 13440 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
211: loss=43.045, avg loss=41.196, last=none, best=none, next=1000, rate=0.00000258, load 64=698.6 milliseconds, train=3.4 seconds, 13504 images, time remaining=5.8 hours
212: loss=45.248, avg loss=41.602, last=none, best=none, next=1000, rate=0.00000263, load 64=394.5 milliseconds, train=3.4 seconds, 13568 images, time remaining=5.8 hours
213: loss=42.972, avg loss=41.739, last=none, best=none, next=1000, rate=0.00000268, load 64=514.5 milliseconds, train=3.4 seconds, 13632 images, time remaining=5.8 hours
214: loss=36.844, avg loss=41.249, last=none, best=none, next=1000, rate=0.00000273, load 64=418.6 milliseconds, train=3.4 seconds, 13696 images, time remaining=5.8 hours
215: loss=36.222, avg loss=40.746, last=none, best=none, next=1000, rate=0.00000278, load 64=863.2 milliseconds, train=3.4 seconds, 13760 images, time remaining=5.8 hours
216: loss=38.854, avg loss=40.557, last=none, best=none, next=1000, rate=0.00000283, load 64=707.6 milliseconds, train=3.4 seconds, 13824 images, time remaining=5.8 hours
217: loss=36.697, avg loss=40.171, last=none, best=none, next=1000, rate=0.00000288, load 64=491.6 milliseconds, train=3.4 seconds, 13888 images, time remaining=5.8 hours
218: loss=42.004, avg loss=40.355, last=none, best=none, next=1000, rate=0.00000294, load 64=584.7 milliseconds, train=3.4 seconds, 13952 images, time remaining=5.8 hours
219: loss=38.250, avg loss=40.144, last=none, best=none, next=1000, rate=0.00000299, load 64=555.9 milliseconds, train=3.4 seconds, 14016 images, time remaining=5.8 hours
220: loss=35.963, avg loss=39.726, last=none, best=none, next=1000, rate=0.00000305, load 64=544.8 milliseconds, train=3.4 seconds, 14080 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
221: loss=36.454, avg loss=39.399, last=none, best=none, next=1000, rate=0.00000310, load 64=478.8 milliseconds, train=4.5 seconds, 14144 images, time remaining=5.9 hours
222: loss=37.300, avg loss=39.189, last=none, best=none, next=1000, rate=0.00000316, load 64=848.6 milliseconds, train=4.5 seconds, 14208 images, time remaining=5.9 hours
223: loss=42.252, avg loss=39.495, last=none, best=none, next=1000, rate=0.00000321, load 64=671.4 milliseconds, train=4.5 seconds, 14272 images, time remaining=5.9 hours
224: loss=33.522, avg loss=38.898, last=none, best=none, next=1000, rate=0.00000327, load 64=545.7 milliseconds, train=4.5 seconds, 14336 images, time remaining=5.9 hours
225: loss=37.370, avg loss=38.745, last=none, best=none, next=1000, rate=0.00000333, load 64=401.7 milliseconds, train=4.5 seconds, 14400 images, time remaining=5.9 hours
226: loss=38.236, avg loss=38.694, last=none, best=none, next=1000, rate=0.00000339, load 64=547.3 milliseconds, train=4.5 seconds, 14464 images, time remaining=6 hours
227: loss=35.679, avg loss=38.393, last=none, best=none, next=1000, rate=0.00000345, load 64=546.4 milliseconds, train=4.5 seconds, 14528 images, time remaining=6 hours
228: loss=36.176, avg loss=38.171, last=none, best=none, next=1000, rate=0.00000351, load 64=871.8 milliseconds, train=4.5 seconds, 14592 images, time remaining=6 hours
229: loss=38.392, avg loss=38.193, last=none, best=none, next=1000, rate=0.00000358, load 64=1.0 seconds, train=4.5 seconds, 14656 images, time remaining=6 hours
230: loss=35.141, avg loss=37.888, last=none, best=none, next=1000, rate=0.00000364, load 64=617.0 milliseconds, train=4.5 seconds, 14720 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1448b6000000
231: loss=34.614, avg loss=37.561, last=none, best=none, next=1000, rate=0.00000370, load 64=732.7 milliseconds, train=3.4 seconds, 14784 images, time remaining=6 hours
232: loss=35.865, avg loss=37.391, last=none, best=none, next=1000, rate=0.00000377, load 64=730.7 milliseconds, train=3.4 seconds, 14848 images, time remaining=6 hours
233: loss=34.825, avg loss=37.134, last=none, best=none, next=1000, rate=0.00000383, load 64=574.5 milliseconds, train=3.4 seconds, 14912 images, time remaining=6 hours
234: loss=34.420, avg loss=36.863, last=none, best=none, next=1000, rate=0.00000390, load 64=704.3 milliseconds, train=3.4 seconds, 14976 images, time remaining=6 hours
235: loss=32.084, avg loss=36.385, last=none, best=none, next=1000, rate=0.00000396, load 64=450.2 milliseconds, train=3.4 seconds, 15040 images, time remaining=6 hours
236: loss=29.969, avg loss=35.743, last=none, best=none, next=1000, rate=0.00000403, load 64=731.3 milliseconds, train=3.4 seconds, 15104 images, time remaining=6 hours
237: loss=33.363, avg loss=35.505, last=none, best=none, next=1000, rate=0.00000410, load 64=612.8 milliseconds, train=3.4 seconds, 15168 images, time remaining=6.1 hours
238: loss=31.257, avg loss=35.080, last=none, best=none, next=1000, rate=0.00000417, load 64=933.6 milliseconds, train=3.4 seconds, 15232 images, time remaining=6.1 hours
239: loss=34.568, avg loss=35.029, last=none, best=none, next=1000, rate=0.00000424, load 64=759.5 milliseconds, train=3.4 seconds, 15296 images, time remaining=6.1 hours
240: loss=31.088, avg loss=34.635, last=none, best=none, next=1000, rate=0.00000431, load 64=617.5 milliseconds, train=3.4 seconds, 15360 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144d4c000000
241: loss=38.741, avg loss=35.046, last=none, best=none, next=1000, rate=0.00000439, load 64=747.5 milliseconds, train=2.1 seconds, 15424 images, time remaining=6.1 hours
242: loss=31.818, avg loss=34.723, last=none, best=none, next=1000, rate=0.00000446, load 64=569.1 milliseconds, train=2.1 seconds, 15488 images, time remaining=6.1 hours
243: loss=31.993, avg loss=34.450, last=none, best=none, next=1000, rate=0.00000453, load 64=589.9 milliseconds, train=2.1 seconds, 15552 images, time remaining=6.1 hours
244: loss=31.824, avg loss=34.187, last=none, best=none, next=1000, rate=0.00000461, load 64=546.3 milliseconds, train=2.1 seconds, 15616 images, time remaining=6 hours
245: loss=36.127, avg loss=34.381, last=none, best=none, next=1000, rate=0.00000468, load 64=774.2 milliseconds, train=2.1 seconds, 15680 images, time remaining=6 hours
246: loss=33.969, avg loss=34.340, last=none, best=none, next=1000, rate=0.00000476, load 64=534.8 milliseconds, train=2.1 seconds, 15744 images, time remaining=6 hours
247: loss=32.886, avg loss=34.195, last=none, best=none, next=1000, rate=0.00000484, load 64=595.9 milliseconds, train=2.1 seconds, 15808 images, time remaining=6 hours
248: loss=31.746, avg loss=33.950, last=none, best=none, next=1000, rate=0.00000492, load 64=509.8 milliseconds, train=2.1 seconds, 15872 images, time remaining=6 hours
249: loss=29.969, avg loss=33.552, last=none, best=none, next=1000, rate=0.00000500, load 64=707.2 milliseconds, train=2.1 seconds, 15936 images, time remaining=6 hours
250: loss=31.866, avg loss=33.383, last=none, best=none, next=1000, rate=0.00000508, load 64=834.3 milliseconds, train=2.1 seconds, 16000 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1459de000000
251: loss=32.351, avg loss=33.280, last=none, best=none, next=1000, rate=0.00000516, load 64=644.6 milliseconds, train=1.6 seconds, 16064 images, time remaining=6 hours
252: loss=35.232, avg loss=33.475, last=none, best=none, next=1000, rate=0.00000524, load 64=692.0 milliseconds, train=1.7 seconds, 16128 images, time remaining=6 hours
253: loss=32.569, avg loss=33.384, last=none, best=none, next=1000, rate=0.00000533, load 64=603.3 milliseconds, train=1.6 seconds, 16192 images, time remaining=6 hours
254: loss=29.050, avg loss=32.951, last=none, best=none, next=1000, rate=0.00000541, load 64=543.8 milliseconds, train=1.6 seconds, 16256 images, time remaining=6 hours
255: loss=31.852, avg loss=32.841, last=none, best=none, next=1000, rate=0.00000550, load 64=554.5 milliseconds, train=1.7 seconds, 16320 images, time remaining=6 hours
256: loss=25.900, avg loss=32.147, last=none, best=none, next=1000, rate=0.00000558, load 64=432.5 milliseconds, train=1.6 seconds, 16384 images, time remaining=5.9 hours
257: loss=30.203, avg loss=31.953, last=none, best=none, next=1000, rate=0.00000567, load 64=468.3 milliseconds, train=1.6 seconds, 16448 images, time remaining=5.9 hours
258: loss=32.215, avg loss=31.979, last=none, best=none, next=1000, rate=0.00000576, load 64=520.8 milliseconds, train=1.6 seconds, 16512 images, time remaining=5.9 hours
259: loss=31.925, avg loss=31.974, last=none, best=none, next=1000, rate=0.00000585, load 64=909.4 milliseconds, train=1.6 seconds, 16576 images, time remaining=5.9 hours
260: loss=33.832, avg loss=32.159, last=none, best=none, next=1000, rate=0.00000594, load 64=758.5 milliseconds, train=1.6 seconds, 16640 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144816000000
261: loss=31.718, avg loss=32.115, last=none, best=none, next=1000, rate=0.00000603, load 64=523.0 milliseconds, train=4.6 seconds, 16704 images, time remaining=6 hours
262: loss=31.407, avg loss=32.044, last=none, best=none, next=1000, rate=0.00000613, load 64=510.1 milliseconds, train=4.6 seconds, 16768 images, time remaining=6 hours
263: loss=36.029, avg loss=32.443, last=none, best=none, next=1000, rate=0.00000622, load 64=505.7 milliseconds, train=4.6 seconds, 16832 images, time remaining=6 hours
264: loss=33.081, avg loss=32.507, last=none, best=none, next=1000, rate=0.00000631, load 64=688.9 milliseconds, train=4.6 seconds, 16896 images, time remaining=6 hours
265: loss=27.373, avg loss=31.993, last=none, best=none, next=1000, rate=0.00000641, load 64=678.4 milliseconds, train=4.6 seconds, 16960 images, time remaining=6 hours
266: loss=32.991, avg loss=32.093, last=none, best=none, next=1000, rate=0.00000651, load 64=541.0 milliseconds, train=4.6 seconds, 17024 images, time remaining=6 hours
267: loss=29.125, avg loss=31.796, last=none, best=none, next=1000, rate=0.00000661, load 64=748.7 milliseconds, train=4.6 seconds, 17088 images, time remaining=6 hours
268: loss=27.524, avg loss=31.369, last=none, best=none, next=1000, rate=0.00000671, load 64=897.5 milliseconds, train=4.6 seconds, 17152 images, time remaining=6.1 hours
269: loss=31.056, avg loss=31.338, last=none, best=none, next=1000, rate=0.00000681, load 64=543.8 milliseconds, train=4.6 seconds, 17216 images, time remaining=6.1 hours
270: loss=36.083, avg loss=31.812, last=none, best=none, next=1000, rate=0.00000691, load 64=1.7 seconds, train=4.6 seconds, 17280 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144816000000
271: loss=38.118, avg loss=32.443, last=none, best=none, next=1000, rate=0.00000701, load 64=614.8 milliseconds, train=3.4 seconds, 17344 images, time remaining=6.1 hours
272: loss=29.208, avg loss=32.119, last=none, best=none, next=1000, rate=0.00000712, load 64=419.3 milliseconds, train=3.4 seconds, 17408 images, time remaining=6.1 hours
273: loss=33.151, avg loss=32.223, last=none, best=none, next=1000, rate=0.00000722, load 64=510.7 milliseconds, train=3.4 seconds, 17472 images, time remaining=6.1 hours
274: loss=32.084, avg loss=32.209, last=none, best=none, next=1000, rate=0.00000733, load 64=669.9 milliseconds, train=3.4 seconds, 17536 images, time remaining=6.1 hours
275: loss=32.320, avg loss=32.220, last=none, best=none, next=1000, rate=0.00000743, load 64=670.6 milliseconds, train=3.4 seconds, 17600 images, time remaining=6.1 hours
276: loss=32.538, avg loss=32.252, last=none, best=none, next=1000, rate=0.00000754, load 64=740.8 milliseconds, train=3.4 seconds, 17664 images, time remaining=6.1 hours
277: loss=35.857, avg loss=32.612, last=none, best=none, next=1000, rate=0.00000765, load 64=805.5 milliseconds, train=3.4 seconds, 17728 images, time remaining=6.1 hours
278: loss=30.431, avg loss=32.394, last=none, best=none, next=1000, rate=0.00000776, load 64=736.2 milliseconds, train=3.4 seconds, 17792 images, time remaining=6.1 hours
279: loss=27.083, avg loss=31.863, last=none, best=none, next=1000, rate=0.00000788, load 64=715.0 milliseconds, train=3.3 seconds, 17856 images, time remaining=6.1 hours
280: loss=33.599, avg loss=32.037, last=none, best=none, next=1000, rate=0.00000799, load 64=505.1 milliseconds, train=3.4 seconds, 17920 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144816000000
281: loss=30.629, avg loss=31.896, last=none, best=none, next=1000, rate=0.00000811, load 64=484.2 milliseconds, train=3.3 seconds, 17984 images, time remaining=6.1 hours
282: loss=32.788, avg loss=31.985, last=none, best=none, next=1000, rate=0.00000822, load 64=667.2 milliseconds, train=3.3 seconds, 18048 images, time remaining=6.1 hours
283: loss=28.267, avg loss=31.613, last=none, best=none, next=1000, rate=0.00000834, load 64=497.7 milliseconds, train=3.3 seconds, 18112 images, time remaining=6.1 hours
284: loss=33.133, avg loss=31.765, last=none, best=none, next=1000, rate=0.00000846, load 64=796.5 milliseconds, train=3.3 seconds, 18176 images, time remaining=6.1 hours
285: loss=35.882, avg loss=32.177, last=none, best=none, next=1000, rate=0.00000858, load 64=685.3 milliseconds, train=3.3 seconds, 18240 images, time remaining=6.1 hours
286: loss=28.882, avg loss=31.847, last=none, best=none, next=1000, rate=0.00000870, load 64=610.6 milliseconds, train=3.3 seconds, 18304 images, time remaining=6.1 hours
287: loss=29.968, avg loss=31.659, last=none, best=none, next=1000, rate=0.00000882, load 64=474.1 milliseconds, train=3.3 seconds, 18368 images, time remaining=6.1 hours
288: loss=27.054, avg loss=31.199, last=none, best=none, next=1000, rate=0.00000894, load 64=565.7 milliseconds, train=3.3 seconds, 18432 images, time remaining=6.1 hours
289: loss=32.161, avg loss=31.295, last=none, best=none, next=1000, rate=0.00000907, load 64=688.6 milliseconds, train=3.3 seconds, 18496 images, time remaining=6.1 hours
290: loss=29.673, avg loss=31.133, last=none, best=none, next=1000, rate=0.00000919, load 64=638.5 milliseconds, train=3.3 seconds, 18560 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14476a000000
291: loss=31.029, avg loss=31.122, last=none, best=none, next=1000, rate=0.00000932, load 64=935.9 milliseconds, train=4.9 seconds, 18624 images, time remaining=6.2 hours
292: loss=33.412, avg loss=31.351, last=none, best=none, next=1000, rate=0.00000945, load 64=768.3 milliseconds, train=4.9 seconds, 18688 images, time remaining=6.2 hours
293: loss=28.616, avg loss=31.078, last=none, best=none, next=1000, rate=0.00000958, load 64=543.2 milliseconds, train=4.9 seconds, 18752 images, time remaining=6.2 hours
294: loss=28.008, avg loss=30.771, last=none, best=none, next=1000, rate=0.00000971, load 64=849.0 milliseconds, train=4.9 seconds, 18816 images, time remaining=6.2 hours
295: loss=26.810, avg loss=30.375, last=none, best=none, next=1000, rate=0.00000985, load 64=670.8 milliseconds, train=4.9 seconds, 18880 images, time remaining=6.2 hours
296: loss=27.861, avg loss=30.123, last=none, best=none, next=1000, rate=0.00000998, load 64=577.1 milliseconds, train=4.9 seconds, 18944 images, time remaining=6.3 hours
297: loss=32.124, avg loss=30.323, last=none, best=none, next=1000, rate=0.00001012, load 64=626.6 milliseconds, train=4.9 seconds, 19008 images, time remaining=6.3 hours
298: loss=28.785, avg loss=30.170, last=none, best=none, next=1000, rate=0.00001025, load 64=582.3 milliseconds, train=4.9 seconds, 19072 images, time remaining=6.3 hours
299: loss=33.706, avg loss=30.523, last=none, best=none, next=1000, rate=0.00001039, load 64=502.3 milliseconds, train=4.9 seconds, 19136 images, time remaining=6.3 hours
300: loss=31.571, avg loss=30.628, last=none, best=none, next=1000, rate=0.00001053, load 64=784.5 milliseconds, train=4.9 seconds, 19200 images, time remaining=6.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447be000000
301: loss=31.675, avg loss=30.733, last=none, best=none, next=1000, rate=0.00001067, load 64=631.4 milliseconds, train=3.6 seconds, 19264 images, time remaining=6.3 hours
302: loss=29.930, avg loss=30.652, last=none, best=none, next=1000, rate=0.00001081, load 64=949.8 milliseconds, train=3.6 seconds, 19328 images, time remaining=6.3 hours
303: loss=30.923, avg loss=30.680, last=none, best=none, next=1000, rate=0.00001096, load 64=1.1 seconds, train=3.6 seconds, 19392 images, time remaining=6.3 hours
304: loss=29.972, avg loss=30.609, last=none, best=none, next=1000, rate=0.00001110, load 64=405.4 milliseconds, train=3.6 seconds, 19456 images, time remaining=6.3 hours
305: loss=32.274, avg loss=30.775, last=none, best=none, next=1000, rate=0.00001125, load 64=479.0 milliseconds, train=3.6 seconds, 19520 images, time remaining=6.3 hours
306: loss=29.972, avg loss=30.695, last=none, best=none, next=1000, rate=0.00001140, load 64=617.8 milliseconds, train=3.6 seconds, 19584 images, time remaining=6.3 hours
307: loss=29.925, avg loss=30.618, last=none, best=none, next=1000, rate=0.00001155, load 64=753.2 milliseconds, train=3.6 seconds, 19648 images, time remaining=6.3 hours
308: loss=30.923, avg loss=30.648, last=none, best=none, next=1000, rate=0.00001170, load 64=909.8 milliseconds, train=3.6 seconds, 19712 images, time remaining=6.3 hours
309: loss=28.404, avg loss=30.424, last=none, best=none, next=1000, rate=0.00001185, load 64=669.8 milliseconds, train=3.6 seconds, 19776 images, time remaining=6.4 hours
310: loss=29.980, avg loss=30.380, last=none, best=none, next=1000, rate=0.00001201, load 64=763.5 milliseconds, train=3.6 seconds, 19840 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144ad0000000
311: loss=26.859, avg loss=30.028, last=none, best=none, next=1000, rate=0.00001216, load 64=463.7 milliseconds, train=2.1 seconds, 19904 images, time remaining=6.4 hours
312: loss=26.689, avg loss=29.694, last=none, best=none, next=1000, rate=0.00001232, load 64=729.8 milliseconds, train=2.1 seconds, 19968 images, time remaining=6.3 hours
313: loss=28.725, avg loss=29.597, last=none, best=none, next=1000, rate=0.00001248, load 64=438.0 milliseconds, train=2.1 seconds, 20032 images, time remaining=6.3 hours
314: loss=32.079, avg loss=29.845, last=none, best=none, next=1000, rate=0.00001264, load 64=460.2 milliseconds, train=2.1 seconds, 20096 images, time remaining=6.3 hours
315: loss=29.141, avg loss=29.775, last=none, best=none, next=1000, rate=0.00001280, load 64=648.6 milliseconds, train=2.1 seconds, 20160 images, time remaining=6.3 hours
316: loss=30.926, avg loss=29.890, last=none, best=none, next=1000, rate=0.00001296, load 64=548.3 milliseconds, train=2.1 seconds, 20224 images, time remaining=6.3 hours
317: loss=25.544, avg loss=29.455, last=none, best=none, next=1000, rate=0.00001313, load 64=692.7 milliseconds, train=2.1 seconds, 20288 images, time remaining=6.3 hours
318: loss=29.402, avg loss=29.450, last=none, best=none, next=1000, rate=0.00001329, load 64=487.0 milliseconds, train=2.1 seconds, 20352 images, time remaining=6.3 hours
319: loss=31.955, avg loss=29.700, last=none, best=none, next=1000, rate=0.00001346, load 64=520.0 milliseconds, train=2.1 seconds, 20416 images, time remaining=6.3 hours
320: loss=33.696, avg loss=30.100, last=none, best=none, next=1000, rate=0.00001363, load 64=675.7 milliseconds, train=2.1 seconds, 20480 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1464ac000000
321: loss=25.781, avg loss=29.668, last=none, best=none, next=1000, rate=0.00001380, load 64=852.2 milliseconds, train=1.2 seconds, 20544 images, time remaining=6.3 hours
322: loss=26.025, avg loss=29.304, last=none, best=none, next=1000, rate=0.00001398, load 64=705.9 milliseconds, train=1.2 seconds, 20608 images, time remaining=6.3 hours
323: loss=25.872, avg loss=28.961, last=none, best=none, next=1000, rate=0.00001415, load 64=630.7 milliseconds, train=1.2 seconds, 20672 images, time remaining=6.3 hours
324: loss=23.656, avg loss=28.430, last=none, best=none, next=1000, rate=0.00001433, load 64=770.0 milliseconds, train=1.2 seconds, 20736 images, time remaining=6.2 hours
325: loss=25.382, avg loss=28.125, last=none, best=none, next=1000, rate=0.00001450, load 64=569.7 milliseconds, train=1.2 seconds, 20800 images, time remaining=6.2 hours
326: loss=27.540, avg loss=28.067, last=none, best=none, next=1000, rate=0.00001468, load 64=453.1 milliseconds, train=1.2 seconds, 20864 images, time remaining=6.2 hours
327: loss=25.050, avg loss=27.765, last=none, best=none, next=1000, rate=0.00001486, load 64=892.1 milliseconds, train=1.2 seconds, 20928 images, time remaining=6.2 hours
328: loss=25.241, avg loss=27.513, last=none, best=none, next=1000, rate=0.00001505, load 64=825.6 milliseconds, train=1.2 seconds, 20992 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
329: loss=28.452, avg loss=27.607, last=none, best=none, next=1000, rate=0.00001523, load 64=2.0 seconds, train=1.2 seconds, 21056 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
330: loss=26.404, avg loss=27.486, last=none, best=none, next=1000, rate=0.00001542, load 64=2.1 seconds, train=1.2 seconds, 21120 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x1464ac000000
331: loss=25.797, avg loss=27.317, last=none, best=none, next=1000, rate=0.00001560, load 64=358.2 milliseconds, train=1.1 seconds, 21184 images, time remaining=6.2 hours
332: loss=22.612, avg loss=26.847, last=none, best=none, next=1000, rate=0.00001579, load 64=558.9 milliseconds, train=1.1 seconds, 21248 images, time remaining=6.2 hours
333: loss=22.508, avg loss=26.413, last=none, best=none, next=1000, rate=0.00001599, load 64=397.4 milliseconds, train=1.1 seconds, 21312 images, time remaining=6.2 hours
334: loss=28.772, avg loss=26.649, last=none, best=none, next=1000, rate=0.00001618, load 64=723.7 milliseconds, train=1.1 seconds, 21376 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
335: loss=23.004, avg loss=26.284, last=none, best=none, next=1000, rate=0.00001637, load 64=2.8 seconds, train=1.1 seconds, 21440 images, time remaining=6.1 hours
336: loss=27.824, avg loss=26.438, last=none, best=none, next=1000, rate=0.00001657, load 64=422.6 milliseconds, train=1.1 seconds, 21504 images, time remaining=6.1 hours
337: loss=25.941, avg loss=26.389, last=none, best=none, next=1000, rate=0.00001677, load 64=332.7 milliseconds, train=1.1 seconds, 21568 images, time remaining=6.1 hours
338: loss=27.149, avg loss=26.465, last=none, best=none, next=1000, rate=0.00001697, load 64=612.2 milliseconds, train=1.2 seconds, 21632 images, time remaining=6.1 hours
339: loss=24.355, avg loss=26.254, last=none, best=none, next=1000, rate=0.00001717, load 64=697.9 milliseconds, train=1.1 seconds, 21696 images, time remaining=6.1 hours
340: loss=27.032, avg loss=26.332, last=none, best=none, next=1000, rate=0.00001737, load 64=547.6 milliseconds, train=1.1 seconds, 21760 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144ad0000000
341: loss=26.733, avg loss=26.372, last=none, best=none, next=1000, rate=0.00001758, load 64=376.7 milliseconds, train=1.9 seconds, 21824 images, time remaining=6.1 hours
342: loss=28.172, avg loss=26.552, last=none, best=none, next=1000, rate=0.00001778, load 64=660.1 milliseconds, train=2.0 seconds, 21888 images, time remaining=6.1 hours
343: loss=26.076, avg loss=26.504, last=none, best=none, next=1000, rate=0.00001799, load 64=515.6 milliseconds, train=2.0 seconds, 21952 images, time remaining=6.1 hours
344: loss=27.961, avg loss=26.650, last=none, best=none, next=1000, rate=0.00001820, load 64=611.5 milliseconds, train=2.0 seconds, 22016 images, time remaining=6.1 hours
345: loss=27.362, avg loss=26.721, last=none, best=none, next=1000, rate=0.00001842, load 64=632.9 milliseconds, train=2.0 seconds, 22080 images, time remaining=6.1 hours
346: loss=26.347, avg loss=26.684, last=none, best=none, next=1000, rate=0.00001863, load 64=727.3 milliseconds, train=2.0 seconds, 22144 images, time remaining=6.1 hours
347: loss=29.920, avg loss=27.007, last=none, best=none, next=1000, rate=0.00001885, load 64=743.6 milliseconds, train=2.0 seconds, 22208 images, time remaining=6 hours
348: loss=28.394, avg loss=27.146, last=none, best=none, next=1000, rate=0.00001907, load 64=808.8 milliseconds, train=2.0 seconds, 22272 images, time remaining=6 hours
349: loss=28.121, avg loss=27.244, last=none, best=none, next=1000, rate=0.00001929, load 64=717.7 milliseconds, train=2.0 seconds, 22336 images, time remaining=6 hours
350: loss=27.709, avg loss=27.290, last=none, best=none, next=1000, rate=0.00001951, load 64=443.3 milliseconds, train=2.0 seconds, 22400 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447be000000
351: loss=31.881, avg loss=27.749, last=none, best=none, next=1000, rate=0.00001973, load 64=494.1 milliseconds, train=4.6 seconds, 22464 images, time remaining=6.1 hours
352: loss=32.493, avg loss=28.224, last=none, best=none, next=1000, rate=0.00001996, load 64=554.0 milliseconds, train=4.6 seconds, 22528 images, time remaining=6.1 hours
353: loss=30.833, avg loss=28.484, last=none, best=none, next=1000, rate=0.00002019, load 64=839.5 milliseconds, train=4.6 seconds, 22592 images, time remaining=6.1 hours
354: loss=27.466, avg loss=28.383, last=none, best=none, next=1000, rate=0.00002042, load 64=463.1 milliseconds, train=4.6 seconds, 22656 images, time remaining=6.1 hours
355: loss=29.925, avg loss=28.537, last=none, best=none, next=1000, rate=0.00002065, load 64=516.1 milliseconds, train=4.6 seconds, 22720 images, time remaining=6.1 hours
356: loss=28.923, avg loss=28.575, last=none, best=none, next=1000, rate=0.00002088, load 64=701.1 milliseconds, train=4.6 seconds, 22784 images, time remaining=6.1 hours
357: loss=30.241, avg loss=28.742, last=none, best=none, next=1000, rate=0.00002112, load 64=1.6 seconds, train=4.6 seconds, 22848 images, time remaining=6.1 hours
358: loss=34.146, avg loss=29.282, last=none, best=none, next=1000, rate=0.00002135, load 64=437.4 milliseconds, train=4.6 seconds, 22912 images, time remaining=6.1 hours
359: loss=26.649, avg loss=29.019, last=none, best=none, next=1000, rate=0.00002159, load 64=417.9 milliseconds, train=4.6 seconds, 22976 images, time remaining=6.1 hours
360: loss=26.020, avg loss=28.719, last=none, best=none, next=1000, rate=0.00002184, load 64=2.3 seconds, train=4.6 seconds, 23040 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447be000000
361: loss=24.329, avg loss=28.280, last=none, best=none, next=1000, rate=0.00002208, load 64=511.9 milliseconds, train=4.1 seconds, 23104 images, time remaining=6.2 hours
362: loss=32.077, avg loss=28.660, last=none, best=none, next=1000, rate=0.00002232, load 64=494.0 milliseconds, train=4.1 seconds, 23168 images, time remaining=6.2 hours
363: loss=28.771, avg loss=28.671, last=none, best=none, next=1000, rate=0.00002257, load 64=543.7 milliseconds, train=4.1 seconds, 23232 images, time remaining=6.2 hours
364: loss=26.778, avg loss=28.482, last=none, best=none, next=1000, rate=0.00002282, load 64=561.2 milliseconds, train=4.1 seconds, 23296 images, time remaining=6.2 hours
365: loss=27.442, avg loss=28.378, last=none, best=none, next=1000, rate=0.00002307, load 64=739.5 milliseconds, train=4.1 seconds, 23360 images, time remaining=6.2 hours
366: loss=29.005, avg loss=28.440, last=none, best=none, next=1000, rate=0.00002333, load 64=684.6 milliseconds, train=4.1 seconds, 23424 images, time remaining=6.2 hours
367: loss=28.038, avg loss=28.400, last=none, best=none, next=1000, rate=0.00002358, load 64=788.8 milliseconds, train=4.1 seconds, 23488 images, time remaining=6.2 hours
368: loss=27.023, avg loss=28.263, last=none, best=none, next=1000, rate=0.00002384, load 64=703.2 milliseconds, train=4.1 seconds, 23552 images, time remaining=6.2 hours
369: loss=26.981, avg loss=28.134, last=none, best=none, next=1000, rate=0.00002410, load 64=585.5 milliseconds, train=4.1 seconds, 23616 images, time remaining=6.2 hours
370: loss=28.496, avg loss=28.171, last=none, best=none, next=1000, rate=0.00002436, load 64=388.4 milliseconds, train=4.1 seconds, 23680 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144ad0000000
371: loss=27.982, avg loss=28.152, last=none, best=none, next=1000, rate=0.00002463, load 64=568.3 milliseconds, train=2.1 seconds, 23744 images, time remaining=6.2 hours
372: loss=26.589, avg loss=27.995, last=none, best=none, next=1000, rate=0.00002490, load 64=586.7 milliseconds, train=2.1 seconds, 23808 images, time remaining=6.2 hours
373: loss=24.342, avg loss=27.630, last=none, best=none, next=1000, rate=0.00002516, load 64=871.2 milliseconds, train=2.1 seconds, 23872 images, time remaining=6.2 hours
374: loss=23.728, avg loss=27.240, last=none, best=none, next=1000, rate=0.00002543, load 64=702.5 milliseconds, train=2.1 seconds, 23936 images, time remaining=6.2 hours
375: loss=27.643, avg loss=27.280, last=none, best=none, next=1000, rate=0.00002571, load 64=643.7 milliseconds, train=2.1 seconds, 24000 images, time remaining=6.2 hours
376: loss=31.309, avg loss=27.683, last=none, best=none, next=1000, rate=0.00002598, load 64=741.0 milliseconds, train=2.1 seconds, 24064 images, time remaining=6.2 hours
377: loss=25.507, avg loss=27.465, last=none, best=none, next=1000, rate=0.00002626, load 64=484.5 milliseconds, train=2.1 seconds, 24128 images, time remaining=6.2 hours
378: loss=25.010, avg loss=27.220, last=none, best=none, next=1000, rate=0.00002654, load 64=362.1 milliseconds, train=2.1 seconds, 24192 images, time remaining=6.2 hours
379: loss=28.747, avg loss=27.373, last=none, best=none, next=1000, rate=0.00002682, load 64=611.5 milliseconds, train=2.1 seconds, 24256 images, time remaining=6.2 hours
380: loss=25.586, avg loss=27.194, last=none, best=none, next=1000, rate=0.00002711, load 64=655.8 milliseconds, train=2.1 seconds, 24320 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144ad0000000
381: loss=25.125, avg loss=26.987, last=none, best=none, next=1000, rate=0.00002739, load 64=787.5 milliseconds, train=1.9 seconds, 24384 images, time remaining=6.2 hours
382: loss=30.745, avg loss=27.363, last=none, best=none, next=1000, rate=0.00002768, load 64=559.4 milliseconds, train=2.0 seconds, 24448 images, time remaining=6.2 hours
383: loss=26.340, avg loss=27.261, last=none, best=none, next=1000, rate=0.00002797, load 64=860.2 milliseconds, train=1.9 seconds, 24512 images, time remaining=6.1 hours
384: loss=26.360, avg loss=27.171, last=none, best=none, next=1000, rate=0.00002827, load 64=407.5 milliseconds, train=2.0 seconds, 24576 images, time remaining=6.1 hours
385: loss=25.844, avg loss=27.038, last=none, best=none, next=1000, rate=0.00002856, load 64=760.4 milliseconds, train=2.0 seconds, 24640 images, time remaining=6.1 hours
386: loss=26.078, avg loss=26.942, last=none, best=none, next=1000, rate=0.00002886, load 64=474.3 milliseconds, train=2.0 seconds, 24704 images, time remaining=6.1 hours
387: loss=26.983, avg loss=26.946, last=none, best=none, next=1000, rate=0.00002916, load 64=577.2 milliseconds, train=2.0 seconds, 24768 images, time remaining=6.1 hours
388: loss=23.083, avg loss=26.560, last=none, best=none, next=1000, rate=0.00002946, load 64=483.8 milliseconds, train=2.0 seconds, 24832 images, time remaining=6.1 hours
389: loss=24.112, avg loss=26.315, last=none, best=none, next=1000, rate=0.00002977, load 64=401.4 milliseconds, train=2.0 seconds, 24896 images, time remaining=6.1 hours
390: loss=28.832, avg loss=26.567, last=none, best=none, next=1000, rate=0.00003007, load 64=522.7 milliseconds, train=2.0 seconds, 24960 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
391: loss=31.314, avg loss=27.041, last=none, best=none, next=1000, rate=0.00003038, load 64=683.0 milliseconds, train=5.1 seconds, 25024 images, time remaining=6.1 hours
392: loss=27.838, avg loss=27.121, last=none, best=none, next=1000, rate=0.00003070, load 64=567.6 milliseconds, train=5.1 seconds, 25088 images, time remaining=6.2 hours
393: loss=29.309, avg loss=27.340, last=none, best=none, next=1000, rate=0.00003101, load 64=654.5 milliseconds, train=5.1 seconds, 25152 images, time remaining=6.2 hours
394: loss=26.618, avg loss=27.268, last=none, best=none, next=1000, rate=0.00003133, load 64=503.8 milliseconds, train=5.1 seconds, 25216 images, time remaining=6.2 hours
395: loss=30.157, avg loss=27.557, last=none, best=none, next=1000, rate=0.00003165, load 64=520.8 milliseconds, train=5.1 seconds, 25280 images, time remaining=6.2 hours
396: loss=27.443, avg loss=27.545, last=none, best=none, next=1000, rate=0.00003197, load 64=929.3 milliseconds, train=5.1 seconds, 25344 images, time remaining=6.2 hours
397: loss=29.604, avg loss=27.751, last=none, best=none, next=1000, rate=0.00003229, load 64=765.3 milliseconds, train=5.1 seconds, 25408 images, time remaining=6.2 hours
398: loss=31.846, avg loss=28.161, last=none, best=none, next=1000, rate=0.00003262, load 64=759.0 milliseconds, train=5.1 seconds, 25472 images, time remaining=6.2 hours
399: loss=27.161, avg loss=28.061, last=none, best=none, next=1000, rate=0.00003295, load 64=696.5 milliseconds, train=5.1 seconds, 25536 images, time remaining=6.2 hours
400: loss=26.916, avg loss=27.946, last=none, best=none, next=1000, rate=0.00003328, load 64=725.7 milliseconds, train=5.1 seconds, 25600 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
401: loss=23.958, avg loss=27.547, last=none, best=none, next=1000, rate=0.00003361, load 64=694.6 milliseconds, train=4.8 seconds, 25664 images, time remaining=6.3 hours
402: loss=27.162, avg loss=27.509, last=none, best=none, next=1000, rate=0.00003395, load 64=698.8 milliseconds, train=4.8 seconds, 25728 images, time remaining=6.3 hours
403: loss=22.535, avg loss=27.011, last=none, best=none, next=1000, rate=0.00003429, load 64=922.4 milliseconds, train=4.8 seconds, 25792 images, time remaining=6.3 hours
404: loss=28.013, avg loss=27.112, last=none, best=none, next=1000, rate=0.00003463, load 64=587.7 milliseconds, train=4.8 seconds, 25856 images, time remaining=6.3 hours
405: loss=25.666, avg loss=26.967, last=none, best=none, next=1000, rate=0.00003498, load 64=633.2 milliseconds, train=4.8 seconds, 25920 images, time remaining=6.3 hours
406: loss=25.387, avg loss=26.809, last=none, best=none, next=1000, rate=0.00003532, load 64=497.4 milliseconds, train=4.8 seconds, 25984 images, time remaining=6.3 hours
407: loss=29.540, avg loss=27.082, last=none, best=none, next=1000, rate=0.00003567, load 64=618.4 milliseconds, train=4.8 seconds, 26048 images, time remaining=6.3 hours
408: loss=30.296, avg loss=27.403, last=none, best=none, next=1000, rate=0.00003602, load 64=905.7 milliseconds, train=4.8 seconds, 26112 images, time remaining=6.3 hours
409: loss=26.910, avg loss=27.354, last=none, best=none, next=1000, rate=0.00003638, load 64=1.0 seconds, train=4.8 seconds, 26176 images, time remaining=6.3 hours
410: loss=28.327, avg loss=27.451, last=none, best=none, next=1000, rate=0.00003673, load 64=617.2 milliseconds, train=4.8 seconds, 26240 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
411: loss=22.826, avg loss=26.989, last=none, best=none, next=1000, rate=0.00003709, load 64=736.7 milliseconds, train=3.4 seconds, 26304 images, time remaining=6.3 hours
412: loss=25.000, avg loss=26.790, last=none, best=none, next=1000, rate=0.00003746, load 64=554.5 milliseconds, train=3.4 seconds, 26368 images, time remaining=6.3 hours
413: loss=25.459, avg loss=26.657, last=none, best=none, next=1000, rate=0.00003782, load 64=1.1 seconds, train=3.4 seconds, 26432 images, time remaining=6.3 hours
414: loss=28.518, avg loss=26.843, last=none, best=none, next=1000, rate=0.00003819, load 64=1.1 seconds, train=3.4 seconds, 26496 images, time remaining=6.3 hours
415: loss=25.645, avg loss=26.723, last=none, best=none, next=1000, rate=0.00003856, load 64=2.9 seconds, train=3.4 seconds, 26560 images, time remaining=6.3 hours
416: loss=27.465, avg loss=26.797, last=none, best=none, next=1000, rate=0.00003893, load 64=490.9 milliseconds, train=3.4 seconds, 26624 images, time remaining=6.3 hours
417: loss=26.456, avg loss=26.763, last=none, best=none, next=1000, rate=0.00003931, load 64=665.1 milliseconds, train=3.4 seconds, 26688 images, time remaining=6.3 hours
418: loss=28.499, avg loss=26.937, last=none, best=none, next=1000, rate=0.00003969, load 64=781.5 milliseconds, train=3.4 seconds, 26752 images, time remaining=6.3 hours
419: loss=27.382, avg loss=26.981, last=none, best=none, next=1000, rate=0.00004007, load 64=995.2 milliseconds, train=3.4 seconds, 26816 images, time remaining=6.3 hours
420: loss=30.493, avg loss=27.333, last=none, best=none, next=1000, rate=0.00004045, load 64=752.4 milliseconds, train=3.4 seconds, 26880 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a40000000
421: loss=28.392, avg loss=27.439, last=none, best=none, next=1000, rate=0.00004084, load 64=1.3 seconds, train=2.1 seconds, 26944 images, time remaining=6.3 hours
422: loss=24.788, avg loss=27.173, last=none, best=none, next=1000, rate=0.00004123, load 64=611.1 milliseconds, train=2.1 seconds, 27008 images, time remaining=6.3 hours
423: loss=24.656, avg loss=26.922, last=none, best=none, next=1000, rate=0.00004162, load 64=599.0 milliseconds, train=2.1 seconds, 27072 images, time remaining=6.3 hours
424: loss=27.320, avg loss=26.962, last=none, best=none, next=1000, rate=0.00004202, load 64=1.7 seconds, train=2.1 seconds, 27136 images, time remaining=6.3 hours
425: loss=22.864, avg loss=26.552, last=none, best=none, next=1000, rate=0.00004241, load 64=462.9 milliseconds, train=2.1 seconds, 27200 images, time remaining=6.3 hours
426: loss=22.323, avg loss=26.129, last=none, best=none, next=1000, rate=0.00004281, load 64=632.8 milliseconds, train=2.1 seconds, 27264 images, time remaining=6.3 hours
427: loss=26.355, avg loss=26.151, last=none, best=none, next=1000, rate=0.00004322, load 64=1.0 seconds, train=2.1 seconds, 27328 images, time remaining=6.3 hours
428: loss=26.488, avg loss=26.185, last=none, best=none, next=1000, rate=0.00004362, load 64=960.9 milliseconds, train=2.1 seconds, 27392 images, time remaining=6.3 hours
429: loss=24.680, avg loss=26.035, last=none, best=none, next=1000, rate=0.00004403, load 64=740.5 milliseconds, train=2.1 seconds, 27456 images, time remaining=6.3 hours
430: loss=26.993, avg loss=26.130, last=none, best=none, next=1000, rate=0.00004444, load 64=547.0 milliseconds, train=2.1 seconds, 27520 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
431: loss=24.380, avg loss=25.955, last=none, best=none, next=1000, rate=0.00004486, load 64=826.4 milliseconds, train=3.6 seconds, 27584 images, time remaining=6.3 hours
432: loss=25.540, avg loss=25.914, last=none, best=none, next=1000, rate=0.00004528, load 64=828.8 milliseconds, train=3.6 seconds, 27648 images, time remaining=6.3 hours
433: loss=23.480, avg loss=25.670, last=none, best=none, next=1000, rate=0.00004570, load 64=1.7 seconds, train=3.6 seconds, 27712 images, time remaining=6.3 hours
434: loss=27.029, avg loss=25.806, last=none, best=none, next=1000, rate=0.00004612, load 64=969.6 milliseconds, train=3.6 seconds, 27776 images, time remaining=6.3 hours
435: loss=24.426, avg loss=25.668, last=none, best=none, next=1000, rate=0.00004655, load 64=684.3 milliseconds, train=3.6 seconds, 27840 images, time remaining=6.3 hours
436: loss=25.950, avg loss=25.696, last=none, best=none, next=1000, rate=0.00004698, load 64=623.0 milliseconds, train=3.6 seconds, 27904 images, time remaining=6.3 hours
437: loss=25.028, avg loss=25.630, last=none, best=none, next=1000, rate=0.00004741, load 64=581.6 milliseconds, train=3.6 seconds, 27968 images, time remaining=6.3 hours
438: loss=27.614, avg loss=25.828, last=none, best=none, next=1000, rate=0.00004785, load 64=587.1 milliseconds, train=3.6 seconds, 28032 images, time remaining=6.3 hours
439: loss=24.216, avg loss=25.667, last=none, best=none, next=1000, rate=0.00004828, load 64=891.7 milliseconds, train=3.6 seconds, 28096 images, time remaining=6.3 hours
440: loss=22.061, avg loss=25.306, last=none, best=none, next=1000, rate=0.00004873, load 64=596.2 milliseconds, train=3.6 seconds, 28160 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x14637e000000
441: loss=24.164, avg loss=25.192, last=none, best=none, next=1000, rate=0.00004917, load 64=597.5 milliseconds, train=1.4 seconds, 28224 images, time remaining=6.3 hours
442: loss=23.524, avg loss=25.025, last=none, best=none, next=1000, rate=0.00004962, load 64=616.1 milliseconds, train=1.5 seconds, 28288 images, time remaining=6.3 hours
443: loss=25.627, avg loss=25.086, last=none, best=none, next=1000, rate=0.00005007, load 64=474.6 milliseconds, train=1.5 seconds, 28352 images, time remaining=6.3 hours
444: loss=26.380, avg loss=25.215, last=none, best=none, next=1000, rate=0.00005052, load 64=476.4 milliseconds, train=1.4 seconds, 28416 images, time remaining=6.3 hours
445: loss=27.955, avg loss=25.489, last=none, best=none, next=1000, rate=0.00005098, load 64=698.6 milliseconds, train=1.5 seconds, 28480 images, time remaining=6.3 hours
446: loss=27.209, avg loss=25.661, last=none, best=none, next=1000, rate=0.00005144, load 64=417.1 milliseconds, train=1.5 seconds, 28544 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
447: loss=23.284, avg loss=25.423, last=none, best=none, next=1000, rate=0.00005190, load 64=1.5 seconds, train=1.4 seconds, 28608 images, time remaining=6.3 hours
448: loss=22.021, avg loss=25.083, last=none, best=none, next=1000, rate=0.00005237, load 64=561.8 milliseconds, train=1.5 seconds, 28672 images, time remaining=6.3 hours
449: loss=25.353, avg loss=25.110, last=none, best=none, next=1000, rate=0.00005284, load 64=535.0 milliseconds, train=1.4 seconds, 28736 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
450: loss=26.268, avg loss=25.226, last=none, best=none, next=1000, rate=0.00005331, load 64=2.7 seconds, train=1.4 seconds, 28800 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a40000000
451: loss=25.706, avg loss=25.274, last=none, best=none, next=1000, rate=0.00005378, load 64=575.1 milliseconds, train=1.9 seconds, 28864 images, time remaining=6.3 hours
452: loss=23.323, avg loss=25.079, last=none, best=none, next=1000, rate=0.00005426, load 64=870.9 milliseconds, train=1.9 seconds, 28928 images, time remaining=6.3 hours
453: loss=25.357, avg loss=25.107, last=none, best=none, next=1000, rate=0.00005474, load 64=677.3 milliseconds, train=1.9 seconds, 28992 images, time remaining=6.2 hours
454: loss=23.566, avg loss=24.952, last=none, best=none, next=1000, rate=0.00005523, load 64=1.1 seconds, train=1.9 seconds, 29056 images, time remaining=6.2 hours
455: loss=25.331, avg loss=24.990, last=none, best=none, next=1000, rate=0.00005572, load 64=1.1 seconds, train=1.9 seconds, 29120 images, time remaining=6.2 hours
456: loss=24.460, avg loss=24.937, last=none, best=none, next=1000, rate=0.00005621, load 64=621.1 milliseconds, train=1.9 seconds, 29184 images, time remaining=6.2 hours
457: loss=25.304, avg loss=24.974, last=none, best=none, next=1000, rate=0.00005670, load 64=320.8 milliseconds, train=1.9 seconds, 29248 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
458: loss=22.286, avg loss=24.705, last=none, best=none, next=1000, rate=0.00005720, load 64=2.1 seconds, train=1.9 seconds, 29312 images, time remaining=6.2 hours
459: loss=26.642, avg loss=24.899, last=none, best=none, next=1000, rate=0.00005770, load 64=656.4 milliseconds, train=1.9 seconds, 29376 images, time remaining=6.2 hours
460: loss=25.224, avg loss=24.931, last=none, best=none, next=1000, rate=0.00005821, load 64=784.5 milliseconds, train=1.9 seconds, 29440 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a40000000
461: loss=25.101, avg loss=24.948, last=none, best=none, next=1000, rate=0.00005871, load 64=822.3 milliseconds, train=1.9 seconds, 29504 images, time remaining=6.2 hours
462: loss=21.631, avg loss=24.616, last=none, best=none, next=1000, rate=0.00005923, load 64=603.3 milliseconds, train=1.9 seconds, 29568 images, time remaining=6.2 hours
463: loss=25.293, avg loss=24.684, last=none, best=none, next=1000, rate=0.00005974, load 64=1.1 seconds, train=1.9 seconds, 29632 images, time remaining=6.2 hours
464: loss=24.123, avg loss=24.628, last=none, best=none, next=1000, rate=0.00006026, load 64=616.0 milliseconds, train=1.9 seconds, 29696 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
465: loss=21.435, avg loss=24.309, last=none, best=none, next=1000, rate=0.00006078, load 64=3.1 seconds, train=1.9 seconds, 29760 images, time remaining=6.2 hours
466: loss=24.627, avg loss=24.341, last=none, best=none, next=1000, rate=0.00006130, load 64=553.0 milliseconds, train=1.9 seconds, 29824 images, time remaining=6.2 hours
467: loss=24.350, avg loss=24.342, last=none, best=none, next=1000, rate=0.00006183, load 64=1.4 seconds, train=1.9 seconds, 29888 images, time remaining=6.2 hours
468: loss=25.733, avg loss=24.481, last=none, best=none, next=1000, rate=0.00006236, load 64=460.2 milliseconds, train=1.9 seconds, 29952 images, time remaining=6.2 hours
469: loss=24.668, avg loss=24.499, last=none, best=none, next=1000, rate=0.00006290, load 64=1.8 seconds, train=1.9 seconds, 30016 images, time remaining=6.2 hours
470: loss=23.094, avg loss=24.359, last=none, best=none, next=1000, rate=0.00006344, load 64=407.0 milliseconds, train=1.9 seconds, 30080 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
471: loss=25.224, avg loss=24.445, last=none, best=none, next=1000, rate=0.00006398, load 64=1.6 seconds, train=4.5 seconds, 30144 images, time remaining=6.2 hours
472: loss=25.419, avg loss=24.543, last=none, best=none, next=1000, rate=0.00006452, load 64=626.0 milliseconds, train=4.5 seconds, 30208 images, time remaining=6.2 hours
473: loss=27.139, avg loss=24.802, last=none, best=none, next=1000, rate=0.00006507, load 64=439.3 milliseconds, train=4.5 seconds, 30272 images, time remaining=6.2 hours
474: loss=26.986, avg loss=25.021, last=none, best=none, next=1000, rate=0.00006562, load 64=685.9 milliseconds, train=4.5 seconds, 30336 images, time remaining=6.2 hours
475: loss=25.444, avg loss=25.063, last=none, best=none, next=1000, rate=0.00006618, load 64=1.2 seconds, train=4.5 seconds, 30400 images, time remaining=6.2 hours
476: loss=21.538, avg loss=24.711, last=none, best=none, next=1000, rate=0.00006674, load 64=1.5 seconds, train=4.5 seconds, 30464 images, time remaining=6.2 hours
477: loss=24.652, avg loss=24.705, last=none, best=none, next=1000, rate=0.00006730, load 64=831.5 milliseconds, train=4.5 seconds, 30528 images, time remaining=6.2 hours
478: loss=24.333, avg loss=24.668, last=none, best=none, next=1000, rate=0.00006787, load 64=2.4 seconds, train=4.5 seconds, 30592 images, time remaining=6.2 hours
479: loss=23.214, avg loss=24.522, last=none, best=none, next=1000, rate=0.00006844, load 64=795.1 milliseconds, train=4.5 seconds, 30656 images, time remaining=6.2 hours
480: loss=26.743, avg loss=24.744, last=none, best=none, next=1000, rate=0.00006901, load 64=846.7 milliseconds, train=4.5 seconds, 30720 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
481: loss=23.634, avg loss=24.633, last=none, best=none, next=1000, rate=0.00006959, load 64=629.0 milliseconds, train=3.4 seconds, 30784 images, time remaining=6.2 hours
482: loss=23.410, avg loss=24.511, last=none, best=none, next=1000, rate=0.00007017, load 64=1.3 seconds, train=3.4 seconds, 30848 images, time remaining=6.2 hours
483: loss=23.339, avg loss=24.394, last=none, best=none, next=1000, rate=0.00007075, load 64=2.6 seconds, train=3.4 seconds, 30912 images, time remaining=6.2 hours
484: loss=21.750, avg loss=24.129, last=none, best=none, next=1000, rate=0.00007134, load 64=2.2 seconds, train=3.4 seconds, 30976 images, time remaining=6.2 hours
485: loss=24.859, avg loss=24.202, last=none, best=none, next=1000, rate=0.00007193, load 64=1.2 seconds, train=3.4 seconds, 31040 images, time remaining=6.2 hours
486: loss=26.043, avg loss=24.386, last=none, best=none, next=1000, rate=0.00007253, load 64=1.2 seconds, train=3.4 seconds, 31104 images, time remaining=6.2 hours
487: loss=24.516, avg loss=24.399, last=none, best=none, next=1000, rate=0.00007312, load 64=3.2 seconds, train=3.4 seconds, 31168 images, time remaining=6.2 hours
488: loss=23.636, avg loss=24.323, last=none, best=none, next=1000, rate=0.00007373, load 64=739.8 milliseconds, train=3.4 seconds, 31232 images, time remaining=6.2 hours
489: loss=23.403, avg loss=24.231, last=none, best=none, next=1000, rate=0.00007433, load 64=567.8 milliseconds, train=3.4 seconds, 31296 images, time remaining=6.2 hours
490: loss=23.675, avg loss=24.175, last=none, best=none, next=1000, rate=0.00007494, load 64=854.8 milliseconds, train=3.4 seconds, 31360 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x14472e000000
491: loss=25.902, avg loss=24.348, last=none, best=none, next=1000, rate=0.00007556, load 64=3.1 seconds, train=3.5 seconds, 31424 images, time remaining=6.3 hours
492: loss=24.145, avg loss=24.328, last=none, best=none, next=1000, rate=0.00007617, load 64=632.2 milliseconds, train=3.5 seconds, 31488 images, time remaining=6.3 hours
493: loss=26.325, avg loss=24.528, last=none, best=none, next=1000, rate=0.00007679, load 64=1.2 seconds, train=3.5 seconds, 31552 images, time remaining=6.3 hours
494: loss=24.547, avg loss=24.530, last=none, best=none, next=1000, rate=0.00007742, load 64=2.9 seconds, train=3.5 seconds, 31616 images, time remaining=6.3 hours
495: loss=20.028, avg loss=24.079, last=none, best=none, next=1000, rate=0.00007805, load 64=673.4 milliseconds, train=3.5 seconds, 31680 images, time remaining=6.3 hours
496: loss=20.978, avg loss=23.769, last=none, best=none, next=1000, rate=0.00007868, load 64=1.9 seconds, train=3.5 seconds, 31744 images, time remaining=6.3 hours
497: loss=22.657, avg loss=23.658, last=none, best=none, next=1000, rate=0.00007932, load 64=1.3 seconds, train=3.5 seconds, 31808 images, time remaining=6.3 hours
498: loss=21.258, avg loss=23.418, last=none, best=none, next=1000, rate=0.00007996, load 64=2.7 seconds, train=3.5 seconds, 31872 images, time remaining=6.3 hours
499: loss=22.222, avg loss=23.298, last=none, best=none, next=1000, rate=0.00008060, load 64=796.0 milliseconds, train=3.5 seconds, 31936 images, time remaining=6.3 hours
500: loss=19.854, avg loss=22.954, last=none, best=none, next=1000, rate=0.00008125, load 64=1.1 seconds, train=3.5 seconds, 32000 images, time remaining=6.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144746000000
501: loss=20.091, avg loss=22.668, last=none, best=none, next=1000, rate=0.00008190, load 64=2.8 seconds, train=3.9 seconds, 32064 images, time remaining=6.3 hours
502: loss=21.391, avg loss=22.540, last=none, best=none, next=1000, rate=0.00008256, load 64=752.1 milliseconds, train=3.9 seconds, 32128 images, time remaining=6.3 hours
503: loss=20.343, avg loss=22.320, last=none, best=none, next=1000, rate=0.00008322, load 64=1.6 seconds, train=3.9 seconds, 32192 images, time remaining=6.3 hours
504: loss=19.336, avg loss=22.022, last=none, best=none, next=1000, rate=0.00008388, load 64=872.1 milliseconds, train=3.9 seconds, 32256 images, time remaining=6.3 hours
505: loss=21.023, avg loss=21.922, last=none, best=none, next=1000, rate=0.00008455, load 64=1.2 seconds, train=3.9 seconds, 32320 images, time remaining=6.3 hours
506: loss=24.130, avg loss=22.143, last=none, best=none, next=1000, rate=0.00008522, load 64=2.6 seconds, train=3.9 seconds, 32384 images, time remaining=6.3 hours
507: loss=23.050, avg loss=22.233, last=none, best=none, next=1000, rate=0.00008590, load 64=2.2 seconds, train=3.9 seconds, 32448 images, time remaining=6.3 hours
508: loss=22.180, avg loss=22.228, last=none, best=none, next=1000, rate=0.00008658, load 64=1.3 seconds, train=3.9 seconds, 32512 images, time remaining=6.3 hours
509: loss=23.771, avg loss=22.382, last=none, best=none, next=1000, rate=0.00008726, load 64=2.6 seconds, train=3.9 seconds, 32576 images, time remaining=6.3 hours
510: loss=20.989, avg loss=22.243, last=none, best=none, next=1000, rate=0.00008795, load 64=2.8 seconds, train=3.9 seconds, 32640 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
511: loss=23.470, avg loss=22.366, last=none, best=none, next=1000, rate=0.00008864, load 64=1.8 seconds, train=1.8 seconds, 32704 images, time remaining=6.3 hours
512: loss=21.148, avg loss=22.244, last=none, best=none, next=1000, rate=0.00008934, load 64=729.8 milliseconds, train=1.8 seconds, 32768 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
513: loss=24.512, avg loss=22.471, last=none, best=none, next=1000, rate=0.00009004, load 64=2.5 seconds, train=1.8 seconds, 32832 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
514: loss=23.629, avg loss=22.587, last=none, best=none, next=1000, rate=0.00009074, load 64=1.9 seconds, train=1.8 seconds, 32896 images, time remaining=6.3 hours
515: loss=25.235, avg loss=22.852, last=none, best=none, next=1000, rate=0.00009145, load 64=702.8 milliseconds, train=1.8 seconds, 32960 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
516: loss=19.987, avg loss=22.565, last=none, best=none, next=1000, rate=0.00009216, load 64=2.4 seconds, train=1.8 seconds, 33024 images, time remaining=6.3 hours
517: loss=21.984, avg loss=22.507, last=none, best=none, next=1000, rate=0.00009288, load 64=1.0 seconds, train=1.8 seconds, 33088 images, time remaining=6.3 hours
518: loss=20.013, avg loss=22.258, last=none, best=none, next=1000, rate=0.00009360, load 64=1.2 seconds, train=1.8 seconds, 33152 images, time remaining=6.3 hours
519: loss=21.781, avg loss=22.210, last=none, best=none, next=1000, rate=0.00009432, load 64=1.2 seconds, train=1.8 seconds, 33216 images, time remaining=6.3 hours
520: loss=17.269, avg loss=21.716, last=none, best=none, next=1000, rate=0.00009505, load 64=983.3 milliseconds, train=1.8 seconds, 33280 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
521: loss=22.611, avg loss=21.805, last=none, best=none, next=1000, rate=0.00009578, load 64=1.1 seconds, train=1.9 seconds, 33344 images, time remaining=6.3 hours
522: loss=21.241, avg loss=21.749, last=none, best=none, next=1000, rate=0.00009652, load 64=666.8 milliseconds, train=1.9 seconds, 33408 images, time remaining=6.3 hours
523: loss=22.798, avg loss=21.854, last=none, best=none, next=1000, rate=0.00009726, load 64=864.4 milliseconds, train=1.9 seconds, 33472 images, time remaining=6.3 hours
524: loss=19.925, avg loss=21.661, last=none, best=none, next=1000, rate=0.00009801, load 64=815.6 milliseconds, train=1.9 seconds, 33536 images, time remaining=6.3 hours
525: loss=20.159, avg loss=21.511, last=none, best=none, next=1000, rate=0.00009876, load 64=786.5 milliseconds, train=1.9 seconds, 33600 images, time remaining=6.3 hours
526: loss=19.256, avg loss=21.285, last=none, best=none, next=1000, rate=0.00009951, load 64=652.6 milliseconds, train=1.9 seconds, 33664 images, time remaining=6.2 hours
527: loss=19.340, avg loss=21.091, last=none, best=none, next=1000, rate=0.00010027, load 64=1.0 seconds, train=1.9 seconds, 33728 images, time remaining=6.2 hours
528: loss=19.342, avg loss=20.916, last=none, best=none, next=1000, rate=0.00010104, load 64=728.0 milliseconds, train=1.9 seconds, 33792 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
529: loss=18.099, avg loss=20.634, last=none, best=none, next=1000, rate=0.00010180, load 64=1.9 seconds, train=1.9 seconds, 33856 images, time remaining=6.2 hours
530: loss=19.450, avg loss=20.516, last=none, best=none, next=1000, rate=0.00010258, load 64=878.9 milliseconds, train=1.9 seconds, 33920 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1464b0000000
531: loss=19.861, avg loss=20.450, last=none, best=none, next=1000, rate=0.00010335, load 64=1.2 seconds, train=1.4 seconds, 33984 images, time remaining=6.2 hours
532: loss=19.785, avg loss=20.384, last=none, best=none, next=1000, rate=0.00010413, load 64=660.9 milliseconds, train=1.4 seconds, 34048 images, time remaining=6.2 hours
533: loss=20.359, avg loss=20.381, last=none, best=none, next=1000, rate=0.00010492, load 64=1.3 seconds, train=1.4 seconds, 34112 images, time remaining=6.2 hours
534: loss=20.377, avg loss=20.381, last=none, best=none, next=1000, rate=0.00010571, load 64=794.5 milliseconds, train=1.4 seconds, 34176 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
535: loss=21.159, avg loss=20.459, last=none, best=none, next=1000, rate=0.00010650, load 64=2.7 seconds, train=1.4 seconds, 34240 images, time remaining=6.2 hours
536: loss=20.069, avg loss=20.420, last=none, best=none, next=1000, rate=0.00010730, load 64=902.5 milliseconds, train=1.4 seconds, 34304 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
537: loss=18.336, avg loss=20.211, last=none, best=none, next=1000, rate=0.00010810, load 64=2.1 seconds, train=1.4 seconds, 34368 images, time remaining=6.2 hours
538: loss=19.406, avg loss=20.131, last=none, best=none, next=1000, rate=0.00010891, load 64=734.9 milliseconds, train=1.5 seconds, 34432 images, time remaining=6.2 hours
539: loss=21.338, avg loss=20.252, last=none, best=none, next=1000, rate=0.00010972, load 64=769.6 milliseconds, train=1.5 seconds, 34496 images, time remaining=6.2 hours
540: loss=17.834, avg loss=20.010, last=none, best=none, next=1000, rate=0.00011054, load 64=771.3 milliseconds, train=1.4 seconds, 34560 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
541: loss=24.559, avg loss=20.465, last=none, best=none, next=1000, rate=0.00011136, load 64=1.7 seconds, train=3.4 seconds, 34624 images, time remaining=6.2 hours
542: loss=21.420, avg loss=20.560, last=none, best=none, next=1000, rate=0.00011219, load 64=2.0 seconds, train=3.4 seconds, 34688 images, time remaining=6.2 hours
543: loss=20.840, avg loss=20.588, last=none, best=none, next=1000, rate=0.00011302, load 64=1.9 seconds, train=3.4 seconds, 34752 images, time remaining=6.2 hours
544: loss=22.426, avg loss=20.772, last=none, best=none, next=1000, rate=0.00011385, load 64=2.8 seconds, train=3.4 seconds, 34816 images, time remaining=6.2 hours
545: loss=19.692, avg loss=20.664, last=none, best=none, next=1000, rate=0.00011469, load 64=749.5 milliseconds, train=3.4 seconds, 34880 images, time remaining=6.2 hours
546: loss=20.070, avg loss=20.605, last=none, best=none, next=1000, rate=0.00011554, load 64=657.6 milliseconds, train=3.4 seconds, 34944 images, time remaining=6.2 hours
547: loss=19.443, avg loss=20.488, last=none, best=none, next=1000, rate=0.00011638, load 64=1.6 seconds, train=3.4 seconds, 35008 images, time remaining=6.2 hours
548: loss=20.491, avg loss=20.489, last=none, best=none, next=1000, rate=0.00011724, load 64=1.2 seconds, train=3.4 seconds, 35072 images, time remaining=6.2 hours
549: loss=20.326, avg loss=20.472, last=none, best=none, next=1000, rate=0.00011810, load 64=2.5 seconds, train=3.4 seconds, 35136 images, time remaining=6.2 hours
550: loss=18.445, avg loss=20.270, last=none, best=none, next=1000, rate=0.00011896, load 64=1.2 seconds, train=3.4 seconds, 35200 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
551: loss=18.000, avg loss=20.043, last=none, best=none, next=1000, rate=0.00011983, load 64=3.2 seconds, train=3.3 seconds, 35264 images, time remaining=6.2 hours
552: loss=19.246, avg loss=19.963, last=none, best=none, next=1000, rate=0.00012070, load 64=1.1 seconds, train=3.3 seconds, 35328 images, time remaining=6.2 hours
553: loss=19.104, avg loss=19.877, last=none, best=none, next=1000, rate=0.00012157, load 64=1.7 seconds, train=3.3 seconds, 35392 images, time remaining=6.2 hours
554: loss=21.229, avg loss=20.012, last=none, best=none, next=1000, rate=0.00012246, load 64=850.1 milliseconds, train=3.3 seconds, 35456 images, time remaining=6.2 hours
555: loss=20.566, avg loss=20.068, last=none, best=none, next=1000, rate=0.00012334, load 64=869.0 milliseconds, train=3.3 seconds, 35520 images, time remaining=6.2 hours
556: loss=20.596, avg loss=20.120, last=none, best=none, next=1000, rate=0.00012423, load 64=960.0 milliseconds, train=3.3 seconds, 35584 images, time remaining=6.2 hours
557: loss=18.079, avg loss=19.916, last=none, best=none, next=1000, rate=0.00012513, load 64=951.7 milliseconds, train=3.3 seconds, 35648 images, time remaining=6.2 hours
558: loss=19.008, avg loss=19.825, last=none, best=none, next=1000, rate=0.00012603, load 64=1.1 seconds, train=3.3 seconds, 35712 images, time remaining=6.2 hours
559: loss=21.056, avg loss=19.948, last=none, best=none, next=1000, rate=0.00012694, load 64=1.4 seconds, train=3.3 seconds, 35776 images, time remaining=6.2 hours
560: loss=19.051, avg loss=19.859, last=none, best=none, next=1000, rate=0.00012785, load 64=1.2 seconds, train=3.3 seconds, 35840 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145fe9000000
561: loss=18.893, avg loss=19.762, last=none, best=none, next=1000, rate=0.00012876, load 64=664.9 milliseconds, train=1.3 seconds, 35904 images, time remaining=6.2 hours
562: loss=15.465, avg loss=19.332, last=none, best=none, next=1000, rate=0.00012968, load 64=836.7 milliseconds, train=1.2 seconds, 35968 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
563: loss=20.544, avg loss=19.454, last=none, best=none, next=1000, rate=0.00013061, load 64=1.4 seconds, train=1.2 seconds, 36032 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
564: loss=18.390, avg loss=19.347, last=none, best=none, next=1000, rate=0.00013154, load 64=1.5 seconds, train=1.2 seconds, 36096 images, time remaining=6.2 hours
565: loss=16.167, avg loss=19.029, last=none, best=none, next=1000, rate=0.00013248, load 64=567.2 milliseconds, train=1.2 seconds, 36160 images, time remaining=6.2 hours
566: loss=16.196, avg loss=18.746, last=none, best=none, next=1000, rate=0.00013342, load 64=704.5 milliseconds, train=1.2 seconds, 36224 images, time remaining=6.2 hours
567: loss=18.014, avg loss=18.673, last=none, best=none, next=1000, rate=0.00013436, load 64=1.1 seconds, train=1.2 seconds, 36288 images, time remaining=6.2 hours
568: loss=19.801, avg loss=18.785, last=none, best=none, next=1000, rate=0.00013531, load 64=873.0 milliseconds, train=1.2 seconds, 36352 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
569: loss=17.910, avg loss=18.698, last=none, best=none, next=1000, rate=0.00013627, load 64=3.3 seconds, train=1.2 seconds, 36416 images, time remaining=6.2 hours
570: loss=17.580, avg loss=18.586, last=none, best=none, next=1000, rate=0.00013723, load 64=902.0 milliseconds, train=1.2 seconds, 36480 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145a74000000
571: loss=21.394, avg loss=18.867, last=none, best=none, next=1000, rate=0.00013819, load 64=1.5 seconds, train=1.6 seconds, 36544 images, time remaining=6.1 hours
572: loss=19.306, avg loss=18.911, last=none, best=none, next=1000, rate=0.00013916, load 64=994.3 milliseconds, train=1.6 seconds, 36608 images, time remaining=6.1 hours
573: loss=19.899, avg loss=19.010, last=none, best=none, next=1000, rate=0.00014014, load 64=1.2 seconds, train=1.6 seconds, 36672 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
574: loss=19.045, avg loss=19.013, last=none, best=none, next=1000, rate=0.00014112, load 64=2.8 seconds, train=1.6 seconds, 36736 images, time remaining=6.1 hours
575: loss=18.299, avg loss=18.942, last=none, best=none, next=1000, rate=0.00014211, load 64=509.2 milliseconds, train=1.6 seconds, 36800 images, time remaining=6.1 hours
576: loss=18.818, avg loss=18.929, last=none, best=none, next=1000, rate=0.00014310, load 64=651.5 milliseconds, train=1.6 seconds, 36864 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
577: loss=19.373, avg loss=18.974, last=none, best=none, next=1000, rate=0.00014409, load 64=2.2 seconds, train=1.6 seconds, 36928 images, time remaining=6.1 hours
578: loss=17.922, avg loss=18.869, last=none, best=none, next=1000, rate=0.00014510, load 64=1.1 seconds, train=1.6 seconds, 36992 images, time remaining=6.1 hours
579: loss=16.459, avg loss=18.628, last=none, best=none, next=1000, rate=0.00014610, load 64=550.2 milliseconds, train=1.7 seconds, 37056 images, time remaining=6.1 hours
580: loss=18.506, avg loss=18.615, last=none, best=none, next=1000, rate=0.00014711, load 64=1.2 seconds, train=1.6 seconds, 37120 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
581: loss=19.455, avg loss=18.699, last=none, best=none, next=1000, rate=0.00014813, load 64=785.5 milliseconds, train=4.1 seconds, 37184 images, time remaining=6.1 hours
582: loss=22.189, avg loss=19.048, last=none, best=none, next=1000, rate=0.00014915, load 64=1.1 seconds, train=4.1 seconds, 37248 images, time remaining=6.1 hours
583: loss=21.399, avg loss=19.283, last=none, best=none, next=1000, rate=0.00015018, load 64=747.0 milliseconds, train=4.1 seconds, 37312 images, time remaining=6.1 hours
584: loss=19.511, avg loss=19.306, last=none, best=none, next=1000, rate=0.00015121, load 64=773.8 milliseconds, train=4.1 seconds, 37376 images, time remaining=6.1 hours
585: loss=18.380, avg loss=19.213, last=none, best=none, next=1000, rate=0.00015225, load 64=579.0 milliseconds, train=4.1 seconds, 37440 images, time remaining=6.1 hours
586: loss=20.949, avg loss=19.387, last=none, best=none, next=1000, rate=0.00015330, load 64=965.9 milliseconds, train=4.1 seconds, 37504 images, time remaining=6.1 hours
587: loss=20.560, avg loss=19.504, last=none, best=none, next=1000, rate=0.00015435, load 64=957.4 milliseconds, train=4.1 seconds, 37568 images, time remaining=6.1 hours
588: loss=17.786, avg loss=19.332, last=none, best=none, next=1000, rate=0.00015540, load 64=1.2 seconds, train=4.1 seconds, 37632 images, time remaining=6.1 hours
589: loss=20.816, avg loss=19.481, last=none, best=none, next=1000, rate=0.00015646, load 64=849.4 milliseconds, train=4.1 seconds, 37696 images, time remaining=6.1 hours
590: loss=18.027, avg loss=19.335, last=none, best=none, next=1000, rate=0.00015753, load 64=2.4 seconds, train=4.1 seconds, 37760 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1024x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
591: loss=16.043, avg loss=19.006, last=none, best=none, next=1000, rate=0.00015860, load 64=2.7 seconds, train=3.2 seconds, 37824 images, time remaining=6.2 hours
592: loss=15.389, avg loss=18.644, last=none, best=none, next=1000, rate=0.00015967, load 64=894.1 milliseconds, train=3.2 seconds, 37888 images, time remaining=6.2 hours
593: loss=15.935, avg loss=18.374, last=none, best=none, next=1000, rate=0.00016075, load 64=1.9 seconds, train=3.2 seconds, 37952 images, time remaining=6.2 hours
594: loss=16.255, avg loss=18.162, last=none, best=none, next=1000, rate=0.00016184, load 64=973.4 milliseconds, train=3.2 seconds, 38016 images, time remaining=6.2 hours
595: loss=20.625, avg loss=18.408, last=none, best=none, next=1000, rate=0.00016293, load 64=824.3 milliseconds, train=3.2 seconds, 38080 images, time remaining=6.2 hours
596: loss=19.026, avg loss=18.470, last=none, best=none, next=1000, rate=0.00016403, load 64=612.7 milliseconds, train=3.2 seconds, 38144 images, time remaining=6.2 hours
597: loss=17.997, avg loss=18.422, last=none, best=none, next=1000, rate=0.00016514, load 64=811.1 milliseconds, train=3.2 seconds, 38208 images, time remaining=6.2 hours
598: loss=19.135, avg loss=18.494, last=none, best=none, next=1000, rate=0.00016624, load 64=582.0 milliseconds, train=3.2 seconds, 38272 images, time remaining=6.2 hours
599: loss=17.296, avg loss=18.374, last=none, best=none, next=1000, rate=0.00016736, load 64=674.1 milliseconds, train=3.2 seconds, 38336 images, time remaining=6.2 hours
600: loss=15.019, avg loss=18.038, last=none, best=none, next=1000, rate=0.00016848, load 64=756.1 milliseconds, train=3.2 seconds, 38400 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
601: loss=18.851, avg loss=18.120, last=none, best=none, next=1000, rate=0.00016961, load 64=1.2 seconds, train=3.4 seconds, 38464 images, time remaining=6.2 hours
602: loss=17.633, avg loss=18.071, last=none, best=none, next=1000, rate=0.00017074, load 64=495.6 milliseconds, train=3.4 seconds, 38528 images, time remaining=6.2 hours
603: loss=17.313, avg loss=17.995, last=none, best=none, next=1000, rate=0.00017187, load 64=677.1 milliseconds, train=3.4 seconds, 38592 images, time remaining=6.2 hours
604: loss=17.789, avg loss=17.975, last=none, best=none, next=1000, rate=0.00017302, load 64=851.3 milliseconds, train=3.4 seconds, 38656 images, time remaining=6.2 hours
605: loss=17.720, avg loss=17.949, last=none, best=none, next=1000, rate=0.00017417, load 64=460.7 milliseconds, train=3.4 seconds, 38720 images, time remaining=6.2 hours
606: loss=17.114, avg loss=17.866, last=none, best=none, next=1000, rate=0.00017532, load 64=910.4 milliseconds, train=3.4 seconds, 38784 images, time remaining=6.2 hours
607: loss=22.017, avg loss=18.281, last=none, best=none, next=1000, rate=0.00017648, load 64=458.4 milliseconds, train=3.4 seconds, 38848 images, time remaining=6.2 hours
608: loss=17.454, avg loss=18.198, last=none, best=none, next=1000, rate=0.00017765, load 64=619.3 milliseconds, train=3.4 seconds, 38912 images, time remaining=6.2 hours
609: loss=18.611, avg loss=18.239, last=none, best=none, next=1000, rate=0.00017882, load 64=710.2 milliseconds, train=3.4 seconds, 38976 images, time remaining=6.2 hours
610: loss=17.353, avg loss=18.151, last=none, best=none, next=1000, rate=0.00018000, load 64=647.4 milliseconds, train=3.4 seconds, 39040 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146034000000
611: loss=16.817, avg loss=18.017, last=none, best=none, next=1000, rate=0.00018118, load 64=641.3 milliseconds, train=1.5 seconds, 39104 images, time remaining=6.2 hours
612: loss=17.128, avg loss=17.928, last=none, best=none, next=1000, rate=0.00018237, load 64=564.2 milliseconds, train=1.5 seconds, 39168 images, time remaining=6.2 hours
613: loss=15.572, avg loss=17.693, last=none, best=none, next=1000, rate=0.00018356, load 64=1.4 seconds, train=1.5 seconds, 39232 images, time remaining=6.2 hours
614: loss=18.975, avg loss=17.821, last=none, best=none, next=1000, rate=0.00018476, load 64=679.2 milliseconds, train=1.5 seconds, 39296 images, time remaining=6.1 hours
615: loss=16.213, avg loss=17.660, last=none, best=none, next=1000, rate=0.00018597, load 64=759.0 milliseconds, train=1.5 seconds, 39360 images, time remaining=6.1 hours
616: loss=16.641, avg loss=17.558, last=none, best=none, next=1000, rate=0.00018718, load 64=483.9 milliseconds, train=1.5 seconds, 39424 images, time remaining=6.1 hours
617: loss=18.292, avg loss=17.632, last=none, best=none, next=1000, rate=0.00018840, load 64=780.7 milliseconds, train=1.5 seconds, 39488 images, time remaining=6.1 hours
618: loss=14.856, avg loss=17.354, last=none, best=none, next=1000, rate=0.00018963, load 64=692.6 milliseconds, train=1.5 seconds, 39552 images, time remaining=6.1 hours
619: loss=16.849, avg loss=17.304, last=none, best=none, next=1000, rate=0.00019086, load 64=488.4 milliseconds, train=1.5 seconds, 39616 images, time remaining=6.1 hours
620: loss=16.292, avg loss=17.203, last=none, best=none, next=1000, rate=0.00019209, load 64=1.5 seconds, train=1.5 seconds, 39680 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
621: loss=20.624, avg loss=17.545, last=none, best=none, next=1000, rate=0.00019333, load 64=549.1 milliseconds, train=3.3 seconds, 39744 images, time remaining=6.1 hours
622: loss=16.857, avg loss=17.476, last=none, best=none, next=1000, rate=0.00019458, load 64=714.7 milliseconds, train=3.3 seconds, 39808 images, time remaining=6.1 hours
623: loss=16.629, avg loss=17.391, last=none, best=none, next=1000, rate=0.00019584, load 64=668.6 milliseconds, train=3.2 seconds, 39872 images, time remaining=6.1 hours
624: loss=18.415, avg loss=17.494, last=none, best=none, next=1000, rate=0.00019710, load 64=423.9 milliseconds, train=3.3 seconds, 39936 images, time remaining=6.1 hours
625: loss=18.764, avg loss=17.621, last=none, best=none, next=1000, rate=0.00019836, load 64=634.9 milliseconds, train=3.3 seconds, 40000 images, time remaining=6.1 hours
626: loss=17.407, avg loss=17.599, last=none, best=none, next=1000, rate=0.00019964, load 64=617.8 milliseconds, train=3.3 seconds, 40064 images, time remaining=6.1 hours
627: loss=18.862, avg loss=17.726, last=none, best=none, next=1000, rate=0.00020092, load 64=902.1 milliseconds, train=3.3 seconds, 40128 images, time remaining=6.1 hours
628: loss=16.520, avg loss=17.605, last=none, best=none, next=1000, rate=0.00020220, load 64=1.4 seconds, train=3.3 seconds, 40192 images, time remaining=6.1 hours
629: loss=18.293, avg loss=17.674, last=none, best=none, next=1000, rate=0.00020349, load 64=789.7 milliseconds, train=3.3 seconds, 40256 images, time remaining=6.1 hours
630: loss=16.173, avg loss=17.524, last=none, best=none, next=1000, rate=0.00020479, load 64=1.3 seconds, train=3.3 seconds, 40320 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x1447bc000000
631: loss=17.102, avg loss=17.482, last=none, best=none, next=1000, rate=0.00020609, load 64=766.6 milliseconds, train=3.9 seconds, 40384 images, time remaining=6.1 hours
632: loss=17.636, avg loss=17.497, last=none, best=none, next=1000, rate=0.00020740, load 64=2.8 seconds, train=3.9 seconds, 40448 images, time remaining=6.1 hours
633: loss=18.425, avg loss=17.590, last=none, best=none, next=1000, rate=0.00020872, load 64=501.8 milliseconds, train=3.9 seconds, 40512 images, time remaining=6.1 hours
634: loss=16.931, avg loss=17.524, last=none, best=none, next=1000, rate=0.00021004, load 64=936.1 milliseconds, train=3.9 seconds, 40576 images, time remaining=6.1 hours
635: loss=16.823, avg loss=17.454, last=none, best=none, next=1000, rate=0.00021137, load 64=2.7 seconds, train=3.9 seconds, 40640 images, time remaining=6.1 hours
636: loss=18.224, avg loss=17.531, last=none, best=none, next=1000, rate=0.00021270, load 64=605.6 milliseconds, train=3.9 seconds, 40704 images, time remaining=6.1 hours
637: loss=19.211, avg loss=17.699, last=none, best=none, next=1000, rate=0.00021404, load 64=977.3 milliseconds, train=3.9 seconds, 40768 images, time remaining=6.1 hours
638: loss=16.935, avg loss=17.622, last=none, best=none, next=1000, rate=0.00021539, load 64=719.6 milliseconds, train=3.9 seconds, 40832 images, time remaining=6.1 hours
639: loss=20.358, avg loss=17.896, last=none, best=none, next=1000, rate=0.00021674, load 64=633.5 milliseconds, train=3.9 seconds, 40896 images, time remaining=6.1 hours
640: loss=17.335, avg loss=17.840, last=none, best=none, next=1000, rate=0.00021810, load 64=1.1 seconds, train=3.9 seconds, 40960 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
641: loss=17.693, avg loss=17.825, last=none, best=none, next=1000, rate=0.00021947, load 64=960.8 milliseconds, train=5.1 seconds, 41024 images, time remaining=6.2 hours
642: loss=17.445, avg loss=17.787, last=none, best=none, next=1000, rate=0.00022084, load 64=979.7 milliseconds, train=5.1 seconds, 41088 images, time remaining=6.2 hours
643: loss=20.930, avg loss=18.101, last=none, best=none, next=1000, rate=0.00022222, load 64=836.2 milliseconds, train=5.1 seconds, 41152 images, time remaining=6.2 hours
644: loss=16.946, avg loss=17.986, last=none, best=none, next=1000, rate=0.00022361, load 64=972.6 milliseconds, train=5.1 seconds, 41216 images, time remaining=6.2 hours
645: loss=17.503, avg loss=17.938, last=none, best=none, next=1000, rate=0.00022500, load 64=971.7 milliseconds, train=5.1 seconds, 41280 images, time remaining=6.2 hours
646: loss=15.365, avg loss=17.680, last=none, best=none, next=1000, rate=0.00022640, load 64=734.0 milliseconds, train=5.1 seconds, 41344 images, time remaining=6.2 hours
647: loss=15.399, avg loss=17.452, last=none, best=none, next=1000, rate=0.00022780, load 64=1.0 seconds, train=5.1 seconds, 41408 images, time remaining=6.2 hours
648: loss=17.422, avg loss=17.449, last=none, best=none, next=1000, rate=0.00022922, load 64=659.5 milliseconds, train=5.1 seconds, 41472 images, time remaining=6.2 hours
649: loss=19.485, avg loss=17.653, last=none, best=none, next=1000, rate=0.00023063, load 64=1.0 seconds, train=5.1 seconds, 41536 images, time remaining=6.2 hours
650: loss=18.224, avg loss=17.710, last=none, best=none, next=1000, rate=0.00023206, load 64=690.5 milliseconds, train=5.1 seconds, 41600 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
651: loss=18.209, avg loss=17.760, last=none, best=none, next=1000, rate=0.00023349, load 64=559.0 milliseconds, train=4.1 seconds, 41664 images, time remaining=6.2 hours
652: loss=15.519, avg loss=17.536, last=none, best=none, next=1000, rate=0.00023493, load 64=830.9 milliseconds, train=4.1 seconds, 41728 images, time remaining=6.2 hours
653: loss=15.414, avg loss=17.324, last=none, best=none, next=1000, rate=0.00023637, load 64=2.4 seconds, train=4.1 seconds, 41792 images, time remaining=6.2 hours
654: loss=17.753, avg loss=17.367, last=none, best=none, next=1000, rate=0.00023782, load 64=2.6 seconds, train=4.1 seconds, 41856 images, time remaining=6.2 hours
655: loss=16.261, avg loss=17.256, last=none, best=none, next=1000, rate=0.00023928, load 64=468.0 milliseconds, train=4.1 seconds, 41920 images, time remaining=6.2 hours
656: loss=16.937, avg loss=17.224, last=none, best=none, next=1000, rate=0.00024075, load 64=813.1 milliseconds, train=4.1 seconds, 41984 images, time remaining=6.2 hours
657: loss=17.369, avg loss=17.238, last=none, best=none, next=1000, rate=0.00024222, load 64=2.1 seconds, train=4.1 seconds, 42048 images, time remaining=6.2 hours
658: loss=17.660, avg loss=17.281, last=none, best=none, next=1000, rate=0.00024370, load 64=770.8 milliseconds, train=4.1 seconds, 42112 images, time remaining=6.2 hours
659: loss=16.767, avg loss=17.229, last=none, best=none, next=1000, rate=0.00024518, load 64=673.4 milliseconds, train=4.1 seconds, 42176 images, time remaining=6.2 hours
660: loss=17.843, avg loss=17.291, last=none, best=none, next=1000, rate=0.00024667, load 64=3.1 seconds, train=4.1 seconds, 42240 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
661: loss=16.820, avg loss=17.244, last=none, best=none, next=1000, rate=0.00024817, load 64=1.7 seconds, train=4.6 seconds, 42304 images, time remaining=6.3 hours
662: loss=20.210, avg loss=17.540, last=none, best=none, next=1000, rate=0.00024968, load 64=858.7 milliseconds, train=4.6 seconds, 42368 images, time remaining=6.3 hours
663: loss=16.190, avg loss=17.405, last=none, best=none, next=1000, rate=0.00025119, load 64=541.0 milliseconds, train=4.6 seconds, 42432 images, time remaining=6.3 hours
664: loss=18.782, avg loss=17.543, last=none, best=none, next=1000, rate=0.00025271, load 64=1.1 seconds, train=4.6 seconds, 42496 images, time remaining=6.3 hours
665: loss=17.233, avg loss=17.512, last=none, best=none, next=1000, rate=0.00025423, load 64=3.0 seconds, train=4.6 seconds, 42560 images, time remaining=6.3 hours
666: loss=17.167, avg loss=17.477, last=none, best=none, next=1000, rate=0.00025576, load 64=882.6 milliseconds, train=4.6 seconds, 42624 images, time remaining=6.3 hours
667: loss=15.636, avg loss=17.293, last=none, best=none, next=1000, rate=0.00025730, load 64=777.1 milliseconds, train=4.6 seconds, 42688 images, time remaining=6.3 hours
668: loss=15.918, avg loss=17.156, last=none, best=none, next=1000, rate=0.00025885, load 64=705.0 milliseconds, train=4.6 seconds, 42752 images, time remaining=6.3 hours
669: loss=15.558, avg loss=16.996, last=none, best=none, next=1000, rate=0.00026040, load 64=879.3 milliseconds, train=4.6 seconds, 42816 images, time remaining=6.3 hours
670: loss=17.627, avg loss=17.059, last=none, best=none, next=1000, rate=0.00026196, load 64=726.8 milliseconds, train=4.6 seconds, 42880 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
671: loss=15.974, avg loss=16.951, last=none, best=none, next=1000, rate=0.00026353, load 64=940.5 milliseconds, train=4.4 seconds, 42944 images, time remaining=6.3 hours
672: loss=15.289, avg loss=16.784, last=none, best=none, next=1000, rate=0.00026511, load 64=946.5 milliseconds, train=4.4 seconds, 43008 images, time remaining=6.3 hours
673: loss=14.158, avg loss=16.522, last=none, best=none, next=1000, rate=0.00026669, load 64=699.8 milliseconds, train=4.4 seconds, 43072 images, time remaining=6.3 hours
674: loss=17.202, avg loss=16.590, last=none, best=none, next=1000, rate=0.00026828, load 64=1.3 seconds, train=4.4 seconds, 43136 images, time remaining=6.3 hours
675: loss=18.183, avg loss=16.749, last=none, best=none, next=1000, rate=0.00026987, load 64=1.3 seconds, train=4.4 seconds, 43200 images, time remaining=6.3 hours
676: loss=17.449, avg loss=16.819, last=none, best=none, next=1000, rate=0.00027148, load 64=719.3 milliseconds, train=4.4 seconds, 43264 images, time remaining=6.3 hours
677: loss=16.929, avg loss=16.830, last=none, best=none, next=1000, rate=0.00027309, load 64=1.1 seconds, train=4.4 seconds, 43328 images, time remaining=6.3 hours
678: loss=14.883, avg loss=16.635, last=none, best=none, next=1000, rate=0.00027470, load 64=854.2 milliseconds, train=4.4 seconds, 43392 images, time remaining=6.3 hours
679: loss=14.116, avg loss=16.383, last=none, best=none, next=1000, rate=0.00027633, load 64=2.0 seconds, train=4.4 seconds, 43456 images, time remaining=6.3 hours
680: loss=15.433, avg loss=16.288, last=none, best=none, next=1000, rate=0.00027796, load 64=1.3 seconds, train=4.4 seconds, 43520 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463c6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
681: loss=16.431, avg loss=16.303, last=none, best=none, next=1000, rate=0.00027960, load 64=2.6 seconds, train=1.5 seconds, 43584 images, time remaining=6.3 hours
682: loss=17.209, avg loss=16.393, last=none, best=none, next=1000, rate=0.00028124, load 64=900.1 milliseconds, train=1.5 seconds, 43648 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
683: loss=16.746, avg loss=16.429, last=none, best=none, next=1000, rate=0.00028290, load 64=2.3 seconds, train=1.5 seconds, 43712 images, time remaining=6.3 hours
684: loss=19.552, avg loss=16.741, last=none, best=none, next=1000, rate=0.00028456, load 64=1.3 seconds, train=1.5 seconds, 43776 images, time remaining=6.3 hours
685: loss=17.151, avg loss=16.782, last=none, best=none, next=1000, rate=0.00028622, load 64=1.1 seconds, train=1.5 seconds, 43840 images, time remaining=6.3 hours
686: loss=16.220, avg loss=16.726, last=none, best=none, next=1000, rate=0.00028790, load 64=1.2 seconds, train=1.5 seconds, 43904 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
687: loss=19.044, avg loss=16.957, last=none, best=none, next=1000, rate=0.00028958, load 64=3.0 seconds, train=1.5 seconds, 43968 images, time remaining=6.3 hours
688: loss=16.918, avg loss=16.954, last=none, best=none, next=1000, rate=0.00029127, load 64=786.7 milliseconds, train=1.5 seconds, 44032 images, time remaining=6.3 hours
689: loss=14.690, avg loss=16.727, last=none, best=none, next=1000, rate=0.00029297, load 64=1.2 seconds, train=1.5 seconds, 44096 images, time remaining=6.3 hours
690: loss=16.499, avg loss=16.704, last=none, best=none, next=1000, rate=0.00029467, load 64=879.9 milliseconds, train=1.5 seconds, 44160 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463c6000000
691: loss=14.545, avg loss=16.488, last=none, best=none, next=1000, rate=0.00029638, load 64=586.5 milliseconds, train=1.5 seconds, 44224 images, time remaining=6.3 hours
692: loss=14.059, avg loss=16.245, last=none, best=none, next=1000, rate=0.00029810, load 64=935.8 milliseconds, train=1.5 seconds, 44288 images, time remaining=6.3 hours
693: loss=15.570, avg loss=16.178, last=none, best=none, next=1000, rate=0.00029983, load 64=905.0 milliseconds, train=1.5 seconds, 44352 images, time remaining=6.3 hours
694: loss=14.634, avg loss=16.024, last=none, best=none, next=1000, rate=0.00030157, load 64=1.3 seconds, train=1.5 seconds, 44416 images, time remaining=6.3 hours
695: loss=16.254, avg loss=16.047, last=none, best=none, next=1000, rate=0.00030331, load 64=1.2 seconds, train=1.5 seconds, 44480 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
696: loss=13.668, avg loss=15.809, last=none, best=none, next=1000, rate=0.00030506, load 64=1.6 seconds, train=1.5 seconds, 44544 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
697: loss=15.838, avg loss=15.812, last=none, best=none, next=1000, rate=0.00030681, load 64=1.6 seconds, train=1.5 seconds, 44608 images, time remaining=6.2 hours
698: loss=14.650, avg loss=15.695, last=none, best=none, next=1000, rate=0.00030858, load 64=748.5 milliseconds, train=1.5 seconds, 44672 images, time remaining=6.2 hours
699: loss=13.398, avg loss=15.466, last=none, best=none, next=1000, rate=0.00031035, load 64=872.5 milliseconds, train=1.5 seconds, 44736 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
700: loss=14.501, avg loss=15.369, last=none, best=none, next=1000, rate=0.00031213, load 64=3.0 seconds, train=1.5 seconds, 44800 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x146400000000
701: loss=13.877, avg loss=15.220, last=none, best=none, next=1000, rate=0.00031392, load 64=978.0 milliseconds, train=1.4 seconds, 44864 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
702: loss=14.451, avg loss=15.143, last=none, best=none, next=1000, rate=0.00031571, load 64=2.0 seconds, train=1.4 seconds, 44928 images, time remaining=6.2 hours
703: loss=14.664, avg loss=15.095, last=none, best=none, next=1000, rate=0.00031752, load 64=646.4 milliseconds, train=1.4 seconds, 44992 images, time remaining=6.2 hours
704: loss=14.974, avg loss=15.083, last=none, best=none, next=1000, rate=0.00031933, load 64=953.9 milliseconds, train=1.4 seconds, 45056 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
705: loss=12.484, avg loss=14.823, last=none, best=none, next=1000, rate=0.00032114, load 64=3.1 seconds, train=1.4 seconds, 45120 images, time remaining=6.2 hours
706: loss=16.181, avg loss=14.959, last=none, best=none, next=1000, rate=0.00032297, load 64=1.1 seconds, train=1.4 seconds, 45184 images, time remaining=6.2 hours
707: loss=15.026, avg loss=14.966, last=none, best=none, next=1000, rate=0.00032480, load 64=917.0 milliseconds, train=1.4 seconds, 45248 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
708: loss=13.771, avg loss=14.846, last=none, best=none, next=1000, rate=0.00032665, load 64=1.6 seconds, train=1.4 seconds, 45312 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
709: loss=13.892, avg loss=14.751, last=none, best=none, next=1000, rate=0.00032849, load 64=2.4 seconds, train=1.4 seconds, 45376 images, time remaining=6.2 hours
710: loss=12.966, avg loss=14.572, last=none, best=none, next=1000, rate=0.00033035, load 64=897.4 milliseconds, train=1.4 seconds, 45440 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
711: loss=20.114, avg loss=15.127, last=none, best=none, next=1000, rate=0.00033222, load 64=1.3 seconds, train=3.8 seconds, 45504 images, time remaining=6.2 hours
712: loss=14.952, avg loss=15.109, last=none, best=none, next=1000, rate=0.00033409, load 64=1.1 seconds, train=3.8 seconds, 45568 images, time remaining=6.2 hours
713: loss=17.046, avg loss=15.303, last=none, best=none, next=1000, rate=0.00033597, load 64=2.0 seconds, train=3.7 seconds, 45632 images, time remaining=6.2 hours
714: loss=17.296, avg loss=15.502, last=none, best=none, next=1000, rate=0.00033786, load 64=775.1 milliseconds, train=3.8 seconds, 45696 images, time remaining=6.2 hours
715: loss=16.173, avg loss=15.569, last=none, best=none, next=1000, rate=0.00033976, load 64=1.8 seconds, train=3.8 seconds, 45760 images, time remaining=6.2 hours
716: loss=17.673, avg loss=15.779, last=none, best=none, next=1000, rate=0.00034166, load 64=820.0 milliseconds, train=3.8 seconds, 45824 images, time remaining=6.2 hours
717: loss=16.676, avg loss=15.869, last=none, best=none, next=1000, rate=0.00034357, load 64=2.0 seconds, train=3.8 seconds, 45888 images, time remaining=6.2 hours
718: loss=17.204, avg loss=16.003, last=none, best=none, next=1000, rate=0.00034549, load 64=2.4 seconds, train=3.8 seconds, 45952 images, time remaining=6.2 hours
719: loss=19.950, avg loss=16.397, last=none, best=none, next=1000, rate=0.00034742, load 64=995.5 milliseconds, train=3.8 seconds, 46016 images, time remaining=6.2 hours
720: loss=17.070, avg loss=16.465, last=none, best=none, next=1000, rate=0.00034936, load 64=1.5 seconds, train=3.8 seconds, 46080 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a5c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
721: loss=16.390, avg loss=16.457, last=none, best=none, next=1000, rate=0.00035131, load 64=2.4 seconds, train=2.0 seconds, 46144 images, time remaining=6.2 hours
722: loss=15.764, avg loss=16.388, last=none, best=none, next=1000, rate=0.00035326, load 64=1.4 seconds, train=2.0 seconds, 46208 images, time remaining=6.2 hours
723: loss=16.333, avg loss=16.382, last=none, best=none, next=1000, rate=0.00035522, load 64=694.7 milliseconds, train=2.0 seconds, 46272 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
724: loss=15.480, avg loss=16.292, last=none, best=none, next=1000, rate=0.00035719, load 64=2.5 seconds, train=1.9 seconds, 46336 images, time remaining=6.2 hours
725: loss=17.927, avg loss=16.456, last=none, best=none, next=1000, rate=0.00035917, load 64=1.1 seconds, train=2.0 seconds, 46400 images, time remaining=6.2 hours
726: loss=12.095, avg loss=16.020, last=none, best=none, next=1000, rate=0.00036115, load 64=1.0 seconds, train=2.0 seconds, 46464 images, time remaining=6.2 hours
727: loss=16.069, avg loss=16.024, last=none, best=none, next=1000, rate=0.00036315, load 64=1.7 seconds, train=2.0 seconds, 46528 images, time remaining=6.2 hours
728: loss=15.236, avg loss=15.946, last=none, best=none, next=1000, rate=0.00036515, load 64=2.0 seconds, train=2.0 seconds, 46592 images, time remaining=6.2 hours
729: loss=14.995, avg loss=15.851, last=none, best=none, next=1000, rate=0.00036716, load 64=1.6 seconds, train=2.0 seconds, 46656 images, time remaining=6.2 hours
730: loss=15.660, avg loss=15.832, last=none, best=none, next=1000, rate=0.00036918, load 64=1.5 seconds, train=2.0 seconds, 46720 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
731: loss=19.676, avg loss=16.216, last=none, best=none, next=1000, rate=0.00037120, load 64=2.6 seconds, train=4.6 seconds, 46784 images, time remaining=6.2 hours
732: loss=17.845, avg loss=16.379, last=none, best=none, next=1000, rate=0.00037324, load 64=673.8 milliseconds, train=4.6 seconds, 46848 images, time remaining=6.2 hours
733: loss=17.174, avg loss=16.458, last=none, best=none, next=1000, rate=0.00037528, load 64=1.1 seconds, train=4.6 seconds, 46912 images, time remaining=6.2 hours
734: loss=17.462, avg loss=16.559, last=none, best=none, next=1000, rate=0.00037734, load 64=1.0 seconds, train=4.6 seconds, 46976 images, time remaining=6.2 hours
735: loss=18.056, avg loss=16.708, last=none, best=none, next=1000, rate=0.00037940, load 64=978.7 milliseconds, train=4.6 seconds, 47040 images, time remaining=6.2 hours
736: loss=19.545, avg loss=16.992, last=none, best=none, next=1000, rate=0.00038146, load 64=1.9 seconds, train=4.6 seconds, 47104 images, time remaining=6.2 hours
737: loss=17.607, avg loss=17.054, last=none, best=none, next=1000, rate=0.00038354, load 64=2.2 seconds, train=4.6 seconds, 47168 images, time remaining=6.2 hours
738: loss=14.861, avg loss=16.834, last=none, best=none, next=1000, rate=0.00038563, load 64=2.1 seconds, train=4.6 seconds, 47232 images, time remaining=6.2 hours
739: loss=15.873, avg loss=16.738, last=none, best=none, next=1000, rate=0.00038772, load 64=1.8 seconds, train=4.6 seconds, 47296 images, time remaining=6.2 hours
740: loss=17.708, avg loss=16.835, last=none, best=none, next=1000, rate=0.00038983, load 64=2.9 seconds, train=4.6 seconds, 47360 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x14474a000000
741: loss=19.163, avg loss=17.068, last=none, best=none, next=1000, rate=0.00039194, load 64=1.1 seconds, train=4.2 seconds, 47424 images, time remaining=6.2 hours
742: loss=14.006, avg loss=16.762, last=none, best=none, next=1000, rate=0.00039406, load 64=968.1 milliseconds, train=4.1 seconds, 47488 images, time remaining=6.2 hours
743: loss=15.905, avg loss=16.676, last=none, best=none, next=1000, rate=0.00039619, load 64=938.3 milliseconds, train=4.1 seconds, 47552 images, time remaining=6.2 hours
744: loss=14.739, avg loss=16.482, last=none, best=none, next=1000, rate=0.00039832, load 64=1.1 seconds, train=4.1 seconds, 47616 images, time remaining=6.2 hours
745: loss=17.110, avg loss=16.545, last=none, best=none, next=1000, rate=0.00040047, load 64=818.2 milliseconds, train=4.1 seconds, 47680 images, time remaining=6.2 hours
746: loss=14.108, avg loss=16.301, last=none, best=none, next=1000, rate=0.00040262, load 64=861.6 milliseconds, train=4.1 seconds, 47744 images, time remaining=6.2 hours
747: loss=15.869, avg loss=16.258, last=none, best=none, next=1000, rate=0.00040479, load 64=971.1 milliseconds, train=4.1 seconds, 47808 images, time remaining=6.2 hours
748: loss=13.821, avg loss=16.014, last=none, best=none, next=1000, rate=0.00040696, load 64=1.1 seconds, train=4.1 seconds, 47872 images, time remaining=6.2 hours
749: loss=15.604, avg loss=15.973, last=none, best=none, next=1000, rate=0.00040914, load 64=1.1 seconds, train=4.1 seconds, 47936 images, time remaining=6.2 hours
750: loss=13.630, avg loss=15.739, last=none, best=none, next=1000, rate=0.00041133, load 64=919.8 milliseconds, train=4.1 seconds, 48000 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a5c000000
751: loss=13.900, avg loss=15.555, last=none, best=none, next=1000, rate=0.00041353, load 64=835.0 milliseconds, train=2.1 seconds, 48064 images, time remaining=6.2 hours
752: loss=15.630, avg loss=15.563, last=none, best=none, next=1000, rate=0.00041573, load 64=699.5 milliseconds, train=2.1 seconds, 48128 images, time remaining=6.2 hours
753: loss=13.925, avg loss=15.399, last=none, best=none, next=1000, rate=0.00041795, load 64=806.9 milliseconds, train=2.1 seconds, 48192 images, time remaining=6.2 hours
754: loss=15.540, avg loss=15.413, last=none, best=none, next=1000, rate=0.00042017, load 64=760.1 milliseconds, train=2.1 seconds, 48256 images, time remaining=6.2 hours
755: loss=13.653, avg loss=15.237, last=none, best=none, next=1000, rate=0.00042241, load 64=853.7 milliseconds, train=2.1 seconds, 48320 images, time remaining=6.2 hours
756: loss=12.890, avg loss=15.002, last=none, best=none, next=1000, rate=0.00042465, load 64=678.5 milliseconds, train=2.1 seconds, 48384 images, time remaining=6.2 hours
757: loss=16.216, avg loss=15.124, last=none, best=none, next=1000, rate=0.00042690, load 64=859.4 milliseconds, train=2.1 seconds, 48448 images, time remaining=6.2 hours
758: loss=13.244, avg loss=14.936, last=none, best=none, next=1000, rate=0.00042916, load 64=878.0 milliseconds, train=2.1 seconds, 48512 images, time remaining=6.2 hours
759: loss=13.244, avg loss=14.766, last=none, best=none, next=1000, rate=0.00043143, load 64=1.1 seconds, train=2.1 seconds, 48576 images, time remaining=6.2 hours
760: loss=13.317, avg loss=14.622, last=none, best=none, next=1000, rate=0.00043371, load 64=1.4 seconds, train=2.1 seconds, 48640 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146478000000
761: loss=14.696, avg loss=14.629, last=none, best=none, next=1000, rate=0.00043600, load 64=1.1 seconds, train=1.2 seconds, 48704 images, time remaining=6.2 hours
762: loss=15.361, avg loss=14.702, last=none, best=none, next=1000, rate=0.00043829, load 64=1.0 seconds, train=1.2 seconds, 48768 images, time remaining=6.2 hours
763: loss=14.345, avg loss=14.666, last=none, best=none, next=1000, rate=0.00044060, load 64=703.2 milliseconds, train=1.2 seconds, 48832 images, time remaining=6.2 hours
764: loss=13.028, avg loss=14.503, last=none, best=none, next=1000, rate=0.00044291, load 64=833.9 milliseconds, train=1.2 seconds, 48896 images, time remaining=6.2 hours
765: loss=15.072, avg loss=14.560, last=none, best=none, next=1000, rate=0.00044523, load 64=1.0 seconds, train=1.2 seconds, 48960 images, time remaining=6.2 hours
766: loss=13.765, avg loss=14.480, last=none, best=none, next=1000, rate=0.00044757, load 64=922.5 milliseconds, train=1.2 seconds, 49024 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
767: loss=12.742, avg loss=14.306, last=none, best=none, next=1000, rate=0.00044991, load 64=2.3 seconds, train=1.2 seconds, 49088 images, time remaining=6.2 hours
768: loss=13.223, avg loss=14.198, last=none, best=none, next=1000, rate=0.00045226, load 64=856.0 milliseconds, train=1.2 seconds, 49152 images, time remaining=6.2 hours
769: loss=13.673, avg loss=14.145, last=none, best=none, next=1000, rate=0.00045462, load 64=957.7 milliseconds, train=1.2 seconds, 49216 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
770: loss=12.973, avg loss=14.028, last=none, best=none, next=1000, rate=0.00045699, load 64=1.7 seconds, train=1.2 seconds, 49280 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1464b0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
771: loss=14.102, avg loss=14.036, last=none, best=none, next=1000, rate=0.00045937, load 64=1.5 seconds, train=1.4 seconds, 49344 images, time remaining=6.2 hours
772: loss=12.071, avg loss=13.839, last=none, best=none, next=1000, rate=0.00046176, load 64=852.7 milliseconds, train=1.4 seconds, 49408 images, time remaining=6.2 hours
773: loss=11.795, avg loss=13.635, last=none, best=none, next=1000, rate=0.00046415, load 64=743.8 milliseconds, train=1.4 seconds, 49472 images, time remaining=6.2 hours
774: loss=11.830, avg loss=13.454, last=none, best=none, next=1000, rate=0.00046656, load 64=842.2 milliseconds, train=1.4 seconds, 49536 images, time remaining=6.1 hours
775: loss=13.684, avg loss=13.477, last=none, best=none, next=1000, rate=0.00046898, load 64=958.1 milliseconds, train=1.4 seconds, 49600 images, time remaining=6.1 hours
776: loss=12.272, avg loss=13.357, last=none, best=none, next=1000, rate=0.00047140, load 64=737.8 milliseconds, train=1.4 seconds, 49664 images, time remaining=6.1 hours
777: loss=12.521, avg loss=13.273, last=none, best=none, next=1000, rate=0.00047384, load 64=1.3 seconds, train=1.4 seconds, 49728 images, time remaining=6.1 hours
778: loss=12.825, avg loss=13.228, last=none, best=none, next=1000, rate=0.00047628, load 64=929.0 milliseconds, train=1.4 seconds, 49792 images, time remaining=6.1 hours
779: loss=11.913, avg loss=13.097, last=none, best=none, next=1000, rate=0.00047873, load 64=540.4 milliseconds, train=1.4 seconds, 49856 images, time remaining=6.1 hours
780: loss=11.600, avg loss=12.947, last=none, best=none, next=1000, rate=0.00048120, load 64=733.0 milliseconds, train=1.4 seconds, 49920 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
781: loss=13.490, avg loss=13.001, last=none, best=none, next=1000, rate=0.00048367, load 64=1.2 seconds, train=2.1 seconds, 49984 images, time remaining=6.1 hours
782: loss=13.405, avg loss=13.042, last=none, best=none, next=1000, rate=0.00048615, load 64=583.8 milliseconds, train=2.1 seconds, 50048 images, time remaining=6.1 hours
783: loss=12.507, avg loss=12.988, last=none, best=none, next=1000, rate=0.00048864, load 64=509.3 milliseconds, train=2.1 seconds, 50112 images, time remaining=6.1 hours
784: loss=16.761, avg loss=13.366, last=none, best=none, next=1000, rate=0.00049114, load 64=770.9 milliseconds, train=2.1 seconds, 50176 images, time remaining=6.1 hours
785: loss=12.134, avg loss=13.242, last=none, best=none, next=1000, rate=0.00049365, load 64=656.4 milliseconds, train=2.1 seconds, 50240 images, time remaining=6.1 hours
786: loss=14.777, avg loss=13.396, last=none, best=none, next=1000, rate=0.00049617, load 64=749.4 milliseconds, train=2.1 seconds, 50304 images, time remaining=6.1 hours
787: loss=12.231, avg loss=13.279, last=none, best=none, next=1000, rate=0.00049870, load 64=760.2 milliseconds, train=2.1 seconds, 50368 images, time remaining=6.1 hours
788: loss=14.689, avg loss=13.420, last=none, best=none, next=1000, rate=0.00050124, load 64=1.3 seconds, train=2.1 seconds, 50432 images, time remaining=6.1 hours
789: loss=11.920, avg loss=13.270, last=none, best=none, next=1000, rate=0.00050379, load 64=1.0 seconds, train=2.1 seconds, 50496 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
790: loss=12.618, avg loss=13.205, last=none, best=none, next=1000, rate=0.00050635, load 64=2.2 seconds, train=2.1 seconds, 50560 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
791: loss=15.754, avg loss=13.460, last=none, best=none, next=1000, rate=0.00050892, load 64=1.1 seconds, train=4.4 seconds, 50624 images, time remaining=6.1 hours
792: loss=14.491, avg loss=13.563, last=none, best=none, next=1000, rate=0.00051150, load 64=2.5 seconds, train=4.4 seconds, 50688 images, time remaining=6.1 hours
793: loss=14.044, avg loss=13.611, last=none, best=none, next=1000, rate=0.00051409, load 64=1.8 seconds, train=4.4 seconds, 50752 images, time remaining=6.1 hours
794: loss=16.319, avg loss=13.882, last=none, best=none, next=1000, rate=0.00051668, load 64=715.0 milliseconds, train=4.4 seconds, 50816 images, time remaining=6.1 hours
795: loss=15.698, avg loss=14.064, last=none, best=none, next=1000, rate=0.00051929, load 64=730.7 milliseconds, train=4.4 seconds, 50880 images, time remaining=6.1 hours
796: loss=16.900, avg loss=14.347, last=none, best=none, next=1000, rate=0.00052191, load 64=2.6 seconds, train=4.4 seconds, 50944 images, time remaining=6.1 hours
797: loss=14.383, avg loss=14.351, last=none, best=none, next=1000, rate=0.00052454, load 64=1.4 seconds, train=4.4 seconds, 51008 images, time remaining=6.1 hours
798: loss=15.475, avg loss=14.463, last=none, best=none, next=1000, rate=0.00052718, load 64=785.1 milliseconds, train=4.4 seconds, 51072 images, time remaining=6.1 hours
799: loss=15.608, avg loss=14.578, last=none, best=none, next=1000, rate=0.00052982, load 64=1.9 seconds, train=4.4 seconds, 51136 images, time remaining=6.1 hours
800: loss=14.853, avg loss=14.605, last=none, best=none, next=1000, rate=0.00053248, load 64=3.3 seconds, train=4.4 seconds, 51200 images, time remaining=6.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
801: loss=15.086, avg loss=14.653, last=none, best=none, next=1000, rate=0.00053515, load 64=997.5 milliseconds, train=3.3 seconds, 51264 images, time remaining=6.1 hours
802: loss=13.723, avg loss=14.560, last=none, best=none, next=1000, rate=0.00053782, load 64=1.4 seconds, train=3.3 seconds, 51328 images, time remaining=6.1 hours
803: loss=14.816, avg loss=14.586, last=none, best=none, next=1000, rate=0.00054051, load 64=1.0 seconds, train=3.3 seconds, 51392 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
804: loss=14.302, avg loss=14.557, last=none, best=none, next=1000, rate=0.00054321, load 64=4.2 seconds, train=3.3 seconds, 51456 images, time remaining=6.1 hours
805: loss=13.561, avg loss=14.458, last=none, best=none, next=1000, rate=0.00054592, load 64=1.0 seconds, train=3.3 seconds, 51520 images, time remaining=6.1 hours
806: loss=15.071, avg loss=14.519, last=none, best=none, next=1000, rate=0.00054864, load 64=2.4 seconds, train=3.3 seconds, 51584 images, time remaining=6.1 hours
807: loss=13.983, avg loss=14.466, last=none, best=none, next=1000, rate=0.00055136, load 64=1.3 seconds, train=3.3 seconds, 51648 images, time remaining=6.1 hours
808: loss=13.328, avg loss=14.352, last=none, best=none, next=1000, rate=0.00055410, load 64=891.5 milliseconds, train=3.3 seconds, 51712 images, time remaining=6.1 hours
809: loss=12.767, avg loss=14.193, last=none, best=none, next=1000, rate=0.00055685, load 64=914.5 milliseconds, train=3.3 seconds, 51776 images, time remaining=6.1 hours
810: loss=15.854, avg loss=14.359, last=none, best=none, next=1000, rate=0.00055961, load 64=794.6 milliseconds, train=3.3 seconds, 51840 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145bb2000000
811: loss=14.810, avg loss=14.404, last=none, best=none, next=1000, rate=0.00056238, load 64=653.8 milliseconds, train=1.4 seconds, 51904 images, time remaining=6.1 hours
812: loss=12.227, avg loss=14.187, last=none, best=none, next=1000, rate=0.00056515, load 64=701.0 milliseconds, train=1.4 seconds, 51968 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
813: loss=14.113, avg loss=14.179, last=none, best=none, next=1000, rate=0.00056794, load 64=2.4 seconds, train=1.4 seconds, 52032 images, time remaining=6.1 hours
814: loss=15.418, avg loss=14.303, last=none, best=none, next=1000, rate=0.00057074, load 64=668.6 milliseconds, train=1.4 seconds, 52096 images, time remaining=6.1 hours
815: loss=11.914, avg loss=14.064, last=none, best=none, next=1000, rate=0.00057355, load 64=825.3 milliseconds, train=1.4 seconds, 52160 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
816: loss=14.279, avg loss=14.086, last=none, best=none, next=1000, rate=0.00057637, load 64=2.1 seconds, train=1.4 seconds, 52224 images, time remaining=6.1 hours
817: loss=13.746, avg loss=14.052, last=none, best=none, next=1000, rate=0.00057920, load 64=931.1 milliseconds, train=1.4 seconds, 52288 images, time remaining=6.1 hours
818: loss=14.745, avg loss=14.121, last=none, best=none, next=1000, rate=0.00058205, load 64=884.5 milliseconds, train=1.4 seconds, 52352 images, time remaining=6.1 hours
819: loss=14.149, avg loss=14.124, last=none, best=none, next=1000, rate=0.00058490, load 64=518.2 milliseconds, train=1.4 seconds, 52416 images, time remaining=6.1 hours
820: loss=13.774, avg loss=14.089, last=none, best=none, next=1000, rate=0.00058776, load 64=570.3 milliseconds, train=1.4 seconds, 52480 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
821: loss=23.428, avg loss=15.023, last=none, best=none, next=1000, rate=0.00059063, load 64=606.0 milliseconds, train=4.7 seconds, 52544 images, time remaining=6.1 hours
822: loss=15.196, avg loss=15.040, last=none, best=none, next=1000, rate=0.00059351, load 64=913.4 milliseconds, train=4.6 seconds, 52608 images, time remaining=6.1 hours
823: loss=17.703, avg loss=15.306, last=none, best=none, next=1000, rate=0.00059641, load 64=817.0 milliseconds, train=4.6 seconds, 52672 images, time remaining=6.1 hours
824: loss=18.154, avg loss=15.591, last=none, best=none, next=1000, rate=0.00059931, load 64=735.3 milliseconds, train=4.7 seconds, 52736 images, time remaining=6.1 hours
825: loss=15.831, avg loss=15.615, last=none, best=none, next=1000, rate=0.00060223, load 64=1.3 seconds, train=4.6 seconds, 52800 images, time remaining=6.1 hours
826: loss=19.254, avg loss=15.979, last=none, best=none, next=1000, rate=0.00060515, load 64=1.8 seconds, train=4.6 seconds, 52864 images, time remaining=6.1 hours
827: loss=16.418, avg loss=16.023, last=none, best=none, next=1000, rate=0.00060809, load 64=1.5 seconds, train=4.6 seconds, 52928 images, time remaining=6.1 hours
828: loss=16.474, avg loss=16.068, last=none, best=none, next=1000, rate=0.00061103, load 64=3.1 seconds, train=4.6 seconds, 52992 images, time remaining=6.1 hours
829: loss=16.855, avg loss=16.147, last=none, best=none, next=1000, rate=0.00061399, load 64=1.2 seconds, train=4.6 seconds, 53056 images, time remaining=6.1 hours
830: loss=15.288, avg loss=16.061, last=none, best=none, next=1000, rate=0.00061696, load 64=844.2 milliseconds, train=4.6 seconds, 53120 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
831: loss=17.375, avg loss=16.192, last=none, best=none, next=1000, rate=0.00061994, load 64=2.2 seconds, train=5.0 seconds, 53184 images, time remaining=6.1 hours
832: loss=16.989, avg loss=16.272, last=none, best=none, next=1000, rate=0.00062293, load 64=1.2 seconds, train=4.9 seconds, 53248 images, time remaining=6.1 hours
833: loss=16.951, avg loss=16.340, last=none, best=none, next=1000, rate=0.00062593, load 64=1.3 seconds, train=4.9 seconds, 53312 images, time remaining=6.1 hours
834: loss=16.032, avg loss=16.309, last=none, best=none, next=1000, rate=0.00062894, load 64=606.3 milliseconds, train=4.9 seconds, 53376 images, time remaining=6.1 hours
835: loss=15.172, avg loss=16.195, last=none, best=none, next=1000, rate=0.00063196, load 64=720.2 milliseconds, train=4.9 seconds, 53440 images, time remaining=6.1 hours
836: loss=18.222, avg loss=16.398, last=none, best=none, next=1000, rate=0.00063499, load 64=1.2 seconds, train=4.9 seconds, 53504 images, time remaining=6.2 hours
837: loss=15.774, avg loss=16.336, last=none, best=none, next=1000, rate=0.00063804, load 64=882.1 milliseconds, train=4.9 seconds, 53568 images, time remaining=6.2 hours
838: loss=16.593, avg loss=16.361, last=none, best=none, next=1000, rate=0.00064109, load 64=1.0 seconds, train=4.9 seconds, 53632 images, time remaining=6.2 hours
839: loss=15.100, avg loss=16.235, last=none, best=none, next=1000, rate=0.00064416, load 64=2.1 seconds, train=4.9 seconds, 53696 images, time remaining=6.2 hours
840: loss=16.010, avg loss=16.213, last=none, best=none, next=1000, rate=0.00064723, load 64=3.3 seconds, train=4.9 seconds, 53760 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
841: loss=14.390, avg loss=16.030, last=none, best=none, next=1000, rate=0.00065032, load 64=942.7 milliseconds, train=4.5 seconds, 53824 images, time remaining=6.2 hours
842: loss=14.121, avg loss=15.840, last=none, best=none, next=1000, rate=0.00065342, load 64=898.9 milliseconds, train=4.5 seconds, 53888 images, time remaining=6.2 hours
843: loss=16.672, avg loss=15.923, last=none, best=none, next=1000, rate=0.00065653, load 64=884.4 milliseconds, train=4.5 seconds, 53952 images, time remaining=6.2 hours
844: loss=15.459, avg loss=15.876, last=none, best=none, next=1000, rate=0.00065965, load 64=862.4 milliseconds, train=4.5 seconds, 54016 images, time remaining=6.2 hours
845: loss=16.695, avg loss=15.958, last=none, best=none, next=1000, rate=0.00066278, load 64=720.5 milliseconds, train=4.5 seconds, 54080 images, time remaining=6.2 hours
846: loss=15.125, avg loss=15.875, last=none, best=none, next=1000, rate=0.00066592, load 64=1.4 seconds, train=4.5 seconds, 54144 images, time remaining=6.2 hours
847: loss=14.229, avg loss=15.710, last=none, best=none, next=1000, rate=0.00066908, load 64=700.6 milliseconds, train=4.5 seconds, 54208 images, time remaining=6.2 hours
848: loss=14.107, avg loss=15.550, last=none, best=none, next=1000, rate=0.00067224, load 64=1.4 seconds, train=4.5 seconds, 54272 images, time remaining=6.2 hours
849: loss=13.962, avg loss=15.391, last=none, best=none, next=1000, rate=0.00067542, load 64=611.9 milliseconds, train=4.5 seconds, 54336 images, time remaining=6.2 hours
850: loss=15.686, avg loss=15.421, last=none, best=none, next=1000, rate=0.00067861, load 64=1.0 seconds, train=4.5 seconds, 54400 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145d25e00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
851: loss=17.710, avg loss=15.650, last=none, best=none, next=1000, rate=0.00068181, load 64=2.8 seconds, train=1.4 seconds, 54464 images, time remaining=6.2 hours
852: loss=14.432, avg loss=15.528, last=none, best=none, next=1000, rate=0.00068502, load 64=750.7 milliseconds, train=1.4 seconds, 54528 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
853: loss=16.681, avg loss=15.643, last=none, best=none, next=1000, rate=0.00068824, load 64=2.0 seconds, train=1.4 seconds, 54592 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
854: loss=16.756, avg loss=15.754, last=none, best=none, next=1000, rate=0.00069147, load 64=1.8 seconds, train=1.4 seconds, 54656 images, time remaining=6.2 hours
855: loss=14.634, avg loss=15.642, last=none, best=none, next=1000, rate=0.00069472, load 64=817.9 milliseconds, train=1.4 seconds, 54720 images, time remaining=6.2 hours
856: loss=13.701, avg loss=15.448, last=none, best=none, next=1000, rate=0.00069797, load 64=800.5 milliseconds, train=1.4 seconds, 54784 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
857: loss=14.702, avg loss=15.374, last=none, best=none, next=1000, rate=0.00070124, load 64=4.4 seconds, train=1.4 seconds, 54848 images, time remaining=6.2 hours
858: loss=15.238, avg loss=15.360, last=none, best=none, next=1000, rate=0.00070452, load 64=1.3 seconds, train=1.4 seconds, 54912 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
859: loss=13.438, avg loss=15.168, last=none, best=none, next=1000, rate=0.00070781, load 64=3.7 seconds, train=1.4 seconds, 54976 images, time remaining=6.2 hours
860: loss=13.919, avg loss=15.043, last=none, best=none, next=1000, rate=0.00071111, load 64=866.4 milliseconds, train=1.4 seconds, 55040 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
861: loss=19.002, avg loss=15.439, last=none, best=none, next=1000, rate=0.00071442, load 64=2.4 seconds, train=3.9 seconds, 55104 images, time remaining=6.2 hours
862: loss=16.148, avg loss=15.510, last=none, best=none, next=1000, rate=0.00071775, load 64=952.2 milliseconds, train=3.9 seconds, 55168 images, time remaining=6.2 hours
863: loss=17.279, avg loss=15.687, last=none, best=none, next=1000, rate=0.00072109, load 64=732.7 milliseconds, train=3.9 seconds, 55232 images, time remaining=6.2 hours
864: loss=16.108, avg loss=15.729, last=none, best=none, next=1000, rate=0.00072443, load 64=924.6 milliseconds, train=3.9 seconds, 55296 images, time remaining=6.2 hours
865: loss=16.714, avg loss=15.827, last=none, best=none, next=1000, rate=0.00072779, load 64=723.8 milliseconds, train=3.9 seconds, 55360 images, time remaining=6.2 hours
866: loss=16.401, avg loss=15.885, last=none, best=none, next=1000, rate=0.00073116, load 64=2.6 seconds, train=3.9 seconds, 55424 images, time remaining=6.2 hours
867: loss=15.619, avg loss=15.858, last=none, best=none, next=1000, rate=0.00073455, load 64=965.7 milliseconds, train=3.9 seconds, 55488 images, time remaining=6.2 hours
868: loss=15.001, avg loss=15.773, last=none, best=none, next=1000, rate=0.00073794, load 64=1.0 seconds, train=3.9 seconds, 55552 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
869: loss=17.062, avg loss=15.902, last=none, best=none, next=1000, rate=0.00074135, load 64=4.0 seconds, train=3.9 seconds, 55616 images, time remaining=6.2 hours
870: loss=17.152, avg loss=16.027, last=none, best=none, next=1000, rate=0.00074477, load 64=1.2 seconds, train=3.9 seconds, 55680 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
871: loss=17.309, avg loss=16.155, last=none, best=none, next=1000, rate=0.00074820, load 64=3.0 seconds, train=3.6 seconds, 55744 images, time remaining=6.2 hours
872: loss=15.304, avg loss=16.070, last=none, best=none, next=1000, rate=0.00075164, load 64=959.6 milliseconds, train=3.5 seconds, 55808 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
873: loss=14.619, avg loss=15.925, last=none, best=none, next=1000, rate=0.00075509, load 64=3.8 seconds, train=3.5 seconds, 55872 images, time remaining=6.2 hours
874: loss=14.476, avg loss=15.780, last=none, best=none, next=1000, rate=0.00075856, load 64=1.0 seconds, train=3.5 seconds, 55936 images, time remaining=6.2 hours
875: loss=13.935, avg loss=15.595, last=none, best=none, next=1000, rate=0.00076204, load 64=2.3 seconds, train=3.5 seconds, 56000 images, time remaining=6.2 hours
876: loss=16.422, avg loss=15.678, last=none, best=none, next=1000, rate=0.00076553, load 64=616.7 milliseconds, train=3.5 seconds, 56064 images, time remaining=6.2 hours
877: loss=13.746, avg loss=15.485, last=none, best=none, next=1000, rate=0.00076903, load 64=911.1 milliseconds, train=3.5 seconds, 56128 images, time remaining=6.2 hours
878: loss=14.092, avg loss=15.346, last=none, best=none, next=1000, rate=0.00077254, load 64=2.8 seconds, train=3.5 seconds, 56192 images, time remaining=6.2 hours
879: loss=13.202, avg loss=15.131, last=none, best=none, next=1000, rate=0.00077607, load 64=767.8 milliseconds, train=3.5 seconds, 56256 images, time remaining=6.2 hours
880: loss=14.566, avg loss=15.075, last=none, best=none, next=1000, rate=0.00077960, load 64=2.4 seconds, train=3.5 seconds, 56320 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
881: loss=13.402, avg loss=14.907, last=none, best=none, next=1000, rate=0.00078315, load 64=802.3 milliseconds, train=3.4 seconds, 56384 images, time remaining=6.2 hours
882: loss=14.416, avg loss=14.858, last=none, best=none, next=1000, rate=0.00078672, load 64=627.4 milliseconds, train=3.4 seconds, 56448 images, time remaining=6.2 hours
883: loss=13.875, avg loss=14.760, last=none, best=none, next=1000, rate=0.00079029, load 64=2.7 seconds, train=3.4 seconds, 56512 images, time remaining=6.2 hours
884: loss=14.148, avg loss=14.699, last=none, best=none, next=1000, rate=0.00079388, load 64=591.9 milliseconds, train=3.4 seconds, 56576 images, time remaining=6.2 hours
885: loss=13.039, avg loss=14.533, last=none, best=none, next=1000, rate=0.00079747, load 64=949.8 milliseconds, train=3.4 seconds, 56640 images, time remaining=6.2 hours
886: loss=13.002, avg loss=14.380, last=none, best=none, next=1000, rate=0.00080108, load 64=2.3 seconds, train=3.4 seconds, 56704 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
887: loss=12.620, avg loss=14.204, last=none, best=none, next=1000, rate=0.00080471, load 64=3.8 seconds, train=3.4 seconds, 56768 images, time remaining=6.2 hours
888: loss=15.733, avg loss=14.357, last=none, best=none, next=1000, rate=0.00080834, load 64=1.2 seconds, train=3.4 seconds, 56832 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
889: loss=13.638, avg loss=14.285, last=none, best=none, next=1000, rate=0.00081199, load 64=3.4 seconds, train=3.4 seconds, 56896 images, time remaining=6.2 hours
890: loss=13.262, avg loss=14.183, last=none, best=none, next=1000, rate=0.00081565, load 64=747.5 milliseconds, train=3.4 seconds, 56960 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446ce000000
891: loss=13.519, avg loss=14.116, last=none, best=none, next=1000, rate=0.00081932, load 64=2.4 seconds, train=4.6 seconds, 57024 images, time remaining=6.2 hours
892: loss=14.868, avg loss=14.191, last=none, best=none, next=1000, rate=0.00082301, load 64=1.2 seconds, train=4.7 seconds, 57088 images, time remaining=6.2 hours
893: loss=14.958, avg loss=14.268, last=none, best=none, next=1000, rate=0.00082670, load 64=2.5 seconds, train=4.6 seconds, 57152 images, time remaining=6.2 hours
894: loss=13.578, avg loss=14.199, last=none, best=none, next=1000, rate=0.00083041, load 64=2.4 seconds, train=4.6 seconds, 57216 images, time remaining=6.2 hours
895: loss=13.720, avg loss=14.151, last=none, best=none, next=1000, rate=0.00083413, load 64=2.1 seconds, train=4.6 seconds, 57280 images, time remaining=6.2 hours
896: loss=12.943, avg loss=14.030, last=none, best=none, next=1000, rate=0.00083787, load 64=2.9 seconds, train=4.6 seconds, 57344 images, time remaining=6.2 hours
897: loss=13.701, avg loss=13.997, last=none, best=none, next=1000, rate=0.00084161, load 64=2.9 seconds, train=4.6 seconds, 57408 images, time remaining=6.2 hours
898: loss=13.901, avg loss=13.988, last=none, best=none, next=1000, rate=0.00084537, load 64=3.1 seconds, train=4.6 seconds, 57472 images, time remaining=6.2 hours
899: loss=12.737, avg loss=13.863, last=none, best=none, next=1000, rate=0.00084915, load 64=2.9 seconds, train=4.6 seconds, 57536 images, time remaining=6.2 hours
900: loss=13.441, avg loss=13.820, last=none, best=none, next=1000, rate=0.00085293, load 64=3.1 seconds, train=4.6 seconds, 57600 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x1449e0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
901: loss=14.294, avg loss=13.868, last=none, best=none, next=1000, rate=0.00085673, load 64=3.0 seconds, train=2.0 seconds, 57664 images, time remaining=6.2 hours
902: loss=12.475, avg loss=13.728, last=none, best=none, next=1000, rate=0.00086054, load 64=774.6 milliseconds, train=2.0 seconds, 57728 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
903: loss=15.828, avg loss=13.938, last=none, best=none, next=1000, rate=0.00086436, load 64=3.2 seconds, train=2.0 seconds, 57792 images, time remaining=6.2 hours
904: loss=14.988, avg loss=14.043, last=none, best=none, next=1000, rate=0.00086819, load 64=1.3 seconds, train=2.0 seconds, 57856 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
905: loss=15.063, avg loss=14.145, last=none, best=none, next=1000, rate=0.00087204, load 64=2.4 seconds, train=2.0 seconds, 57920 images, time remaining=6.2 hours
906: loss=10.723, avg loss=13.803, last=none, best=none, next=1000, rate=0.00087590, load 64=919.8 milliseconds, train=2.0 seconds, 57984 images, time remaining=6.2 hours
907: loss=14.296, avg loss=13.852, last=none, best=none, next=1000, rate=0.00087978, load 64=512.5 milliseconds, train=2.0 seconds, 58048 images, time remaining=6.2 hours
908: loss=12.988, avg loss=13.766, last=none, best=none, next=1000, rate=0.00088366, load 64=1.2 seconds, train=2.0 seconds, 58112 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
909: loss=12.927, avg loss=13.682, last=none, best=none, next=1000, rate=0.00088756, load 64=2.8 seconds, train=2.0 seconds, 58176 images, time remaining=6.2 hours
910: loss=13.201, avg loss=13.634, last=none, best=none, next=1000, rate=0.00089147, load 64=550.2 milliseconds, train=2.0 seconds, 58240 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1464c4400000
911: loss=10.993, avg loss=13.370, last=none, best=none, next=1000, rate=0.00089540, load 64=957.1 milliseconds, train=1.2 seconds, 58304 images, time remaining=6.2 hours
912: loss=11.628, avg loss=13.196, last=none, best=none, next=1000, rate=0.00089934, load 64=490.6 milliseconds, train=1.2 seconds, 58368 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
913: loss=10.064, avg loss=12.882, last=none, best=none, next=1000, rate=0.00090329, load 64=2.7 seconds, train=1.2 seconds, 58432 images, time remaining=6.2 hours
914: loss=11.297, avg loss=12.724, last=none, best=none, next=1000, rate=0.00090725, load 64=696.5 milliseconds, train=1.2 seconds, 58496 images, time remaining=6.2 hours
915: loss=11.727, avg loss=12.624, last=none, best=none, next=1000, rate=0.00091123, load 64=1.0 seconds, train=1.2 seconds, 58560 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
916: loss=13.144, avg loss=12.676, last=none, best=none, next=1000, rate=0.00091522, load 64=1.2 seconds, train=1.2 seconds, 58624 images, time remaining=6.2 hours
917: loss=11.646, avg loss=12.573, last=none, best=none, next=1000, rate=0.00091922, load 64=584.2 milliseconds, train=1.2 seconds, 58688 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
918: loss=11.441, avg loss=12.460, last=none, best=none, next=1000, rate=0.00092324, load 64=2.1 seconds, train=1.2 seconds, 58752 images, time remaining=6.2 hours
919: loss=12.641, avg loss=12.478, last=none, best=none, next=1000, rate=0.00092727, load 64=741.8 milliseconds, train=1.2 seconds, 58816 images, time remaining=6.2 hours
920: loss=10.670, avg loss=12.297, last=none, best=none, next=1000, rate=0.00093131, load 64=570.4 milliseconds, train=1.2 seconds, 58880 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1464c8000000
921: loss=9.530, avg loss=12.021, last=none, best=none, next=1000, rate=0.00093537, load 64=1.3 seconds, train=1.4 seconds, 58944 images, time remaining=6.2 hours
922: loss=12.219, avg loss=12.040, last=none, best=none, next=1000, rate=0.00093944, load 64=641.3 milliseconds, train=1.4 seconds, 59008 images, time remaining=6.2 hours
923: loss=11.886, avg loss=12.025, last=none, best=none, next=1000, rate=0.00094352, load 64=1.2 seconds, train=1.4 seconds, 59072 images, time remaining=6.1 hours
924: loss=10.492, avg loss=11.872, last=none, best=none, next=1000, rate=0.00094761, load 64=1.1 seconds, train=1.4 seconds, 59136 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
925: loss=10.630, avg loss=11.747, last=none, best=none, next=1000, rate=0.00095172, load 64=2.9 seconds, train=1.4 seconds, 59200 images, time remaining=6.1 hours
926: loss=10.027, avg loss=11.575, last=none, best=none, next=1000, rate=0.00095584, load 64=944.3 milliseconds, train=1.4 seconds, 59264 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
927: loss=12.429, avg loss=11.661, last=none, best=none, next=1000, rate=0.00095998, load 64=3.1 seconds, train=1.4 seconds, 59328 images, time remaining=6.1 hours
928: loss=10.473, avg loss=11.542, last=none, best=none, next=1000, rate=0.00096413, load 64=807.1 milliseconds, train=1.4 seconds, 59392 images, time remaining=6.1 hours
929: loss=12.236, avg loss=11.611, last=none, best=none, next=1000, rate=0.00096829, load 64=794.5 milliseconds, train=1.4 seconds, 59456 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
930: loss=10.184, avg loss=11.469, last=none, best=none, next=1000, rate=0.00097247, load 64=2.6 seconds, train=1.4 seconds, 59520 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14595e000000
931: loss=11.085, avg loss=11.430, last=none, best=none, next=1000, rate=0.00097666, load 64=571.3 milliseconds, train=1.6 seconds, 59584 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
932: loss=10.343, avg loss=11.322, last=none, best=none, next=1000, rate=0.00098086, load 64=2.5 seconds, train=1.6 seconds, 59648 images, time remaining=6.1 hours
933: loss=10.534, avg loss=11.243, last=none, best=none, next=1000, rate=0.00098508, load 64=682.6 milliseconds, train=1.7 seconds, 59712 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
934: loss=12.229, avg loss=11.341, last=none, best=none, next=1000, rate=0.00098931, load 64=3.4 seconds, train=1.6 seconds, 59776 images, time remaining=6.1 hours
935: loss=11.135, avg loss=11.321, last=none, best=none, next=1000, rate=0.00099355, load 64=509.1 milliseconds, train=1.6 seconds, 59840 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
936: loss=9.926, avg loss=11.181, last=none, best=none, next=1000, rate=0.00099781, load 64=3.0 seconds, train=1.6 seconds, 59904 images, time remaining=6.1 hours
937: loss=12.364, avg loss=11.300, last=none, best=none, next=1000, rate=0.00100208, load 64=1.3 seconds, train=1.7 seconds, 59968 images, time remaining=6.1 hours
938: loss=10.267, avg loss=11.196, last=none, best=none, next=1000, rate=0.00100636, load 64=906.5 milliseconds, train=1.6 seconds, 60032 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
939: loss=13.493, avg loss=11.426, last=none, best=none, next=1000, rate=0.00101066, load 64=2.8 seconds, train=1.6 seconds, 60096 images, time remaining=6.1 hours
940: loss=10.339, avg loss=11.317, last=none, best=none, next=1000, rate=0.00101497, load 64=1.1 seconds, train=1.6 seconds, 60160 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14595e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
941: loss=10.642, avg loss=11.250, last=none, best=none, next=1000, rate=0.00101930, load 64=2.9 seconds, train=1.6 seconds, 60224 images, time remaining=6.1 hours
942: loss=9.598, avg loss=11.085, last=none, best=none, next=1000, rate=0.00102364, load 64=524.8 milliseconds, train=1.6 seconds, 60288 images, time remaining=6.1 hours
943: loss=10.603, avg loss=11.036, last=none, best=none, next=1000, rate=0.00102799, load 64=559.1 milliseconds, train=1.6 seconds, 60352 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
944: loss=10.673, avg loss=11.000, last=none, best=none, next=1000, rate=0.00103236, load 64=2.2 seconds, train=1.6 seconds, 60416 images, time remaining=6.1 hours
945: loss=10.727, avg loss=10.973, last=none, best=none, next=1000, rate=0.00103674, load 64=1.6 seconds, train=1.6 seconds, 60480 images, time remaining=6.1 hours
946: loss=9.032, avg loss=10.779, last=none, best=none, next=1000, rate=0.00104114, load 64=1.2 seconds, train=1.6 seconds, 60544 images, time remaining=6.1 hours
947: loss=10.852, avg loss=10.786, last=none, best=none, next=1000, rate=0.00104555, load 64=1.4 seconds, train=1.7 seconds, 60608 images, time remaining=6.1 hours
948: loss=9.785, avg loss=10.686, last=none, best=none, next=1000, rate=0.00104997, load 64=872.7 milliseconds, train=1.6 seconds, 60672 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
949: loss=10.690, avg loss=10.686, last=none, best=none, next=1000, rate=0.00105441, load 64=2.3 seconds, train=1.6 seconds, 60736 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
950: loss=11.485, avg loss=10.766, last=none, best=none, next=1000, rate=0.00105886, load 64=1.9 seconds, train=1.6 seconds, 60800 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
951: loss=13.263, avg loss=11.016, last=none, best=none, next=1000, rate=0.00106332, load 64=822.0 milliseconds, train=4.5 seconds, 60864 images, time remaining=6.1 hours
952: loss=13.006, avg loss=11.215, last=none, best=none, next=1000, rate=0.00106780, load 64=3.7 seconds, train=4.5 seconds, 60928 images, time remaining=6.1 hours
953: loss=14.328, avg loss=11.526, last=none, best=none, next=1000, rate=0.00107230, load 64=1.5 seconds, train=4.5 seconds, 60992 images, time remaining=6.1 hours
954: loss=11.673, avg loss=11.541, last=none, best=none, next=1000, rate=0.00107680, load 64=921.0 milliseconds, train=4.5 seconds, 61056 images, time remaining=6.1 hours
955: loss=15.297, avg loss=11.917, last=none, best=none, next=1000, rate=0.00108133, load 64=1.1 seconds, train=4.5 seconds, 61120 images, time remaining=6.1 hours
956: loss=13.705, avg loss=12.095, last=none, best=none, next=1000, rate=0.00108586, load 64=826.2 milliseconds, train=4.5 seconds, 61184 images, time remaining=6.1 hours
957: loss=13.935, avg loss=12.279, last=none, best=none, next=1000, rate=0.00109041, load 64=1.8 seconds, train=4.5 seconds, 61248 images, time remaining=6.1 hours
958: loss=14.163, avg loss=12.468, last=none, best=none, next=1000, rate=0.00109498, load 64=3.0 seconds, train=4.5 seconds, 61312 images, time remaining=6.1 hours
959: loss=11.884, avg loss=12.409, last=none, best=none, next=1000, rate=0.00109956, load 64=1.1 seconds, train=4.5 seconds, 61376 images, time remaining=6.1 hours
960: loss=12.874, avg loss=12.456, last=none, best=none, next=1000, rate=0.00110415, load 64=2.0 seconds, train=4.5 seconds, 61440 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
961: loss=12.112, avg loss=12.422, last=none, best=none, next=1000, rate=0.00110876, load 64=861.8 milliseconds, train=2.0 seconds, 61504 images, time remaining=6.1 hours
962: loss=13.045, avg loss=12.484, last=none, best=none, next=1000, rate=0.00111338, load 64=718.5 milliseconds, train=2.0 seconds, 61568 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
963: loss=12.579, avg loss=12.493, last=none, best=none, next=1000, rate=0.00111802, load 64=2.9 seconds, train=2.0 seconds, 61632 images, time remaining=6.1 hours
964: loss=12.477, avg loss=12.492, last=none, best=none, next=1000, rate=0.00112267, load 64=1.7 seconds, train=2.0 seconds, 61696 images, time remaining=6.1 hours
965: loss=11.647, avg loss=12.407, last=none, best=none, next=1000, rate=0.00112733, load 64=1.6 seconds, train=2.0 seconds, 61760 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
966: loss=12.260, avg loss=12.393, last=none, best=none, next=1000, rate=0.00113201, load 64=3.6 seconds, train=2.0 seconds, 61824 images, time remaining=6.1 hours
967: loss=12.273, avg loss=12.381, last=none, best=none, next=1000, rate=0.00113671, load 64=708.8 milliseconds, train=2.0 seconds, 61888 images, time remaining=6.1 hours
968: loss=11.047, avg loss=12.247, last=none, best=none, next=1000, rate=0.00114142, load 64=1.0 seconds, train=2.0 seconds, 61952 images, time remaining=6.1 hours
969: loss=11.750, avg loss=12.197, last=none, best=none, next=1000, rate=0.00114614, load 64=1.5 seconds, train=2.1 seconds, 62016 images, time remaining=6.1 hours
970: loss=11.405, avg loss=12.118, last=none, best=none, next=1000, rate=0.00115088, load 64=505.2 milliseconds, train=2.0 seconds, 62080 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
971: loss=11.779, avg loss=12.084, last=none, best=none, next=1000, rate=0.00115563, load 64=789.1 milliseconds, train=3.4 seconds, 62144 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
972: loss=11.514, avg loss=12.027, last=none, best=none, next=1000, rate=0.00116040, load 64=4.3 seconds, train=3.4 seconds, 62208 images, time remaining=6.1 hours
973: loss=11.630, avg loss=11.988, last=none, best=none, next=1000, rate=0.00116518, load 64=810.0 milliseconds, train=3.4 seconds, 62272 images, time remaining=6.1 hours
974: loss=11.914, avg loss=11.980, last=none, best=none, next=1000, rate=0.00116998, load 64=1.6 seconds, train=3.4 seconds, 62336 images, time remaining=6.1 hours
975: loss=11.922, avg loss=11.974, last=none, best=none, next=1000, rate=0.00117479, load 64=2.4 seconds, train=3.4 seconds, 62400 images, time remaining=6.1 hours
976: loss=12.301, avg loss=12.007, last=none, best=none, next=1000, rate=0.00117962, load 64=1.0 seconds, train=3.4 seconds, 62464 images, time remaining=6.1 hours
977: loss=11.752, avg loss=11.981, last=none, best=none, next=1000, rate=0.00118446, load 64=3.0 seconds, train=3.4 seconds, 62528 images, time remaining=6.1 hours
978: loss=11.801, avg loss=11.963, last=none, best=none, next=1000, rate=0.00118932, load 64=1.1 seconds, train=3.4 seconds, 62592 images, time remaining=6.1 hours
979: loss=11.656, avg loss=11.933, last=none, best=none, next=1000, rate=0.00119419, load 64=3.1 seconds, train=3.4 seconds, 62656 images, time remaining=6.1 hours
980: loss=11.063, avg loss=11.846, last=none, best=none, next=1000, rate=0.00119908, load 64=935.9 milliseconds, train=3.4 seconds, 62720 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
981: loss=12.157, avg loss=11.877, last=none, best=none, next=1000, rate=0.00120398, load 64=880.6 milliseconds, train=3.9 seconds, 62784 images, time remaining=6.1 hours
982: loss=11.581, avg loss=11.847, last=none, best=none, next=1000, rate=0.00120890, load 64=3.2 seconds, train=3.9 seconds, 62848 images, time remaining=6.1 hours
983: loss=14.642, avg loss=12.127, last=none, best=none, next=1000, rate=0.00121383, load 64=987.2 milliseconds, train=3.9 seconds, 62912 images, time remaining=6.1 hours
984: loss=12.502, avg loss=12.164, last=none, best=none, next=1000, rate=0.00121878, load 64=1.1 seconds, train=3.9 seconds, 62976 images, time remaining=6.1 hours
985: loss=12.433, avg loss=12.191, last=none, best=none, next=1000, rate=0.00122374, load 64=2.2 seconds, train=3.9 seconds, 63040 images, time remaining=6.1 hours
986: loss=11.716, avg loss=12.144, last=none, best=none, next=1000, rate=0.00122871, load 64=3.3 seconds, train=3.9 seconds, 63104 images, time remaining=6.1 hours
987: loss=11.930, avg loss=12.122, last=none, best=none, next=1000, rate=0.00123371, load 64=643.9 milliseconds, train=3.9 seconds, 63168 images, time remaining=6.1 hours
988: loss=11.092, avg loss=12.019, last=none, best=none, next=1000, rate=0.00123871, load 64=3.2 seconds, train=3.9 seconds, 63232 images, time remaining=6.1 hours
989: loss=11.682, avg loss=11.986, last=none, best=none, next=1000, rate=0.00124374, load 64=1.1 seconds, train=3.9 seconds, 63296 images, time remaining=6.1 hours
990: loss=12.419, avg loss=12.029, last=none, best=none, next=1000, rate=0.00124877, load 64=2.1 seconds, train=3.9 seconds, 63360 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
991: loss=12.937, avg loss=12.120, last=none, best=none, next=1000, rate=0.00125383, load 64=1.2 seconds, train=5.0 seconds, 63424 images, time remaining=6.1 hours
992: loss=13.926, avg loss=12.300, last=none, best=none, next=1000, rate=0.00125890, load 64=618.7 milliseconds, train=5.0 seconds, 63488 images, time remaining=6.1 hours
993: loss=10.285, avg loss=12.099, last=none, best=none, next=1000, rate=0.00126398, load 64=679.5 milliseconds, train=4.9 seconds, 63552 images, time remaining=6.1 hours
994: loss=12.851, avg loss=12.174, last=none, best=none, next=1000, rate=0.00126908, load 64=1.4 seconds, train=4.9 seconds, 63616 images, time remaining=6.1 hours
995: loss=12.694, avg loss=12.226, last=none, best=none, next=1000, rate=0.00127419, load 64=1.2 seconds, train=4.9 seconds, 63680 images, time remaining=6.1 hours
996: loss=13.178, avg loss=12.321, last=none, best=none, next=1000, rate=0.00127932, load 64=643.8 milliseconds, train=4.9 seconds, 63744 images, time remaining=6.1 hours
997: loss=12.505, avg loss=12.340, last=none, best=none, next=1000, rate=0.00128447, load 64=2.0 seconds, train=4.9 seconds, 63808 images, time remaining=6.1 hours
998: loss=13.804, avg loss=12.486, last=none, best=none, next=1000, rate=0.00128963, load 64=3.6 seconds, train=5.0 seconds, 63872 images, time remaining=6.1 hours
999: loss=11.839, avg loss=12.421, last=none, best=none, next=1000, rate=0.00129481, load 64=928.8 milliseconds, train=4.9 seconds, 63936 images, time remaining=6.1 hours
1000: loss=11.700, avg loss=12.349, last=none, best=none, next=1000, rate=0.00130000, load 64=1.0 seconds, train=4.9 seconds, 64000 images, time remaining=6.1 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=499472, unique_truth_count=57264
rank=0 of ranks=499472rank=100 of ranks=499472rank=200 of ranks=499472rank=300 of ranks=499472rank=400 of ranks=499472rank=500 of ranks=499472rank=600 of ranks=499472rank=700 of ranks=499472rank=800 of ranks=499472rank=900 of ranks=499472rank=1000 of ranks=499472rank=1100 of ranks=499472rank=1200 of ranks=499472rank=1300 of ranks=499472rank=1400 of ranks=499472rank=1500 of ranks=499472rank=1600 of ranks=499472rank=1700 of ranks=499472rank=1800 of ranks=499472rank=1900 of ranks=499472rank=2000 of ranks=499472rank=2100 of ranks=499472rank=2200 of ranks=499472rank=2300 of ranks=499472rank=2400 of ranks=499472rank=2500 of ranks=499472rank=2600 of ranks=499472rank=2700 of ranks=499472rank=2800 of ranks=499472rank=2900 of ranks=499472rank=3000 of ranks=499472rank=3100 of ranks=499472rank=3200 of ranks=499472rank=3300 of ranks=499472rank=3400 of ranks=499472rank=3500 of ranks=499472rank=3600 of ranks=499472rank=3700 of ranks=499472rank=3800 of ranks=499472rank=3900 of ranks=499472rank=4000 of ranks=499472rank=4100 of ranks=499472rank=4200 of ranks=499472rank=4300 of ranks=499472rank=4400 of ranks=499472rank=4500 of ranks=499472rank=4600 of ranks=499472rank=4700 of ranks=499472rank=4800 of ranks=499472rank=4900 of ranks=499472rank=5000 of ranks=499472rank=5100 of ranks=499472rank=5200 of ranks=499472rank=5300 of ranks=499472rank=5400 of ranks=499472rank=5500 of ranks=499472rank=5600 of ranks=499472rank=5700 of ranks=499472rank=5800 of ranks=499472rank=5900 of ranks=499472rank=6000 of ranks=499472rank=6100 of ranks=499472rank=6200 of ranks=499472rank=6300 of ranks=499472rank=6400 of ranks=499472rank=6500 of ranks=499472rank=6600 of ranks=499472rank=6700 of ranks=499472rank=6800 of ranks=499472rank=6900 of ranks=499472rank=7000 of ranks=499472rank=7100 of ranks=499472rank=7200 of ranks=499472rank=7300 of ranks=499472rank=7400 of ranks=499472rank=7500 of ranks=499472rank=7600 of ranks=499472rank=7700 of ranks=499472rank=7800 of ranks=499472rank=7900 of ranks=499472rank=8000 of ranks=499472rank=8100 of ranks=499472rank=8200 of ranks=499472rank=8300 of ranks=499472rank=8400 of ranks=499472rank=8500 of ranks=499472rank=8600 of ranks=499472rank=8700 of ranks=499472rank=8800 of ranks=499472rank=8900 of ranks=499472rank=9000 of ranks=499472rank=9100 of ranks=499472rank=9200 of ranks=499472rank=9300 of ranks=499472rank=9400 of ranks=499472rank=9500 of ranks=499472rank=9600 of ranks=499472rank=9700 of ranks=499472rank=9800 of ranks=499472rank=9900 of ranks=499472rank=10000 of ranks=499472rank=10100 of ranks=499472rank=10200 of ranks=499472rank=10300 of ranks=499472rank=10400 of ranks=499472rank=10500 of ranks=499472rank=10600 of ranks=499472rank=10700 of ranks=499472rank=10800 of ranks=499472rank=10900 of ranks=499472rank=11000 of ranks=499472rank=11100 of ranks=499472rank=11200 of ranks=499472rank=11300 of ranks=499472rank=11400 of ranks=499472rank=11500 of ranks=499472rank=11600 of ranks=499472rank=11700 of ranks=499472rank=11800 of ranks=499472rank=11900 of ranks=499472rank=12000 of ranks=499472rank=12100 of ranks=499472rank=12200 of ranks=499472rank=12300 of ranks=499472rank=12400 of ranks=499472rank=12500 of ranks=499472rank=12600 of ranks=499472rank=12700 of ranks=499472rank=12800 of ranks=499472rank=12900 of ranks=499472rank=13000 of ranks=499472rank=13100 of ranks=499472rank=13200 of ranks=499472rank=13300 of ranks=499472rank=13400 of ranks=499472rank=13500 of ranks=499472rank=13600 of ranks=499472rank=13700 of ranks=499472rank=13800 of ranks=499472rank=13900 of ranks=499472rank=14000 of ranks=499472rank=14100 of ranks=499472rank=14200 of ranks=499472rank=14300 of ranks=499472rank=14400 of ranks=499472rank=14500 of ranks=499472rank=14600 of ranks=499472rank=14700 of ranks=499472rank=14800 of ranks=499472rank=14900 of ranks=499472rank=15000 of ranks=499472rank=15100 of ranks=499472rank=15200 of ranks=499472rank=15300 of ranks=499472rank=15400 of ranks=499472rank=15500 of ranks=499472rank=15600 of ranks=499472rank=15700 of ranks=499472rank=15800 of ranks=499472rank=15900 of ranks=499472rank=16000 of ranks=499472rank=16100 of ranks=499472rank=16200 of ranks=499472rank=16300 of ranks=499472rank=16400 of ranks=499472rank=16500 of ranks=499472rank=16600 of ranks=499472rank=16700 of ranks=499472rank=16800 of ranks=499472rank=16900 of ranks=499472rank=17000 of ranks=499472rank=17100 of ranks=499472rank=17200 of ranks=499472rank=17300 of ranks=499472rank=17400 of ranks=499472rank=17500 of ranks=499472rank=17600 of ranks=499472rank=17700 of ranks=499472rank=17800 of ranks=499472rank=17900 of ranks=499472rank=18000 of ranks=499472rank=18100 of ranks=499472rank=18200 of ranks=499472rank=18300 of ranks=499472rank=18400 of ranks=499472rank=18500 of ranks=499472rank=18600 of ranks=499472rank=18700 of ranks=499472rank=18800 of ranks=499472rank=18900 of ranks=499472rank=19000 of ranks=499472rank=19100 of ranks=499472rank=19200 of ranks=499472rank=19300 of ranks=499472rank=19400 of ranks=499472rank=19500 of ranks=499472rank=19600 of ranks=499472rank=19700 of ranks=499472rank=19800 of ranks=499472rank=19900 of ranks=499472rank=20000 of ranks=499472rank=20100 of ranks=499472rank=20200 of ranks=499472rank=20300 of ranks=499472rank=20400 of ranks=499472rank=20500 of ranks=499472rank=20600 of ranks=499472rank=20700 of ranks=499472rank=20800 of ranks=499472rank=20900 of ranks=499472rank=21000 of ranks=499472rank=21100 of ranks=499472rank=21200 of ranks=499472rank=21300 of ranks=499472rank=21400 of ranks=499472rank=21500 of ranks=499472rank=21600 of ranks=499472rank=21700 of ranks=499472rank=21800 of ranks=499472rank=21900 of ranks=499472rank=22000 of ranks=499472rank=22100 of ranks=499472rank=22200 of ranks=499472rank=22300 of ranks=499472rank=22400 of ranks=499472rank=22500 of ranks=499472rank=22600 of ranks=499472rank=22700 of ranks=499472rank=22800 of ranks=499472rank=22900 of ranks=499472rank=23000 of ranks=499472rank=23100 of ranks=499472rank=23200 of ranks=499472rank=23300 of ranks=499472rank=23400 of ranks=499472rank=23500 of ranks=499472rank=23600 of ranks=499472rank=23700 of ranks=499472rank=23800 of ranks=499472rank=23900 of ranks=499472rank=24000 of ranks=499472rank=24100 of ranks=499472rank=24200 of ranks=499472rank=24300 of ranks=499472rank=24400 of ranks=499472rank=24500 of ranks=499472rank=24600 of ranks=499472rank=24700 of ranks=499472rank=24800 of ranks=499472rank=24900 of ranks=499472rank=25000 of ranks=499472rank=25100 of ranks=499472rank=25200 of ranks=499472rank=25300 of ranks=499472rank=25400 of ranks=499472rank=25500 of ranks=499472rank=25600 of ranks=499472rank=25700 of ranks=499472rank=25800 of ranks=499472rank=25900 of ranks=499472rank=26000 of ranks=499472rank=26100 of ranks=499472rank=26200 of ranks=499472rank=26300 of ranks=499472rank=26400 of ranks=499472rank=26500 of ranks=499472rank=26600 of ranks=499472rank=26700 of ranks=499472rank=26800 of ranks=499472rank=26900 of ranks=499472rank=27000 of ranks=499472rank=27100 of ranks=499472rank=27200 of ranks=499472rank=27300 of ranks=499472rank=27400 of ranks=499472rank=27500 of ranks=499472rank=27600 of ranks=499472rank=27700 of ranks=499472rank=27800 of ranks=499472rank=27900 of ranks=499472rank=28000 of ranks=499472rank=28100 of ranks=499472rank=28200 of ranks=499472rank=28300 of ranks=499472rank=28400 of ranks=499472rank=28500 of ranks=499472rank=28600 of ranks=499472rank=28700 of ranks=499472rank=28800 of ranks=499472rank=28900 of ranks=499472rank=29000 of ranks=499472rank=29100 of ranks=499472rank=29200 of ranks=499472rank=29300 of ranks=499472rank=29400 of ranks=499472rank=29500 of ranks=499472rank=29600 of ranks=499472rank=29700 of ranks=499472rank=29800 of ranks=499472rank=29900 of ranks=499472rank=30000 of ranks=499472rank=30100 of ranks=499472rank=30200 of ranks=499472rank=30300 of ranks=499472rank=30400 of ranks=499472rank=30500 of ranks=499472rank=30600 of ranks=499472rank=30700 of ranks=499472rank=30800 of ranks=499472rank=30900 of ranks=499472rank=31000 of ranks=499472rank=31100 of ranks=499472rank=31200 of ranks=499472rank=31300 of ranks=499472rank=31400 of ranks=499472rank=31500 of ranks=499472rank=31600 of ranks=499472rank=31700 of ranks=499472rank=31800 of ranks=499472rank=31900 of ranks=499472rank=32000 of ranks=499472rank=32100 of ranks=499472rank=32200 of ranks=499472rank=32300 of ranks=499472rank=32400 of ranks=499472rank=32500 of ranks=499472rank=32600 of ranks=499472rank=32700 of ranks=499472rank=32800 of ranks=499472rank=32900 of ranks=499472rank=33000 of ranks=499472rank=33100 of ranks=499472rank=33200 of ranks=499472rank=33300 of ranks=499472rank=33400 of ranks=499472rank=33500 of ranks=499472rank=33600 of ranks=499472rank=33700 of ranks=499472rank=33800 of ranks=499472rank=33900 of ranks=499472rank=34000 of ranks=499472rank=34100 of ranks=499472rank=34200 of ranks=499472rank=34300 of ranks=499472rank=34400 of ranks=499472rank=34500 of ranks=499472rank=34600 of ranks=499472rank=34700 of ranks=499472rank=34800 of ranks=499472rank=34900 of ranks=499472rank=35000 of ranks=499472rank=35100 of ranks=499472rank=35200 of ranks=499472rank=35300 of ranks=499472rank=35400 of ranks=499472rank=35500 of ranks=499472rank=35600 of ranks=499472rank=35700 of ranks=499472rank=35800 of ranks=499472rank=35900 of ranks=499472rank=36000 of ranks=499472rank=36100 of ranks=499472rank=36200 of ranks=499472rank=36300 of ranks=499472rank=36400 of ranks=499472rank=36500 of ranks=499472rank=36600 of ranks=499472rank=36700 of ranks=499472rank=36800 of ranks=499472rank=36900 of ranks=499472rank=37000 of ranks=499472rank=37100 of ranks=499472rank=37200 of ranks=499472rank=37300 of ranks=499472rank=37400 of ranks=499472rank=37500 of ranks=499472rank=37600 of ranks=499472rank=37700 of ranks=499472rank=37800 of ranks=499472rank=37900 of ranks=499472rank=38000 of ranks=499472rank=38100 of ranks=499472rank=38200 of ranks=499472rank=38300 of ranks=499472rank=38400 of ranks=499472rank=38500 of ranks=499472rank=38600 of ranks=499472rank=38700 of ranks=499472rank=38800 of ranks=499472rank=38900 of ranks=499472rank=39000 of ranks=499472rank=39100 of ranks=499472rank=39200 of ranks=499472rank=39300 of ranks=499472rank=39400 of ranks=499472rank=39500 of ranks=499472rank=39600 of ranks=499472rank=39700 of ranks=499472rank=39800 of ranks=499472rank=39900 of ranks=499472rank=40000 of ranks=499472rank=40100 of ranks=499472rank=40200 of ranks=499472rank=40300 of ranks=499472rank=40400 of ranks=499472rank=40500 of ranks=499472rank=40600 of ranks=499472rank=40700 of ranks=499472rank=40800 of ranks=499472rank=40900 of ranks=499472rank=41000 of ranks=499472rank=41100 of ranks=499472rank=41200 of ranks=499472rank=41300 of ranks=499472rank=41400 of ranks=499472rank=41500 of ranks=499472rank=41600 of ranks=499472rank=41700 of ranks=499472rank=41800 of ranks=499472rank=41900 of ranks=499472rank=42000 of ranks=499472rank=42100 of ranks=499472rank=42200 of ranks=499472rank=42300 of ranks=499472rank=42400 of ranks=499472rank=42500 of ranks=499472rank=42600 of ranks=499472rank=42700 of ranks=499472rank=42800 of ranks=499472rank=42900 of ranks=499472rank=43000 of ranks=499472rank=43100 of ranks=499472rank=43200 of ranks=499472rank=43300 of ranks=499472rank=43400 of ranks=499472rank=43500 of ranks=499472rank=43600 of ranks=499472rank=43700 of ranks=499472rank=43800 of ranks=499472rank=43900 of ranks=499472rank=44000 of ranks=499472rank=44100 of ranks=499472rank=44200 of ranks=499472rank=44300 of ranks=499472rank=44400 of ranks=499472rank=44500 of ranks=499472rank=44600 of ranks=499472rank=44700 of ranks=499472rank=44800 of ranks=499472rank=44900 of ranks=499472rank=45000 of ranks=499472rank=45100 of ranks=499472rank=45200 of ranks=499472rank=45300 of ranks=499472rank=45400 of ranks=499472rank=45500 of ranks=499472rank=45600 of ranks=499472rank=45700 of ranks=499472rank=45800 of ranks=499472rank=45900 of ranks=499472rank=46000 of ranks=499472rank=46100 of ranks=499472rank=46200 of ranks=499472rank=46300 of ranks=499472rank=46400 of ranks=499472rank=46500 of ranks=499472rank=46600 of ranks=499472rank=46700 of ranks=499472rank=46800 of ranks=499472rank=46900 of ranks=499472rank=47000 of ranks=499472rank=47100 of ranks=499472rank=47200 of ranks=499472rank=47300 of ranks=499472rank=47400 of ranks=499472rank=47500 of ranks=499472rank=47600 of ranks=499472rank=47700 of ranks=499472rank=47800 of ranks=499472rank=47900 of ranks=499472rank=48000 of ranks=499472rank=48100 of ranks=499472rank=48200 of ranks=499472rank=48300 of ranks=499472rank=48400 of ranks=499472rank=48500 of ranks=499472rank=48600 of ranks=499472rank=48700 of ranks=499472rank=48800 of ranks=499472rank=48900 of ranks=499472rank=49000 of ranks=499472rank=49100 of ranks=499472rank=49200 of ranks=499472rank=49300 of ranks=499472rank=49400 of ranks=499472rank=49500 of ranks=499472rank=49600 of ranks=499472rank=49700 of ranks=499472rank=49800 of ranks=499472rank=49900 of ranks=499472rank=50000 of ranks=499472rank=50100 of ranks=499472rank=50200 of ranks=499472rank=50300 of ranks=499472rank=50400 of ranks=499472rank=50500 of ranks=499472rank=50600 of ranks=499472rank=50700 of ranks=499472rank=50800 of ranks=499472rank=50900 of ranks=499472rank=51000 of ranks=499472rank=51100 of ranks=499472rank=51200 of ranks=499472rank=51300 of ranks=499472rank=51400 of ranks=499472rank=51500 of ranks=499472rank=51600 of ranks=499472rank=51700 of ranks=499472rank=51800 of ranks=499472rank=51900 of ranks=499472rank=52000 of ranks=499472rank=52100 of ranks=499472rank=52200 of ranks=499472rank=52300 of ranks=499472rank=52400 of ranks=499472rank=52500 of ranks=499472rank=52600 of ranks=499472rank=52700 of ranks=499472rank=52800 of ranks=499472rank=52900 of ranks=499472rank=53000 of ranks=499472rank=53100 of ranks=499472rank=53200 of ranks=499472rank=53300 of ranks=499472rank=53400 of ranks=499472rank=53500 of ranks=499472rank=53600 of ranks=499472rank=53700 of ranks=499472rank=53800 of ranks=499472rank=53900 of ranks=499472rank=54000 of ranks=499472rank=54100 of ranks=499472rank=54200 of ranks=499472rank=54300 of ranks=499472rank=54400 of ranks=499472rank=54500 of ranks=499472rank=54600 of ranks=499472rank=54700 of ranks=499472rank=54800 of ranks=499472rank=54900 of ranks=499472rank=55000 of ranks=499472rank=55100 of ranks=499472rank=55200 of ranks=499472rank=55300 of ranks=499472rank=55400 of ranks=499472rank=55500 of ranks=499472rank=55600 of ranks=499472rank=55700 of ranks=499472rank=55800 of ranks=499472rank=55900 of ranks=499472rank=56000 of ranks=499472rank=56100 of ranks=499472rank=56200 of ranks=499472rank=56300 of ranks=499472rank=56400 of ranks=499472rank=56500 of ranks=499472rank=56600 of ranks=499472rank=56700 of ranks=499472rank=56800 of ranks=499472rank=56900 of ranks=499472rank=57000 of ranks=499472rank=57100 of ranks=499472rank=57200 of ranks=499472rank=57300 of ranks=499472rank=57400 of ranks=499472rank=57500 of ranks=499472rank=57600 of ranks=499472rank=57700 of ranks=499472rank=57800 of ranks=499472rank=57900 of ranks=499472rank=58000 of ranks=499472rank=58100 of ranks=499472rank=58200 of ranks=499472rank=58300 of ranks=499472rank=58400 of ranks=499472rank=58500 of ranks=499472rank=58600 of ranks=499472rank=58700 of ranks=499472rank=58800 of ranks=499472rank=58900 of ranks=499472rank=59000 of ranks=499472rank=59100 of ranks=499472rank=59200 of ranks=499472rank=59300 of ranks=499472rank=59400 of ranks=499472rank=59500 of ranks=499472rank=59600 of ranks=499472rank=59700 of ranks=499472rank=59800 of ranks=499472rank=59900 of ranks=499472rank=60000 of ranks=499472rank=60100 of ranks=499472rank=60200 of ranks=499472rank=60300 of ranks=499472rank=60400 of ranks=499472rank=60500 of ranks=499472rank=60600 of ranks=499472rank=60700 of ranks=499472rank=60800 of ranks=499472rank=60900 of ranks=499472rank=61000 of ranks=499472rank=61100 of ranks=499472rank=61200 of ranks=499472rank=61300 of ranks=499472rank=61400 of ranks=499472rank=61500 of ranks=499472rank=61600 of ranks=499472rank=61700 of ranks=499472rank=61800 of ranks=499472rank=61900 of ranks=499472rank=62000 of ranks=499472rank=62100 of ranks=499472rank=62200 of ranks=499472rank=62300 of ranks=499472rank=62400 of ranks=499472rank=62500 of ranks=499472rank=62600 of ranks=499472rank=62700 of ranks=499472rank=62800 of ranks=499472rank=62900 of ranks=499472rank=63000 of ranks=499472rank=63100 of ranks=499472rank=63200 of ranks=499472rank=63300 of ranks=499472rank=63400 of ranks=499472rank=63500 of ranks=499472rank=63600 of ranks=499472rank=63700 of ranks=499472rank=63800 of ranks=499472rank=63900 of ranks=499472rank=64000 of ranks=499472rank=64100 of ranks=499472rank=64200 of ranks=499472rank=64300 of ranks=499472rank=64400 of ranks=499472rank=64500 of ranks=499472rank=64600 of ranks=499472rank=64700 of ranks=499472rank=64800 of ranks=499472rank=64900 of ranks=499472rank=65000 of ranks=499472rank=65100 of ranks=499472rank=65200 of ranks=499472rank=65300 of ranks=499472rank=65400 of ranks=499472rank=65500 of ranks=499472rank=65600 of ranks=499472rank=65700 of ranks=499472rank=65800 of ranks=499472rank=65900 of ranks=499472rank=66000 of ranks=499472rank=66100 of ranks=499472rank=66200 of ranks=499472rank=66300 of ranks=499472rank=66400 of ranks=499472rank=66500 of ranks=499472rank=66600 of ranks=499472rank=66700 of ranks=499472rank=66800 of ranks=499472rank=66900 of ranks=499472rank=67000 of ranks=499472rank=67100 of ranks=499472rank=67200 of ranks=499472rank=67300 of ranks=499472rank=67400 of ranks=499472rank=67500 of ranks=499472rank=67600 of ranks=499472rank=67700 of ranks=499472rank=67800 of ranks=499472rank=67900 of ranks=499472rank=68000 of ranks=499472rank=68100 of ranks=499472rank=68200 of ranks=499472rank=68300 of ranks=499472rank=68400 of ranks=499472rank=68500 of ranks=499472rank=68600 of ranks=499472rank=68700 of ranks=499472rank=68800 of ranks=499472rank=68900 of ranks=499472rank=69000 of ranks=499472rank=69100 of ranks=499472rank=69200 of ranks=499472rank=69300 of ranks=499472rank=69400 of ranks=499472rank=69500 of ranks=499472rank=69600 of ranks=499472rank=69700 of ranks=499472rank=69800 of ranks=499472rank=69900 of ranks=499472rank=70000 of ranks=499472rank=70100 of ranks=499472rank=70200 of ranks=499472rank=70300 of ranks=499472rank=70400 of ranks=499472rank=70500 of ranks=499472rank=70600 of ranks=499472rank=70700 of ranks=499472rank=70800 of ranks=499472rank=70900 of ranks=499472rank=71000 of ranks=499472rank=71100 of ranks=499472rank=71200 of ranks=499472rank=71300 of ranks=499472rank=71400 of ranks=499472rank=71500 of ranks=499472rank=71600 of ranks=499472rank=71700 of ranks=499472rank=71800 of ranks=499472rank=71900 of ranks=499472rank=72000 of ranks=499472rank=72100 of ranks=499472rank=72200 of ranks=499472rank=72300 of ranks=499472rank=72400 of ranks=499472rank=72500 of ranks=499472rank=72600 of ranks=499472rank=72700 of ranks=499472rank=72800 of ranks=499472rank=72900 of ranks=499472rank=73000 of ranks=499472rank=73100 of ranks=499472rank=73200 of ranks=499472rank=73300 of ranks=499472rank=73400 of ranks=499472rank=73500 of ranks=499472rank=73600 of ranks=499472rank=73700 of ranks=499472rank=73800 of ranks=499472rank=73900 of ranks=499472rank=74000 of ranks=499472rank=74100 of ranks=499472rank=74200 of ranks=499472rank=74300 of ranks=499472rank=74400 of ranks=499472rank=74500 of ranks=499472rank=74600 of ranks=499472rank=74700 of ranks=499472rank=74800 of ranks=499472rank=74900 of ranks=499472rank=75000 of ranks=499472rank=75100 of ranks=499472rank=75200 of ranks=499472rank=75300 of ranks=499472rank=75400 of ranks=499472rank=75500 of ranks=499472rank=75600 of ranks=499472rank=75700 of ranks=499472rank=75800 of ranks=499472rank=75900 of ranks=499472rank=76000 of ranks=499472rank=76100 of ranks=499472rank=76200 of ranks=499472rank=76300 of ranks=499472rank=76400 of ranks=499472rank=76500 of ranks=499472rank=76600 of ranks=499472rank=76700 of ranks=499472rank=76800 of ranks=499472rank=76900 of ranks=499472rank=77000 of ranks=499472rank=77100 of ranks=499472rank=77200 of ranks=499472rank=77300 of ranks=499472rank=77400 of ranks=499472rank=77500 of ranks=499472rank=77600 of ranks=499472rank=77700 of ranks=499472rank=77800 of ranks=499472rank=77900 of ranks=499472rank=78000 of ranks=499472rank=78100 of ranks=499472rank=78200 of ranks=499472rank=78300 of ranks=499472rank=78400 of ranks=499472rank=78500 of ranks=499472rank=78600 of ranks=499472rank=78700 of ranks=499472rank=78800 of ranks=499472rank=78900 of ranks=499472rank=79000 of ranks=499472rank=79100 of ranks=499472rank=79200 of ranks=499472rank=79300 of ranks=499472rank=79400 of ranks=499472rank=79500 of ranks=499472rank=79600 of ranks=499472rank=79700 of ranks=499472rank=79800 of ranks=499472rank=79900 of ranks=499472rank=80000 of ranks=499472rank=80100 of ranks=499472rank=80200 of ranks=499472rank=80300 of ranks=499472rank=80400 of ranks=499472rank=80500 of ranks=499472rank=80600 of ranks=499472rank=80700 of ranks=499472rank=80800 of ranks=499472rank=80900 of ranks=499472rank=81000 of ranks=499472rank=81100 of ranks=499472rank=81200 of ranks=499472rank=81300 of ranks=499472rank=81400 of ranks=499472rank=81500 of ranks=499472rank=81600 of ranks=499472rank=81700 of ranks=499472rank=81800 of ranks=499472rank=81900 of ranks=499472rank=82000 of ranks=499472rank=82100 of ranks=499472rank=82200 of ranks=499472rank=82300 of ranks=499472rank=82400 of ranks=499472rank=82500 of ranks=499472rank=82600 of ranks=499472rank=82700 of ranks=499472rank=82800 of ranks=499472rank=82900 of ranks=499472rank=83000 of ranks=499472rank=83100 of ranks=499472rank=83200 of ranks=499472rank=83300 of ranks=499472rank=83400 of ranks=499472rank=83500 of ranks=499472rank=83600 of ranks=499472rank=83700 of ranks=499472rank=83800 of ranks=499472rank=83900 of ranks=499472rank=84000 of ranks=499472rank=84100 of ranks=499472rank=84200 of ranks=499472rank=84300 of ranks=499472rank=84400 of ranks=499472rank=84500 of ranks=499472rank=84600 of ranks=499472rank=84700 of ranks=499472rank=84800 of ranks=499472rank=84900 of ranks=499472rank=85000 of ranks=499472rank=85100 of ranks=499472rank=85200 of ranks=499472rank=85300 of ranks=499472rank=85400 of ranks=499472rank=85500 of ranks=499472rank=85600 of ranks=499472rank=85700 of ranks=499472rank=85800 of ranks=499472rank=85900 of ranks=499472rank=86000 of ranks=499472rank=86100 of ranks=499472rank=86200 of ranks=499472rank=86300 of ranks=499472rank=86400 of ranks=499472rank=86500 of ranks=499472rank=86600 of ranks=499472rank=86700 of ranks=499472rank=86800 of ranks=499472rank=86900 of ranks=499472rank=87000 of ranks=499472rank=87100 of ranks=499472rank=87200 of ranks=499472rank=87300 of ranks=499472rank=87400 of ranks=499472rank=87500 of ranks=499472rank=87600 of ranks=499472rank=87700 of ranks=499472rank=87800 of ranks=499472rank=87900 of ranks=499472rank=88000 of ranks=499472rank=88100 of ranks=499472rank=88200 of ranks=499472rank=88300 of ranks=499472rank=88400 of ranks=499472rank=88500 of ranks=499472rank=88600 of ranks=499472rank=88700 of ranks=499472rank=88800 of ranks=499472rank=88900 of ranks=499472rank=89000 of ranks=499472rank=89100 of ranks=499472rank=89200 of ranks=499472rank=89300 of ranks=499472rank=89400 of ranks=499472rank=89500 of ranks=499472rank=89600 of ranks=499472rank=89700 of ranks=499472rank=89800 of ranks=499472rank=89900 of ranks=499472rank=90000 of ranks=499472rank=90100 of ranks=499472rank=90200 of ranks=499472rank=90300 of ranks=499472rank=90400 of ranks=499472rank=90500 of ranks=499472rank=90600 of ranks=499472rank=90700 of ranks=499472rank=90800 of ranks=499472rank=90900 of ranks=499472rank=91000 of ranks=499472rank=91100 of ranks=499472rank=91200 of ranks=499472rank=91300 of ranks=499472rank=91400 of ranks=499472rank=91500 of ranks=499472rank=91600 of ranks=499472rank=91700 of ranks=499472rank=91800 of ranks=499472rank=91900 of ranks=499472rank=92000 of ranks=499472rank=92100 of ranks=499472rank=92200 of ranks=499472rank=92300 of ranks=499472rank=92400 of ranks=499472rank=92500 of ranks=499472rank=92600 of ranks=499472rank=92700 of ranks=499472rank=92800 of ranks=499472rank=92900 of ranks=499472rank=93000 of ranks=499472rank=93100 of ranks=499472rank=93200 of ranks=499472rank=93300 of ranks=499472rank=93400 of ranks=499472rank=93500 of ranks=499472rank=93600 of ranks=499472rank=93700 of ranks=499472rank=93800 of ranks=499472rank=93900 of ranks=499472rank=94000 of ranks=499472rank=94100 of ranks=499472rank=94200 of ranks=499472rank=94300 of ranks=499472rank=94400 of ranks=499472rank=94500 of ranks=499472rank=94600 of ranks=499472rank=94700 of ranks=499472rank=94800 of ranks=499472rank=94900 of ranks=499472rank=95000 of ranks=499472rank=95100 of ranks=499472rank=95200 of ranks=499472rank=95300 of ranks=499472rank=95400 of ranks=499472rank=95500 of ranks=499472rank=95600 of ranks=499472rank=95700 of ranks=499472rank=95800 of ranks=499472rank=95900 of ranks=499472rank=96000 of ranks=499472rank=96100 of ranks=499472rank=96200 of ranks=499472rank=96300 of ranks=499472rank=96400 of ranks=499472rank=96500 of ranks=499472rank=96600 of ranks=499472rank=96700 of ranks=499472rank=96800 of ranks=499472rank=96900 of ranks=499472rank=97000 of ranks=499472rank=97100 of ranks=499472rank=97200 of ranks=499472rank=97300 of ranks=499472rank=97400 of ranks=499472rank=97500 of ranks=499472rank=97600 of ranks=499472rank=97700 of ranks=499472rank=97800 of ranks=499472rank=97900 of ranks=499472rank=98000 of ranks=499472rank=98100 of ranks=499472rank=98200 of ranks=499472rank=98300 of ranks=499472rank=98400 of ranks=499472rank=98500 of ranks=499472rank=98600 of ranks=499472rank=98700 of ranks=499472rank=98800 of ranks=499472rank=98900 of ranks=499472rank=99000 of ranks=499472rank=99100 of ranks=499472rank=99200 of ranks=499472rank=99300 of ranks=499472rank=99400 of ranks=499472rank=99500 of ranks=499472rank=99600 of ranks=499472rank=99700 of ranks=499472rank=99800 of ranks=499472rank=99900 of ranks=499472rank=100000 of ranks=499472rank=100100 of ranks=499472rank=100200 of ranks=499472rank=100300 of ranks=499472rank=100400 of ranks=499472rank=100500 of ranks=499472rank=100600 of ranks=499472rank=100700 of ranks=499472rank=100800 of ranks=499472rank=100900 of ranks=499472rank=101000 of ranks=499472rank=101100 of ranks=499472rank=101200 of ranks=499472rank=101300 of ranks=499472rank=101400 of ranks=499472rank=101500 of ranks=499472rank=101600 of ranks=499472rank=101700 of ranks=499472rank=101800 of ranks=499472rank=101900 of ranks=499472rank=102000 of ranks=499472rank=102100 of ranks=499472rank=102200 of ranks=499472rank=102300 of ranks=499472rank=102400 of ranks=499472rank=102500 of ranks=499472rank=102600 of ranks=499472rank=102700 of ranks=499472rank=102800 of ranks=499472rank=102900 of ranks=499472rank=103000 of ranks=499472rank=103100 of ranks=499472rank=103200 of ranks=499472rank=103300 of ranks=499472rank=103400 of ranks=499472rank=103500 of ranks=499472rank=103600 of ranks=499472rank=103700 of ranks=499472rank=103800 of ranks=499472rank=103900 of ranks=499472rank=104000 of ranks=499472rank=104100 of ranks=499472rank=104200 of ranks=499472rank=104300 of ranks=499472rank=104400 of ranks=499472rank=104500 of ranks=499472rank=104600 of ranks=499472rank=104700 of ranks=499472rank=104800 of ranks=499472rank=104900 of ranks=499472rank=105000 of ranks=499472rank=105100 of ranks=499472rank=105200 of ranks=499472rank=105300 of ranks=499472rank=105400 of ranks=499472rank=105500 of ranks=499472rank=105600 of ranks=499472rank=105700 of ranks=499472rank=105800 of ranks=499472rank=105900 of ranks=499472rank=106000 of ranks=499472rank=106100 of ranks=499472rank=106200 of ranks=499472rank=106300 of ranks=499472rank=106400 of ranks=499472rank=106500 of ranks=499472rank=106600 of ranks=499472rank=106700 of ranks=499472rank=106800 of ranks=499472rank=106900 of ranks=499472rank=107000 of ranks=499472rank=107100 of ranks=499472rank=107200 of ranks=499472rank=107300 of ranks=499472rank=107400 of ranks=499472rank=107500 of ranks=499472rank=107600 of ranks=499472rank=107700 of ranks=499472rank=107800 of ranks=499472rank=107900 of ranks=499472rank=108000 of ranks=499472rank=108100 of ranks=499472rank=108200 of ranks=499472rank=108300 of ranks=499472rank=108400 of ranks=499472rank=108500 of ranks=499472rank=108600 of ranks=499472rank=108700 of ranks=499472rank=108800 of ranks=499472rank=108900 of ranks=499472rank=109000 of ranks=499472rank=109100 of ranks=499472rank=109200 of ranks=499472rank=109300 of ranks=499472rank=109400 of ranks=499472rank=109500 of ranks=499472rank=109600 of ranks=499472rank=109700 of ranks=499472rank=109800 of ranks=499472rank=109900 of ranks=499472rank=110000 of ranks=499472rank=110100 of ranks=499472rank=110200 of ranks=499472rank=110300 of ranks=499472rank=110400 of ranks=499472rank=110500 of ranks=499472rank=110600 of ranks=499472rank=110700 of ranks=499472rank=110800 of ranks=499472rank=110900 of ranks=499472rank=111000 of ranks=499472rank=111100 of ranks=499472rank=111200 of ranks=499472rank=111300 of ranks=499472rank=111400 of ranks=499472rank=111500 of ranks=499472rank=111600 of ranks=499472rank=111700 of ranks=499472rank=111800 of ranks=499472rank=111900 of ranks=499472rank=112000 of ranks=499472rank=112100 of ranks=499472rank=112200 of ranks=499472rank=112300 of ranks=499472rank=112400 of ranks=499472rank=112500 of ranks=499472rank=112600 of ranks=499472rank=112700 of ranks=499472rank=112800 of ranks=499472rank=112900 of ranks=499472rank=113000 of ranks=499472rank=113100 of ranks=499472rank=113200 of ranks=499472rank=113300 of ranks=499472rank=113400 of ranks=499472rank=113500 of ranks=499472rank=113600 of ranks=499472rank=113700 of ranks=499472rank=113800 of ranks=499472rank=113900 of ranks=499472rank=114000 of ranks=499472rank=114100 of ranks=499472rank=114200 of ranks=499472rank=114300 of ranks=499472rank=114400 of ranks=499472rank=114500 of ranks=499472rank=114600 of ranks=499472rank=114700 of ranks=499472rank=114800 of ranks=499472rank=114900 of ranks=499472rank=115000 of ranks=499472rank=115100 of ranks=499472rank=115200 of ranks=499472rank=115300 of ranks=499472rank=115400 of ranks=499472rank=115500 of ranks=499472rank=115600 of ranks=499472rank=115700 of ranks=499472rank=115800 of ranks=499472rank=115900 of ranks=499472rank=116000 of ranks=499472rank=116100 of ranks=499472rank=116200 of ranks=499472rank=116300 of ranks=499472rank=116400 of ranks=499472rank=116500 of ranks=499472rank=116600 of ranks=499472rank=116700 of ranks=499472rank=116800 of ranks=499472rank=116900 of ranks=499472rank=117000 of ranks=499472rank=117100 of ranks=499472rank=117200 of ranks=499472rank=117300 of ranks=499472rank=117400 of ranks=499472rank=117500 of ranks=499472rank=117600 of ranks=499472rank=117700 of ranks=499472rank=117800 of ranks=499472rank=117900 of ranks=499472rank=118000 of ranks=499472rank=118100 of ranks=499472rank=118200 of ranks=499472rank=118300 of ranks=499472rank=118400 of ranks=499472rank=118500 of ranks=499472rank=118600 of ranks=499472rank=118700 of ranks=499472rank=118800 of ranks=499472rank=118900 of ranks=499472rank=119000 of ranks=499472rank=119100 of ranks=499472rank=119200 of ranks=499472rank=119300 of ranks=499472rank=119400 of ranks=499472rank=119500 of ranks=499472rank=119600 of ranks=499472rank=119700 of ranks=499472rank=119800 of ranks=499472rank=119900 of ranks=499472rank=120000 of ranks=499472rank=120100 of ranks=499472rank=120200 of ranks=499472rank=120300 of ranks=499472rank=120400 of ranks=499472rank=120500 of ranks=499472rank=120600 of ranks=499472rank=120700 of ranks=499472rank=120800 of ranks=499472rank=120900 of ranks=499472rank=121000 of ranks=499472rank=121100 of ranks=499472rank=121200 of ranks=499472rank=121300 of ranks=499472rank=121400 of ranks=499472rank=121500 of ranks=499472rank=121600 of ranks=499472rank=121700 of ranks=499472rank=121800 of ranks=499472rank=121900 of ranks=499472rank=122000 of ranks=499472rank=122100 of ranks=499472rank=122200 of ranks=499472rank=122300 of ranks=499472rank=122400 of ranks=499472rank=122500 of ranks=499472rank=122600 of ranks=499472rank=122700 of ranks=499472rank=122800 of ranks=499472rank=122900 of ranks=499472rank=123000 of ranks=499472rank=123100 of ranks=499472rank=123200 of ranks=499472rank=123300 of ranks=499472rank=123400 of ranks=499472rank=123500 of ranks=499472rank=123600 of ranks=499472rank=123700 of ranks=499472rank=123800 of ranks=499472rank=123900 of ranks=499472rank=124000 of ranks=499472rank=124100 of ranks=499472rank=124200 of ranks=499472rank=124300 of ranks=499472rank=124400 of ranks=499472rank=124500 of ranks=499472rank=124600 of ranks=499472rank=124700 of ranks=499472rank=124800 of ranks=499472rank=124900 of ranks=499472rank=125000 of ranks=499472rank=125100 of ranks=499472rank=125200 of ranks=499472rank=125300 of ranks=499472rank=125400 of ranks=499472rank=125500 of ranks=499472rank=125600 of ranks=499472rank=125700 of ranks=499472rank=125800 of ranks=499472rank=125900 of ranks=499472rank=126000 of ranks=499472rank=126100 of ranks=499472rank=126200 of ranks=499472rank=126300 of ranks=499472rank=126400 of ranks=499472rank=126500 of ranks=499472rank=126600 of ranks=499472rank=126700 of ranks=499472rank=126800 of ranks=499472rank=126900 of ranks=499472rank=127000 of ranks=499472rank=127100 of ranks=499472rank=127200 of ranks=499472rank=127300 of ranks=499472rank=127400 of ranks=499472rank=127500 of ranks=499472rank=127600 of ranks=499472rank=127700 of ranks=499472rank=127800 of ranks=499472rank=127900 of ranks=499472rank=128000 of ranks=499472rank=128100 of ranks=499472rank=128200 of ranks=499472rank=128300 of ranks=499472rank=128400 of ranks=499472rank=128500 of ranks=499472rank=128600 of ranks=499472rank=128700 of ranks=499472rank=128800 of ranks=499472rank=128900 of ranks=499472rank=129000 of ranks=499472rank=129100 of ranks=499472rank=129200 of ranks=499472rank=129300 of ranks=499472rank=129400 of ranks=499472rank=129500 of ranks=499472rank=129600 of ranks=499472rank=129700 of ranks=499472rank=129800 of ranks=499472rank=129900 of ranks=499472rank=130000 of ranks=499472rank=130100 of ranks=499472rank=130200 of ranks=499472rank=130300 of ranks=499472rank=130400 of ranks=499472rank=130500 of ranks=499472rank=130600 of ranks=499472rank=130700 of ranks=499472rank=130800 of ranks=499472rank=130900 of ranks=499472rank=131000 of ranks=499472rank=131100 of ranks=499472rank=131200 of ranks=499472rank=131300 of ranks=499472rank=131400 of ranks=499472rank=131500 of ranks=499472rank=131600 of ranks=499472rank=131700 of ranks=499472rank=131800 of ranks=499472rank=131900 of ranks=499472rank=132000 of ranks=499472rank=132100 of ranks=499472rank=132200 of ranks=499472rank=132300 of ranks=499472rank=132400 of ranks=499472rank=132500 of ranks=499472rank=132600 of ranks=499472rank=132700 of ranks=499472rank=132800 of ranks=499472rank=132900 of ranks=499472rank=133000 of ranks=499472rank=133100 of ranks=499472rank=133200 of ranks=499472rank=133300 of ranks=499472rank=133400 of ranks=499472rank=133500 of ranks=499472rank=133600 of ranks=499472rank=133700 of ranks=499472rank=133800 of ranks=499472rank=133900 of ranks=499472rank=134000 of ranks=499472rank=134100 of ranks=499472rank=134200 of ranks=499472rank=134300 of ranks=499472rank=134400 of ranks=499472rank=134500 of ranks=499472rank=134600 of ranks=499472rank=134700 of ranks=499472rank=134800 of ranks=499472rank=134900 of ranks=499472rank=135000 of ranks=499472rank=135100 of ranks=499472rank=135200 of ranks=499472rank=135300 of ranks=499472rank=135400 of ranks=499472rank=135500 of ranks=499472rank=135600 of ranks=499472rank=135700 of ranks=499472rank=135800 of ranks=499472rank=135900 of ranks=499472rank=136000 of ranks=499472rank=136100 of ranks=499472rank=136200 of ranks=499472rank=136300 of ranks=499472rank=136400 of ranks=499472rank=136500 of ranks=499472rank=136600 of ranks=499472rank=136700 of ranks=499472rank=136800 of ranks=499472rank=136900 of ranks=499472rank=137000 of ranks=499472rank=137100 of ranks=499472rank=137200 of ranks=499472rank=137300 of ranks=499472rank=137400 of ranks=499472rank=137500 of ranks=499472rank=137600 of ranks=499472rank=137700 of ranks=499472rank=137800 of ranks=499472rank=137900 of ranks=499472rank=138000 of ranks=499472rank=138100 of ranks=499472rank=138200 of ranks=499472rank=138300 of ranks=499472rank=138400 of ranks=499472rank=138500 of ranks=499472rank=138600 of ranks=499472rank=138700 of ranks=499472rank=138800 of ranks=499472rank=138900 of ranks=499472rank=139000 of ranks=499472rank=139100 of ranks=499472rank=139200 of ranks=499472rank=139300 of ranks=499472rank=139400 of ranks=499472rank=139500 of ranks=499472rank=139600 of ranks=499472rank=139700 of ranks=499472rank=139800 of ranks=499472rank=139900 of ranks=499472rank=140000 of ranks=499472rank=140100 of ranks=499472rank=140200 of ranks=499472rank=140300 of ranks=499472rank=140400 of ranks=499472rank=140500 of ranks=499472rank=140600 of ranks=499472rank=140700 of ranks=499472rank=140800 of ranks=499472rank=140900 of ranks=499472rank=141000 of ranks=499472rank=141100 of ranks=499472rank=141200 of ranks=499472rank=141300 of ranks=499472rank=141400 of ranks=499472rank=141500 of ranks=499472rank=141600 of ranks=499472rank=141700 of ranks=499472rank=141800 of ranks=499472rank=141900 of ranks=499472rank=142000 of ranks=499472rank=142100 of ranks=499472rank=142200 of ranks=499472rank=142300 of ranks=499472rank=142400 of ranks=499472rank=142500 of ranks=499472rank=142600 of ranks=499472rank=142700 of ranks=499472rank=142800 of ranks=499472rank=142900 of ranks=499472rank=143000 of ranks=499472rank=143100 of ranks=499472rank=143200 of ranks=499472rank=143300 of ranks=499472rank=143400 of ranks=499472rank=143500 of ranks=499472rank=143600 of ranks=499472rank=143700 of ranks=499472rank=143800 of ranks=499472rank=143900 of ranks=499472rank=144000 of ranks=499472rank=144100 of ranks=499472rank=144200 of ranks=499472rank=144300 of ranks=499472rank=144400 of ranks=499472rank=144500 of ranks=499472rank=144600 of ranks=499472rank=144700 of ranks=499472rank=144800 of ranks=499472rank=144900 of ranks=499472rank=145000 of ranks=499472rank=145100 of ranks=499472rank=145200 of ranks=499472rank=145300 of ranks=499472rank=145400 of ranks=499472rank=145500 of ranks=499472rank=145600 of ranks=499472rank=145700 of ranks=499472rank=145800 of ranks=499472rank=145900 of ranks=499472rank=146000 of ranks=499472rank=146100 of ranks=499472rank=146200 of ranks=499472rank=146300 of ranks=499472rank=146400 of ranks=499472rank=146500 of ranks=499472rank=146600 of ranks=499472rank=146700 of ranks=499472rank=146800 of ranks=499472rank=146900 of ranks=499472rank=147000 of ranks=499472rank=147100 of ranks=499472rank=147200 of ranks=499472rank=147300 of ranks=499472rank=147400 of ranks=499472rank=147500 of ranks=499472rank=147600 of ranks=499472rank=147700 of ranks=499472rank=147800 of ranks=499472rank=147900 of ranks=499472rank=148000 of ranks=499472rank=148100 of ranks=499472rank=148200 of ranks=499472rank=148300 of ranks=499472rank=148400 of ranks=499472rank=148500 of ranks=499472rank=148600 of ranks=499472rank=148700 of ranks=499472rank=148800 of ranks=499472rank=148900 of ranks=499472rank=149000 of ranks=499472rank=149100 of ranks=499472rank=149200 of ranks=499472rank=149300 of ranks=499472rank=149400 of ranks=499472rank=149500 of ranks=499472rank=149600 of ranks=499472rank=149700 of ranks=499472rank=149800 of ranks=499472rank=149900 of ranks=499472rank=150000 of ranks=499472rank=150100 of ranks=499472rank=150200 of ranks=499472rank=150300 of ranks=499472rank=150400 of ranks=499472rank=150500 of ranks=499472rank=150600 of ranks=499472rank=150700 of ranks=499472rank=150800 of ranks=499472rank=150900 of ranks=499472rank=151000 of ranks=499472rank=151100 of ranks=499472rank=151200 of ranks=499472rank=151300 of ranks=499472rank=151400 of ranks=499472rank=151500 of ranks=499472rank=151600 of ranks=499472rank=151700 of ranks=499472rank=151800 of ranks=499472rank=151900 of ranks=499472rank=152000 of ranks=499472rank=152100 of ranks=499472rank=152200 of ranks=499472rank=152300 of ranks=499472rank=152400 of ranks=499472rank=152500 of ranks=499472rank=152600 of ranks=499472rank=152700 of ranks=499472rank=152800 of ranks=499472rank=152900 of ranks=499472rank=153000 of ranks=499472rank=153100 of ranks=499472rank=153200 of ranks=499472rank=153300 of ranks=499472rank=153400 of ranks=499472rank=153500 of ranks=499472rank=153600 of ranks=499472rank=153700 of ranks=499472rank=153800 of ranks=499472rank=153900 of ranks=499472rank=154000 of ranks=499472rank=154100 of ranks=499472rank=154200 of ranks=499472rank=154300 of ranks=499472rank=154400 of ranks=499472rank=154500 of ranks=499472rank=154600 of ranks=499472rank=154700 of ranks=499472rank=154800 of ranks=499472rank=154900 of ranks=499472rank=155000 of ranks=499472rank=155100 of ranks=499472rank=155200 of ranks=499472rank=155300 of ranks=499472rank=155400 of ranks=499472rank=155500 of ranks=499472rank=155600 of ranks=499472rank=155700 of ranks=499472rank=155800 of ranks=499472rank=155900 of ranks=499472rank=156000 of ranks=499472rank=156100 of ranks=499472rank=156200 of ranks=499472rank=156300 of ranks=499472rank=156400 of ranks=499472rank=156500 of ranks=499472rank=156600 of ranks=499472rank=156700 of ranks=499472rank=156800 of ranks=499472rank=156900 of ranks=499472rank=157000 of ranks=499472rank=157100 of ranks=499472rank=157200 of ranks=499472rank=157300 of ranks=499472rank=157400 of ranks=499472rank=157500 of ranks=499472rank=157600 of ranks=499472rank=157700 of ranks=499472rank=157800 of ranks=499472rank=157900 of ranks=499472rank=158000 of ranks=499472rank=158100 of ranks=499472rank=158200 of ranks=499472rank=158300 of ranks=499472rank=158400 of ranks=499472rank=158500 of ranks=499472rank=158600 of ranks=499472rank=158700 of ranks=499472rank=158800 of ranks=499472rank=158900 of ranks=499472rank=159000 of ranks=499472rank=159100 of ranks=499472rank=159200 of ranks=499472rank=159300 of ranks=499472rank=159400 of ranks=499472rank=159500 of ranks=499472rank=159600 of ranks=499472rank=159700 of ranks=499472rank=159800 of ranks=499472rank=159900 of ranks=499472rank=160000 of ranks=499472rank=160100 of ranks=499472rank=160200 of ranks=499472rank=160300 of ranks=499472rank=160400 of ranks=499472rank=160500 of ranks=499472rank=160600 of ranks=499472rank=160700 of ranks=499472rank=160800 of ranks=499472rank=160900 of ranks=499472rank=161000 of ranks=499472rank=161100 of ranks=499472rank=161200 of ranks=499472rank=161300 of ranks=499472rank=161400 of ranks=499472rank=161500 of ranks=499472rank=161600 of ranks=499472rank=161700 of ranks=499472rank=161800 of ranks=499472rank=161900 of ranks=499472rank=162000 of ranks=499472rank=162100 of ranks=499472rank=162200 of ranks=499472rank=162300 of ranks=499472rank=162400 of ranks=499472rank=162500 of ranks=499472rank=162600 of ranks=499472rank=162700 of ranks=499472rank=162800 of ranks=499472rank=162900 of ranks=499472rank=163000 of ranks=499472rank=163100 of ranks=499472rank=163200 of ranks=499472rank=163300 of ranks=499472rank=163400 of ranks=499472rank=163500 of ranks=499472rank=163600 of ranks=499472rank=163700 of ranks=499472rank=163800 of ranks=499472rank=163900 of ranks=499472rank=164000 of ranks=499472rank=164100 of ranks=499472rank=164200 of ranks=499472rank=164300 of ranks=499472rank=164400 of ranks=499472rank=164500 of ranks=499472rank=164600 of ranks=499472rank=164700 of ranks=499472rank=164800 of ranks=499472rank=164900 of ranks=499472rank=165000 of ranks=499472rank=165100 of ranks=499472rank=165200 of ranks=499472rank=165300 of ranks=499472rank=165400 of ranks=499472rank=165500 of ranks=499472rank=165600 of ranks=499472rank=165700 of ranks=499472rank=165800 of ranks=499472rank=165900 of ranks=499472rank=166000 of ranks=499472rank=166100 of ranks=499472rank=166200 of ranks=499472rank=166300 of ranks=499472rank=166400 of ranks=499472rank=166500 of ranks=499472rank=166600 of ranks=499472rank=166700 of ranks=499472rank=166800 of ranks=499472rank=166900 of ranks=499472rank=167000 of ranks=499472rank=167100 of ranks=499472rank=167200 of ranks=499472rank=167300 of ranks=499472rank=167400 of ranks=499472rank=167500 of ranks=499472rank=167600 of ranks=499472rank=167700 of ranks=499472rank=167800 of ranks=499472rank=167900 of ranks=499472rank=168000 of ranks=499472rank=168100 of ranks=499472rank=168200 of ranks=499472rank=168300 of ranks=499472rank=168400 of ranks=499472rank=168500 of ranks=499472rank=168600 of ranks=499472rank=168700 of ranks=499472rank=168800 of ranks=499472rank=168900 of ranks=499472rank=169000 of ranks=499472rank=169100 of ranks=499472rank=169200 of ranks=499472rank=169300 of ranks=499472rank=169400 of ranks=499472rank=169500 of ranks=499472rank=169600 of ranks=499472rank=169700 of ranks=499472rank=169800 of ranks=499472rank=169900 of ranks=499472rank=170000 of ranks=499472rank=170100 of ranks=499472rank=170200 of ranks=499472rank=170300 of ranks=499472rank=170400 of ranks=499472rank=170500 of ranks=499472rank=170600 of ranks=499472rank=170700 of ranks=499472rank=170800 of ranks=499472rank=170900 of ranks=499472rank=171000 of ranks=499472rank=171100 of ranks=499472rank=171200 of ranks=499472rank=171300 of ranks=499472rank=171400 of ranks=499472rank=171500 of ranks=499472rank=171600 of ranks=499472rank=171700 of ranks=499472rank=171800 of ranks=499472rank=171900 of ranks=499472rank=172000 of ranks=499472rank=172100 of ranks=499472rank=172200 of ranks=499472rank=172300 of ranks=499472rank=172400 of ranks=499472rank=172500 of ranks=499472rank=172600 of ranks=499472rank=172700 of ranks=499472rank=172800 of ranks=499472rank=172900 of ranks=499472rank=173000 of ranks=499472rank=173100 of ranks=499472rank=173200 of ranks=499472rank=173300 of ranks=499472rank=173400 of ranks=499472rank=173500 of ranks=499472rank=173600 of ranks=499472rank=173700 of ranks=499472rank=173800 of ranks=499472rank=173900 of ranks=499472rank=174000 of ranks=499472rank=174100 of ranks=499472rank=174200 of ranks=499472rank=174300 of ranks=499472rank=174400 of ranks=499472rank=174500 of ranks=499472rank=174600 of ranks=499472rank=174700 of ranks=499472rank=174800 of ranks=499472rank=174900 of ranks=499472rank=175000 of ranks=499472rank=175100 of ranks=499472rank=175200 of ranks=499472rank=175300 of ranks=499472rank=175400 of ranks=499472rank=175500 of ranks=499472rank=175600 of ranks=499472rank=175700 of ranks=499472rank=175800 of ranks=499472rank=175900 of ranks=499472rank=176000 of ranks=499472rank=176100 of ranks=499472rank=176200 of ranks=499472rank=176300 of ranks=499472rank=176400 of ranks=499472rank=176500 of ranks=499472rank=176600 of ranks=499472rank=176700 of ranks=499472rank=176800 of ranks=499472rank=176900 of ranks=499472rank=177000 of ranks=499472rank=177100 of ranks=499472rank=177200 of ranks=499472rank=177300 of ranks=499472rank=177400 of ranks=499472rank=177500 of ranks=499472rank=177600 of ranks=499472rank=177700 of ranks=499472rank=177800 of ranks=499472rank=177900 of ranks=499472rank=178000 of ranks=499472rank=178100 of ranks=499472rank=178200 of ranks=499472rank=178300 of ranks=499472rank=178400 of ranks=499472rank=178500 of ranks=499472rank=178600 of ranks=499472rank=178700 of ranks=499472rank=178800 of ranks=499472rank=178900 of ranks=499472rank=179000 of ranks=499472rank=179100 of ranks=499472rank=179200 of ranks=499472rank=179300 of ranks=499472rank=179400 of ranks=499472rank=179500 of ranks=499472rank=179600 of ranks=499472rank=179700 of ranks=499472rank=179800 of ranks=499472rank=179900 of ranks=499472rank=180000 of ranks=499472rank=180100 of ranks=499472rank=180200 of ranks=499472rank=180300 of ranks=499472rank=180400 of ranks=499472rank=180500 of ranks=499472rank=180600 of ranks=499472rank=180700 of ranks=499472rank=180800 of ranks=499472rank=180900 of ranks=499472rank=181000 of ranks=499472rank=181100 of ranks=499472rank=181200 of ranks=499472rank=181300 of ranks=499472rank=181400 of ranks=499472rank=181500 of ranks=499472rank=181600 of ranks=499472rank=181700 of ranks=499472rank=181800 of ranks=499472rank=181900 of ranks=499472rank=182000 of ranks=499472rank=182100 of ranks=499472rank=182200 of ranks=499472rank=182300 of ranks=499472rank=182400 of ranks=499472rank=182500 of ranks=499472rank=182600 of ranks=499472rank=182700 of ranks=499472rank=182800 of ranks=499472rank=182900 of ranks=499472rank=183000 of ranks=499472rank=183100 of ranks=499472rank=183200 of ranks=499472rank=183300 of ranks=499472rank=183400 of ranks=499472rank=183500 of ranks=499472rank=183600 of ranks=499472rank=183700 of ranks=499472rank=183800 of ranks=499472rank=183900 of ranks=499472rank=184000 of ranks=499472rank=184100 of ranks=499472rank=184200 of ranks=499472rank=184300 of ranks=499472rank=184400 of ranks=499472rank=184500 of ranks=499472rank=184600 of ranks=499472rank=184700 of ranks=499472rank=184800 of ranks=499472rank=184900 of ranks=499472rank=185000 of ranks=499472rank=185100 of ranks=499472rank=185200 of ranks=499472rank=185300 of ranks=499472rank=185400 of ranks=499472rank=185500 of ranks=499472rank=185600 of ranks=499472rank=185700 of ranks=499472rank=185800 of ranks=499472rank=185900 of ranks=499472rank=186000 of ranks=499472rank=186100 of ranks=499472rank=186200 of ranks=499472rank=186300 of ranks=499472rank=186400 of ranks=499472rank=186500 of ranks=499472rank=186600 of ranks=499472rank=186700 of ranks=499472rank=186800 of ranks=499472rank=186900 of ranks=499472rank=187000 of ranks=499472rank=187100 of ranks=499472rank=187200 of ranks=499472rank=187300 of ranks=499472rank=187400 of ranks=499472rank=187500 of ranks=499472rank=187600 of ranks=499472rank=187700 of ranks=499472rank=187800 of ranks=499472rank=187900 of ranks=499472rank=188000 of ranks=499472rank=188100 of ranks=499472rank=188200 of ranks=499472rank=188300 of ranks=499472rank=188400 of ranks=499472rank=188500 of ranks=499472rank=188600 of ranks=499472rank=188700 of ranks=499472rank=188800 of ranks=499472rank=188900 of ranks=499472rank=189000 of ranks=499472rank=189100 of ranks=499472rank=189200 of ranks=499472rank=189300 of ranks=499472rank=189400 of ranks=499472rank=189500 of ranks=499472rank=189600 of ranks=499472rank=189700 of ranks=499472rank=189800 of ranks=499472rank=189900 of ranks=499472rank=190000 of ranks=499472rank=190100 of ranks=499472rank=190200 of ranks=499472rank=190300 of ranks=499472rank=190400 of ranks=499472rank=190500 of ranks=499472rank=190600 of ranks=499472rank=190700 of ranks=499472rank=190800 of ranks=499472rank=190900 of ranks=499472rank=191000 of ranks=499472rank=191100 of ranks=499472rank=191200 of ranks=499472rank=191300 of ranks=499472rank=191400 of ranks=499472rank=191500 of ranks=499472rank=191600 of ranks=499472rank=191700 of ranks=499472rank=191800 of ranks=499472rank=191900 of ranks=499472rank=192000 of ranks=499472rank=192100 of ranks=499472rank=192200 of ranks=499472rank=192300 of ranks=499472rank=192400 of ranks=499472rank=192500 of ranks=499472rank=192600 of ranks=499472rank=192700 of ranks=499472rank=192800 of ranks=499472rank=192900 of ranks=499472rank=193000 of ranks=499472rank=193100 of ranks=499472rank=193200 of ranks=499472rank=193300 of ranks=499472rank=193400 of ranks=499472rank=193500 of ranks=499472rank=193600 of ranks=499472rank=193700 of ranks=499472rank=193800 of ranks=499472rank=193900 of ranks=499472rank=194000 of ranks=499472rank=194100 of ranks=499472rank=194200 of ranks=499472rank=194300 of ranks=499472rank=194400 of ranks=499472rank=194500 of ranks=499472rank=194600 of ranks=499472rank=194700 of ranks=499472rank=194800 of ranks=499472rank=194900 of ranks=499472rank=195000 of ranks=499472rank=195100 of ranks=499472rank=195200 of ranks=499472rank=195300 of ranks=499472rank=195400 of ranks=499472rank=195500 of ranks=499472rank=195600 of ranks=499472rank=195700 of ranks=499472rank=195800 of ranks=499472rank=195900 of ranks=499472rank=196000 of ranks=499472rank=196100 of ranks=499472rank=196200 of ranks=499472rank=196300 of ranks=499472rank=196400 of ranks=499472rank=196500 of ranks=499472rank=196600 of ranks=499472rank=196700 of ranks=499472rank=196800 of ranks=499472rank=196900 of ranks=499472rank=197000 of ranks=499472rank=197100 of ranks=499472rank=197200 of ranks=499472rank=197300 of ranks=499472rank=197400 of ranks=499472rank=197500 of ranks=499472rank=197600 of ranks=499472rank=197700 of ranks=499472rank=197800 of ranks=499472rank=197900 of ranks=499472rank=198000 of ranks=499472rank=198100 of ranks=499472rank=198200 of ranks=499472rank=198300 of ranks=499472rank=198400 of ranks=499472rank=198500 of ranks=499472rank=198600 of ranks=499472rank=198700 of ranks=499472rank=198800 of ranks=499472rank=198900 of ranks=499472rank=199000 of ranks=499472rank=199100 of ranks=499472rank=199200 of ranks=499472rank=199300 of ranks=499472rank=199400 of ranks=499472rank=199500 of ranks=499472rank=199600 of ranks=499472rank=199700 of ranks=499472rank=199800 of ranks=499472rank=199900 of ranks=499472rank=200000 of ranks=499472rank=200100 of ranks=499472rank=200200 of ranks=499472rank=200300 of ranks=499472rank=200400 of ranks=499472rank=200500 of ranks=499472rank=200600 of ranks=499472rank=200700 of ranks=499472rank=200800 of ranks=499472rank=200900 of ranks=499472rank=201000 of ranks=499472rank=201100 of ranks=499472rank=201200 of ranks=499472rank=201300 of ranks=499472rank=201400 of ranks=499472rank=201500 of ranks=499472rank=201600 of ranks=499472rank=201700 of ranks=499472rank=201800 of ranks=499472rank=201900 of ranks=499472rank=202000 of ranks=499472rank=202100 of ranks=499472rank=202200 of ranks=499472rank=202300 of ranks=499472rank=202400 of ranks=499472rank=202500 of ranks=499472rank=202600 of ranks=499472rank=202700 of ranks=499472rank=202800 of ranks=499472rank=202900 of ranks=499472rank=203000 of ranks=499472rank=203100 of ranks=499472rank=203200 of ranks=499472rank=203300 of ranks=499472rank=203400 of ranks=499472rank=203500 of ranks=499472rank=203600 of ranks=499472rank=203700 of ranks=499472rank=203800 of ranks=499472rank=203900 of ranks=499472rank=204000 of ranks=499472rank=204100 of ranks=499472rank=204200 of ranks=499472rank=204300 of ranks=499472rank=204400 of ranks=499472rank=204500 of ranks=499472rank=204600 of ranks=499472rank=204700 of ranks=499472rank=204800 of ranks=499472rank=204900 of ranks=499472rank=205000 of ranks=499472rank=205100 of ranks=499472rank=205200 of ranks=499472rank=205300 of ranks=499472rank=205400 of ranks=499472rank=205500 of ranks=499472rank=205600 of ranks=499472rank=205700 of ranks=499472rank=205800 of ranks=499472rank=205900 of ranks=499472rank=206000 of ranks=499472rank=206100 of ranks=499472rank=206200 of ranks=499472rank=206300 of ranks=499472rank=206400 of ranks=499472rank=206500 of ranks=499472rank=206600 of ranks=499472rank=206700 of ranks=499472rank=206800 of ranks=499472rank=206900 of ranks=499472rank=207000 of ranks=499472rank=207100 of ranks=499472rank=207200 of ranks=499472rank=207300 of ranks=499472rank=207400 of ranks=499472rank=207500 of ranks=499472rank=207600 of ranks=499472rank=207700 of ranks=499472rank=207800 of ranks=499472rank=207900 of ranks=499472rank=208000 of ranks=499472rank=208100 of ranks=499472rank=208200 of ranks=499472rank=208300 of ranks=499472rank=208400 of ranks=499472rank=208500 of ranks=499472rank=208600 of ranks=499472rank=208700 of ranks=499472rank=208800 of ranks=499472rank=208900 of ranks=499472rank=209000 of ranks=499472rank=209100 of ranks=499472rank=209200 of ranks=499472rank=209300 of ranks=499472rank=209400 of ranks=499472rank=209500 of ranks=499472rank=209600 of ranks=499472rank=209700 of ranks=499472rank=209800 of ranks=499472rank=209900 of ranks=499472rank=210000 of ranks=499472rank=210100 of ranks=499472rank=210200 of ranks=499472rank=210300 of ranks=499472rank=210400 of ranks=499472rank=210500 of ranks=499472rank=210600 of ranks=499472rank=210700 of ranks=499472rank=210800 of ranks=499472rank=210900 of ranks=499472rank=211000 of ranks=499472rank=211100 of ranks=499472rank=211200 of ranks=499472rank=211300 of ranks=499472rank=211400 of ranks=499472rank=211500 of ranks=499472rank=211600 of ranks=499472rank=211700 of ranks=499472rank=211800 of ranks=499472rank=211900 of ranks=499472rank=212000 of ranks=499472rank=212100 of ranks=499472rank=212200 of ranks=499472rank=212300 of ranks=499472rank=212400 of ranks=499472rank=212500 of ranks=499472rank=212600 of ranks=499472rank=212700 of ranks=499472rank=212800 of ranks=499472rank=212900 of ranks=499472rank=213000 of ranks=499472rank=213100 of ranks=499472rank=213200 of ranks=499472rank=213300 of ranks=499472rank=213400 of ranks=499472rank=213500 of ranks=499472rank=213600 of ranks=499472rank=213700 of ranks=499472rank=213800 of ranks=499472rank=213900 of ranks=499472rank=214000 of ranks=499472rank=214100 of ranks=499472rank=214200 of ranks=499472rank=214300 of ranks=499472rank=214400 of ranks=499472rank=214500 of ranks=499472rank=214600 of ranks=499472rank=214700 of ranks=499472rank=214800 of ranks=499472rank=214900 of ranks=499472rank=215000 of ranks=499472rank=215100 of ranks=499472rank=215200 of ranks=499472rank=215300 of ranks=499472rank=215400 of ranks=499472rank=215500 of ranks=499472rank=215600 of ranks=499472rank=215700 of ranks=499472rank=215800 of ranks=499472rank=215900 of ranks=499472rank=216000 of ranks=499472rank=216100 of ranks=499472rank=216200 of ranks=499472rank=216300 of ranks=499472rank=216400 of ranks=499472rank=216500 of ranks=499472rank=216600 of ranks=499472rank=216700 of ranks=499472rank=216800 of ranks=499472rank=216900 of ranks=499472rank=217000 of ranks=499472rank=217100 of ranks=499472rank=217200 of ranks=499472rank=217300 of ranks=499472rank=217400 of ranks=499472rank=217500 of ranks=499472rank=217600 of ranks=499472rank=217700 of ranks=499472rank=217800 of ranks=499472rank=217900 of ranks=499472rank=218000 of ranks=499472rank=218100 of ranks=499472rank=218200 of ranks=499472rank=218300 of ranks=499472rank=218400 of ranks=499472rank=218500 of ranks=499472rank=218600 of ranks=499472rank=218700 of ranks=499472rank=218800 of ranks=499472rank=218900 of ranks=499472rank=219000 of ranks=499472rank=219100 of ranks=499472rank=219200 of ranks=499472rank=219300 of ranks=499472rank=219400 of ranks=499472rank=219500 of ranks=499472rank=219600 of ranks=499472rank=219700 of ranks=499472rank=219800 of ranks=499472rank=219900 of ranks=499472rank=220000 of ranks=499472rank=220100 of ranks=499472rank=220200 of ranks=499472rank=220300 of ranks=499472rank=220400 of ranks=499472rank=220500 of ranks=499472rank=220600 of ranks=499472rank=220700 of ranks=499472rank=220800 of ranks=499472rank=220900 of ranks=499472rank=221000 of ranks=499472rank=221100 of ranks=499472rank=221200 of ranks=499472rank=221300 of ranks=499472rank=221400 of ranks=499472rank=221500 of ranks=499472rank=221600 of ranks=499472rank=221700 of ranks=499472rank=221800 of ranks=499472rank=221900 of ranks=499472rank=222000 of ranks=499472rank=222100 of ranks=499472rank=222200 of ranks=499472rank=222300 of ranks=499472rank=222400 of ranks=499472rank=222500 of ranks=499472rank=222600 of ranks=499472rank=222700 of ranks=499472rank=222800 of ranks=499472rank=222900 of ranks=499472rank=223000 of ranks=499472rank=223100 of ranks=499472rank=223200 of ranks=499472rank=223300 of ranks=499472rank=223400 of ranks=499472rank=223500 of ranks=499472rank=223600 of ranks=499472rank=223700 of ranks=499472rank=223800 of ranks=499472rank=223900 of ranks=499472rank=224000 of ranks=499472rank=224100 of ranks=499472rank=224200 of ranks=499472rank=224300 of ranks=499472rank=224400 of ranks=499472rank=224500 of ranks=499472rank=224600 of ranks=499472rank=224700 of ranks=499472rank=224800 of ranks=499472rank=224900 of ranks=499472rank=225000 of ranks=499472rank=225100 of ranks=499472rank=225200 of ranks=499472rank=225300 of ranks=499472rank=225400 of ranks=499472rank=225500 of ranks=499472rank=225600 of ranks=499472rank=225700 of ranks=499472rank=225800 of ranks=499472rank=225900 of ranks=499472rank=226000 of ranks=499472rank=226100 of ranks=499472rank=226200 of ranks=499472rank=226300 of ranks=499472rank=226400 of ranks=499472rank=226500 of ranks=499472rank=226600 of ranks=499472rank=226700 of ranks=499472rank=226800 of ranks=499472rank=226900 of ranks=499472rank=227000 of ranks=499472rank=227100 of ranks=499472rank=227200 of ranks=499472rank=227300 of ranks=499472rank=227400 of ranks=499472rank=227500 of ranks=499472rank=227600 of ranks=499472rank=227700 of ranks=499472rank=227800 of ranks=499472rank=227900 of ranks=499472rank=228000 of ranks=499472rank=228100 of ranks=499472rank=228200 of ranks=499472rank=228300 of ranks=499472rank=228400 of ranks=499472rank=228500 of ranks=499472rank=228600 of ranks=499472rank=228700 of ranks=499472rank=228800 of ranks=499472rank=228900 of ranks=499472rank=229000 of ranks=499472rank=229100 of ranks=499472rank=229200 of ranks=499472rank=229300 of ranks=499472rank=229400 of ranks=499472rank=229500 of ranks=499472rank=229600 of ranks=499472rank=229700 of ranks=499472rank=229800 of ranks=499472rank=229900 of ranks=499472rank=230000 of ranks=499472rank=230100 of ranks=499472rank=230200 of ranks=499472rank=230300 of ranks=499472rank=230400 of ranks=499472rank=230500 of ranks=499472rank=230600 of ranks=499472rank=230700 of ranks=499472rank=230800 of ranks=499472rank=230900 of ranks=499472rank=231000 of ranks=499472rank=231100 of ranks=499472rank=231200 of ranks=499472rank=231300 of ranks=499472rank=231400 of ranks=499472rank=231500 of ranks=499472rank=231600 of ranks=499472rank=231700 of ranks=499472rank=231800 of ranks=499472rank=231900 of ranks=499472rank=232000 of ranks=499472rank=232100 of ranks=499472rank=232200 of ranks=499472rank=232300 of ranks=499472rank=232400 of ranks=499472rank=232500 of ranks=499472rank=232600 of ranks=499472rank=232700 of ranks=499472rank=232800 of ranks=499472rank=232900 of ranks=499472rank=233000 of ranks=499472rank=233100 of ranks=499472rank=233200 of ranks=499472rank=233300 of ranks=499472rank=233400 of ranks=499472rank=233500 of ranks=499472rank=233600 of ranks=499472rank=233700 of ranks=499472rank=233800 of ranks=499472rank=233900 of ranks=499472rank=234000 of ranks=499472rank=234100 of ranks=499472rank=234200 of ranks=499472rank=234300 of ranks=499472rank=234400 of ranks=499472rank=234500 of ranks=499472rank=234600 of ranks=499472rank=234700 of ranks=499472rank=234800 of ranks=499472rank=234900 of ranks=499472rank=235000 of ranks=499472rank=235100 of ranks=499472rank=235200 of ranks=499472rank=235300 of ranks=499472rank=235400 of ranks=499472rank=235500 of ranks=499472rank=235600 of ranks=499472rank=235700 of ranks=499472rank=235800 of ranks=499472rank=235900 of ranks=499472rank=236000 of ranks=499472rank=236100 of ranks=499472rank=236200 of ranks=499472rank=236300 of ranks=499472rank=236400 of ranks=499472rank=236500 of ranks=499472rank=236600 of ranks=499472rank=236700 of ranks=499472rank=236800 of ranks=499472rank=236900 of ranks=499472rank=237000 of ranks=499472rank=237100 of ranks=499472rank=237200 of ranks=499472rank=237300 of ranks=499472rank=237400 of ranks=499472rank=237500 of ranks=499472rank=237600 of ranks=499472rank=237700 of ranks=499472rank=237800 of ranks=499472rank=237900 of ranks=499472rank=238000 of ranks=499472rank=238100 of ranks=499472rank=238200 of ranks=499472rank=238300 of ranks=499472rank=238400 of ranks=499472rank=238500 of ranks=499472rank=238600 of ranks=499472rank=238700 of ranks=499472rank=238800 of ranks=499472rank=238900 of ranks=499472rank=239000 of ranks=499472rank=239100 of ranks=499472rank=239200 of ranks=499472rank=239300 of ranks=499472rank=239400 of ranks=499472rank=239500 of ranks=499472rank=239600 of ranks=499472rank=239700 of ranks=499472rank=239800 of ranks=499472rank=239900 of ranks=499472rank=240000 of ranks=499472rank=240100 of ranks=499472rank=240200 of ranks=499472rank=240300 of ranks=499472rank=240400 of ranks=499472rank=240500 of ranks=499472rank=240600 of ranks=499472rank=240700 of ranks=499472rank=240800 of ranks=499472rank=240900 of ranks=499472rank=241000 of ranks=499472rank=241100 of ranks=499472rank=241200 of ranks=499472rank=241300 of ranks=499472rank=241400 of ranks=499472rank=241500 of ranks=499472rank=241600 of ranks=499472rank=241700 of ranks=499472rank=241800 of ranks=499472rank=241900 of ranks=499472rank=242000 of ranks=499472rank=242100 of ranks=499472rank=242200 of ranks=499472rank=242300 of ranks=499472rank=242400 of ranks=499472rank=242500 of ranks=499472rank=242600 of ranks=499472rank=242700 of ranks=499472rank=242800 of ranks=499472rank=242900 of ranks=499472rank=243000 of ranks=499472rank=243100 of ranks=499472rank=243200 of ranks=499472rank=243300 of ranks=499472rank=243400 of ranks=499472rank=243500 of ranks=499472rank=243600 of ranks=499472rank=243700 of ranks=499472rank=243800 of ranks=499472rank=243900 of ranks=499472rank=244000 of ranks=499472rank=244100 of ranks=499472rank=244200 of ranks=499472rank=244300 of ranks=499472rank=244400 of ranks=499472rank=244500 of ranks=499472rank=244600 of ranks=499472rank=244700 of ranks=499472rank=244800 of ranks=499472rank=244900 of ranks=499472rank=245000 of ranks=499472rank=245100 of ranks=499472rank=245200 of ranks=499472rank=245300 of ranks=499472rank=245400 of ranks=499472rank=245500 of ranks=499472rank=245600 of ranks=499472rank=245700 of ranks=499472rank=245800 of ranks=499472rank=245900 of ranks=499472rank=246000 of ranks=499472rank=246100 of ranks=499472rank=246200 of ranks=499472rank=246300 of ranks=499472rank=246400 of ranks=499472rank=246500 of ranks=499472rank=246600 of ranks=499472rank=246700 of ranks=499472rank=246800 of ranks=499472rank=246900 of ranks=499472rank=247000 of ranks=499472rank=247100 of ranks=499472rank=247200 of ranks=499472rank=247300 of ranks=499472rank=247400 of ranks=499472rank=247500 of ranks=499472rank=247600 of ranks=499472rank=247700 of ranks=499472rank=247800 of ranks=499472rank=247900 of ranks=499472rank=248000 of ranks=499472rank=248100 of ranks=499472rank=248200 of ranks=499472rank=248300 of ranks=499472rank=248400 of ranks=499472rank=248500 of ranks=499472rank=248600 of ranks=499472rank=248700 of ranks=499472rank=248800 of ranks=499472rank=248900 of ranks=499472rank=249000 of ranks=499472rank=249100 of ranks=499472rank=249200 of ranks=499472rank=249300 of ranks=499472rank=249400 of ranks=499472rank=249500 of ranks=499472rank=249600 of ranks=499472rank=249700 of ranks=499472rank=249800 of ranks=499472rank=249900 of ranks=499472rank=250000 of ranks=499472rank=250100 of ranks=499472rank=250200 of ranks=499472rank=250300 of ranks=499472rank=250400 of ranks=499472rank=250500 of ranks=499472rank=250600 of ranks=499472rank=250700 of ranks=499472rank=250800 of ranks=499472rank=250900 of ranks=499472rank=251000 of ranks=499472rank=251100 of ranks=499472rank=251200 of ranks=499472rank=251300 of ranks=499472rank=251400 of ranks=499472rank=251500 of ranks=499472rank=251600 of ranks=499472rank=251700 of ranks=499472rank=251800 of ranks=499472rank=251900 of ranks=499472rank=252000 of ranks=499472rank=252100 of ranks=499472rank=252200 of ranks=499472rank=252300 of ranks=499472rank=252400 of ranks=499472rank=252500 of ranks=499472rank=252600 of ranks=499472rank=252700 of ranks=499472rank=252800 of ranks=499472rank=252900 of ranks=499472rank=253000 of ranks=499472rank=253100 of ranks=499472rank=253200 of ranks=499472rank=253300 of ranks=499472rank=253400 of ranks=499472rank=253500 of ranks=499472rank=253600 of ranks=499472rank=253700 of ranks=499472rank=253800 of ranks=499472rank=253900 of ranks=499472rank=254000 of ranks=499472rank=254100 of ranks=499472rank=254200 of ranks=499472rank=254300 of ranks=499472rank=254400 of ranks=499472rank=254500 of ranks=499472rank=254600 of ranks=499472rank=254700 of ranks=499472rank=254800 of ranks=499472rank=254900 of ranks=499472rank=255000 of ranks=499472rank=255100 of ranks=499472rank=255200 of ranks=499472rank=255300 of ranks=499472rank=255400 of ranks=499472rank=255500 of ranks=499472rank=255600 of ranks=499472rank=255700 of ranks=499472rank=255800 of ranks=499472rank=255900 of ranks=499472rank=256000 of ranks=499472rank=256100 of ranks=499472rank=256200 of ranks=499472rank=256300 of ranks=499472rank=256400 of ranks=499472rank=256500 of ranks=499472rank=256600 of ranks=499472rank=256700 of ranks=499472rank=256800 of ranks=499472rank=256900 of ranks=499472rank=257000 of ranks=499472rank=257100 of ranks=499472rank=257200 of ranks=499472rank=257300 of ranks=499472rank=257400 of ranks=499472rank=257500 of ranks=499472rank=257600 of ranks=499472rank=257700 of ranks=499472rank=257800 of ranks=499472rank=257900 of ranks=499472rank=258000 of ranks=499472rank=258100 of ranks=499472rank=258200 of ranks=499472rank=258300 of ranks=499472rank=258400 of ranks=499472rank=258500 of ranks=499472rank=258600 of ranks=499472rank=258700 of ranks=499472rank=258800 of ranks=499472rank=258900 of ranks=499472rank=259000 of ranks=499472rank=259100 of ranks=499472rank=259200 of ranks=499472rank=259300 of ranks=499472rank=259400 of ranks=499472rank=259500 of ranks=499472rank=259600 of ranks=499472rank=259700 of ranks=499472rank=259800 of ranks=499472rank=259900 of ranks=499472rank=260000 of ranks=499472rank=260100 of ranks=499472rank=260200 of ranks=499472rank=260300 of ranks=499472rank=260400 of ranks=499472rank=260500 of ranks=499472rank=260600 of ranks=499472rank=260700 of ranks=499472rank=260800 of ranks=499472rank=260900 of ranks=499472rank=261000 of ranks=499472rank=261100 of ranks=499472rank=261200 of ranks=499472rank=261300 of ranks=499472rank=261400 of ranks=499472rank=261500 of ranks=499472rank=261600 of ranks=499472rank=261700 of ranks=499472rank=261800 of ranks=499472rank=261900 of ranks=499472rank=262000 of ranks=499472rank=262100 of ranks=499472rank=262200 of ranks=499472rank=262300 of ranks=499472rank=262400 of ranks=499472rank=262500 of ranks=499472rank=262600 of ranks=499472rank=262700 of ranks=499472rank=262800 of ranks=499472rank=262900 of ranks=499472rank=263000 of ranks=499472rank=263100 of ranks=499472rank=263200 of ranks=499472rank=263300 of ranks=499472rank=263400 of ranks=499472rank=263500 of ranks=499472rank=263600 of ranks=499472rank=263700 of ranks=499472rank=263800 of ranks=499472rank=263900 of ranks=499472rank=264000 of ranks=499472rank=264100 of ranks=499472rank=264200 of ranks=499472rank=264300 of ranks=499472rank=264400 of ranks=499472rank=264500 of ranks=499472rank=264600 of ranks=499472rank=264700 of ranks=499472rank=264800 of ranks=499472rank=264900 of ranks=499472rank=265000 of ranks=499472rank=265100 of ranks=499472rank=265200 of ranks=499472rank=265300 of ranks=499472rank=265400 of ranks=499472rank=265500 of ranks=499472rank=265600 of ranks=499472rank=265700 of ranks=499472rank=265800 of ranks=499472rank=265900 of ranks=499472rank=266000 of ranks=499472rank=266100 of ranks=499472rank=266200 of ranks=499472rank=266300 of ranks=499472rank=266400 of ranks=499472rank=266500 of ranks=499472rank=266600 of ranks=499472rank=266700 of ranks=499472rank=266800 of ranks=499472rank=266900 of ranks=499472rank=267000 of ranks=499472rank=267100 of ranks=499472rank=267200 of ranks=499472rank=267300 of ranks=499472rank=267400 of ranks=499472rank=267500 of ranks=499472rank=267600 of ranks=499472rank=267700 of ranks=499472rank=267800 of ranks=499472rank=267900 of ranks=499472rank=268000 of ranks=499472rank=268100 of ranks=499472rank=268200 of ranks=499472rank=268300 of ranks=499472rank=268400 of ranks=499472rank=268500 of ranks=499472rank=268600 of ranks=499472rank=268700 of ranks=499472rank=268800 of ranks=499472rank=268900 of ranks=499472rank=269000 of ranks=499472rank=269100 of ranks=499472rank=269200 of ranks=499472rank=269300 of ranks=499472rank=269400 of ranks=499472rank=269500 of ranks=499472rank=269600 of ranks=499472rank=269700 of ranks=499472rank=269800 of ranks=499472rank=269900 of ranks=499472rank=270000 of ranks=499472rank=270100 of ranks=499472rank=270200 of ranks=499472rank=270300 of ranks=499472rank=270400 of ranks=499472rank=270500 of ranks=499472rank=270600 of ranks=499472rank=270700 of ranks=499472rank=270800 of ranks=499472rank=270900 of ranks=499472rank=271000 of ranks=499472rank=271100 of ranks=499472rank=271200 of ranks=499472rank=271300 of ranks=499472rank=271400 of ranks=499472rank=271500 of ranks=499472rank=271600 of ranks=499472rank=271700 of ranks=499472rank=271800 of ranks=499472rank=271900 of ranks=499472rank=272000 of ranks=499472rank=272100 of ranks=499472rank=272200 of ranks=499472rank=272300 of ranks=499472rank=272400 of ranks=499472rank=272500 of ranks=499472rank=272600 of ranks=499472rank=272700 of ranks=499472rank=272800 of ranks=499472rank=272900 of ranks=499472rank=273000 of ranks=499472rank=273100 of ranks=499472rank=273200 of ranks=499472rank=273300 of ranks=499472rank=273400 of ranks=499472rank=273500 of ranks=499472rank=273600 of ranks=499472rank=273700 of ranks=499472rank=273800 of ranks=499472rank=273900 of ranks=499472rank=274000 of ranks=499472rank=274100 of ranks=499472rank=274200 of ranks=499472rank=274300 of ranks=499472rank=274400 of ranks=499472rank=274500 of ranks=499472rank=274600 of ranks=499472rank=274700 of ranks=499472rank=274800 of ranks=499472rank=274900 of ranks=499472rank=275000 of ranks=499472rank=275100 of ranks=499472rank=275200 of ranks=499472rank=275300 of ranks=499472rank=275400 of ranks=499472rank=275500 of ranks=499472rank=275600 of ranks=499472rank=275700 of ranks=499472rank=275800 of ranks=499472rank=275900 of ranks=499472rank=276000 of ranks=499472rank=276100 of ranks=499472rank=276200 of ranks=499472rank=276300 of ranks=499472rank=276400 of ranks=499472rank=276500 of ranks=499472rank=276600 of ranks=499472rank=276700 of ranks=499472rank=276800 of ranks=499472rank=276900 of ranks=499472rank=277000 of ranks=499472rank=277100 of ranks=499472rank=277200 of ranks=499472rank=277300 of ranks=499472rank=277400 of ranks=499472rank=277500 of ranks=499472rank=277600 of ranks=499472rank=277700 of ranks=499472rank=277800 of ranks=499472rank=277900 of ranks=499472rank=278000 of ranks=499472rank=278100 of ranks=499472rank=278200 of ranks=499472rank=278300 of ranks=499472rank=278400 of ranks=499472rank=278500 of ranks=499472rank=278600 of ranks=499472rank=278700 of ranks=499472rank=278800 of ranks=499472rank=278900 of ranks=499472rank=279000 of ranks=499472rank=279100 of ranks=499472rank=279200 of ranks=499472rank=279300 of ranks=499472rank=279400 of ranks=499472rank=279500 of ranks=499472rank=279600 of ranks=499472rank=279700 of ranks=499472rank=279800 of ranks=499472rank=279900 of ranks=499472rank=280000 of ranks=499472rank=280100 of ranks=499472rank=280200 of ranks=499472rank=280300 of ranks=499472rank=280400 of ranks=499472rank=280500 of ranks=499472rank=280600 of ranks=499472rank=280700 of ranks=499472rank=280800 of ranks=499472rank=280900 of ranks=499472rank=281000 of ranks=499472rank=281100 of ranks=499472rank=281200 of ranks=499472rank=281300 of ranks=499472rank=281400 of ranks=499472rank=281500 of ranks=499472rank=281600 of ranks=499472rank=281700 of ranks=499472rank=281800 of ranks=499472rank=281900 of ranks=499472rank=282000 of ranks=499472rank=282100 of ranks=499472rank=282200 of ranks=499472rank=282300 of ranks=499472rank=282400 of ranks=499472rank=282500 of ranks=499472rank=282600 of ranks=499472rank=282700 of ranks=499472rank=282800 of ranks=499472rank=282900 of ranks=499472rank=283000 of ranks=499472rank=283100 of ranks=499472rank=283200 of ranks=499472rank=283300 of ranks=499472rank=283400 of ranks=499472rank=283500 of ranks=499472rank=283600 of ranks=499472rank=283700 of ranks=499472rank=283800 of ranks=499472rank=283900 of ranks=499472rank=284000 of ranks=499472rank=284100 of ranks=499472rank=284200 of ranks=499472rank=284300 of ranks=499472rank=284400 of ranks=499472rank=284500 of ranks=499472rank=284600 of ranks=499472rank=284700 of ranks=499472rank=284800 of ranks=499472rank=284900 of ranks=499472rank=285000 of ranks=499472rank=285100 of ranks=499472rank=285200 of ranks=499472rank=285300 of ranks=499472rank=285400 of ranks=499472rank=285500 of ranks=499472rank=285600 of ranks=499472rank=285700 of ranks=499472rank=285800 of ranks=499472rank=285900 of ranks=499472rank=286000 of ranks=499472rank=286100 of ranks=499472rank=286200 of ranks=499472rank=286300 of ranks=499472rank=286400 of ranks=499472rank=286500 of ranks=499472rank=286600 of ranks=499472rank=286700 of ranks=499472rank=286800 of ranks=499472rank=286900 of ranks=499472rank=287000 of ranks=499472rank=287100 of ranks=499472rank=287200 of ranks=499472rank=287300 of ranks=499472rank=287400 of ranks=499472rank=287500 of ranks=499472rank=287600 of ranks=499472rank=287700 of ranks=499472rank=287800 of ranks=499472rank=287900 of ranks=499472rank=288000 of ranks=499472rank=288100 of ranks=499472rank=288200 of ranks=499472rank=288300 of ranks=499472rank=288400 of ranks=499472rank=288500 of ranks=499472rank=288600 of ranks=499472rank=288700 of ranks=499472rank=288800 of ranks=499472rank=288900 of ranks=499472rank=289000 of ranks=499472rank=289100 of ranks=499472rank=289200 of ranks=499472rank=289300 of ranks=499472rank=289400 of ranks=499472rank=289500 of ranks=499472rank=289600 of ranks=499472rank=289700 of ranks=499472rank=289800 of ranks=499472rank=289900 of ranks=499472rank=290000 of ranks=499472rank=290100 of ranks=499472rank=290200 of ranks=499472rank=290300 of ranks=499472rank=290400 of ranks=499472rank=290500 of ranks=499472rank=290600 of ranks=499472rank=290700 of ranks=499472rank=290800 of ranks=499472rank=290900 of ranks=499472rank=291000 of ranks=499472rank=291100 of ranks=499472rank=291200 of ranks=499472rank=291300 of ranks=499472rank=291400 of ranks=499472rank=291500 of ranks=499472rank=291600 of ranks=499472rank=291700 of ranks=499472rank=291800 of ranks=499472rank=291900 of ranks=499472rank=292000 of ranks=499472rank=292100 of ranks=499472rank=292200 of ranks=499472rank=292300 of ranks=499472rank=292400 of ranks=499472rank=292500 of ranks=499472rank=292600 of ranks=499472rank=292700 of ranks=499472rank=292800 of ranks=499472rank=292900 of ranks=499472rank=293000 of ranks=499472rank=293100 of ranks=499472rank=293200 of ranks=499472rank=293300 of ranks=499472rank=293400 of ranks=499472rank=293500 of ranks=499472rank=293600 of ranks=499472rank=293700 of ranks=499472rank=293800 of ranks=499472rank=293900 of ranks=499472rank=294000 of ranks=499472rank=294100 of ranks=499472rank=294200 of ranks=499472rank=294300 of ranks=499472rank=294400 of ranks=499472rank=294500 of ranks=499472rank=294600 of ranks=499472rank=294700 of ranks=499472rank=294800 of ranks=499472rank=294900 of ranks=499472rank=295000 of ranks=499472rank=295100 of ranks=499472rank=295200 of ranks=499472rank=295300 of ranks=499472rank=295400 of ranks=499472rank=295500 of ranks=499472rank=295600 of ranks=499472rank=295700 of ranks=499472rank=295800 of ranks=499472rank=295900 of ranks=499472rank=296000 of ranks=499472rank=296100 of ranks=499472rank=296200 of ranks=499472rank=296300 of ranks=499472rank=296400 of ranks=499472rank=296500 of ranks=499472rank=296600 of ranks=499472rank=296700 of ranks=499472rank=296800 of ranks=499472rank=296900 of ranks=499472rank=297000 of ranks=499472rank=297100 of ranks=499472rank=297200 of ranks=499472rank=297300 of ranks=499472rank=297400 of ranks=499472rank=297500 of ranks=499472rank=297600 of ranks=499472rank=297700 of ranks=499472rank=297800 of ranks=499472rank=297900 of ranks=499472rank=298000 of ranks=499472rank=298100 of ranks=499472rank=298200 of ranks=499472rank=298300 of ranks=499472rank=298400 of ranks=499472rank=298500 of ranks=499472rank=298600 of ranks=499472rank=298700 of ranks=499472rank=298800 of ranks=499472rank=298900 of ranks=499472rank=299000 of ranks=499472rank=299100 of ranks=499472rank=299200 of ranks=499472rank=299300 of ranks=499472rank=299400 of ranks=499472rank=299500 of ranks=499472rank=299600 of ranks=499472rank=299700 of ranks=499472rank=299800 of ranks=499472rank=299900 of ranks=499472rank=300000 of ranks=499472rank=300100 of ranks=499472rank=300200 of ranks=499472rank=300300 of ranks=499472rank=300400 of ranks=499472rank=300500 of ranks=499472rank=300600 of ranks=499472rank=300700 of ranks=499472rank=300800 of ranks=499472rank=300900 of ranks=499472rank=301000 of ranks=499472rank=301100 of ranks=499472rank=301200 of ranks=499472rank=301300 of ranks=499472rank=301400 of ranks=499472rank=301500 of ranks=499472rank=301600 of ranks=499472rank=301700 of ranks=499472rank=301800 of ranks=499472rank=301900 of ranks=499472rank=302000 of ranks=499472rank=302100 of ranks=499472rank=302200 of ranks=499472rank=302300 of ranks=499472rank=302400 of ranks=499472rank=302500 of ranks=499472rank=302600 of ranks=499472rank=302700 of ranks=499472rank=302800 of ranks=499472rank=302900 of ranks=499472rank=303000 of ranks=499472rank=303100 of ranks=499472rank=303200 of ranks=499472rank=303300 of ranks=499472rank=303400 of ranks=499472rank=303500 of ranks=499472rank=303600 of ranks=499472rank=303700 of ranks=499472rank=303800 of ranks=499472rank=303900 of ranks=499472rank=304000 of ranks=499472rank=304100 of ranks=499472rank=304200 of ranks=499472rank=304300 of ranks=499472rank=304400 of ranks=499472rank=304500 of ranks=499472rank=304600 of ranks=499472rank=304700 of ranks=499472rank=304800 of ranks=499472rank=304900 of ranks=499472rank=305000 of ranks=499472rank=305100 of ranks=499472rank=305200 of ranks=499472rank=305300 of ranks=499472rank=305400 of ranks=499472rank=305500 of ranks=499472rank=305600 of ranks=499472rank=305700 of ranks=499472rank=305800 of ranks=499472rank=305900 of ranks=499472rank=306000 of ranks=499472rank=306100 of ranks=499472rank=306200 of ranks=499472rank=306300 of ranks=499472rank=306400 of ranks=499472rank=306500 of ranks=499472rank=306600 of ranks=499472rank=306700 of ranks=499472rank=306800 of ranks=499472rank=306900 of ranks=499472rank=307000 of ranks=499472rank=307100 of ranks=499472rank=307200 of ranks=499472rank=307300 of ranks=499472rank=307400 of ranks=499472rank=307500 of ranks=499472rank=307600 of ranks=499472rank=307700 of ranks=499472rank=307800 of ranks=499472rank=307900 of ranks=499472rank=308000 of ranks=499472rank=308100 of ranks=499472rank=308200 of ranks=499472rank=308300 of ranks=499472rank=308400 of ranks=499472rank=308500 of ranks=499472rank=308600 of ranks=499472rank=308700 of ranks=499472rank=308800 of ranks=499472rank=308900 of ranks=499472rank=309000 of ranks=499472rank=309100 of ranks=499472rank=309200 of ranks=499472rank=309300 of ranks=499472rank=309400 of ranks=499472rank=309500 of ranks=499472rank=309600 of ranks=499472rank=309700 of ranks=499472rank=309800 of ranks=499472rank=309900 of ranks=499472rank=310000 of ranks=499472rank=310100 of ranks=499472rank=310200 of ranks=499472rank=310300 of ranks=499472rank=310400 of ranks=499472rank=310500 of ranks=499472rank=310600 of ranks=499472rank=310700 of ranks=499472rank=310800 of ranks=499472rank=310900 of ranks=499472rank=311000 of ranks=499472rank=311100 of ranks=499472rank=311200 of ranks=499472rank=311300 of ranks=499472rank=311400 of ranks=499472rank=311500 of ranks=499472rank=311600 of ranks=499472rank=311700 of ranks=499472rank=311800 of ranks=499472rank=311900 of ranks=499472rank=312000 of ranks=499472rank=312100 of ranks=499472rank=312200 of ranks=499472rank=312300 of ranks=499472rank=312400 of ranks=499472rank=312500 of ranks=499472rank=312600 of ranks=499472rank=312700 of ranks=499472rank=312800 of ranks=499472rank=312900 of ranks=499472rank=313000 of ranks=499472rank=313100 of ranks=499472rank=313200 of ranks=499472rank=313300 of ranks=499472rank=313400 of ranks=499472rank=313500 of ranks=499472rank=313600 of ranks=499472rank=313700 of ranks=499472rank=313800 of ranks=499472rank=313900 of ranks=499472rank=314000 of ranks=499472rank=314100 of ranks=499472rank=314200 of ranks=499472rank=314300 of ranks=499472rank=314400 of ranks=499472rank=314500 of ranks=499472rank=314600 of ranks=499472rank=314700 of ranks=499472rank=314800 of ranks=499472rank=314900 of ranks=499472rank=315000 of ranks=499472rank=315100 of ranks=499472rank=315200 of ranks=499472rank=315300 of ranks=499472rank=315400 of ranks=499472rank=315500 of ranks=499472rank=315600 of ranks=499472rank=315700 of ranks=499472rank=315800 of ranks=499472rank=315900 of ranks=499472rank=316000 of ranks=499472rank=316100 of ranks=499472rank=316200 of ranks=499472rank=316300 of ranks=499472rank=316400 of ranks=499472rank=316500 of ranks=499472rank=316600 of ranks=499472rank=316700 of ranks=499472rank=316800 of ranks=499472rank=316900 of ranks=499472rank=317000 of ranks=499472rank=317100 of ranks=499472rank=317200 of ranks=499472rank=317300 of ranks=499472rank=317400 of ranks=499472rank=317500 of ranks=499472rank=317600 of ranks=499472rank=317700 of ranks=499472rank=317800 of ranks=499472rank=317900 of ranks=499472rank=318000 of ranks=499472rank=318100 of ranks=499472rank=318200 of ranks=499472rank=318300 of ranks=499472rank=318400 of ranks=499472rank=318500 of ranks=499472rank=318600 of ranks=499472rank=318700 of ranks=499472rank=318800 of ranks=499472rank=318900 of ranks=499472rank=319000 of ranks=499472rank=319100 of ranks=499472rank=319200 of ranks=499472rank=319300 of ranks=499472rank=319400 of ranks=499472rank=319500 of ranks=499472rank=319600 of ranks=499472rank=319700 of ranks=499472rank=319800 of ranks=499472rank=319900 of ranks=499472rank=320000 of ranks=499472rank=320100 of ranks=499472rank=320200 of ranks=499472rank=320300 of ranks=499472rank=320400 of ranks=499472rank=320500 of ranks=499472rank=320600 of ranks=499472rank=320700 of ranks=499472rank=320800 of ranks=499472rank=320900 of ranks=499472rank=321000 of ranks=499472rank=321100 of ranks=499472rank=321200 of ranks=499472rank=321300 of ranks=499472rank=321400 of ranks=499472rank=321500 of ranks=499472rank=321600 of ranks=499472rank=321700 of ranks=499472rank=321800 of ranks=499472rank=321900 of ranks=499472rank=322000 of ranks=499472rank=322100 of ranks=499472rank=322200 of ranks=499472rank=322300 of ranks=499472rank=322400 of ranks=499472rank=322500 of ranks=499472rank=322600 of ranks=499472rank=322700 of ranks=499472rank=322800 of ranks=499472rank=322900 of ranks=499472rank=323000 of ranks=499472rank=323100 of ranks=499472rank=323200 of ranks=499472rank=323300 of ranks=499472rank=323400 of ranks=499472rank=323500 of ranks=499472rank=323600 of ranks=499472rank=323700 of ranks=499472rank=323800 of ranks=499472rank=323900 of ranks=499472rank=324000 of ranks=499472rank=324100 of ranks=499472rank=324200 of ranks=499472rank=324300 of ranks=499472rank=324400 of ranks=499472rank=324500 of ranks=499472rank=324600 of ranks=499472rank=324700 of ranks=499472rank=324800 of ranks=499472rank=324900 of ranks=499472rank=325000 of ranks=499472rank=325100 of ranks=499472rank=325200 of ranks=499472rank=325300 of ranks=499472rank=325400 of ranks=499472rank=325500 of ranks=499472rank=325600 of ranks=499472rank=325700 of ranks=499472rank=325800 of ranks=499472rank=325900 of ranks=499472rank=326000 of ranks=499472rank=326100 of ranks=499472rank=326200 of ranks=499472rank=326300 of ranks=499472rank=326400 of ranks=499472rank=326500 of ranks=499472rank=326600 of ranks=499472rank=326700 of ranks=499472rank=326800 of ranks=499472rank=326900 of ranks=499472rank=327000 of ranks=499472rank=327100 of ranks=499472rank=327200 of ranks=499472rank=327300 of ranks=499472rank=327400 of ranks=499472rank=327500 of ranks=499472rank=327600 of ranks=499472rank=327700 of ranks=499472rank=327800 of ranks=499472rank=327900 of ranks=499472rank=328000 of ranks=499472rank=328100 of ranks=499472rank=328200 of ranks=499472rank=328300 of ranks=499472rank=328400 of ranks=499472rank=328500 of ranks=499472rank=328600 of ranks=499472rank=328700 of ranks=499472rank=328800 of ranks=499472rank=328900 of ranks=499472rank=329000 of ranks=499472rank=329100 of ranks=499472rank=329200 of ranks=499472rank=329300 of ranks=499472rank=329400 of ranks=499472rank=329500 of ranks=499472rank=329600 of ranks=499472rank=329700 of ranks=499472rank=329800 of ranks=499472rank=329900 of ranks=499472rank=330000 of ranks=499472rank=330100 of ranks=499472rank=330200 of ranks=499472rank=330300 of ranks=499472rank=330400 of ranks=499472rank=330500 of ranks=499472rank=330600 of ranks=499472rank=330700 of ranks=499472rank=330800 of ranks=499472rank=330900 of ranks=499472rank=331000 of ranks=499472rank=331100 of ranks=499472rank=331200 of ranks=499472rank=331300 of ranks=499472rank=331400 of ranks=499472rank=331500 of ranks=499472rank=331600 of ranks=499472rank=331700 of ranks=499472rank=331800 of ranks=499472rank=331900 of ranks=499472rank=332000 of ranks=499472rank=332100 of ranks=499472rank=332200 of ranks=499472rank=332300 of ranks=499472rank=332400 of ranks=499472rank=332500 of ranks=499472rank=332600 of ranks=499472rank=332700 of ranks=499472rank=332800 of ranks=499472rank=332900 of ranks=499472rank=333000 of ranks=499472rank=333100 of ranks=499472rank=333200 of ranks=499472rank=333300 of ranks=499472rank=333400 of ranks=499472rank=333500 of ranks=499472rank=333600 of ranks=499472rank=333700 of ranks=499472rank=333800 of ranks=499472rank=333900 of ranks=499472rank=334000 of ranks=499472rank=334100 of ranks=499472rank=334200 of ranks=499472rank=334300 of ranks=499472rank=334400 of ranks=499472rank=334500 of ranks=499472rank=334600 of ranks=499472rank=334700 of ranks=499472rank=334800 of ranks=499472rank=334900 of ranks=499472rank=335000 of ranks=499472rank=335100 of ranks=499472rank=335200 of ranks=499472rank=335300 of ranks=499472rank=335400 of ranks=499472rank=335500 of ranks=499472rank=335600 of ranks=499472rank=335700 of ranks=499472rank=335800 of ranks=499472rank=335900 of ranks=499472rank=336000 of ranks=499472rank=336100 of ranks=499472rank=336200 of ranks=499472rank=336300 of ranks=499472rank=336400 of ranks=499472rank=336500 of ranks=499472rank=336600 of ranks=499472rank=336700 of ranks=499472rank=336800 of ranks=499472rank=336900 of ranks=499472rank=337000 of ranks=499472rank=337100 of ranks=499472rank=337200 of ranks=499472rank=337300 of ranks=499472rank=337400 of ranks=499472rank=337500 of ranks=499472rank=337600 of ranks=499472rank=337700 of ranks=499472rank=337800 of ranks=499472rank=337900 of ranks=499472rank=338000 of ranks=499472rank=338100 of ranks=499472rank=338200 of ranks=499472rank=338300 of ranks=499472rank=338400 of ranks=499472rank=338500 of ranks=499472rank=338600 of ranks=499472rank=338700 of ranks=499472rank=338800 of ranks=499472rank=338900 of ranks=499472rank=339000 of ranks=499472rank=339100 of ranks=499472rank=339200 of ranks=499472rank=339300 of ranks=499472rank=339400 of ranks=499472rank=339500 of ranks=499472rank=339600 of ranks=499472rank=339700 of ranks=499472rank=339800 of ranks=499472rank=339900 of ranks=499472rank=340000 of ranks=499472rank=340100 of ranks=499472rank=340200 of ranks=499472rank=340300 of ranks=499472rank=340400 of ranks=499472rank=340500 of ranks=499472rank=340600 of ranks=499472rank=340700 of ranks=499472rank=340800 of ranks=499472rank=340900 of ranks=499472rank=341000 of ranks=499472rank=341100 of ranks=499472rank=341200 of ranks=499472rank=341300 of ranks=499472rank=341400 of ranks=499472rank=341500 of ranks=499472rank=341600 of ranks=499472rank=341700 of ranks=499472rank=341800 of ranks=499472rank=341900 of ranks=499472rank=342000 of ranks=499472rank=342100 of ranks=499472rank=342200 of ranks=499472rank=342300 of ranks=499472rank=342400 of ranks=499472rank=342500 of ranks=499472rank=342600 of ranks=499472rank=342700 of ranks=499472rank=342800 of ranks=499472rank=342900 of ranks=499472rank=343000 of ranks=499472rank=343100 of ranks=499472rank=343200 of ranks=499472rank=343300 of ranks=499472rank=343400 of ranks=499472rank=343500 of ranks=499472rank=343600 of ranks=499472rank=343700 of ranks=499472rank=343800 of ranks=499472rank=343900 of ranks=499472rank=344000 of ranks=499472rank=344100 of ranks=499472rank=344200 of ranks=499472rank=344300 of ranks=499472rank=344400 of ranks=499472rank=344500 of ranks=499472rank=344600 of ranks=499472rank=344700 of ranks=499472rank=344800 of ranks=499472rank=344900 of ranks=499472rank=345000 of ranks=499472rank=345100 of ranks=499472rank=345200 of ranks=499472rank=345300 of ranks=499472rank=345400 of ranks=499472rank=345500 of ranks=499472rank=345600 of ranks=499472rank=345700 of ranks=499472rank=345800 of ranks=499472rank=345900 of ranks=499472rank=346000 of ranks=499472rank=346100 of ranks=499472rank=346200 of ranks=499472rank=346300 of ranks=499472rank=346400 of ranks=499472rank=346500 of ranks=499472rank=346600 of ranks=499472rank=346700 of ranks=499472rank=346800 of ranks=499472rank=346900 of ranks=499472rank=347000 of ranks=499472rank=347100 of ranks=499472rank=347200 of ranks=499472rank=347300 of ranks=499472rank=347400 of ranks=499472rank=347500 of ranks=499472rank=347600 of ranks=499472rank=347700 of ranks=499472rank=347800 of ranks=499472rank=347900 of ranks=499472rank=348000 of ranks=499472rank=348100 of ranks=499472rank=348200 of ranks=499472rank=348300 of ranks=499472rank=348400 of ranks=499472rank=348500 of ranks=499472rank=348600 of ranks=499472rank=348700 of ranks=499472rank=348800 of ranks=499472rank=348900 of ranks=499472rank=349000 of ranks=499472rank=349100 of ranks=499472rank=349200 of ranks=499472rank=349300 of ranks=499472rank=349400 of ranks=499472rank=349500 of ranks=499472rank=349600 of ranks=499472rank=349700 of ranks=499472rank=349800 of ranks=499472rank=349900 of ranks=499472rank=350000 of ranks=499472rank=350100 of ranks=499472rank=350200 of ranks=499472rank=350300 of ranks=499472rank=350400 of ranks=499472rank=350500 of ranks=499472rank=350600 of ranks=499472rank=350700 of ranks=499472rank=350800 of ranks=499472rank=350900 of ranks=499472rank=351000 of ranks=499472rank=351100 of ranks=499472rank=351200 of ranks=499472rank=351300 of ranks=499472rank=351400 of ranks=499472rank=351500 of ranks=499472rank=351600 of ranks=499472rank=351700 of ranks=499472rank=351800 of ranks=499472rank=351900 of ranks=499472rank=352000 of ranks=499472rank=352100 of ranks=499472rank=352200 of ranks=499472rank=352300 of ranks=499472rank=352400 of ranks=499472rank=352500 of ranks=499472rank=352600 of ranks=499472rank=352700 of ranks=499472rank=352800 of ranks=499472rank=352900 of ranks=499472rank=353000 of ranks=499472rank=353100 of ranks=499472rank=353200 of ranks=499472rank=353300 of ranks=499472rank=353400 of ranks=499472rank=353500 of ranks=499472rank=353600 of ranks=499472rank=353700 of ranks=499472rank=353800 of ranks=499472rank=353900 of ranks=499472rank=354000 of ranks=499472rank=354100 of ranks=499472rank=354200 of ranks=499472rank=354300 of ranks=499472rank=354400 of ranks=499472rank=354500 of ranks=499472rank=354600 of ranks=499472rank=354700 of ranks=499472rank=354800 of ranks=499472rank=354900 of ranks=499472rank=355000 of ranks=499472rank=355100 of ranks=499472rank=355200 of ranks=499472rank=355300 of ranks=499472rank=355400 of ranks=499472rank=355500 of ranks=499472rank=355600 of ranks=499472rank=355700 of ranks=499472rank=355800 of ranks=499472rank=355900 of ranks=499472rank=356000 of ranks=499472rank=356100 of ranks=499472rank=356200 of ranks=499472rank=356300 of ranks=499472rank=356400 of ranks=499472rank=356500 of ranks=499472rank=356600 of ranks=499472rank=356700 of ranks=499472rank=356800 of ranks=499472rank=356900 of ranks=499472rank=357000 of ranks=499472rank=357100 of ranks=499472rank=357200 of ranks=499472rank=357300 of ranks=499472rank=357400 of ranks=499472rank=357500 of ranks=499472rank=357600 of ranks=499472rank=357700 of ranks=499472rank=357800 of ranks=499472rank=357900 of ranks=499472rank=358000 of ranks=499472rank=358100 of ranks=499472rank=358200 of ranks=499472rank=358300 of ranks=499472rank=358400 of ranks=499472rank=358500 of ranks=499472rank=358600 of ranks=499472rank=358700 of ranks=499472rank=358800 of ranks=499472rank=358900 of ranks=499472rank=359000 of ranks=499472rank=359100 of ranks=499472rank=359200 of ranks=499472rank=359300 of ranks=499472rank=359400 of ranks=499472rank=359500 of ranks=499472rank=359600 of ranks=499472rank=359700 of ranks=499472rank=359800 of ranks=499472rank=359900 of ranks=499472rank=360000 of ranks=499472rank=360100 of ranks=499472rank=360200 of ranks=499472rank=360300 of ranks=499472rank=360400 of ranks=499472rank=360500 of ranks=499472rank=360600 of ranks=499472rank=360700 of ranks=499472rank=360800 of ranks=499472rank=360900 of ranks=499472rank=361000 of ranks=499472rank=361100 of ranks=499472rank=361200 of ranks=499472rank=361300 of ranks=499472rank=361400 of ranks=499472rank=361500 of ranks=499472rank=361600 of ranks=499472rank=361700 of ranks=499472rank=361800 of ranks=499472rank=361900 of ranks=499472rank=362000 of ranks=499472rank=362100 of ranks=499472rank=362200 of ranks=499472rank=362300 of ranks=499472rank=362400 of ranks=499472rank=362500 of ranks=499472rank=362600 of ranks=499472rank=362700 of ranks=499472rank=362800 of ranks=499472rank=362900 of ranks=499472rank=363000 of ranks=499472rank=363100 of ranks=499472rank=363200 of ranks=499472rank=363300 of ranks=499472rank=363400 of ranks=499472rank=363500 of ranks=499472rank=363600 of ranks=499472rank=363700 of ranks=499472rank=363800 of ranks=499472rank=363900 of ranks=499472rank=364000 of ranks=499472rank=364100 of ranks=499472rank=364200 of ranks=499472rank=364300 of ranks=499472rank=364400 of ranks=499472rank=364500 of ranks=499472rank=364600 of ranks=499472rank=364700 of ranks=499472rank=364800 of ranks=499472rank=364900 of ranks=499472rank=365000 of ranks=499472rank=365100 of ranks=499472rank=365200 of ranks=499472rank=365300 of ranks=499472rank=365400 of ranks=499472rank=365500 of ranks=499472rank=365600 of ranks=499472rank=365700 of ranks=499472rank=365800 of ranks=499472rank=365900 of ranks=499472rank=366000 of ranks=499472rank=366100 of ranks=499472rank=366200 of ranks=499472rank=366300 of ranks=499472rank=366400 of ranks=499472rank=366500 of ranks=499472rank=366600 of ranks=499472rank=366700 of ranks=499472rank=366800 of ranks=499472rank=366900 of ranks=499472rank=367000 of ranks=499472rank=367100 of ranks=499472rank=367200 of ranks=499472rank=367300 of ranks=499472rank=367400 of ranks=499472rank=367500 of ranks=499472rank=367600 of ranks=499472rank=367700 of ranks=499472rank=367800 of ranks=499472rank=367900 of ranks=499472rank=368000 of ranks=499472rank=368100 of ranks=499472rank=368200 of ranks=499472rank=368300 of ranks=499472rank=368400 of ranks=499472rank=368500 of ranks=499472rank=368600 of ranks=499472rank=368700 of ranks=499472rank=368800 of ranks=499472rank=368900 of ranks=499472rank=369000 of ranks=499472rank=369100 of ranks=499472rank=369200 of ranks=499472rank=369300 of ranks=499472rank=369400 of ranks=499472rank=369500 of ranks=499472rank=369600 of ranks=499472rank=369700 of ranks=499472rank=369800 of ranks=499472rank=369900 of ranks=499472rank=370000 of ranks=499472rank=370100 of ranks=499472rank=370200 of ranks=499472rank=370300 of ranks=499472rank=370400 of ranks=499472rank=370500 of ranks=499472rank=370600 of ranks=499472rank=370700 of ranks=499472rank=370800 of ranks=499472rank=370900 of ranks=499472rank=371000 of ranks=499472rank=371100 of ranks=499472rank=371200 of ranks=499472rank=371300 of ranks=499472rank=371400 of ranks=499472rank=371500 of ranks=499472rank=371600 of ranks=499472rank=371700 of ranks=499472rank=371800 of ranks=499472rank=371900 of ranks=499472rank=372000 of ranks=499472rank=372100 of ranks=499472rank=372200 of ranks=499472rank=372300 of ranks=499472rank=372400 of ranks=499472rank=372500 of ranks=499472rank=372600 of ranks=499472rank=372700 of ranks=499472rank=372800 of ranks=499472rank=372900 of ranks=499472rank=373000 of ranks=499472rank=373100 of ranks=499472rank=373200 of ranks=499472rank=373300 of ranks=499472rank=373400 of ranks=499472rank=373500 of ranks=499472rank=373600 of ranks=499472rank=373700 of ranks=499472rank=373800 of ranks=499472rank=373900 of ranks=499472rank=374000 of ranks=499472rank=374100 of ranks=499472rank=374200 of ranks=499472rank=374300 of ranks=499472rank=374400 of ranks=499472rank=374500 of ranks=499472rank=374600 of ranks=499472rank=374700 of ranks=499472rank=374800 of ranks=499472rank=374900 of ranks=499472rank=375000 of ranks=499472rank=375100 of ranks=499472rank=375200 of ranks=499472rank=375300 of ranks=499472rank=375400 of ranks=499472rank=375500 of ranks=499472rank=375600 of ranks=499472rank=375700 of ranks=499472rank=375800 of ranks=499472rank=375900 of ranks=499472rank=376000 of ranks=499472rank=376100 of ranks=499472rank=376200 of ranks=499472rank=376300 of ranks=499472rank=376400 of ranks=499472rank=376500 of ranks=499472rank=376600 of ranks=499472rank=376700 of ranks=499472rank=376800 of ranks=499472rank=376900 of ranks=499472rank=377000 of ranks=499472rank=377100 of ranks=499472rank=377200 of ranks=499472rank=377300 of ranks=499472rank=377400 of ranks=499472rank=377500 of ranks=499472rank=377600 of ranks=499472rank=377700 of ranks=499472rank=377800 of ranks=499472rank=377900 of ranks=499472rank=378000 of ranks=499472rank=378100 of ranks=499472rank=378200 of ranks=499472rank=378300 of ranks=499472rank=378400 of ranks=499472rank=378500 of ranks=499472rank=378600 of ranks=499472rank=378700 of ranks=499472rank=378800 of ranks=499472rank=378900 of ranks=499472rank=379000 of ranks=499472rank=379100 of ranks=499472rank=379200 of ranks=499472rank=379300 of ranks=499472rank=379400 of ranks=499472rank=379500 of ranks=499472rank=379600 of ranks=499472rank=379700 of ranks=499472rank=379800 of ranks=499472rank=379900 of ranks=499472rank=380000 of ranks=499472rank=380100 of ranks=499472rank=380200 of ranks=499472rank=380300 of ranks=499472rank=380400 of ranks=499472rank=380500 of ranks=499472rank=380600 of ranks=499472rank=380700 of ranks=499472rank=380800 of ranks=499472rank=380900 of ranks=499472rank=381000 of ranks=499472rank=381100 of ranks=499472rank=381200 of ranks=499472rank=381300 of ranks=499472rank=381400 of ranks=499472rank=381500 of ranks=499472rank=381600 of ranks=499472rank=381700 of ranks=499472rank=381800 of ranks=499472rank=381900 of ranks=499472rank=382000 of ranks=499472rank=382100 of ranks=499472rank=382200 of ranks=499472rank=382300 of ranks=499472rank=382400 of ranks=499472rank=382500 of ranks=499472rank=382600 of ranks=499472rank=382700 of ranks=499472rank=382800 of ranks=499472rank=382900 of ranks=499472rank=383000 of ranks=499472rank=383100 of ranks=499472rank=383200 of ranks=499472rank=383300 of ranks=499472rank=383400 of ranks=499472rank=383500 of ranks=499472rank=383600 of ranks=499472rank=383700 of ranks=499472rank=383800 of ranks=499472rank=383900 of ranks=499472rank=384000 of ranks=499472rank=384100 of ranks=499472rank=384200 of ranks=499472rank=384300 of ranks=499472rank=384400 of ranks=499472rank=384500 of ranks=499472rank=384600 of ranks=499472rank=384700 of ranks=499472rank=384800 of ranks=499472rank=384900 of ranks=499472rank=385000 of ranks=499472rank=385100 of ranks=499472rank=385200 of ranks=499472rank=385300 of ranks=499472rank=385400 of ranks=499472rank=385500 of ranks=499472rank=385600 of ranks=499472rank=385700 of ranks=499472rank=385800 of ranks=499472rank=385900 of ranks=499472rank=386000 of ranks=499472rank=386100 of ranks=499472rank=386200 of ranks=499472rank=386300 of ranks=499472rank=386400 of ranks=499472rank=386500 of ranks=499472rank=386600 of ranks=499472rank=386700 of ranks=499472rank=386800 of ranks=499472rank=386900 of ranks=499472rank=387000 of ranks=499472rank=387100 of ranks=499472rank=387200 of ranks=499472rank=387300 of ranks=499472rank=387400 of ranks=499472rank=387500 of ranks=499472rank=387600 of ranks=499472rank=387700 of ranks=499472rank=387800 of ranks=499472rank=387900 of ranks=499472rank=388000 of ranks=499472rank=388100 of ranks=499472rank=388200 of ranks=499472rank=388300 of ranks=499472rank=388400 of ranks=499472rank=388500 of ranks=499472rank=388600 of ranks=499472rank=388700 of ranks=499472rank=388800 of ranks=499472rank=388900 of ranks=499472rank=389000 of ranks=499472rank=389100 of ranks=499472rank=389200 of ranks=499472rank=389300 of ranks=499472rank=389400 of ranks=499472rank=389500 of ranks=499472rank=389600 of ranks=499472rank=389700 of ranks=499472rank=389800 of ranks=499472rank=389900 of ranks=499472rank=390000 of ranks=499472rank=390100 of ranks=499472rank=390200 of ranks=499472rank=390300 of ranks=499472rank=390400 of ranks=499472rank=390500 of ranks=499472rank=390600 of ranks=499472rank=390700 of ranks=499472rank=390800 of ranks=499472rank=390900 of ranks=499472rank=391000 of ranks=499472rank=391100 of ranks=499472rank=391200 of ranks=499472rank=391300 of ranks=499472rank=391400 of ranks=499472rank=391500 of ranks=499472rank=391600 of ranks=499472rank=391700 of ranks=499472rank=391800 of ranks=499472rank=391900 of ranks=499472rank=392000 of ranks=499472rank=392100 of ranks=499472rank=392200 of ranks=499472rank=392300 of ranks=499472rank=392400 of ranks=499472rank=392500 of ranks=499472rank=392600 of ranks=499472rank=392700 of ranks=499472rank=392800 of ranks=499472rank=392900 of ranks=499472rank=393000 of ranks=499472rank=393100 of ranks=499472rank=393200 of ranks=499472rank=393300 of ranks=499472rank=393400 of ranks=499472rank=393500 of ranks=499472rank=393600 of ranks=499472rank=393700 of ranks=499472rank=393800 of ranks=499472rank=393900 of ranks=499472rank=394000 of ranks=499472rank=394100 of ranks=499472rank=394200 of ranks=499472rank=394300 of ranks=499472rank=394400 of ranks=499472rank=394500 of ranks=499472rank=394600 of ranks=499472rank=394700 of ranks=499472rank=394800 of ranks=499472rank=394900 of ranks=499472rank=395000 of ranks=499472rank=395100 of ranks=499472rank=395200 of ranks=499472rank=395300 of ranks=499472rank=395400 of ranks=499472rank=395500 of ranks=499472rank=395600 of ranks=499472rank=395700 of ranks=499472rank=395800 of ranks=499472rank=395900 of ranks=499472rank=396000 of ranks=499472rank=396100 of ranks=499472rank=396200 of ranks=499472rank=396300 of ranks=499472rank=396400 of ranks=499472rank=396500 of ranks=499472rank=396600 of ranks=499472rank=396700 of ranks=499472rank=396800 of ranks=499472rank=396900 of ranks=499472rank=397000 of ranks=499472rank=397100 of ranks=499472rank=397200 of ranks=499472rank=397300 of ranks=499472rank=397400 of ranks=499472rank=397500 of ranks=499472rank=397600 of ranks=499472rank=397700 of ranks=499472rank=397800 of ranks=499472rank=397900 of ranks=499472rank=398000 of ranks=499472rank=398100 of ranks=499472rank=398200 of ranks=499472rank=398300 of ranks=499472rank=398400 of ranks=499472rank=398500 of ranks=499472rank=398600 of ranks=499472rank=398700 of ranks=499472rank=398800 of ranks=499472rank=398900 of ranks=499472rank=399000 of ranks=499472rank=399100 of ranks=499472rank=399200 of ranks=499472rank=399300 of ranks=499472rank=399400 of ranks=499472rank=399500 of ranks=499472rank=399600 of ranks=499472rank=399700 of ranks=499472rank=399800 of ranks=499472rank=399900 of ranks=499472rank=400000 of ranks=499472rank=400100 of ranks=499472rank=400200 of ranks=499472rank=400300 of ranks=499472rank=400400 of ranks=499472rank=400500 of ranks=499472rank=400600 of ranks=499472rank=400700 of ranks=499472rank=400800 of ranks=499472rank=400900 of ranks=499472rank=401000 of ranks=499472rank=401100 of ranks=499472rank=401200 of ranks=499472rank=401300 of ranks=499472rank=401400 of ranks=499472rank=401500 of ranks=499472rank=401600 of ranks=499472rank=401700 of ranks=499472rank=401800 of ranks=499472rank=401900 of ranks=499472rank=402000 of ranks=499472rank=402100 of ranks=499472rank=402200 of ranks=499472rank=402300 of ranks=499472rank=402400 of ranks=499472rank=402500 of ranks=499472rank=402600 of ranks=499472rank=402700 of ranks=499472rank=402800 of ranks=499472rank=402900 of ranks=499472rank=403000 of ranks=499472rank=403100 of ranks=499472rank=403200 of ranks=499472rank=403300 of ranks=499472rank=403400 of ranks=499472rank=403500 of ranks=499472rank=403600 of ranks=499472rank=403700 of ranks=499472rank=403800 of ranks=499472rank=403900 of ranks=499472rank=404000 of ranks=499472rank=404100 of ranks=499472rank=404200 of ranks=499472rank=404300 of ranks=499472rank=404400 of ranks=499472rank=404500 of ranks=499472rank=404600 of ranks=499472rank=404700 of ranks=499472rank=404800 of ranks=499472rank=404900 of ranks=499472rank=405000 of ranks=499472rank=405100 of ranks=499472rank=405200 of ranks=499472rank=405300 of ranks=499472rank=405400 of ranks=499472rank=405500 of ranks=499472rank=405600 of ranks=499472rank=405700 of ranks=499472rank=405800 of ranks=499472rank=405900 of ranks=499472rank=406000 of ranks=499472rank=406100 of ranks=499472rank=406200 of ranks=499472rank=406300 of ranks=499472rank=406400 of ranks=499472rank=406500 of ranks=499472rank=406600 of ranks=499472rank=406700 of ranks=499472rank=406800 of ranks=499472rank=406900 of ranks=499472rank=407000 of ranks=499472rank=407100 of ranks=499472rank=407200 of ranks=499472rank=407300 of ranks=499472rank=407400 of ranks=499472rank=407500 of ranks=499472rank=407600 of ranks=499472rank=407700 of ranks=499472rank=407800 of ranks=499472rank=407900 of ranks=499472rank=408000 of ranks=499472rank=408100 of ranks=499472rank=408200 of ranks=499472rank=408300 of ranks=499472rank=408400 of ranks=499472rank=408500 of ranks=499472rank=408600 of ranks=499472rank=408700 of ranks=499472rank=408800 of ranks=499472rank=408900 of ranks=499472rank=409000 of ranks=499472rank=409100 of ranks=499472rank=409200 of ranks=499472rank=409300 of ranks=499472rank=409400 of ranks=499472rank=409500 of ranks=499472rank=409600 of ranks=499472rank=409700 of ranks=499472rank=409800 of ranks=499472rank=409900 of ranks=499472rank=410000 of ranks=499472rank=410100 of ranks=499472rank=410200 of ranks=499472rank=410300 of ranks=499472rank=410400 of ranks=499472rank=410500 of ranks=499472rank=410600 of ranks=499472rank=410700 of ranks=499472rank=410800 of ranks=499472rank=410900 of ranks=499472rank=411000 of ranks=499472rank=411100 of ranks=499472rank=411200 of ranks=499472rank=411300 of ranks=499472rank=411400 of ranks=499472rank=411500 of ranks=499472rank=411600 of ranks=499472rank=411700 of ranks=499472rank=411800 of ranks=499472rank=411900 of ranks=499472rank=412000 of ranks=499472rank=412100 of ranks=499472rank=412200 of ranks=499472rank=412300 of ranks=499472rank=412400 of ranks=499472rank=412500 of ranks=499472rank=412600 of ranks=499472rank=412700 of ranks=499472rank=412800 of ranks=499472rank=412900 of ranks=499472rank=413000 of ranks=499472rank=413100 of ranks=499472rank=413200 of ranks=499472rank=413300 of ranks=499472rank=413400 of ranks=499472rank=413500 of ranks=499472rank=413600 of ranks=499472rank=413700 of ranks=499472rank=413800 of ranks=499472rank=413900 of ranks=499472rank=414000 of ranks=499472rank=414100 of ranks=499472rank=414200 of ranks=499472rank=414300 of ranks=499472rank=414400 of ranks=499472rank=414500 of ranks=499472rank=414600 of ranks=499472rank=414700 of ranks=499472rank=414800 of ranks=499472rank=414900 of ranks=499472rank=415000 of ranks=499472rank=415100 of ranks=499472rank=415200 of ranks=499472rank=415300 of ranks=499472rank=415400 of ranks=499472rank=415500 of ranks=499472rank=415600 of ranks=499472rank=415700 of ranks=499472rank=415800 of ranks=499472rank=415900 of ranks=499472rank=416000 of ranks=499472rank=416100 of ranks=499472rank=416200 of ranks=499472rank=416300 of ranks=499472rank=416400 of ranks=499472rank=416500 of ranks=499472rank=416600 of ranks=499472rank=416700 of ranks=499472rank=416800 of ranks=499472rank=416900 of ranks=499472rank=417000 of ranks=499472rank=417100 of ranks=499472rank=417200 of ranks=499472rank=417300 of ranks=499472rank=417400 of ranks=499472rank=417500 of ranks=499472rank=417600 of ranks=499472rank=417700 of ranks=499472rank=417800 of ranks=499472rank=417900 of ranks=499472rank=418000 of ranks=499472rank=418100 of ranks=499472rank=418200 of ranks=499472rank=418300 of ranks=499472rank=418400 of ranks=499472rank=418500 of ranks=499472rank=418600 of ranks=499472rank=418700 of ranks=499472rank=418800 of ranks=499472rank=418900 of ranks=499472rank=419000 of ranks=499472rank=419100 of ranks=499472rank=419200 of ranks=499472rank=419300 of ranks=499472rank=419400 of ranks=499472rank=419500 of ranks=499472rank=419600 of ranks=499472rank=419700 of ranks=499472rank=419800 of ranks=499472rank=419900 of ranks=499472rank=420000 of ranks=499472rank=420100 of ranks=499472rank=420200 of ranks=499472rank=420300 of ranks=499472rank=420400 of ranks=499472rank=420500 of ranks=499472rank=420600 of ranks=499472rank=420700 of ranks=499472rank=420800 of ranks=499472rank=420900 of ranks=499472rank=421000 of ranks=499472rank=421100 of ranks=499472rank=421200 of ranks=499472rank=421300 of ranks=499472rank=421400 of ranks=499472rank=421500 of ranks=499472rank=421600 of ranks=499472rank=421700 of ranks=499472rank=421800 of ranks=499472rank=421900 of ranks=499472rank=422000 of ranks=499472rank=422100 of ranks=499472rank=422200 of ranks=499472rank=422300 of ranks=499472rank=422400 of ranks=499472rank=422500 of ranks=499472rank=422600 of ranks=499472rank=422700 of ranks=499472rank=422800 of ranks=499472rank=422900 of ranks=499472rank=423000 of ranks=499472rank=423100 of ranks=499472rank=423200 of ranks=499472rank=423300 of ranks=499472rank=423400 of ranks=499472rank=423500 of ranks=499472rank=423600 of ranks=499472rank=423700 of ranks=499472rank=423800 of ranks=499472rank=423900 of ranks=499472rank=424000 of ranks=499472rank=424100 of ranks=499472rank=424200 of ranks=499472rank=424300 of ranks=499472rank=424400 of ranks=499472rank=424500 of ranks=499472rank=424600 of ranks=499472rank=424700 of ranks=499472rank=424800 of ranks=499472rank=424900 of ranks=499472rank=425000 of ranks=499472rank=425100 of ranks=499472rank=425200 of ranks=499472rank=425300 of ranks=499472rank=425400 of ranks=499472rank=425500 of ranks=499472rank=425600 of ranks=499472rank=425700 of ranks=499472rank=425800 of ranks=499472rank=425900 of ranks=499472rank=426000 of ranks=499472rank=426100 of ranks=499472rank=426200 of ranks=499472rank=426300 of ranks=499472rank=426400 of ranks=499472rank=426500 of ranks=499472rank=426600 of ranks=499472rank=426700 of ranks=499472rank=426800 of ranks=499472rank=426900 of ranks=499472rank=427000 of ranks=499472rank=427100 of ranks=499472rank=427200 of ranks=499472rank=427300 of ranks=499472rank=427400 of ranks=499472rank=427500 of ranks=499472rank=427600 of ranks=499472rank=427700 of ranks=499472rank=427800 of ranks=499472rank=427900 of ranks=499472rank=428000 of ranks=499472rank=428100 of ranks=499472rank=428200 of ranks=499472rank=428300 of ranks=499472rank=428400 of ranks=499472rank=428500 of ranks=499472rank=428600 of ranks=499472rank=428700 of ranks=499472rank=428800 of ranks=499472rank=428900 of ranks=499472rank=429000 of ranks=499472rank=429100 of ranks=499472rank=429200 of ranks=499472rank=429300 of ranks=499472rank=429400 of ranks=499472rank=429500 of ranks=499472rank=429600 of ranks=499472rank=429700 of ranks=499472rank=429800 of ranks=499472rank=429900 of ranks=499472rank=430000 of ranks=499472rank=430100 of ranks=499472rank=430200 of ranks=499472rank=430300 of ranks=499472rank=430400 of ranks=499472rank=430500 of ranks=499472rank=430600 of ranks=499472rank=430700 of ranks=499472rank=430800 of ranks=499472rank=430900 of ranks=499472rank=431000 of ranks=499472rank=431100 of ranks=499472rank=431200 of ranks=499472rank=431300 of ranks=499472rank=431400 of ranks=499472rank=431500 of ranks=499472rank=431600 of ranks=499472rank=431700 of ranks=499472rank=431800 of ranks=499472rank=431900 of ranks=499472rank=432000 of ranks=499472rank=432100 of ranks=499472rank=432200 of ranks=499472rank=432300 of ranks=499472rank=432400 of ranks=499472rank=432500 of ranks=499472rank=432600 of ranks=499472rank=432700 of ranks=499472rank=432800 of ranks=499472rank=432900 of ranks=499472rank=433000 of ranks=499472rank=433100 of ranks=499472rank=433200 of ranks=499472rank=433300 of ranks=499472rank=433400 of ranks=499472rank=433500 of ranks=499472rank=433600 of ranks=499472rank=433700 of ranks=499472rank=433800 of ranks=499472rank=433900 of ranks=499472rank=434000 of ranks=499472rank=434100 of ranks=499472rank=434200 of ranks=499472rank=434300 of ranks=499472rank=434400 of ranks=499472rank=434500 of ranks=499472rank=434600 of ranks=499472rank=434700 of ranks=499472rank=434800 of ranks=499472rank=434900 of ranks=499472rank=435000 of ranks=499472rank=435100 of ranks=499472rank=435200 of ranks=499472rank=435300 of ranks=499472rank=435400 of ranks=499472rank=435500 of ranks=499472rank=435600 of ranks=499472rank=435700 of ranks=499472rank=435800 of ranks=499472rank=435900 of ranks=499472rank=436000 of ranks=499472rank=436100 of ranks=499472rank=436200 of ranks=499472rank=436300 of ranks=499472rank=436400 of ranks=499472rank=436500 of ranks=499472rank=436600 of ranks=499472rank=436700 of ranks=499472rank=436800 of ranks=499472rank=436900 of ranks=499472rank=437000 of ranks=499472rank=437100 of ranks=499472rank=437200 of ranks=499472rank=437300 of ranks=499472rank=437400 of ranks=499472rank=437500 of ranks=499472rank=437600 of ranks=499472rank=437700 of ranks=499472rank=437800 of ranks=499472rank=437900 of ranks=499472rank=438000 of ranks=499472rank=438100 of ranks=499472rank=438200 of ranks=499472rank=438300 of ranks=499472rank=438400 of ranks=499472rank=438500 of ranks=499472rank=438600 of ranks=499472rank=438700 of ranks=499472rank=438800 of ranks=499472rank=438900 of ranks=499472rank=439000 of ranks=499472rank=439100 of ranks=499472rank=439200 of ranks=499472rank=439300 of ranks=499472rank=439400 of ranks=499472rank=439500 of ranks=499472rank=439600 of ranks=499472rank=439700 of ranks=499472rank=439800 of ranks=499472rank=439900 of ranks=499472rank=440000 of ranks=499472rank=440100 of ranks=499472rank=440200 of ranks=499472rank=440300 of ranks=499472rank=440400 of ranks=499472rank=440500 of ranks=499472rank=440600 of ranks=499472rank=440700 of ranks=499472rank=440800 of ranks=499472rank=440900 of ranks=499472rank=441000 of ranks=499472rank=441100 of ranks=499472rank=441200 of ranks=499472rank=441300 of ranks=499472rank=441400 of ranks=499472rank=441500 of ranks=499472rank=441600 of ranks=499472rank=441700 of ranks=499472rank=441800 of ranks=499472rank=441900 of ranks=499472rank=442000 of ranks=499472rank=442100 of ranks=499472rank=442200 of ranks=499472rank=442300 of ranks=499472rank=442400 of ranks=499472rank=442500 of ranks=499472rank=442600 of ranks=499472rank=442700 of ranks=499472rank=442800 of ranks=499472rank=442900 of ranks=499472rank=443000 of ranks=499472rank=443100 of ranks=499472rank=443200 of ranks=499472rank=443300 of ranks=499472rank=443400 of ranks=499472rank=443500 of ranks=499472rank=443600 of ranks=499472rank=443700 of ranks=499472rank=443800 of ranks=499472rank=443900 of ranks=499472rank=444000 of ranks=499472rank=444100 of ranks=499472rank=444200 of ranks=499472rank=444300 of ranks=499472rank=444400 of ranks=499472rank=444500 of ranks=499472rank=444600 of ranks=499472rank=444700 of ranks=499472rank=444800 of ranks=499472rank=444900 of ranks=499472rank=445000 of ranks=499472rank=445100 of ranks=499472rank=445200 of ranks=499472rank=445300 of ranks=499472rank=445400 of ranks=499472rank=445500 of ranks=499472rank=445600 of ranks=499472rank=445700 of ranks=499472rank=445800 of ranks=499472rank=445900 of ranks=499472rank=446000 of ranks=499472rank=446100 of ranks=499472rank=446200 of ranks=499472rank=446300 of ranks=499472rank=446400 of ranks=499472rank=446500 of ranks=499472rank=446600 of ranks=499472rank=446700 of ranks=499472rank=446800 of ranks=499472rank=446900 of ranks=499472rank=447000 of ranks=499472rank=447100 of ranks=499472rank=447200 of ranks=499472rank=447300 of ranks=499472rank=447400 of ranks=499472rank=447500 of ranks=499472rank=447600 of ranks=499472rank=447700 of ranks=499472rank=447800 of ranks=499472rank=447900 of ranks=499472rank=448000 of ranks=499472rank=448100 of ranks=499472rank=448200 of ranks=499472rank=448300 of ranks=499472rank=448400 of ranks=499472rank=448500 of ranks=499472rank=448600 of ranks=499472rank=448700 of ranks=499472rank=448800 of ranks=499472rank=448900 of ranks=499472rank=449000 of ranks=499472rank=449100 of ranks=499472rank=449200 of ranks=499472rank=449300 of ranks=499472rank=449400 of ranks=499472rank=449500 of ranks=499472rank=449600 of ranks=499472rank=449700 of ranks=499472rank=449800 of ranks=499472rank=449900 of ranks=499472rank=450000 of ranks=499472rank=450100 of ranks=499472rank=450200 of ranks=499472rank=450300 of ranks=499472rank=450400 of ranks=499472rank=450500 of ranks=499472rank=450600 of ranks=499472rank=450700 of ranks=499472rank=450800 of ranks=499472rank=450900 of ranks=499472rank=451000 of ranks=499472rank=451100 of ranks=499472rank=451200 of ranks=499472rank=451300 of ranks=499472rank=451400 of ranks=499472rank=451500 of ranks=499472rank=451600 of ranks=499472rank=451700 of ranks=499472rank=451800 of ranks=499472rank=451900 of ranks=499472rank=452000 of ranks=499472rank=452100 of ranks=499472rank=452200 of ranks=499472rank=452300 of ranks=499472rank=452400 of ranks=499472rank=452500 of ranks=499472rank=452600 of ranks=499472rank=452700 of ranks=499472rank=452800 of ranks=499472rank=452900 of ranks=499472rank=453000 of ranks=499472rank=453100 of ranks=499472rank=453200 of ranks=499472rank=453300 of ranks=499472rank=453400 of ranks=499472rank=453500 of ranks=499472rank=453600 of ranks=499472rank=453700 of ranks=499472rank=453800 of ranks=499472rank=453900 of ranks=499472rank=454000 of ranks=499472rank=454100 of ranks=499472rank=454200 of ranks=499472rank=454300 of ranks=499472rank=454400 of ranks=499472rank=454500 of ranks=499472rank=454600 of ranks=499472rank=454700 of ranks=499472rank=454800 of ranks=499472rank=454900 of ranks=499472rank=455000 of ranks=499472rank=455100 of ranks=499472rank=455200 of ranks=499472rank=455300 of ranks=499472rank=455400 of ranks=499472rank=455500 of ranks=499472rank=455600 of ranks=499472rank=455700 of ranks=499472rank=455800 of ranks=499472rank=455900 of ranks=499472rank=456000 of ranks=499472rank=456100 of ranks=499472rank=456200 of ranks=499472rank=456300 of ranks=499472rank=456400 of ranks=499472rank=456500 of ranks=499472rank=456600 of ranks=499472rank=456700 of ranks=499472rank=456800 of ranks=499472rank=456900 of ranks=499472rank=457000 of ranks=499472rank=457100 of ranks=499472rank=457200 of ranks=499472rank=457300 of ranks=499472rank=457400 of ranks=499472rank=457500 of ranks=499472rank=457600 of ranks=499472rank=457700 of ranks=499472rank=457800 of ranks=499472rank=457900 of ranks=499472rank=458000 of ranks=499472rank=458100 of ranks=499472rank=458200 of ranks=499472rank=458300 of ranks=499472rank=458400 of ranks=499472rank=458500 of ranks=499472rank=458600 of ranks=499472rank=458700 of ranks=499472rank=458800 of ranks=499472rank=458900 of ranks=499472rank=459000 of ranks=499472rank=459100 of ranks=499472rank=459200 of ranks=499472rank=459300 of ranks=499472rank=459400 of ranks=499472rank=459500 of ranks=499472rank=459600 of ranks=499472rank=459700 of ranks=499472rank=459800 of ranks=499472rank=459900 of ranks=499472rank=460000 of ranks=499472rank=460100 of ranks=499472rank=460200 of ranks=499472rank=460300 of ranks=499472rank=460400 of ranks=499472rank=460500 of ranks=499472rank=460600 of ranks=499472rank=460700 of ranks=499472rank=460800 of ranks=499472rank=460900 of ranks=499472rank=461000 of ranks=499472rank=461100 of ranks=499472rank=461200 of ranks=499472rank=461300 of ranks=499472rank=461400 of ranks=499472rank=461500 of ranks=499472rank=461600 of ranks=499472rank=461700 of ranks=499472rank=461800 of ranks=499472rank=461900 of ranks=499472rank=462000 of ranks=499472rank=462100 of ranks=499472rank=462200 of ranks=499472rank=462300 of ranks=499472rank=462400 of ranks=499472rank=462500 of ranks=499472rank=462600 of ranks=499472rank=462700 of ranks=499472rank=462800 of ranks=499472rank=462900 of ranks=499472rank=463000 of ranks=499472rank=463100 of ranks=499472rank=463200 of ranks=499472rank=463300 of ranks=499472rank=463400 of ranks=499472rank=463500 of ranks=499472rank=463600 of ranks=499472rank=463700 of ranks=499472rank=463800 of ranks=499472rank=463900 of ranks=499472rank=464000 of ranks=499472rank=464100 of ranks=499472rank=464200 of ranks=499472rank=464300 of ranks=499472rank=464400 of ranks=499472rank=464500 of ranks=499472rank=464600 of ranks=499472rank=464700 of ranks=499472rank=464800 of ranks=499472rank=464900 of ranks=499472rank=465000 of ranks=499472rank=465100 of ranks=499472rank=465200 of ranks=499472rank=465300 of ranks=499472rank=465400 of ranks=499472rank=465500 of ranks=499472rank=465600 of ranks=499472rank=465700 of ranks=499472rank=465800 of ranks=499472rank=465900 of ranks=499472rank=466000 of ranks=499472rank=466100 of ranks=499472rank=466200 of ranks=499472rank=466300 of ranks=499472rank=466400 of ranks=499472rank=466500 of ranks=499472rank=466600 of ranks=499472rank=466700 of ranks=499472rank=466800 of ranks=499472rank=466900 of ranks=499472rank=467000 of ranks=499472rank=467100 of ranks=499472rank=467200 of ranks=499472rank=467300 of ranks=499472rank=467400 of ranks=499472rank=467500 of ranks=499472rank=467600 of ranks=499472rank=467700 of ranks=499472rank=467800 of ranks=499472rank=467900 of ranks=499472rank=468000 of ranks=499472rank=468100 of ranks=499472rank=468200 of ranks=499472rank=468300 of ranks=499472rank=468400 of ranks=499472rank=468500 of ranks=499472rank=468600 of ranks=499472rank=468700 of ranks=499472rank=468800 of ranks=499472rank=468900 of ranks=499472rank=469000 of ranks=499472rank=469100 of ranks=499472rank=469200 of ranks=499472rank=469300 of ranks=499472rank=469400 of ranks=499472rank=469500 of ranks=499472rank=469600 of ranks=499472rank=469700 of ranks=499472rank=469800 of ranks=499472rank=469900 of ranks=499472rank=470000 of ranks=499472rank=470100 of ranks=499472rank=470200 of ranks=499472rank=470300 of ranks=499472rank=470400 of ranks=499472rank=470500 of ranks=499472rank=470600 of ranks=499472rank=470700 of ranks=499472rank=470800 of ranks=499472rank=470900 of ranks=499472rank=471000 of ranks=499472rank=471100 of ranks=499472rank=471200 of ranks=499472rank=471300 of ranks=499472rank=471400 of ranks=499472rank=471500 of ranks=499472rank=471600 of ranks=499472rank=471700 of ranks=499472rank=471800 of ranks=499472rank=471900 of ranks=499472rank=472000 of ranks=499472rank=472100 of ranks=499472rank=472200 of ranks=499472rank=472300 of ranks=499472rank=472400 of ranks=499472rank=472500 of ranks=499472rank=472600 of ranks=499472rank=472700 of ranks=499472rank=472800 of ranks=499472rank=472900 of ranks=499472rank=473000 of ranks=499472rank=473100 of ranks=499472rank=473200 of ranks=499472rank=473300 of ranks=499472rank=473400 of ranks=499472rank=473500 of ranks=499472rank=473600 of ranks=499472rank=473700 of ranks=499472rank=473800 of ranks=499472rank=473900 of ranks=499472rank=474000 of ranks=499472rank=474100 of ranks=499472rank=474200 of ranks=499472rank=474300 of ranks=499472rank=474400 of ranks=499472rank=474500 of ranks=499472rank=474600 of ranks=499472rank=474700 of ranks=499472rank=474800 of ranks=499472rank=474900 of ranks=499472rank=475000 of ranks=499472rank=475100 of ranks=499472rank=475200 of ranks=499472rank=475300 of ranks=499472rank=475400 of ranks=499472rank=475500 of ranks=499472rank=475600 of ranks=499472rank=475700 of ranks=499472rank=475800 of ranks=499472rank=475900 of ranks=499472rank=476000 of ranks=499472rank=476100 of ranks=499472rank=476200 of ranks=499472rank=476300 of ranks=499472rank=476400 of ranks=499472rank=476500 of ranks=499472rank=476600 of ranks=499472rank=476700 of ranks=499472rank=476800 of ranks=499472rank=476900 of ranks=499472rank=477000 of ranks=499472rank=477100 of ranks=499472rank=477200 of ranks=499472rank=477300 of ranks=499472rank=477400 of ranks=499472rank=477500 of ranks=499472rank=477600 of ranks=499472rank=477700 of ranks=499472rank=477800 of ranks=499472rank=477900 of ranks=499472rank=478000 of ranks=499472rank=478100 of ranks=499472rank=478200 of ranks=499472rank=478300 of ranks=499472rank=478400 of ranks=499472rank=478500 of ranks=499472rank=478600 of ranks=499472rank=478700 of ranks=499472rank=478800 of ranks=499472rank=478900 of ranks=499472rank=479000 of ranks=499472rank=479100 of ranks=499472rank=479200 of ranks=499472rank=479300 of ranks=499472rank=479400 of ranks=499472rank=479500 of ranks=499472rank=479600 of ranks=499472rank=479700 of ranks=499472rank=479800 of ranks=499472rank=479900 of ranks=499472rank=480000 of ranks=499472rank=480100 of ranks=499472rank=480200 of ranks=499472rank=480300 of ranks=499472rank=480400 of ranks=499472rank=480500 of ranks=499472rank=480600 of ranks=499472rank=480700 of ranks=499472rank=480800 of ranks=499472rank=480900 of ranks=499472rank=481000 of ranks=499472rank=481100 of ranks=499472rank=481200 of ranks=499472rank=481300 of ranks=499472rank=481400 of ranks=499472rank=481500 of ranks=499472rank=481600 of ranks=499472rank=481700 of ranks=499472rank=481800 of ranks=499472rank=481900 of ranks=499472rank=482000 of ranks=499472rank=482100 of ranks=499472rank=482200 of ranks=499472rank=482300 of ranks=499472rank=482400 of ranks=499472rank=482500 of ranks=499472rank=482600 of ranks=499472rank=482700 of ranks=499472rank=482800 of ranks=499472rank=482900 of ranks=499472rank=483000 of ranks=499472rank=483100 of ranks=499472rank=483200 of ranks=499472rank=483300 of ranks=499472rank=483400 of ranks=499472rank=483500 of ranks=499472rank=483600 of ranks=499472rank=483700 of ranks=499472rank=483800 of ranks=499472rank=483900 of ranks=499472rank=484000 of ranks=499472rank=484100 of ranks=499472rank=484200 of ranks=499472rank=484300 of ranks=499472rank=484400 of ranks=499472rank=484500 of ranks=499472rank=484600 of ranks=499472rank=484700 of ranks=499472rank=484800 of ranks=499472rank=484900 of ranks=499472rank=485000 of ranks=499472rank=485100 of ranks=499472rank=485200 of ranks=499472rank=485300 of ranks=499472rank=485400 of ranks=499472rank=485500 of ranks=499472rank=485600 of ranks=499472rank=485700 of ranks=499472rank=485800 of ranks=499472rank=485900 of ranks=499472rank=486000 of ranks=499472rank=486100 of ranks=499472rank=486200 of ranks=499472rank=486300 of ranks=499472rank=486400 of ranks=499472rank=486500 of ranks=499472rank=486600 of ranks=499472rank=486700 of ranks=499472rank=486800 of ranks=499472rank=486900 of ranks=499472rank=487000 of ranks=499472rank=487100 of ranks=499472rank=487200 of ranks=499472rank=487300 of ranks=499472rank=487400 of ranks=499472rank=487500 of ranks=499472rank=487600 of ranks=499472rank=487700 of ranks=499472rank=487800 of ranks=499472rank=487900 of ranks=499472rank=488000 of ranks=499472rank=488100 of ranks=499472rank=488200 of ranks=499472rank=488300 of ranks=499472rank=488400 of ranks=499472rank=488500 of ranks=499472rank=488600 of ranks=499472rank=488700 of ranks=499472rank=488800 of ranks=499472rank=488900 of ranks=499472rank=489000 of ranks=499472rank=489100 of ranks=499472rank=489200 of ranks=499472rank=489300 of ranks=499472rank=489400 of ranks=499472rank=489500 of ranks=499472rank=489600 of ranks=499472rank=489700 of ranks=499472rank=489800 of ranks=499472rank=489900 of ranks=499472rank=490000 of ranks=499472rank=490100 of ranks=499472rank=490200 of ranks=499472rank=490300 of ranks=499472rank=490400 of ranks=499472rank=490500 of ranks=499472rank=490600 of ranks=499472rank=490700 of ranks=499472rank=490800 of ranks=499472rank=490900 of ranks=499472rank=491000 of ranks=499472rank=491100 of ranks=499472rank=491200 of ranks=499472rank=491300 of ranks=499472rank=491400 of ranks=499472rank=491500 of ranks=499472rank=491600 of ranks=499472rank=491700 of ranks=499472rank=491800 of ranks=499472rank=491900 of ranks=499472rank=492000 of ranks=499472rank=492100 of ranks=499472rank=492200 of ranks=499472rank=492300 of ranks=499472rank=492400 of ranks=499472rank=492500 of ranks=499472rank=492600 of ranks=499472rank=492700 of ranks=499472rank=492800 of ranks=499472rank=492900 of ranks=499472rank=493000 of ranks=499472rank=493100 of ranks=499472rank=493200 of ranks=499472rank=493300 of ranks=499472rank=493400 of ranks=499472rank=493500 of ranks=499472rank=493600 of ranks=499472rank=493700 of ranks=499472rank=493800 of ranks=499472rank=493900 of ranks=499472rank=494000 of ranks=499472rank=494100 of ranks=499472rank=494200 of ranks=499472rank=494300 of ranks=499472rank=494400 of ranks=499472rank=494500 of ranks=499472rank=494600 of ranks=499472rank=494700 of ranks=499472rank=494800 of ranks=499472rank=494900 of ranks=499472rank=495000 of ranks=499472rank=495100 of ranks=499472rank=495200 of ranks=499472rank=495300 of ranks=499472rank=495400 of ranks=499472rank=495500 of ranks=499472rank=495600 of ranks=499472rank=495700 of ranks=499472rank=495800 of ranks=499472rank=495900 of ranks=499472rank=496000 of ranks=499472rank=496100 of ranks=499472rank=496200 of ranks=499472rank=496300 of ranks=499472rank=496400 of ranks=499472rank=496500 of ranks=499472rank=496600 of ranks=499472rank=496700 of ranks=499472rank=496800 of ranks=499472rank=496900 of ranks=499472rank=497000 of ranks=499472rank=497100 of ranks=499472rank=497200 of ranks=499472rank=497300 of ranks=499472rank=497400 of ranks=499472rank=497500 of ranks=499472rank=497600 of ranks=499472rank=497700 of ranks=499472rank=497800 of ranks=499472rank=497900 of ranks=499472rank=498000 of ranks=499472rank=498100 of ranks=499472rank=498200 of ranks=499472rank=498300 of ranks=499472rank=498400 of ranks=499472rank=498500 of ranks=499472rank=498600 of ranks=499472rank=498700 of ranks=499472rank=498800 of ranks=499472rank=498900 of ranks=499472rank=499000 of ranks=499472rank=499100 of ranks=499472rank=499200 of ranks=499472rank=499300 of ranks=499472rank=499400 of ranks=499472

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              21.2985    359  52523    139    498            0.0000
   1 car                    84.7641  47465 175632   2851  50316           55.8562
   2 truck                  36.9817   1563  80164    262   1825           32.2183
   3 bus                     7.5541    274  46253     92    366            0.0000
   4 pedestrian             39.7229   3067  92172   1192   4259           33.9793

for conf_thresh=0.25, precision=0.72, recall=0.73, F1 score=0.73
for conf_thresh=0.25, TP=42054, FP=16344, FN=15210, average IoU=54.14%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=38.06%
Total detection time: 225 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Saving weights to /workspace/.cache/splits/combined_1000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145fa2000000
1001: loss=13.302, avg loss=12.445, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 64064 images, time remaining=6.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1002: loss=12.199, avg loss=12.420, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 64128 images, time remaining=6.6 hours
1003: loss=11.905, avg loss=12.368, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 64192 images, time remaining=6.6 hours
1004: loss=12.088, avg loss=12.340, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 64256 images, time remaining=6.5 hours
1005: loss=11.692, avg loss=12.276, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 64320 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1006: loss=11.069, avg loss=12.155, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 64384 images, time remaining=6.5 hours
1007: loss=11.716, avg loss=12.111, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 64448 images, time remaining=6.5 hours
1008: loss=12.802, avg loss=12.180, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 64512 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1009: loss=11.206, avg loss=12.083, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.1 seconds, train=1.6 seconds, 64576 images, time remaining=6.5 hours
1010: loss=13.417, avg loss=12.216, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 64640 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145fa2000000
1011: loss=12.608, avg loss=12.255, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=808.3 milliseconds, train=1.6 seconds, 64704 images, time remaining=6.5 hours
1012: loss=10.299, avg loss=12.060, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 64768 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1013: loss=11.286, avg loss=11.982, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.9 seconds, train=1.6 seconds, 64832 images, time remaining=6.5 hours
1014: loss=10.906, avg loss=11.875, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 64896 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1015: loss=12.226, avg loss=11.910, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.3 seconds, train=1.6 seconds, 64960 images, time remaining=6.5 hours
1016: loss=12.818, avg loss=12.001, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=671.3 milliseconds, train=1.7 seconds, 65024 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1017: loss=12.938, avg loss=12.094, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.2 seconds, train=1.6 seconds, 65088 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1018: loss=12.013, avg loss=12.086, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.9 seconds, train=1.6 seconds, 65152 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1019: loss=11.382, avg loss=12.016, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 65216 images, time remaining=6.5 hours
1020: loss=11.745, avg loss=11.989, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 65280 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1021: loss=14.973, avg loss=12.287, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 65344 images, time remaining=6.5 hours
1022: loss=13.342, avg loss=12.393, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=4.6 seconds, 65408 images, time remaining=6.5 hours
1023: loss=10.700, avg loss=12.223, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.3 seconds, train=4.6 seconds, 65472 images, time remaining=6.5 hours
1024: loss=12.953, avg loss=12.296, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=829.6 milliseconds, train=4.6 seconds, 65536 images, time remaining=6.5 hours
1025: loss=13.053, avg loss=12.372, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 65600 images, time remaining=6.5 hours
1026: loss=12.442, avg loss=12.379, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 65664 images, time remaining=6.5 hours
1027: loss=14.910, avg loss=12.632, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=4.7 seconds, 65728 images, time remaining=6.5 hours
1028: loss=15.659, avg loss=12.935, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=769.6 milliseconds, train=4.7 seconds, 65792 images, time remaining=6.5 hours
1029: loss=12.717, avg loss=12.913, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 65856 images, time remaining=6.5 hours
1030: loss=12.008, avg loss=12.823, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=672.4 milliseconds, train=4.6 seconds, 65920 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1031: loss=11.764, avg loss=12.717, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=885.1 milliseconds, train=4.5 seconds, 65984 images, time remaining=6.5 hours
1032: loss=13.748, avg loss=12.820, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=4.5 seconds, 66048 images, time remaining=6.5 hours
1033: loss=12.646, avg loss=12.802, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.6 seconds, train=4.5 seconds, 66112 images, time remaining=6.5 hours
1034: loss=12.783, avg loss=12.801, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=882.2 milliseconds, train=4.5 seconds, 66176 images, time remaining=6.5 hours
1035: loss=12.312, avg loss=12.752, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=4.5 seconds, 66240 images, time remaining=6.5 hours
1036: loss=10.881, avg loss=12.565, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=881.6 milliseconds, train=4.5 seconds, 66304 images, time remaining=6.5 hours
1037: loss=11.809, avg loss=12.489, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 66368 images, time remaining=6.5 hours
1038: loss=12.463, avg loss=12.486, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=4.5 seconds, 66432 images, time remaining=6.5 hours
1039: loss=11.574, avg loss=12.395, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=4.5 seconds, 66496 images, time remaining=6.5 hours
1040: loss=11.493, avg loss=12.305, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=951.6 milliseconds, train=4.5 seconds, 66560 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1041: loss=10.377, avg loss=12.112, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=5.0 seconds, 66624 images, time remaining=6.6 hours
1042: loss=10.786, avg loss=11.980, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=846.0 milliseconds, train=4.9 seconds, 66688 images, time remaining=6.6 hours
1043: loss=11.016, avg loss=11.883, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=5.0 seconds, 66752 images, time remaining=6.6 hours
1044: loss=11.389, avg loss=11.834, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=5.0 seconds, 66816 images, time remaining=6.6 hours
1045: loss=14.459, avg loss=12.096, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=5.0 seconds, 66880 images, time remaining=6.6 hours
1046: loss=12.187, avg loss=12.105, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.4 seconds, train=4.9 seconds, 66944 images, time remaining=6.6 hours
1047: loss=12.221, avg loss=12.117, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=4.9 seconds, 67008 images, time remaining=6.6 hours
1048: loss=12.289, avg loss=12.134, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=800.0 milliseconds, train=4.9 seconds, 67072 images, time remaining=6.6 hours
1049: loss=12.059, avg loss=12.127, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 67136 images, time remaining=6.6 hours
1050: loss=12.973, avg loss=12.211, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=5.0 seconds, 67200 images, time remaining=6.6 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1051: loss=11.252, avg loss=12.115, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 67264 images, time remaining=6.6 hours
1052: loss=14.540, avg loss=12.358, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.3 seconds, train=4.6 seconds, 67328 images, time remaining=6.6 hours
1053: loss=12.181, avg loss=12.340, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 67392 images, time remaining=6.6 hours
1054: loss=10.632, avg loss=12.169, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 67456 images, time remaining=6.6 hours
1055: loss=12.154, avg loss=12.168, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 67520 images, time remaining=6.6 hours
1056: loss=12.635, avg loss=12.214, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.3 seconds, train=4.6 seconds, 67584 images, time remaining=6.6 hours
1057: loss=10.838, avg loss=12.077, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=737.4 milliseconds, train=4.6 seconds, 67648 images, time remaining=6.6 hours
1058: loss=10.908, avg loss=11.960, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=4.6 seconds, 67712 images, time remaining=6.6 hours
1059: loss=10.407, avg loss=11.805, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=817.1 milliseconds, train=4.6 seconds, 67776 images, time remaining=6.6 hours
1060: loss=12.226, avg loss=11.847, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 67840 images, time remaining=6.6 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1061: loss=10.397, avg loss=11.702, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 67904 images, time remaining=6.6 hours
1062: loss=11.098, avg loss=11.641, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 67968 images, time remaining=6.6 hours
1063: loss=11.228, avg loss=11.600, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=3.9 seconds, 68032 images, time remaining=6.6 hours
1064: loss=11.416, avg loss=11.582, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 68096 images, time remaining=6.6 hours
1065: loss=12.191, avg loss=11.643, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 68160 images, time remaining=6.6 hours
1066: loss=11.397, avg loss=11.618, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=842.1 milliseconds, train=3.9 seconds, 68224 images, time remaining=6.6 hours
1067: loss=11.788, avg loss=11.635, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 68288 images, time remaining=6.6 hours
1068: loss=10.147, avg loss=11.486, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 68352 images, time remaining=6.6 hours
1069: loss=10.243, avg loss=11.362, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=704.9 milliseconds, train=3.9 seconds, 68416 images, time remaining=6.6 hours
1070: loss=9.544, avg loss=11.180, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=682.3 milliseconds, train=3.9 seconds, 68480 images, time remaining=6.6 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1071: loss=10.953, avg loss=11.157, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.0 seconds, train=3.9 seconds, 68544 images, time remaining=6.6 hours
1072: loss=11.503, avg loss=11.192, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=742.8 milliseconds, train=3.9 seconds, 68608 images, time remaining=6.6 hours
1073: loss=10.973, avg loss=11.170, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 68672 images, time remaining=6.6 hours
1074: loss=10.141, avg loss=11.067, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.8 seconds, train=3.9 seconds, 68736 images, time remaining=6.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1075: loss=11.734, avg loss=11.134, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.0 seconds, train=3.9 seconds, 68800 images, time remaining=6.6 hours
1076: loss=9.945, avg loss=11.015, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 68864 images, time remaining=6.6 hours
1077: loss=10.853, avg loss=10.999, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.3 seconds, train=3.9 seconds, 68928 images, time remaining=6.6 hours
1078: loss=10.859, avg loss=10.985, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=643.4 milliseconds, train=3.9 seconds, 68992 images, time remaining=6.6 hours
1079: loss=14.178, avg loss=11.304, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 69056 images, time remaining=6.6 hours
1080: loss=11.095, avg loss=11.283, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=902.6 milliseconds, train=3.9 seconds, 69120 images, time remaining=6.6 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1463bc000000
1081: loss=10.809, avg loss=11.236, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=668.2 milliseconds, train=1.6 seconds, 69184 images, time remaining=6.6 hours
1082: loss=12.812, avg loss=11.393, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 69248 images, time remaining=6.6 hours
1083: loss=9.995, avg loss=11.254, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 69312 images, time remaining=6.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1084: loss=12.015, avg loss=11.330, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=1.6 seconds, 69376 images, time remaining=6.6 hours
1085: loss=10.391, avg loss=11.236, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=892.4 milliseconds, train=1.6 seconds, 69440 images, time remaining=6.6 hours
1086: loss=10.654, avg loss=11.178, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 69504 images, time remaining=6.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1087: loss=9.998, avg loss=11.060, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 69568 images, time remaining=6.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1088: loss=12.598, avg loss=11.214, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 69632 images, time remaining=6.6 hours
1089: loss=9.755, avg loss=11.068, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 69696 images, time remaining=6.6 hours
1090: loss=10.307, avg loss=10.992, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 69760 images, time remaining=6.6 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463be000000
1091: loss=11.172, avg loss=11.010, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 69824 images, time remaining=6.6 hours
1092: loss=11.282, avg loss=11.037, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 69888 images, time remaining=6.6 hours
1093: loss=10.083, avg loss=10.941, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 69952 images, time remaining=6.5 hours
1094: loss=11.227, avg loss=10.970, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=414.6 milliseconds, train=1.5 seconds, 70016 images, time remaining=6.5 hours
1095: loss=9.829, avg loss=10.856, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 70080 images, time remaining=6.5 hours
1096: loss=10.537, avg loss=10.824, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=724.6 milliseconds, train=1.5 seconds, 70144 images, time remaining=6.5 hours
1097: loss=9.223, avg loss=10.664, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=688.1 milliseconds, train=1.5 seconds, 70208 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1098: loss=10.260, avg loss=10.623, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 70272 images, time remaining=6.5 hours
1099: loss=10.489, avg loss=10.610, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 70336 images, time remaining=6.5 hours
1100: loss=9.756, avg loss=10.525, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 70400 images, time remaining=6.5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1101: loss=13.805, avg loss=10.853, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 70464 images, time remaining=6.5 hours
1102: loss=10.632, avg loss=10.831, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 70528 images, time remaining=6.5 hours
1103: loss=11.680, avg loss=10.915, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 70592 images, time remaining=6.5 hours
1104: loss=12.899, avg loss=11.114, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=687.2 milliseconds, train=3.9 seconds, 70656 images, time remaining=6.5 hours
1105: loss=10.549, avg loss=11.057, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 70720 images, time remaining=6.5 hours
1106: loss=10.223, avg loss=10.974, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=759.7 milliseconds, train=3.9 seconds, 70784 images, time remaining=6.5 hours
1107: loss=11.752, avg loss=11.052, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=621.7 milliseconds, train=3.9 seconds, 70848 images, time remaining=6.5 hours
1108: loss=10.389, avg loss=10.985, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 70912 images, time remaining=6.5 hours
1109: loss=11.242, avg loss=11.011, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 70976 images, time remaining=6.5 hours
1110: loss=11.704, avg loss=11.080, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=812.8 milliseconds, train=3.9 seconds, 71040 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1464ae000000
1111: loss=11.775, avg loss=11.150, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=591.9 milliseconds, train=1.5 seconds, 71104 images, time remaining=6.5 hours
1112: loss=11.833, avg loss=11.218, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 71168 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1113: loss=10.241, avg loss=11.121, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.0 seconds, train=1.5 seconds, 71232 images, time remaining=6.5 hours
1114: loss=11.661, avg loss=11.175, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 71296 images, time remaining=6.5 hours
1115: loss=12.416, avg loss=11.299, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=943.9 milliseconds, train=1.5 seconds, 71360 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1116: loss=11.179, avg loss=11.287, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 71424 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1117: loss=12.834, avg loss=11.441, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 71488 images, time remaining=6.5 hours
1118: loss=10.166, avg loss=11.314, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=769.8 milliseconds, train=1.5 seconds, 71552 images, time remaining=6.5 hours
1119: loss=10.384, avg loss=11.221, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=947.2 milliseconds, train=1.5 seconds, 71616 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1120: loss=12.316, avg loss=11.330, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 71680 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x145ffb000000
1121: loss=9.980, avg loss=11.195, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.1 seconds, 71744 images, time remaining=6.5 hours
1122: loss=10.237, avg loss=11.099, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=915.4 milliseconds, train=1.1 seconds, 71808 images, time remaining=6.5 hours
1123: loss=11.412, avg loss=11.131, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.1 seconds, 71872 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1124: loss=10.553, avg loss=11.073, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.8 seconds, train=1.1 seconds, 71936 images, time remaining=6.5 hours
1125: loss=11.159, avg loss=11.082, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=466.4 milliseconds, train=1.1 seconds, 72000 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1126: loss=9.819, avg loss=10.955, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=1.1 seconds, 72064 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1127: loss=10.425, avg loss=10.902, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=1.1 seconds, 72128 images, time remaining=6.5 hours
1128: loss=10.576, avg loss=10.870, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=723.6 milliseconds, train=1.1 seconds, 72192 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1129: loss=9.622, avg loss=10.745, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.6 seconds, train=1.1 seconds, 72256 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1130: loss=10.146, avg loss=10.685, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.1 seconds, 72320 images, time remaining=6.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1464ca000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1131: loss=10.012, avg loss=10.618, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 72384 images, time remaining=6.5 hours
1132: loss=9.411, avg loss=10.497, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=671.7 milliseconds, train=1.2 seconds, 72448 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1133: loss=10.399, avg loss=10.487, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.1 seconds, train=1.2 seconds, 72512 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1134: loss=10.072, avg loss=10.446, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 72576 images, time remaining=6.5 hours
1135: loss=9.538, avg loss=10.355, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=809.3 milliseconds, train=1.2 seconds, 72640 images, time remaining=6.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1136: loss=9.822, avg loss=10.302, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 72704 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1137: loss=9.109, avg loss=10.182, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 72768 images, time remaining=6.4 hours
1138: loss=9.250, avg loss=10.089, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=927.4 milliseconds, train=1.2 seconds, 72832 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1139: loss=10.122, avg loss=10.092, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.2 seconds, train=1.2 seconds, 72896 images, time remaining=6.4 hours
1140: loss=8.914, avg loss=9.975, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 72960 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x14499e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1141: loss=12.067, avg loss=10.184, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=2.0 seconds, 73024 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1142: loss=8.598, avg loss=10.025, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=2.0 seconds, 73088 images, time remaining=6.4 hours
1143: loss=10.380, avg loss=10.061, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 73152 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1144: loss=9.577, avg loss=10.012, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=2.0 seconds, 73216 images, time remaining=6.4 hours
1145: loss=10.256, avg loss=10.037, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=590.7 milliseconds, train=2.0 seconds, 73280 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1146: loss=8.994, avg loss=9.932, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.8 seconds, train=2.0 seconds, 73344 images, time remaining=6.4 hours
1147: loss=10.387, avg loss=9.978, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 73408 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1148: loss=10.193, avg loss=9.999, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=5.0 seconds, train=2.0 seconds, 73472 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1149: loss=9.718, avg loss=9.971, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=2.0 seconds, 73536 images, time remaining=6.4 hours
1150: loss=9.417, avg loss=9.916, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=506.4 milliseconds, train=2.0 seconds, 73600 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x14499e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1151: loss=9.960, avg loss=9.920, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.4 seconds, train=2.0 seconds, 73664 images, time remaining=6.4 hours
1152: loss=10.874, avg loss=10.015, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 73728 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1153: loss=9.092, avg loss=9.923, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=2.0 seconds, 73792 images, time remaining=6.4 hours
1154: loss=10.384, avg loss=9.969, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 73856 images, time remaining=6.4 hours
1155: loss=8.302, avg loss=9.803, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 73920 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1156: loss=7.727, avg loss=9.595, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 73984 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1157: loss=10.153, avg loss=9.651, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=2.0 seconds, 74048 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1158: loss=8.980, avg loss=9.584, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.4 seconds, train=2.0 seconds, 74112 images, time remaining=6.4 hours
1159: loss=9.332, avg loss=9.559, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=838.8 milliseconds, train=2.0 seconds, 74176 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1160: loss=8.154, avg loss=9.418, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=2.0 seconds, 74240 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1161: loss=9.384, avg loss=9.415, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 74304 images, time remaining=6.4 hours
1162: loss=9.325, avg loss=9.406, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 74368 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1163: loss=10.577, avg loss=9.523, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=11.2 seconds, train=3.4 seconds, 74432 images, time remaining=6.4 hours
1164: loss=9.428, avg loss=9.513, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.2 seconds, train=3.4 seconds, 74496 images, time remaining=6.4 hours
1165: loss=9.506, avg loss=9.513, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 74560 images, time remaining=6.4 hours
1166: loss=9.261, avg loss=9.488, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=755.7 milliseconds, train=3.4 seconds, 74624 images, time remaining=6.4 hours
1167: loss=9.183, avg loss=9.457, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=3.4 seconds, 74688 images, time remaining=6.4 hours
1168: loss=9.120, avg loss=9.423, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=787.3 milliseconds, train=3.4 seconds, 74752 images, time remaining=6.4 hours
1169: loss=9.123, avg loss=9.393, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 74816 images, time remaining=6.4 hours
1170: loss=11.006, avg loss=9.555, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.4 seconds, train=3.4 seconds, 74880 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145b94000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1171: loss=9.574, avg loss=9.557, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 74944 images, time remaining=6.4 hours
1172: loss=9.287, avg loss=9.530, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=671.6 milliseconds, train=1.4 seconds, 75008 images, time remaining=6.4 hours
1173: loss=9.564, avg loss=9.533, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 75072 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1174: loss=8.936, avg loss=9.473, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 75136 images, time remaining=6.4 hours
1175: loss=10.395, avg loss=9.566, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=865.8 milliseconds, train=1.4 seconds, 75200 images, time remaining=6.4 hours
1176: loss=9.537, avg loss=9.563, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=810.0 milliseconds, train=1.4 seconds, 75264 images, time remaining=6.4 hours
1177: loss=9.848, avg loss=9.591, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=655.6 milliseconds, train=1.5 seconds, 75328 images, time remaining=6.4 hours
1178: loss=8.165, avg loss=9.449, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=406.3 milliseconds, train=1.4 seconds, 75392 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1179: loss=9.039, avg loss=9.408, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=1.4 seconds, 75456 images, time remaining=6.4 hours
1180: loss=8.476, avg loss=9.314, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 75520 images, time remaining=6.4 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145b96000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1181: loss=8.548, avg loss=9.238, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 75584 images, time remaining=6.4 hours
1182: loss=10.065, avg loss=9.320, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=691.1 milliseconds, train=1.4 seconds, 75648 images, time remaining=6.4 hours
1183: loss=9.181, avg loss=9.307, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=611.6 milliseconds, train=1.4 seconds, 75712 images, time remaining=6.4 hours
1184: loss=8.958, avg loss=9.272, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=767.4 milliseconds, train=1.4 seconds, 75776 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1185: loss=8.122, avg loss=9.157, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 75840 images, time remaining=6.4 hours
1186: loss=8.980, avg loss=9.139, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 75904 images, time remaining=6.4 hours
1187: loss=8.854, avg loss=9.111, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=719.5 milliseconds, train=1.4 seconds, 75968 images, time remaining=6.4 hours
1188: loss=7.378, avg loss=8.937, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=594.8 milliseconds, train=1.4 seconds, 76032 images, time remaining=6.4 hours
1189: loss=9.116, avg loss=8.955, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 76096 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1190: loss=9.204, avg loss=8.980, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 76160 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14468c000000
1191: loss=14.926, avg loss=9.575, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 76224 images, time remaining=6.4 hours
1192: loss=12.820, avg loss=9.899, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=4.9 seconds, 76288 images, time remaining=6.4 hours
1193: loss=12.602, avg loss=10.169, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=4.8 seconds, 76352 images, time remaining=6.4 hours
1194: loss=11.699, avg loss=10.322, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=612.9 milliseconds, train=4.8 seconds, 76416 images, time remaining=6.4 hours
1195: loss=11.041, avg loss=10.394, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=4.9 seconds, 76480 images, time remaining=6.4 hours
1196: loss=12.655, avg loss=10.620, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=4.9 seconds, 76544 images, time remaining=6.4 hours
1197: loss=12.918, avg loss=10.850, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.5 seconds, train=4.8 seconds, 76608 images, time remaining=6.4 hours
1198: loss=11.952, avg loss=10.960, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=726.5 milliseconds, train=4.8 seconds, 76672 images, time remaining=6.4 hours
1199: loss=12.688, avg loss=11.133, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=4.8 seconds, 76736 images, time remaining=6.4 hours
1200: loss=12.072, avg loss=11.227, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=4.8 seconds, 76800 images, time remaining=6.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x1449a6000000
1201: loss=12.772, avg loss=11.382, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 76864 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1202: loss=11.276, avg loss=11.371, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 76928 images, time remaining=6.4 hours
1203: loss=11.166, avg loss=11.351, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=488.9 milliseconds, train=1.5 seconds, 76992 images, time remaining=6.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1204: loss=10.787, avg loss=11.294, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 77056 images, time remaining=6.4 hours
1205: loss=11.513, avg loss=11.316, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 77120 images, time remaining=6.4 hours
1206: loss=10.202, avg loss=11.205, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 77184 images, time remaining=6.4 hours
1207: loss=11.583, avg loss=11.243, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=622.5 milliseconds, train=1.5 seconds, 77248 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1208: loss=9.682, avg loss=11.087, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 77312 images, time remaining=6.3 hours
1209: loss=9.468, avg loss=10.925, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=795.8 milliseconds, train=1.5 seconds, 77376 images, time remaining=6.3 hours
1210: loss=9.538, avg loss=10.786, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=661.2 milliseconds, train=1.6 seconds, 77440 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x1449a6000000
1211: loss=10.007, avg loss=10.708, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 77504 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1212: loss=10.316, avg loss=10.669, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 77568 images, time remaining=6.3 hours
1213: loss=9.251, avg loss=10.527, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=852.7 milliseconds, train=1.5 seconds, 77632 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1214: loss=9.154, avg loss=10.390, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 77696 images, time remaining=6.3 hours
1215: loss=10.335, avg loss=10.384, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 77760 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1216: loss=8.173, avg loss=10.163, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 77824 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1217: loss=9.561, avg loss=10.103, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 77888 images, time remaining=6.3 hours
1218: loss=9.379, avg loss=10.030, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=678.9 milliseconds, train=1.5 seconds, 77952 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1219: loss=8.622, avg loss=9.890, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 78016 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1220: loss=8.477, avg loss=9.748, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 78080 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1464c4000000
1221: loss=9.663, avg loss=9.740, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=821.2 milliseconds, train=1.5 seconds, 78144 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1222: loss=10.804, avg loss=9.846, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 78208 images, time remaining=6.3 hours
1223: loss=8.119, avg loss=9.674, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 78272 images, time remaining=6.3 hours
1224: loss=10.010, avg loss=9.707, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=778.9 milliseconds, train=1.5 seconds, 78336 images, time remaining=6.3 hours
1225: loss=9.425, avg loss=9.679, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=886.6 milliseconds, train=1.5 seconds, 78400 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1226: loss=9.603, avg loss=9.671, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 78464 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1227: loss=7.510, avg loss=9.455, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 78528 images, time remaining=6.3 hours
1228: loss=8.600, avg loss=9.370, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 78592 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1229: loss=9.027, avg loss=9.335, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 78656 images, time remaining=6.3 hours
1230: loss=9.161, avg loss=9.318, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 78720 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1231: loss=9.655, avg loss=9.352, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=820.7 milliseconds, train=2.1 seconds, 78784 images, time remaining=6.3 hours
1232: loss=9.522, avg loss=9.369, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 78848 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1233: loss=8.868, avg loss=9.319, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 78912 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1234: loss=8.411, avg loss=9.228, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 78976 images, time remaining=6.3 hours
1235: loss=8.838, avg loss=9.189, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=701.5 milliseconds, train=2.1 seconds, 79040 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1236: loss=9.423, avg loss=9.212, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 79104 images, time remaining=6.3 hours
1237: loss=8.810, avg loss=9.172, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 79168 images, time remaining=6.3 hours
1238: loss=8.985, avg loss=9.153, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=786.1 milliseconds, train=2.1 seconds, 79232 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1239: loss=8.924, avg loss=9.130, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 79296 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1240: loss=8.938, avg loss=9.111, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 79360 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1241: loss=10.942, avg loss=9.294, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 79424 images, time remaining=6.3 hours
1242: loss=9.971, avg loss=9.362, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=666.5 milliseconds, train=4.4 seconds, 79488 images, time remaining=6.3 hours
1243: loss=7.928, avg loss=9.219, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.3 seconds, train=4.4 seconds, 79552 images, time remaining=6.3 hours
1244: loss=10.585, avg loss=9.355, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=917.5 milliseconds, train=4.4 seconds, 79616 images, time remaining=6.3 hours
1245: loss=10.799, avg loss=9.500, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 79680 images, time remaining=6.3 hours
1246: loss=8.760, avg loss=9.426, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 79744 images, time remaining=6.3 hours
1247: loss=11.288, avg loss=9.612, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=4.4 seconds, 79808 images, time remaining=6.3 hours
1248: loss=9.880, avg loss=9.639, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.5 seconds, train=4.4 seconds, 79872 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1249: loss=9.853, avg loss=9.660, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.6 seconds, train=4.4 seconds, 79936 images, time remaining=6.3 hours
1250: loss=10.784, avg loss=9.773, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=807.0 milliseconds, train=4.4 seconds, 80000 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145c06000000
1251: loss=9.415, avg loss=9.737, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=994.2 milliseconds, train=1.5 seconds, 80064 images, time remaining=6.3 hours
1252: loss=9.787, avg loss=9.742, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=502.6 milliseconds, train=1.5 seconds, 80128 images, time remaining=6.3 hours
1253: loss=7.841, avg loss=9.552, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=856.6 milliseconds, train=1.5 seconds, 80192 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1254: loss=10.232, avg loss=9.620, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 80256 images, time remaining=6.3 hours
1255: loss=9.655, avg loss=9.623, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=827.5 milliseconds, train=1.5 seconds, 80320 images, time remaining=6.3 hours
1256: loss=8.857, avg loss=9.547, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=702.6 milliseconds, train=1.5 seconds, 80384 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1257: loss=8.963, avg loss=9.488, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 80448 images, time remaining=6.3 hours
1258: loss=8.229, avg loss=9.362, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=636.2 milliseconds, train=1.5 seconds, 80512 images, time remaining=6.3 hours
1259: loss=8.995, avg loss=9.326, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=778.2 milliseconds, train=1.5 seconds, 80576 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1260: loss=8.048, avg loss=9.198, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 80640 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1261: loss=9.446, avg loss=9.223, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 80704 images, time remaining=6.3 hours
1262: loss=9.273, avg loss=9.228, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=953.3 milliseconds, train=3.4 seconds, 80768 images, time remaining=6.3 hours
1263: loss=7.191, avg loss=9.024, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 80832 images, time remaining=6.3 hours
1264: loss=8.604, avg loss=8.982, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 80896 images, time remaining=6.2 hours
1265: loss=10.642, avg loss=9.148, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 80960 images, time remaining=6.2 hours
1266: loss=9.310, avg loss=9.164, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 81024 images, time remaining=6.2 hours
1267: loss=9.426, avg loss=9.190, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=3.4 seconds, 81088 images, time remaining=6.2 hours
1268: loss=9.715, avg loss=9.243, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 81152 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1269: loss=9.803, avg loss=9.299, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.4 seconds, train=3.4 seconds, 81216 images, time remaining=6.2 hours
1270: loss=8.172, avg loss=9.186, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 81280 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1271: loss=10.250, avg loss=9.293, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 81344 images, time remaining=6.2 hours
1272: loss=9.114, avg loss=9.275, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 81408 images, time remaining=6.2 hours
1273: loss=9.295, avg loss=9.277, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=4.1 seconds, 81472 images, time remaining=6.2 hours
1274: loss=10.062, avg loss=9.355, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.1 seconds, train=4.1 seconds, 81536 images, time remaining=6.2 hours
1275: loss=7.710, avg loss=9.191, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=954.5 milliseconds, train=4.1 seconds, 81600 images, time remaining=6.2 hours
1276: loss=8.306, avg loss=9.102, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=4.1 seconds, 81664 images, time remaining=6.2 hours
1277: loss=9.822, avg loss=9.174, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 81728 images, time remaining=6.2 hours
1278: loss=9.683, avg loss=9.225, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=4.1 seconds, 81792 images, time remaining=6.3 hours
1279: loss=9.952, avg loss=9.298, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.2 seconds, train=4.1 seconds, 81856 images, time remaining=6.3 hours
1280: loss=8.162, avg loss=9.184, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=4.1 seconds, 81920 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1281: loss=8.895, avg loss=9.155, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 81984 images, time remaining=6.3 hours
1282: loss=10.488, avg loss=9.289, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 82048 images, time remaining=6.3 hours
1283: loss=10.098, avg loss=9.370, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=898.9 milliseconds, train=3.9 seconds, 82112 images, time remaining=6.3 hours
1284: loss=10.553, avg loss=9.488, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=929.8 milliseconds, train=3.9 seconds, 82176 images, time remaining=6.3 hours
1285: loss=9.678, avg loss=9.507, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=632.3 milliseconds, train=3.9 seconds, 82240 images, time remaining=6.3 hours
1286: loss=8.293, avg loss=9.385, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 82304 images, time remaining=6.3 hours
1287: loss=10.082, avg loss=9.455, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 82368 images, time remaining=6.3 hours
1288: loss=8.950, avg loss=9.405, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=3.9 seconds, 82432 images, time remaining=6.3 hours
1289: loss=9.052, avg loss=9.369, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 82496 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1290: loss=9.306, avg loss=9.363, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.2 seconds, train=3.9 seconds, 82560 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1291: loss=9.434, avg loss=9.370, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 82624 images, time remaining=6.3 hours
1292: loss=8.936, avg loss=9.327, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=667.6 milliseconds, train=2.1 seconds, 82688 images, time remaining=6.2 hours
1293: loss=7.955, avg loss=9.190, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=761.1 milliseconds, train=2.1 seconds, 82752 images, time remaining=6.2 hours
1294: loss=7.530, avg loss=9.024, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 82816 images, time remaining=6.2 hours
1295: loss=8.983, avg loss=9.020, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 82880 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1296: loss=7.117, avg loss=8.829, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 82944 images, time remaining=6.2 hours
1297: loss=9.643, avg loss=8.911, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=967.2 milliseconds, train=2.1 seconds, 83008 images, time remaining=6.2 hours
1298: loss=9.043, avg loss=8.924, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 83072 images, time remaining=6.2 hours
1299: loss=8.337, avg loss=8.865, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=695.7 milliseconds, train=2.1 seconds, 83136 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1300: loss=8.851, avg loss=8.864, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 83200 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1301: loss=8.240, avg loss=8.801, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=941.2 milliseconds, train=2.1 seconds, 83264 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1302: loss=8.444, avg loss=8.766, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.4 seconds, train=2.1 seconds, 83328 images, time remaining=6.2 hours
1303: loss=8.104, avg loss=8.700, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=978.4 milliseconds, train=2.1 seconds, 83392 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1304: loss=8.818, avg loss=8.711, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.6 seconds, train=2.1 seconds, 83456 images, time remaining=6.2 hours
1305: loss=7.096, avg loss=8.550, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 83520 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1306: loss=9.237, avg loss=8.619, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 83584 images, time remaining=6.2 hours
1307: loss=8.152, avg loss=8.572, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 83648 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1308: loss=8.267, avg loss=8.541, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 83712 images, time remaining=6.2 hours
1309: loss=7.842, avg loss=8.472, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 83776 images, time remaining=6.2 hours
1310: loss=8.465, avg loss=8.471, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=831.9 milliseconds, train=2.1 seconds, 83840 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1311: loss=9.784, avg loss=8.602, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=4.9 seconds, 83904 images, time remaining=6.2 hours
1312: loss=10.114, avg loss=8.753, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.2 seconds, train=4.8 seconds, 83968 images, time remaining=6.2 hours
1313: loss=9.544, avg loss=8.832, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=798.0 milliseconds, train=4.9 seconds, 84032 images, time remaining=6.2 hours
1314: loss=10.679, avg loss=9.017, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=4.9 seconds, 84096 images, time remaining=6.2 hours
1315: loss=10.130, avg loss=9.128, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=4.9 seconds, 84160 images, time remaining=6.2 hours
1316: loss=9.813, avg loss=9.197, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.2 seconds, train=4.9 seconds, 84224 images, time remaining=6.2 hours
1317: loss=10.207, avg loss=9.298, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.0 seconds, train=4.8 seconds, 84288 images, time remaining=6.2 hours
1318: loss=11.267, avg loss=9.495, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=4.9 seconds, 84352 images, time remaining=6.2 hours
1319: loss=9.042, avg loss=9.450, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=4.8 seconds, 84416 images, time remaining=6.2 hours
1320: loss=9.472, avg loss=9.452, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=819.7 milliseconds, train=4.9 seconds, 84480 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145fd0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1321: loss=10.014, avg loss=9.508, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.4 seconds, train=1.6 seconds, 84544 images, time remaining=6.2 hours
1322: loss=9.773, avg loss=9.535, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=659.1 milliseconds, train=1.6 seconds, 84608 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1323: loss=9.629, avg loss=9.544, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 84672 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1324: loss=8.417, avg loss=9.431, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.1 seconds, train=1.6 seconds, 84736 images, time remaining=6.2 hours
1325: loss=9.031, avg loss=9.391, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=707.5 milliseconds, train=1.6 seconds, 84800 images, time remaining=6.2 hours
1326: loss=10.042, avg loss=9.456, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=550.6 milliseconds, train=1.6 seconds, 84864 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1327: loss=8.536, avg loss=9.364, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 84928 images, time remaining=6.2 hours
1328: loss=8.472, avg loss=9.275, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=956.0 milliseconds, train=1.6 seconds, 84992 images, time remaining=6.2 hours
1329: loss=9.060, avg loss=9.254, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=697.7 milliseconds, train=1.7 seconds, 85056 images, time remaining=6.2 hours
1330: loss=9.257, avg loss=9.254, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 85120 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1331: loss=8.438, avg loss=9.172, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=561.4 milliseconds, train=2.0 seconds, 85184 images, time remaining=6.2 hours
1332: loss=8.123, avg loss=9.067, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=648.5 milliseconds, train=2.0 seconds, 85248 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1333: loss=8.332, avg loss=8.994, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 85312 images, time remaining=6.2 hours
1334: loss=8.393, avg loss=8.934, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 85376 images, time remaining=6.2 hours
1335: loss=9.024, avg loss=8.943, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=935.1 milliseconds, train=2.0 seconds, 85440 images, time remaining=6.2 hours
1336: loss=9.296, avg loss=8.978, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=2.0 seconds, 85504 images, time remaining=6.2 hours
1337: loss=9.028, avg loss=8.983, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=774.4 milliseconds, train=2.0 seconds, 85568 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1338: loss=9.976, avg loss=9.082, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.7 seconds, train=2.0 seconds, 85632 images, time remaining=6.2 hours
1339: loss=9.393, avg loss=9.113, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 85696 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1340: loss=9.265, avg loss=9.129, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.7 seconds, train=2.0 seconds, 85760 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1341: loss=8.689, avg loss=9.085, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=4.0 seconds, train=2.1 seconds, 85824 images, time remaining=6.2 hours
1342: loss=7.964, avg loss=8.973, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 85888 images, time remaining=6.2 hours
1343: loss=8.386, avg loss=8.914, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=2.1 seconds, 85952 images, time remaining=6.2 hours
1344: loss=9.192, avg loss=8.942, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=992.7 milliseconds, train=2.1 seconds, 86016 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1345: loss=7.458, avg loss=8.793, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 86080 images, time remaining=6.2 hours
1346: loss=6.786, avg loss=8.593, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=504.7 milliseconds, train=2.1 seconds, 86144 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1347: loss=8.484, avg loss=8.582, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.5 seconds, train=2.1 seconds, 86208 images, time remaining=6.2 hours
1348: loss=8.085, avg loss=8.532, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 86272 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1349: loss=7.827, avg loss=8.462, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 86336 images, time remaining=6.2 hours
1350: loss=6.321, avg loss=8.248, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=897.1 milliseconds, train=2.1 seconds, 86400 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1463c0000000
1351: loss=7.895, avg loss=8.212, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 86464 images, time remaining=6.2 hours
1352: loss=7.701, avg loss=8.161, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=746.1 milliseconds, train=1.5 seconds, 86528 images, time remaining=6.2 hours
1353: loss=7.327, avg loss=8.078, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=752.6 milliseconds, train=1.4 seconds, 86592 images, time remaining=6.2 hours
1354: loss=6.081, avg loss=7.878, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=755.7 milliseconds, train=1.4 seconds, 86656 images, time remaining=6.2 hours
1355: loss=6.370, avg loss=7.727, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=572.3 milliseconds, train=1.4 seconds, 86720 images, time remaining=6.2 hours
1356: loss=7.614, avg loss=7.716, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=600.1 milliseconds, train=1.5 seconds, 86784 images, time remaining=6.2 hours
1357: loss=7.326, avg loss=7.677, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=538.2 milliseconds, train=1.4 seconds, 86848 images, time remaining=6.1 hours
1358: loss=9.405, avg loss=7.850, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=821.1 milliseconds, train=1.5 seconds, 86912 images, time remaining=6.1 hours
1359: loss=7.032, avg loss=7.768, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 86976 images, time remaining=6.1 hours
1360: loss=7.234, avg loss=7.715, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=936.4 milliseconds, train=1.5 seconds, 87040 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463c4000000
1361: loss=6.714, avg loss=7.615, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=976.2 milliseconds, train=1.2 seconds, 87104 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1362: loss=7.674, avg loss=7.620, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=1.2 seconds, 87168 images, time remaining=6.1 hours
1363: loss=7.187, avg loss=7.577, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=483.2 milliseconds, train=1.2 seconds, 87232 images, time remaining=6.1 hours
1364: loss=6.564, avg loss=7.476, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=867.4 milliseconds, train=1.2 seconds, 87296 images, time remaining=6.1 hours
1365: loss=7.336, avg loss=7.462, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=610.0 milliseconds, train=1.2 seconds, 87360 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1366: loss=8.522, avg loss=7.568, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 87424 images, time remaining=6.1 hours
1367: loss=7.992, avg loss=7.610, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=787.8 milliseconds, train=1.2 seconds, 87488 images, time remaining=6.1 hours
1368: loss=7.263, avg loss=7.576, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=600.1 milliseconds, train=1.2 seconds, 87552 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1369: loss=7.916, avg loss=7.610, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=1.2 seconds, 87616 images, time remaining=6.1 hours
1370: loss=7.570, avg loss=7.606, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=636.3 milliseconds, train=1.2 seconds, 87680 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1371: loss=14.283, avg loss=8.273, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 87744 images, time remaining=6.1 hours
1372: loss=12.559, avg loss=8.702, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=952.5 milliseconds, train=5.1 seconds, 87808 images, time remaining=6.1 hours
1373: loss=11.765, avg loss=9.008, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 87872 images, time remaining=6.1 hours
1374: loss=9.586, avg loss=9.066, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=5.1 seconds, 87936 images, time remaining=6.1 hours
1375: loss=10.864, avg loss=9.246, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.6 seconds, train=5.1 seconds, 88000 images, time remaining=6.1 hours
1376: loss=11.289, avg loss=9.450, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 88064 images, time remaining=6.1 hours
1377: loss=11.082, avg loss=9.613, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 88128 images, time remaining=6.1 hours
1378: loss=12.846, avg loss=9.937, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 88192 images, time remaining=6.1 hours
1379: loss=10.849, avg loss=10.028, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 88256 images, time remaining=6.1 hours
1380: loss=10.413, avg loss=10.066, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.5 seconds, train=5.1 seconds, 88320 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1381: loss=9.447, avg loss=10.004, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.8 seconds, train=4.9 seconds, 88384 images, time remaining=6.1 hours
1382: loss=9.385, avg loss=9.943, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=714.9 milliseconds, train=4.8 seconds, 88448 images, time remaining=6.1 hours
1383: loss=9.826, avg loss=9.931, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=745.9 milliseconds, train=4.9 seconds, 88512 images, time remaining=6.1 hours
1384: loss=9.937, avg loss=9.931, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=4.9 seconds, 88576 images, time remaining=6.1 hours
1385: loss=10.619, avg loss=10.000, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=3.1 seconds, train=4.9 seconds, 88640 images, time remaining=6.1 hours
1386: loss=8.968, avg loss=9.897, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=4.8 seconds, 88704 images, time remaining=6.1 hours
1387: loss=9.970, avg loss=9.904, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=717.5 milliseconds, train=4.9 seconds, 88768 images, time remaining=6.1 hours
1388: loss=8.960, avg loss=9.810, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=849.6 milliseconds, train=4.8 seconds, 88832 images, time remaining=6.1 hours
1389: loss=9.003, avg loss=9.729, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.9 seconds, train=4.8 seconds, 88896 images, time remaining=6.1 hours
1390: loss=9.577, avg loss=9.714, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.5 seconds, train=4.8 seconds, 88960 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1459e6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1391: loss=10.028, avg loss=9.745, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 89024 images, time remaining=6.1 hours
1392: loss=9.311, avg loss=9.702, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=662.7 milliseconds, train=1.6 seconds, 89088 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1393: loss=9.551, avg loss=9.687, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 89152 images, time remaining=6.1 hours
1394: loss=9.600, avg loss=9.678, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=653.1 milliseconds, train=1.6 seconds, 89216 images, time remaining=6.1 hours
1395: loss=9.379, avg loss=9.648, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=821.2 milliseconds, train=1.6 seconds, 89280 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1396: loss=9.923, avg loss=9.676, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 89344 images, time remaining=6.1 hours
1397: loss=9.817, avg loss=9.690, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.7 seconds, 89408 images, time remaining=6.1 hours
1398: loss=7.675, avg loss=9.488, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=694.2 milliseconds, train=1.6 seconds, 89472 images, time remaining=6.1 hours
1399: loss=8.561, avg loss=9.396, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.6 seconds, train=1.7 seconds, 89536 images, time remaining=6.1 hours
1400: loss=8.573, avg loss=9.313, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 89600 images, time remaining=6.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x14591be00000
1401: loss=9.224, avg loss=9.304, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 89664 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1402: loss=8.839, avg loss=9.258, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 89728 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1403: loss=8.157, avg loss=9.148, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 89792 images, time remaining=6.1 hours
1404: loss=7.448, avg loss=8.978, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=642.1 milliseconds, train=1.4 seconds, 89856 images, time remaining=6.1 hours
1405: loss=9.363, avg loss=9.016, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 89920 images, time remaining=6.1 hours
1406: loss=6.671, avg loss=8.782, last=38.06%, best=38.06%, next=1406, rate=0.00130000, load 64=864.5 milliseconds, train=1.4 seconds, 89984 images, time remaining=6.1 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=465195, unique_truth_count=57264
rank=0 of ranks=465195rank=100 of ranks=465195rank=200 of ranks=465195rank=300 of ranks=465195rank=400 of ranks=465195rank=500 of ranks=465195rank=600 of ranks=465195rank=700 of ranks=465195rank=800 of ranks=465195rank=900 of ranks=465195rank=1000 of ranks=465195rank=1100 of ranks=465195rank=1200 of ranks=465195rank=1300 of ranks=465195rank=1400 of ranks=465195rank=1500 of ranks=465195rank=1600 of ranks=465195rank=1700 of ranks=465195rank=1800 of ranks=465195rank=1900 of ranks=465195rank=2000 of ranks=465195rank=2100 of ranks=465195rank=2200 of ranks=465195rank=2300 of ranks=465195rank=2400 of ranks=465195rank=2500 of ranks=465195rank=2600 of ranks=465195rank=2700 of ranks=465195rank=2800 of ranks=465195rank=2900 of ranks=465195rank=3000 of ranks=465195rank=3100 of ranks=465195rank=3200 of ranks=465195rank=3300 of ranks=465195rank=3400 of ranks=465195rank=3500 of ranks=465195rank=3600 of ranks=465195rank=3700 of ranks=465195rank=3800 of ranks=465195rank=3900 of ranks=465195rank=4000 of ranks=465195rank=4100 of ranks=465195rank=4200 of ranks=465195rank=4300 of ranks=465195rank=4400 of ranks=465195rank=4500 of ranks=465195rank=4600 of ranks=465195rank=4700 of ranks=465195rank=4800 of ranks=465195rank=4900 of ranks=465195rank=5000 of ranks=465195rank=5100 of ranks=465195rank=5200 of ranks=465195rank=5300 of ranks=465195rank=5400 of ranks=465195rank=5500 of ranks=465195rank=5600 of ranks=465195rank=5700 of ranks=465195rank=5800 of ranks=465195rank=5900 of ranks=465195rank=6000 of ranks=465195rank=6100 of ranks=465195rank=6200 of ranks=465195rank=6300 of ranks=465195rank=6400 of ranks=465195rank=6500 of ranks=465195rank=6600 of ranks=465195rank=6700 of ranks=465195rank=6800 of ranks=465195rank=6900 of ranks=465195rank=7000 of ranks=465195rank=7100 of ranks=465195rank=7200 of ranks=465195rank=7300 of ranks=465195rank=7400 of ranks=465195rank=7500 of ranks=465195rank=7600 of ranks=465195rank=7700 of ranks=465195rank=7800 of ranks=465195rank=7900 of ranks=465195rank=8000 of ranks=465195rank=8100 of ranks=465195rank=8200 of ranks=465195rank=8300 of ranks=465195rank=8400 of ranks=465195rank=8500 of ranks=465195rank=8600 of ranks=465195rank=8700 of ranks=465195rank=8800 of ranks=465195rank=8900 of ranks=465195rank=9000 of ranks=465195rank=9100 of ranks=465195rank=9200 of ranks=465195rank=9300 of ranks=465195rank=9400 of ranks=465195rank=9500 of ranks=465195rank=9600 of ranks=465195rank=9700 of ranks=465195rank=9800 of ranks=465195rank=9900 of ranks=465195rank=10000 of ranks=465195rank=10100 of ranks=465195rank=10200 of ranks=465195rank=10300 of ranks=465195rank=10400 of ranks=465195rank=10500 of ranks=465195rank=10600 of ranks=465195rank=10700 of ranks=465195rank=10800 of ranks=465195rank=10900 of ranks=465195rank=11000 of ranks=465195rank=11100 of ranks=465195rank=11200 of ranks=465195rank=11300 of ranks=465195rank=11400 of ranks=465195rank=11500 of ranks=465195rank=11600 of ranks=465195rank=11700 of ranks=465195rank=11800 of ranks=465195rank=11900 of ranks=465195rank=12000 of ranks=465195rank=12100 of ranks=465195rank=12200 of ranks=465195rank=12300 of ranks=465195rank=12400 of ranks=465195rank=12500 of ranks=465195rank=12600 of ranks=465195rank=12700 of ranks=465195rank=12800 of ranks=465195rank=12900 of ranks=465195rank=13000 of ranks=465195rank=13100 of ranks=465195rank=13200 of ranks=465195rank=13300 of ranks=465195rank=13400 of ranks=465195rank=13500 of ranks=465195rank=13600 of ranks=465195rank=13700 of ranks=465195rank=13800 of ranks=465195rank=13900 of ranks=465195rank=14000 of ranks=465195rank=14100 of ranks=465195rank=14200 of ranks=465195rank=14300 of ranks=465195rank=14400 of ranks=465195rank=14500 of ranks=465195rank=14600 of ranks=465195rank=14700 of ranks=465195rank=14800 of ranks=465195rank=14900 of ranks=465195rank=15000 of ranks=465195rank=15100 of ranks=465195rank=15200 of ranks=465195rank=15300 of ranks=465195rank=15400 of ranks=465195rank=15500 of ranks=465195rank=15600 of ranks=465195rank=15700 of ranks=465195rank=15800 of ranks=465195rank=15900 of ranks=465195rank=16000 of ranks=465195rank=16100 of ranks=465195rank=16200 of ranks=465195rank=16300 of ranks=465195rank=16400 of ranks=465195rank=16500 of ranks=465195rank=16600 of ranks=465195rank=16700 of ranks=465195rank=16800 of ranks=465195rank=16900 of ranks=465195rank=17000 of ranks=465195rank=17100 of ranks=465195rank=17200 of ranks=465195rank=17300 of ranks=465195rank=17400 of ranks=465195rank=17500 of ranks=465195rank=17600 of ranks=465195rank=17700 of ranks=465195rank=17800 of ranks=465195rank=17900 of ranks=465195rank=18000 of ranks=465195rank=18100 of ranks=465195rank=18200 of ranks=465195rank=18300 of ranks=465195rank=18400 of ranks=465195rank=18500 of ranks=465195rank=18600 of ranks=465195rank=18700 of ranks=465195rank=18800 of ranks=465195rank=18900 of ranks=465195rank=19000 of ranks=465195rank=19100 of ranks=465195rank=19200 of ranks=465195rank=19300 of ranks=465195rank=19400 of ranks=465195rank=19500 of ranks=465195rank=19600 of ranks=465195rank=19700 of ranks=465195rank=19800 of ranks=465195rank=19900 of ranks=465195rank=20000 of ranks=465195rank=20100 of ranks=465195rank=20200 of ranks=465195rank=20300 of ranks=465195rank=20400 of ranks=465195rank=20500 of ranks=465195rank=20600 of ranks=465195rank=20700 of ranks=465195rank=20800 of ranks=465195rank=20900 of ranks=465195rank=21000 of ranks=465195rank=21100 of ranks=465195rank=21200 of ranks=465195rank=21300 of ranks=465195rank=21400 of ranks=465195rank=21500 of ranks=465195rank=21600 of ranks=465195rank=21700 of ranks=465195rank=21800 of ranks=465195rank=21900 of ranks=465195rank=22000 of ranks=465195rank=22100 of ranks=465195rank=22200 of ranks=465195rank=22300 of ranks=465195rank=22400 of ranks=465195rank=22500 of ranks=465195rank=22600 of ranks=465195rank=22700 of ranks=465195rank=22800 of ranks=465195rank=22900 of ranks=465195rank=23000 of ranks=465195rank=23100 of ranks=465195rank=23200 of ranks=465195rank=23300 of ranks=465195rank=23400 of ranks=465195rank=23500 of ranks=465195rank=23600 of ranks=465195rank=23700 of ranks=465195rank=23800 of ranks=465195rank=23900 of ranks=465195rank=24000 of ranks=465195rank=24100 of ranks=465195rank=24200 of ranks=465195rank=24300 of ranks=465195rank=24400 of ranks=465195rank=24500 of ranks=465195rank=24600 of ranks=465195rank=24700 of ranks=465195rank=24800 of ranks=465195rank=24900 of ranks=465195rank=25000 of ranks=465195rank=25100 of ranks=465195rank=25200 of ranks=465195rank=25300 of ranks=465195rank=25400 of ranks=465195rank=25500 of ranks=465195rank=25600 of ranks=465195rank=25700 of ranks=465195rank=25800 of ranks=465195rank=25900 of ranks=465195rank=26000 of ranks=465195rank=26100 of ranks=465195rank=26200 of ranks=465195rank=26300 of ranks=465195rank=26400 of ranks=465195rank=26500 of ranks=465195rank=26600 of ranks=465195rank=26700 of ranks=465195rank=26800 of ranks=465195rank=26900 of ranks=465195rank=27000 of ranks=465195rank=27100 of ranks=465195rank=27200 of ranks=465195rank=27300 of ranks=465195rank=27400 of ranks=465195rank=27500 of ranks=465195rank=27600 of ranks=465195rank=27700 of ranks=465195rank=27800 of ranks=465195rank=27900 of ranks=465195rank=28000 of ranks=465195rank=28100 of ranks=465195rank=28200 of ranks=465195rank=28300 of ranks=465195rank=28400 of ranks=465195rank=28500 of ranks=465195rank=28600 of ranks=465195rank=28700 of ranks=465195rank=28800 of ranks=465195rank=28900 of ranks=465195rank=29000 of ranks=465195rank=29100 of ranks=465195rank=29200 of ranks=465195rank=29300 of ranks=465195rank=29400 of ranks=465195rank=29500 of ranks=465195rank=29600 of ranks=465195rank=29700 of ranks=465195rank=29800 of ranks=465195rank=29900 of ranks=465195rank=30000 of ranks=465195rank=30100 of ranks=465195rank=30200 of ranks=465195rank=30300 of ranks=465195rank=30400 of ranks=465195rank=30500 of ranks=465195rank=30600 of ranks=465195rank=30700 of ranks=465195rank=30800 of ranks=465195rank=30900 of ranks=465195rank=31000 of ranks=465195rank=31100 of ranks=465195rank=31200 of ranks=465195rank=31300 of ranks=465195rank=31400 of ranks=465195rank=31500 of ranks=465195rank=31600 of ranks=465195rank=31700 of ranks=465195rank=31800 of ranks=465195rank=31900 of ranks=465195rank=32000 of ranks=465195rank=32100 of ranks=465195rank=32200 of ranks=465195rank=32300 of ranks=465195rank=32400 of ranks=465195rank=32500 of ranks=465195rank=32600 of ranks=465195rank=32700 of ranks=465195rank=32800 of ranks=465195rank=32900 of ranks=465195rank=33000 of ranks=465195rank=33100 of ranks=465195rank=33200 of ranks=465195rank=33300 of ranks=465195rank=33400 of ranks=465195rank=33500 of ranks=465195rank=33600 of ranks=465195rank=33700 of ranks=465195rank=33800 of ranks=465195rank=33900 of ranks=465195rank=34000 of ranks=465195rank=34100 of ranks=465195rank=34200 of ranks=465195rank=34300 of ranks=465195rank=34400 of ranks=465195rank=34500 of ranks=465195rank=34600 of ranks=465195rank=34700 of ranks=465195rank=34800 of ranks=465195rank=34900 of ranks=465195rank=35000 of ranks=465195rank=35100 of ranks=465195rank=35200 of ranks=465195rank=35300 of ranks=465195rank=35400 of ranks=465195rank=35500 of ranks=465195rank=35600 of ranks=465195rank=35700 of ranks=465195rank=35800 of ranks=465195rank=35900 of ranks=465195rank=36000 of ranks=465195rank=36100 of ranks=465195rank=36200 of ranks=465195rank=36300 of ranks=465195rank=36400 of ranks=465195rank=36500 of ranks=465195rank=36600 of ranks=465195rank=36700 of ranks=465195rank=36800 of ranks=465195rank=36900 of ranks=465195rank=37000 of ranks=465195rank=37100 of ranks=465195rank=37200 of ranks=465195rank=37300 of ranks=465195rank=37400 of ranks=465195rank=37500 of ranks=465195rank=37600 of ranks=465195rank=37700 of ranks=465195rank=37800 of ranks=465195rank=37900 of ranks=465195rank=38000 of ranks=465195rank=38100 of ranks=465195rank=38200 of ranks=465195rank=38300 of ranks=465195rank=38400 of ranks=465195rank=38500 of ranks=465195rank=38600 of ranks=465195rank=38700 of ranks=465195rank=38800 of ranks=465195rank=38900 of ranks=465195rank=39000 of ranks=465195rank=39100 of ranks=465195rank=39200 of ranks=465195rank=39300 of ranks=465195rank=39400 of ranks=465195rank=39500 of ranks=465195rank=39600 of ranks=465195rank=39700 of ranks=465195rank=39800 of ranks=465195rank=39900 of ranks=465195rank=40000 of ranks=465195rank=40100 of ranks=465195rank=40200 of ranks=465195rank=40300 of ranks=465195rank=40400 of ranks=465195rank=40500 of ranks=465195rank=40600 of ranks=465195rank=40700 of ranks=465195rank=40800 of ranks=465195rank=40900 of ranks=465195rank=41000 of ranks=465195rank=41100 of ranks=465195rank=41200 of ranks=465195rank=41300 of ranks=465195rank=41400 of ranks=465195rank=41500 of ranks=465195rank=41600 of ranks=465195rank=41700 of ranks=465195rank=41800 of ranks=465195rank=41900 of ranks=465195rank=42000 of ranks=465195rank=42100 of ranks=465195rank=42200 of ranks=465195rank=42300 of ranks=465195rank=42400 of ranks=465195rank=42500 of ranks=465195rank=42600 of ranks=465195rank=42700 of ranks=465195rank=42800 of ranks=465195rank=42900 of ranks=465195rank=43000 of ranks=465195rank=43100 of ranks=465195rank=43200 of ranks=465195rank=43300 of ranks=465195rank=43400 of ranks=465195rank=43500 of ranks=465195rank=43600 of ranks=465195rank=43700 of ranks=465195rank=43800 of ranks=465195rank=43900 of ranks=465195rank=44000 of ranks=465195rank=44100 of ranks=465195rank=44200 of ranks=465195rank=44300 of ranks=465195rank=44400 of ranks=465195rank=44500 of ranks=465195rank=44600 of ranks=465195rank=44700 of ranks=465195rank=44800 of ranks=465195rank=44900 of ranks=465195rank=45000 of ranks=465195rank=45100 of ranks=465195rank=45200 of ranks=465195rank=45300 of ranks=465195rank=45400 of ranks=465195rank=45500 of ranks=465195rank=45600 of ranks=465195rank=45700 of ranks=465195rank=45800 of ranks=465195rank=45900 of ranks=465195rank=46000 of ranks=465195rank=46100 of ranks=465195rank=46200 of ranks=465195rank=46300 of ranks=465195rank=46400 of ranks=465195rank=46500 of ranks=465195rank=46600 of ranks=465195rank=46700 of ranks=465195rank=46800 of ranks=465195rank=46900 of ranks=465195rank=47000 of ranks=465195rank=47100 of ranks=465195rank=47200 of ranks=465195rank=47300 of ranks=465195rank=47400 of ranks=465195rank=47500 of ranks=465195rank=47600 of ranks=465195rank=47700 of ranks=465195rank=47800 of ranks=465195rank=47900 of ranks=465195rank=48000 of ranks=465195rank=48100 of ranks=465195rank=48200 of ranks=465195rank=48300 of ranks=465195rank=48400 of ranks=465195rank=48500 of ranks=465195rank=48600 of ranks=465195rank=48700 of ranks=465195rank=48800 of ranks=465195rank=48900 of ranks=465195rank=49000 of ranks=465195rank=49100 of ranks=465195rank=49200 of ranks=465195rank=49300 of ranks=465195rank=49400 of ranks=465195rank=49500 of ranks=465195rank=49600 of ranks=465195rank=49700 of ranks=465195rank=49800 of ranks=465195rank=49900 of ranks=465195rank=50000 of ranks=465195rank=50100 of ranks=465195rank=50200 of ranks=465195rank=50300 of ranks=465195rank=50400 of ranks=465195rank=50500 of ranks=465195rank=50600 of ranks=465195rank=50700 of ranks=465195rank=50800 of ranks=465195rank=50900 of ranks=465195rank=51000 of ranks=465195rank=51100 of ranks=465195rank=51200 of ranks=465195rank=51300 of ranks=465195rank=51400 of ranks=465195rank=51500 of ranks=465195rank=51600 of ranks=465195rank=51700 of ranks=465195rank=51800 of ranks=465195rank=51900 of ranks=465195rank=52000 of ranks=465195rank=52100 of ranks=465195rank=52200 of ranks=465195rank=52300 of ranks=465195rank=52400 of ranks=465195rank=52500 of ranks=465195rank=52600 of ranks=465195rank=52700 of ranks=465195rank=52800 of ranks=465195rank=52900 of ranks=465195rank=53000 of ranks=465195rank=53100 of ranks=465195rank=53200 of ranks=465195rank=53300 of ranks=465195rank=53400 of ranks=465195rank=53500 of ranks=465195rank=53600 of ranks=465195rank=53700 of ranks=465195rank=53800 of ranks=465195rank=53900 of ranks=465195rank=54000 of ranks=465195rank=54100 of ranks=465195rank=54200 of ranks=465195rank=54300 of ranks=465195rank=54400 of ranks=465195rank=54500 of ranks=465195rank=54600 of ranks=465195rank=54700 of ranks=465195rank=54800 of ranks=465195rank=54900 of ranks=465195rank=55000 of ranks=465195rank=55100 of ranks=465195rank=55200 of ranks=465195rank=55300 of ranks=465195rank=55400 of ranks=465195rank=55500 of ranks=465195rank=55600 of ranks=465195rank=55700 of ranks=465195rank=55800 of ranks=465195rank=55900 of ranks=465195rank=56000 of ranks=465195rank=56100 of ranks=465195rank=56200 of ranks=465195rank=56300 of ranks=465195rank=56400 of ranks=465195rank=56500 of ranks=465195rank=56600 of ranks=465195rank=56700 of ranks=465195rank=56800 of ranks=465195rank=56900 of ranks=465195rank=57000 of ranks=465195rank=57100 of ranks=465195rank=57200 of ranks=465195rank=57300 of ranks=465195rank=57400 of ranks=465195rank=57500 of ranks=465195rank=57600 of ranks=465195rank=57700 of ranks=465195rank=57800 of ranks=465195rank=57900 of ranks=465195rank=58000 of ranks=465195rank=58100 of ranks=465195rank=58200 of ranks=465195rank=58300 of ranks=465195rank=58400 of ranks=465195rank=58500 of ranks=465195rank=58600 of ranks=465195rank=58700 of ranks=465195rank=58800 of ranks=465195rank=58900 of ranks=465195rank=59000 of ranks=465195rank=59100 of ranks=465195rank=59200 of ranks=465195rank=59300 of ranks=465195rank=59400 of ranks=465195rank=59500 of ranks=465195rank=59600 of ranks=465195rank=59700 of ranks=465195rank=59800 of ranks=465195rank=59900 of ranks=465195rank=60000 of ranks=465195rank=60100 of ranks=465195rank=60200 of ranks=465195rank=60300 of ranks=465195rank=60400 of ranks=465195rank=60500 of ranks=465195rank=60600 of ranks=465195rank=60700 of ranks=465195rank=60800 of ranks=465195rank=60900 of ranks=465195rank=61000 of ranks=465195rank=61100 of ranks=465195rank=61200 of ranks=465195rank=61300 of ranks=465195rank=61400 of ranks=465195rank=61500 of ranks=465195rank=61600 of ranks=465195rank=61700 of ranks=465195rank=61800 of ranks=465195rank=61900 of ranks=465195rank=62000 of ranks=465195rank=62100 of ranks=465195rank=62200 of ranks=465195rank=62300 of ranks=465195rank=62400 of ranks=465195rank=62500 of ranks=465195rank=62600 of ranks=465195rank=62700 of ranks=465195rank=62800 of ranks=465195rank=62900 of ranks=465195rank=63000 of ranks=465195rank=63100 of ranks=465195rank=63200 of ranks=465195rank=63300 of ranks=465195rank=63400 of ranks=465195rank=63500 of ranks=465195rank=63600 of ranks=465195rank=63700 of ranks=465195rank=63800 of ranks=465195rank=63900 of ranks=465195rank=64000 of ranks=465195rank=64100 of ranks=465195rank=64200 of ranks=465195rank=64300 of ranks=465195rank=64400 of ranks=465195rank=64500 of ranks=465195rank=64600 of ranks=465195rank=64700 of ranks=465195rank=64800 of ranks=465195rank=64900 of ranks=465195rank=65000 of ranks=465195rank=65100 of ranks=465195rank=65200 of ranks=465195rank=65300 of ranks=465195rank=65400 of ranks=465195rank=65500 of ranks=465195rank=65600 of ranks=465195rank=65700 of ranks=465195rank=65800 of ranks=465195rank=65900 of ranks=465195rank=66000 of ranks=465195rank=66100 of ranks=465195rank=66200 of ranks=465195rank=66300 of ranks=465195rank=66400 of ranks=465195rank=66500 of ranks=465195rank=66600 of ranks=465195rank=66700 of ranks=465195rank=66800 of ranks=465195rank=66900 of ranks=465195rank=67000 of ranks=465195rank=67100 of ranks=465195rank=67200 of ranks=465195rank=67300 of ranks=465195rank=67400 of ranks=465195rank=67500 of ranks=465195rank=67600 of ranks=465195rank=67700 of ranks=465195rank=67800 of ranks=465195rank=67900 of ranks=465195rank=68000 of ranks=465195rank=68100 of ranks=465195rank=68200 of ranks=465195rank=68300 of ranks=465195rank=68400 of ranks=465195rank=68500 of ranks=465195rank=68600 of ranks=465195rank=68700 of ranks=465195rank=68800 of ranks=465195rank=68900 of ranks=465195rank=69000 of ranks=465195rank=69100 of ranks=465195rank=69200 of ranks=465195rank=69300 of ranks=465195rank=69400 of ranks=465195rank=69500 of ranks=465195rank=69600 of ranks=465195rank=69700 of ranks=465195rank=69800 of ranks=465195rank=69900 of ranks=465195rank=70000 of ranks=465195rank=70100 of ranks=465195rank=70200 of ranks=465195rank=70300 of ranks=465195rank=70400 of ranks=465195rank=70500 of ranks=465195rank=70600 of ranks=465195rank=70700 of ranks=465195rank=70800 of ranks=465195rank=70900 of ranks=465195rank=71000 of ranks=465195rank=71100 of ranks=465195rank=71200 of ranks=465195rank=71300 of ranks=465195rank=71400 of ranks=465195rank=71500 of ranks=465195rank=71600 of ranks=465195rank=71700 of ranks=465195rank=71800 of ranks=465195rank=71900 of ranks=465195rank=72000 of ranks=465195rank=72100 of ranks=465195rank=72200 of ranks=465195rank=72300 of ranks=465195rank=72400 of ranks=465195rank=72500 of ranks=465195rank=72600 of ranks=465195rank=72700 of ranks=465195rank=72800 of ranks=465195rank=72900 of ranks=465195rank=73000 of ranks=465195rank=73100 of ranks=465195rank=73200 of ranks=465195rank=73300 of ranks=465195rank=73400 of ranks=465195rank=73500 of ranks=465195rank=73600 of ranks=465195rank=73700 of ranks=465195rank=73800 of ranks=465195rank=73900 of ranks=465195rank=74000 of ranks=465195rank=74100 of ranks=465195rank=74200 of ranks=465195rank=74300 of ranks=465195rank=74400 of ranks=465195rank=74500 of ranks=465195rank=74600 of ranks=465195rank=74700 of ranks=465195rank=74800 of ranks=465195rank=74900 of ranks=465195rank=75000 of ranks=465195rank=75100 of ranks=465195rank=75200 of ranks=465195rank=75300 of ranks=465195rank=75400 of ranks=465195rank=75500 of ranks=465195rank=75600 of ranks=465195rank=75700 of ranks=465195rank=75800 of ranks=465195rank=75900 of ranks=465195rank=76000 of ranks=465195rank=76100 of ranks=465195rank=76200 of ranks=465195rank=76300 of ranks=465195rank=76400 of ranks=465195rank=76500 of ranks=465195rank=76600 of ranks=465195rank=76700 of ranks=465195rank=76800 of ranks=465195rank=76900 of ranks=465195rank=77000 of ranks=465195rank=77100 of ranks=465195rank=77200 of ranks=465195rank=77300 of ranks=465195rank=77400 of ranks=465195rank=77500 of ranks=465195rank=77600 of ranks=465195rank=77700 of ranks=465195rank=77800 of ranks=465195rank=77900 of ranks=465195rank=78000 of ranks=465195rank=78100 of ranks=465195rank=78200 of ranks=465195rank=78300 of ranks=465195rank=78400 of ranks=465195rank=78500 of ranks=465195rank=78600 of ranks=465195rank=78700 of ranks=465195rank=78800 of ranks=465195rank=78900 of ranks=465195rank=79000 of ranks=465195rank=79100 of ranks=465195rank=79200 of ranks=465195rank=79300 of ranks=465195rank=79400 of ranks=465195rank=79500 of ranks=465195rank=79600 of ranks=465195rank=79700 of ranks=465195rank=79800 of ranks=465195rank=79900 of ranks=465195rank=80000 of ranks=465195rank=80100 of ranks=465195rank=80200 of ranks=465195rank=80300 of ranks=465195rank=80400 of ranks=465195rank=80500 of ranks=465195rank=80600 of ranks=465195rank=80700 of ranks=465195rank=80800 of ranks=465195rank=80900 of ranks=465195rank=81000 of ranks=465195rank=81100 of ranks=465195rank=81200 of ranks=465195rank=81300 of ranks=465195rank=81400 of ranks=465195rank=81500 of ranks=465195rank=81600 of ranks=465195rank=81700 of ranks=465195rank=81800 of ranks=465195rank=81900 of ranks=465195rank=82000 of ranks=465195rank=82100 of ranks=465195rank=82200 of ranks=465195rank=82300 of ranks=465195rank=82400 of ranks=465195rank=82500 of ranks=465195rank=82600 of ranks=465195rank=82700 of ranks=465195rank=82800 of ranks=465195rank=82900 of ranks=465195rank=83000 of ranks=465195rank=83100 of ranks=465195rank=83200 of ranks=465195rank=83300 of ranks=465195rank=83400 of ranks=465195rank=83500 of ranks=465195rank=83600 of ranks=465195rank=83700 of ranks=465195rank=83800 of ranks=465195rank=83900 of ranks=465195rank=84000 of ranks=465195rank=84100 of ranks=465195rank=84200 of ranks=465195rank=84300 of ranks=465195rank=84400 of ranks=465195rank=84500 of ranks=465195rank=84600 of ranks=465195rank=84700 of ranks=465195rank=84800 of ranks=465195rank=84900 of ranks=465195rank=85000 of ranks=465195rank=85100 of ranks=465195rank=85200 of ranks=465195rank=85300 of ranks=465195rank=85400 of ranks=465195rank=85500 of ranks=465195rank=85600 of ranks=465195rank=85700 of ranks=465195rank=85800 of ranks=465195rank=85900 of ranks=465195rank=86000 of ranks=465195rank=86100 of ranks=465195rank=86200 of ranks=465195rank=86300 of ranks=465195rank=86400 of ranks=465195rank=86500 of ranks=465195rank=86600 of ranks=465195rank=86700 of ranks=465195rank=86800 of ranks=465195rank=86900 of ranks=465195rank=87000 of ranks=465195rank=87100 of ranks=465195rank=87200 of ranks=465195rank=87300 of ranks=465195rank=87400 of ranks=465195rank=87500 of ranks=465195rank=87600 of ranks=465195rank=87700 of ranks=465195rank=87800 of ranks=465195rank=87900 of ranks=465195rank=88000 of ranks=465195rank=88100 of ranks=465195rank=88200 of ranks=465195rank=88300 of ranks=465195rank=88400 of ranks=465195rank=88500 of ranks=465195rank=88600 of ranks=465195rank=88700 of ranks=465195rank=88800 of ranks=465195rank=88900 of ranks=465195rank=89000 of ranks=465195rank=89100 of ranks=465195rank=89200 of ranks=465195rank=89300 of ranks=465195rank=89400 of ranks=465195rank=89500 of ranks=465195rank=89600 of ranks=465195rank=89700 of ranks=465195rank=89800 of ranks=465195rank=89900 of ranks=465195rank=90000 of ranks=465195rank=90100 of ranks=465195rank=90200 of ranks=465195rank=90300 of ranks=465195rank=90400 of ranks=465195rank=90500 of ranks=465195rank=90600 of ranks=465195rank=90700 of ranks=465195rank=90800 of ranks=465195rank=90900 of ranks=465195rank=91000 of ranks=465195rank=91100 of ranks=465195rank=91200 of ranks=465195rank=91300 of ranks=465195rank=91400 of ranks=465195rank=91500 of ranks=465195rank=91600 of ranks=465195rank=91700 of ranks=465195rank=91800 of ranks=465195rank=91900 of ranks=465195rank=92000 of ranks=465195rank=92100 of ranks=465195rank=92200 of ranks=465195rank=92300 of ranks=465195rank=92400 of ranks=465195rank=92500 of ranks=465195rank=92600 of ranks=465195rank=92700 of ranks=465195rank=92800 of ranks=465195rank=92900 of ranks=465195rank=93000 of ranks=465195rank=93100 of ranks=465195rank=93200 of ranks=465195rank=93300 of ranks=465195rank=93400 of ranks=465195rank=93500 of ranks=465195rank=93600 of ranks=465195rank=93700 of ranks=465195rank=93800 of ranks=465195rank=93900 of ranks=465195rank=94000 of ranks=465195rank=94100 of ranks=465195rank=94200 of ranks=465195rank=94300 of ranks=465195rank=94400 of ranks=465195rank=94500 of ranks=465195rank=94600 of ranks=465195rank=94700 of ranks=465195rank=94800 of ranks=465195rank=94900 of ranks=465195rank=95000 of ranks=465195rank=95100 of ranks=465195rank=95200 of ranks=465195rank=95300 of ranks=465195rank=95400 of ranks=465195rank=95500 of ranks=465195rank=95600 of ranks=465195rank=95700 of ranks=465195rank=95800 of ranks=465195rank=95900 of ranks=465195rank=96000 of ranks=465195rank=96100 of ranks=465195rank=96200 of ranks=465195rank=96300 of ranks=465195rank=96400 of ranks=465195rank=96500 of ranks=465195rank=96600 of ranks=465195rank=96700 of ranks=465195rank=96800 of ranks=465195rank=96900 of ranks=465195rank=97000 of ranks=465195rank=97100 of ranks=465195rank=97200 of ranks=465195rank=97300 of ranks=465195rank=97400 of ranks=465195rank=97500 of ranks=465195rank=97600 of ranks=465195rank=97700 of ranks=465195rank=97800 of ranks=465195rank=97900 of ranks=465195rank=98000 of ranks=465195rank=98100 of ranks=465195rank=98200 of ranks=465195rank=98300 of ranks=465195rank=98400 of ranks=465195rank=98500 of ranks=465195rank=98600 of ranks=465195rank=98700 of ranks=465195rank=98800 of ranks=465195rank=98900 of ranks=465195rank=99000 of ranks=465195rank=99100 of ranks=465195rank=99200 of ranks=465195rank=99300 of ranks=465195rank=99400 of ranks=465195rank=99500 of ranks=465195rank=99600 of ranks=465195rank=99700 of ranks=465195rank=99800 of ranks=465195rank=99900 of ranks=465195rank=100000 of ranks=465195rank=100100 of ranks=465195rank=100200 of ranks=465195rank=100300 of ranks=465195rank=100400 of ranks=465195rank=100500 of ranks=465195rank=100600 of ranks=465195rank=100700 of ranks=465195rank=100800 of ranks=465195rank=100900 of ranks=465195rank=101000 of ranks=465195rank=101100 of ranks=465195rank=101200 of ranks=465195rank=101300 of ranks=465195rank=101400 of ranks=465195rank=101500 of ranks=465195rank=101600 of ranks=465195rank=101700 of ranks=465195rank=101800 of ranks=465195rank=101900 of ranks=465195rank=102000 of ranks=465195rank=102100 of ranks=465195rank=102200 of ranks=465195rank=102300 of ranks=465195rank=102400 of ranks=465195rank=102500 of ranks=465195rank=102600 of ranks=465195rank=102700 of ranks=465195rank=102800 of ranks=465195rank=102900 of ranks=465195rank=103000 of ranks=465195rank=103100 of ranks=465195rank=103200 of ranks=465195rank=103300 of ranks=465195rank=103400 of ranks=465195rank=103500 of ranks=465195rank=103600 of ranks=465195rank=103700 of ranks=465195rank=103800 of ranks=465195rank=103900 of ranks=465195rank=104000 of ranks=465195rank=104100 of ranks=465195rank=104200 of ranks=465195rank=104300 of ranks=465195rank=104400 of ranks=465195rank=104500 of ranks=465195rank=104600 of ranks=465195rank=104700 of ranks=465195rank=104800 of ranks=465195rank=104900 of ranks=465195rank=105000 of ranks=465195rank=105100 of ranks=465195rank=105200 of ranks=465195rank=105300 of ranks=465195rank=105400 of ranks=465195rank=105500 of ranks=465195rank=105600 of ranks=465195rank=105700 of ranks=465195rank=105800 of ranks=465195rank=105900 of ranks=465195rank=106000 of ranks=465195rank=106100 of ranks=465195rank=106200 of ranks=465195rank=106300 of ranks=465195rank=106400 of ranks=465195rank=106500 of ranks=465195rank=106600 of ranks=465195rank=106700 of ranks=465195rank=106800 of ranks=465195rank=106900 of ranks=465195rank=107000 of ranks=465195rank=107100 of ranks=465195rank=107200 of ranks=465195rank=107300 of ranks=465195rank=107400 of ranks=465195rank=107500 of ranks=465195rank=107600 of ranks=465195rank=107700 of ranks=465195rank=107800 of ranks=465195rank=107900 of ranks=465195rank=108000 of ranks=465195rank=108100 of ranks=465195rank=108200 of ranks=465195rank=108300 of ranks=465195rank=108400 of ranks=465195rank=108500 of ranks=465195rank=108600 of ranks=465195rank=108700 of ranks=465195rank=108800 of ranks=465195rank=108900 of ranks=465195rank=109000 of ranks=465195rank=109100 of ranks=465195rank=109200 of ranks=465195rank=109300 of ranks=465195rank=109400 of ranks=465195rank=109500 of ranks=465195rank=109600 of ranks=465195rank=109700 of ranks=465195rank=109800 of ranks=465195rank=109900 of ranks=465195rank=110000 of ranks=465195rank=110100 of ranks=465195rank=110200 of ranks=465195rank=110300 of ranks=465195rank=110400 of ranks=465195rank=110500 of ranks=465195rank=110600 of ranks=465195rank=110700 of ranks=465195rank=110800 of ranks=465195rank=110900 of ranks=465195rank=111000 of ranks=465195rank=111100 of ranks=465195rank=111200 of ranks=465195rank=111300 of ranks=465195rank=111400 of ranks=465195rank=111500 of ranks=465195rank=111600 of ranks=465195rank=111700 of ranks=465195rank=111800 of ranks=465195rank=111900 of ranks=465195rank=112000 of ranks=465195rank=112100 of ranks=465195rank=112200 of ranks=465195rank=112300 of ranks=465195rank=112400 of ranks=465195rank=112500 of ranks=465195rank=112600 of ranks=465195rank=112700 of ranks=465195rank=112800 of ranks=465195rank=112900 of ranks=465195rank=113000 of ranks=465195rank=113100 of ranks=465195rank=113200 of ranks=465195rank=113300 of ranks=465195rank=113400 of ranks=465195rank=113500 of ranks=465195rank=113600 of ranks=465195rank=113700 of ranks=465195rank=113800 of ranks=465195rank=113900 of ranks=465195rank=114000 of ranks=465195rank=114100 of ranks=465195rank=114200 of ranks=465195rank=114300 of ranks=465195rank=114400 of ranks=465195rank=114500 of ranks=465195rank=114600 of ranks=465195rank=114700 of ranks=465195rank=114800 of ranks=465195rank=114900 of ranks=465195rank=115000 of ranks=465195rank=115100 of ranks=465195rank=115200 of ranks=465195rank=115300 of ranks=465195rank=115400 of ranks=465195rank=115500 of ranks=465195rank=115600 of ranks=465195rank=115700 of ranks=465195rank=115800 of ranks=465195rank=115900 of ranks=465195rank=116000 of ranks=465195rank=116100 of ranks=465195rank=116200 of ranks=465195rank=116300 of ranks=465195rank=116400 of ranks=465195rank=116500 of ranks=465195rank=116600 of ranks=465195rank=116700 of ranks=465195rank=116800 of ranks=465195rank=116900 of ranks=465195rank=117000 of ranks=465195rank=117100 of ranks=465195rank=117200 of ranks=465195rank=117300 of ranks=465195rank=117400 of ranks=465195rank=117500 of ranks=465195rank=117600 of ranks=465195rank=117700 of ranks=465195rank=117800 of ranks=465195rank=117900 of ranks=465195rank=118000 of ranks=465195rank=118100 of ranks=465195rank=118200 of ranks=465195rank=118300 of ranks=465195rank=118400 of ranks=465195rank=118500 of ranks=465195rank=118600 of ranks=465195rank=118700 of ranks=465195rank=118800 of ranks=465195rank=118900 of ranks=465195rank=119000 of ranks=465195rank=119100 of ranks=465195rank=119200 of ranks=465195rank=119300 of ranks=465195rank=119400 of ranks=465195rank=119500 of ranks=465195rank=119600 of ranks=465195rank=119700 of ranks=465195rank=119800 of ranks=465195rank=119900 of ranks=465195rank=120000 of ranks=465195rank=120100 of ranks=465195rank=120200 of ranks=465195rank=120300 of ranks=465195rank=120400 of ranks=465195rank=120500 of ranks=465195rank=120600 of ranks=465195rank=120700 of ranks=465195rank=120800 of ranks=465195rank=120900 of ranks=465195rank=121000 of ranks=465195rank=121100 of ranks=465195rank=121200 of ranks=465195rank=121300 of ranks=465195rank=121400 of ranks=465195rank=121500 of ranks=465195rank=121600 of ranks=465195rank=121700 of ranks=465195rank=121800 of ranks=465195rank=121900 of ranks=465195rank=122000 of ranks=465195rank=122100 of ranks=465195rank=122200 of ranks=465195rank=122300 of ranks=465195rank=122400 of ranks=465195rank=122500 of ranks=465195rank=122600 of ranks=465195rank=122700 of ranks=465195rank=122800 of ranks=465195rank=122900 of ranks=465195rank=123000 of ranks=465195rank=123100 of ranks=465195rank=123200 of ranks=465195rank=123300 of ranks=465195rank=123400 of ranks=465195rank=123500 of ranks=465195rank=123600 of ranks=465195rank=123700 of ranks=465195rank=123800 of ranks=465195rank=123900 of ranks=465195rank=124000 of ranks=465195rank=124100 of ranks=465195rank=124200 of ranks=465195rank=124300 of ranks=465195rank=124400 of ranks=465195rank=124500 of ranks=465195rank=124600 of ranks=465195rank=124700 of ranks=465195rank=124800 of ranks=465195rank=124900 of ranks=465195rank=125000 of ranks=465195rank=125100 of ranks=465195rank=125200 of ranks=465195rank=125300 of ranks=465195rank=125400 of ranks=465195rank=125500 of ranks=465195rank=125600 of ranks=465195rank=125700 of ranks=465195rank=125800 of ranks=465195rank=125900 of ranks=465195rank=126000 of ranks=465195rank=126100 of ranks=465195rank=126200 of ranks=465195rank=126300 of ranks=465195rank=126400 of ranks=465195rank=126500 of ranks=465195rank=126600 of ranks=465195rank=126700 of ranks=465195rank=126800 of ranks=465195rank=126900 of ranks=465195rank=127000 of ranks=465195rank=127100 of ranks=465195rank=127200 of ranks=465195rank=127300 of ranks=465195rank=127400 of ranks=465195rank=127500 of ranks=465195rank=127600 of ranks=465195rank=127700 of ranks=465195rank=127800 of ranks=465195rank=127900 of ranks=465195rank=128000 of ranks=465195rank=128100 of ranks=465195rank=128200 of ranks=465195rank=128300 of ranks=465195rank=128400 of ranks=465195rank=128500 of ranks=465195rank=128600 of ranks=465195rank=128700 of ranks=465195rank=128800 of ranks=465195rank=128900 of ranks=465195rank=129000 of ranks=465195rank=129100 of ranks=465195rank=129200 of ranks=465195rank=129300 of ranks=465195rank=129400 of ranks=465195rank=129500 of ranks=465195rank=129600 of ranks=465195rank=129700 of ranks=465195rank=129800 of ranks=465195rank=129900 of ranks=465195rank=130000 of ranks=465195rank=130100 of ranks=465195rank=130200 of ranks=465195rank=130300 of ranks=465195rank=130400 of ranks=465195rank=130500 of ranks=465195rank=130600 of ranks=465195rank=130700 of ranks=465195rank=130800 of ranks=465195rank=130900 of ranks=465195rank=131000 of ranks=465195rank=131100 of ranks=465195rank=131200 of ranks=465195rank=131300 of ranks=465195rank=131400 of ranks=465195rank=131500 of ranks=465195rank=131600 of ranks=465195rank=131700 of ranks=465195rank=131800 of ranks=465195rank=131900 of ranks=465195rank=132000 of ranks=465195rank=132100 of ranks=465195rank=132200 of ranks=465195rank=132300 of ranks=465195rank=132400 of ranks=465195rank=132500 of ranks=465195rank=132600 of ranks=465195rank=132700 of ranks=465195rank=132800 of ranks=465195rank=132900 of ranks=465195rank=133000 of ranks=465195rank=133100 of ranks=465195rank=133200 of ranks=465195rank=133300 of ranks=465195rank=133400 of ranks=465195rank=133500 of ranks=465195rank=133600 of ranks=465195rank=133700 of ranks=465195rank=133800 of ranks=465195rank=133900 of ranks=465195rank=134000 of ranks=465195rank=134100 of ranks=465195rank=134200 of ranks=465195rank=134300 of ranks=465195rank=134400 of ranks=465195rank=134500 of ranks=465195rank=134600 of ranks=465195rank=134700 of ranks=465195rank=134800 of ranks=465195rank=134900 of ranks=465195rank=135000 of ranks=465195rank=135100 of ranks=465195rank=135200 of ranks=465195rank=135300 of ranks=465195rank=135400 of ranks=465195rank=135500 of ranks=465195rank=135600 of ranks=465195rank=135700 of ranks=465195rank=135800 of ranks=465195rank=135900 of ranks=465195rank=136000 of ranks=465195rank=136100 of ranks=465195rank=136200 of ranks=465195rank=136300 of ranks=465195rank=136400 of ranks=465195rank=136500 of ranks=465195rank=136600 of ranks=465195rank=136700 of ranks=465195rank=136800 of ranks=465195rank=136900 of ranks=465195rank=137000 of ranks=465195rank=137100 of ranks=465195rank=137200 of ranks=465195rank=137300 of ranks=465195rank=137400 of ranks=465195rank=137500 of ranks=465195rank=137600 of ranks=465195rank=137700 of ranks=465195rank=137800 of ranks=465195rank=137900 of ranks=465195rank=138000 of ranks=465195rank=138100 of ranks=465195rank=138200 of ranks=465195rank=138300 of ranks=465195rank=138400 of ranks=465195rank=138500 of ranks=465195rank=138600 of ranks=465195rank=138700 of ranks=465195rank=138800 of ranks=465195rank=138900 of ranks=465195rank=139000 of ranks=465195rank=139100 of ranks=465195rank=139200 of ranks=465195rank=139300 of ranks=465195rank=139400 of ranks=465195rank=139500 of ranks=465195rank=139600 of ranks=465195rank=139700 of ranks=465195rank=139800 of ranks=465195rank=139900 of ranks=465195rank=140000 of ranks=465195rank=140100 of ranks=465195rank=140200 of ranks=465195rank=140300 of ranks=465195rank=140400 of ranks=465195rank=140500 of ranks=465195rank=140600 of ranks=465195rank=140700 of ranks=465195rank=140800 of ranks=465195rank=140900 of ranks=465195rank=141000 of ranks=465195rank=141100 of ranks=465195rank=141200 of ranks=465195rank=141300 of ranks=465195rank=141400 of ranks=465195rank=141500 of ranks=465195rank=141600 of ranks=465195rank=141700 of ranks=465195rank=141800 of ranks=465195rank=141900 of ranks=465195rank=142000 of ranks=465195rank=142100 of ranks=465195rank=142200 of ranks=465195rank=142300 of ranks=465195rank=142400 of ranks=465195rank=142500 of ranks=465195rank=142600 of ranks=465195rank=142700 of ranks=465195rank=142800 of ranks=465195rank=142900 of ranks=465195rank=143000 of ranks=465195rank=143100 of ranks=465195rank=143200 of ranks=465195rank=143300 of ranks=465195rank=143400 of ranks=465195rank=143500 of ranks=465195rank=143600 of ranks=465195rank=143700 of ranks=465195rank=143800 of ranks=465195rank=143900 of ranks=465195rank=144000 of ranks=465195rank=144100 of ranks=465195rank=144200 of ranks=465195rank=144300 of ranks=465195rank=144400 of ranks=465195rank=144500 of ranks=465195rank=144600 of ranks=465195rank=144700 of ranks=465195rank=144800 of ranks=465195rank=144900 of ranks=465195rank=145000 of ranks=465195rank=145100 of ranks=465195rank=145200 of ranks=465195rank=145300 of ranks=465195rank=145400 of ranks=465195rank=145500 of ranks=465195rank=145600 of ranks=465195rank=145700 of ranks=465195rank=145800 of ranks=465195rank=145900 of ranks=465195rank=146000 of ranks=465195rank=146100 of ranks=465195rank=146200 of ranks=465195rank=146300 of ranks=465195rank=146400 of ranks=465195rank=146500 of ranks=465195rank=146600 of ranks=465195rank=146700 of ranks=465195rank=146800 of ranks=465195rank=146900 of ranks=465195rank=147000 of ranks=465195rank=147100 of ranks=465195rank=147200 of ranks=465195rank=147300 of ranks=465195rank=147400 of ranks=465195rank=147500 of ranks=465195rank=147600 of ranks=465195rank=147700 of ranks=465195rank=147800 of ranks=465195rank=147900 of ranks=465195rank=148000 of ranks=465195rank=148100 of ranks=465195rank=148200 of ranks=465195rank=148300 of ranks=465195rank=148400 of ranks=465195rank=148500 of ranks=465195rank=148600 of ranks=465195rank=148700 of ranks=465195rank=148800 of ranks=465195rank=148900 of ranks=465195rank=149000 of ranks=465195rank=149100 of ranks=465195rank=149200 of ranks=465195rank=149300 of ranks=465195rank=149400 of ranks=465195rank=149500 of ranks=465195rank=149600 of ranks=465195rank=149700 of ranks=465195rank=149800 of ranks=465195rank=149900 of ranks=465195rank=150000 of ranks=465195rank=150100 of ranks=465195rank=150200 of ranks=465195rank=150300 of ranks=465195rank=150400 of ranks=465195rank=150500 of ranks=465195rank=150600 of ranks=465195rank=150700 of ranks=465195rank=150800 of ranks=465195rank=150900 of ranks=465195rank=151000 of ranks=465195rank=151100 of ranks=465195rank=151200 of ranks=465195rank=151300 of ranks=465195rank=151400 of ranks=465195rank=151500 of ranks=465195rank=151600 of ranks=465195rank=151700 of ranks=465195rank=151800 of ranks=465195rank=151900 of ranks=465195rank=152000 of ranks=465195rank=152100 of ranks=465195rank=152200 of ranks=465195rank=152300 of ranks=465195rank=152400 of ranks=465195rank=152500 of ranks=465195rank=152600 of ranks=465195rank=152700 of ranks=465195rank=152800 of ranks=465195rank=152900 of ranks=465195rank=153000 of ranks=465195rank=153100 of ranks=465195rank=153200 of ranks=465195rank=153300 of ranks=465195rank=153400 of ranks=465195rank=153500 of ranks=465195rank=153600 of ranks=465195rank=153700 of ranks=465195rank=153800 of ranks=465195rank=153900 of ranks=465195rank=154000 of ranks=465195rank=154100 of ranks=465195rank=154200 of ranks=465195rank=154300 of ranks=465195rank=154400 of ranks=465195rank=154500 of ranks=465195rank=154600 of ranks=465195rank=154700 of ranks=465195rank=154800 of ranks=465195rank=154900 of ranks=465195rank=155000 of ranks=465195rank=155100 of ranks=465195rank=155200 of ranks=465195rank=155300 of ranks=465195rank=155400 of ranks=465195rank=155500 of ranks=465195rank=155600 of ranks=465195rank=155700 of ranks=465195rank=155800 of ranks=465195rank=155900 of ranks=465195rank=156000 of ranks=465195rank=156100 of ranks=465195rank=156200 of ranks=465195rank=156300 of ranks=465195rank=156400 of ranks=465195rank=156500 of ranks=465195rank=156600 of ranks=465195rank=156700 of ranks=465195rank=156800 of ranks=465195rank=156900 of ranks=465195rank=157000 of ranks=465195rank=157100 of ranks=465195rank=157200 of ranks=465195rank=157300 of ranks=465195rank=157400 of ranks=465195rank=157500 of ranks=465195rank=157600 of ranks=465195rank=157700 of ranks=465195rank=157800 of ranks=465195rank=157900 of ranks=465195rank=158000 of ranks=465195rank=158100 of ranks=465195rank=158200 of ranks=465195rank=158300 of ranks=465195rank=158400 of ranks=465195rank=158500 of ranks=465195rank=158600 of ranks=465195rank=158700 of ranks=465195rank=158800 of ranks=465195rank=158900 of ranks=465195rank=159000 of ranks=465195rank=159100 of ranks=465195rank=159200 of ranks=465195rank=159300 of ranks=465195rank=159400 of ranks=465195rank=159500 of ranks=465195rank=159600 of ranks=465195rank=159700 of ranks=465195rank=159800 of ranks=465195rank=159900 of ranks=465195rank=160000 of ranks=465195rank=160100 of ranks=465195rank=160200 of ranks=465195rank=160300 of ranks=465195rank=160400 of ranks=465195rank=160500 of ranks=465195rank=160600 of ranks=465195rank=160700 of ranks=465195rank=160800 of ranks=465195rank=160900 of ranks=465195rank=161000 of ranks=465195rank=161100 of ranks=465195rank=161200 of ranks=465195rank=161300 of ranks=465195rank=161400 of ranks=465195rank=161500 of ranks=465195rank=161600 of ranks=465195rank=161700 of ranks=465195rank=161800 of ranks=465195rank=161900 of ranks=465195rank=162000 of ranks=465195rank=162100 of ranks=465195rank=162200 of ranks=465195rank=162300 of ranks=465195rank=162400 of ranks=465195rank=162500 of ranks=465195rank=162600 of ranks=465195rank=162700 of ranks=465195rank=162800 of ranks=465195rank=162900 of ranks=465195rank=163000 of ranks=465195rank=163100 of ranks=465195rank=163200 of ranks=465195rank=163300 of ranks=465195rank=163400 of ranks=465195rank=163500 of ranks=465195rank=163600 of ranks=465195rank=163700 of ranks=465195rank=163800 of ranks=465195rank=163900 of ranks=465195rank=164000 of ranks=465195rank=164100 of ranks=465195rank=164200 of ranks=465195rank=164300 of ranks=465195rank=164400 of ranks=465195rank=164500 of ranks=465195rank=164600 of ranks=465195rank=164700 of ranks=465195rank=164800 of ranks=465195rank=164900 of ranks=465195rank=165000 of ranks=465195rank=165100 of ranks=465195rank=165200 of ranks=465195rank=165300 of ranks=465195rank=165400 of ranks=465195rank=165500 of ranks=465195rank=165600 of ranks=465195rank=165700 of ranks=465195rank=165800 of ranks=465195rank=165900 of ranks=465195rank=166000 of ranks=465195rank=166100 of ranks=465195rank=166200 of ranks=465195rank=166300 of ranks=465195rank=166400 of ranks=465195rank=166500 of ranks=465195rank=166600 of ranks=465195rank=166700 of ranks=465195rank=166800 of ranks=465195rank=166900 of ranks=465195rank=167000 of ranks=465195rank=167100 of ranks=465195rank=167200 of ranks=465195rank=167300 of ranks=465195rank=167400 of ranks=465195rank=167500 of ranks=465195rank=167600 of ranks=465195rank=167700 of ranks=465195rank=167800 of ranks=465195rank=167900 of ranks=465195rank=168000 of ranks=465195rank=168100 of ranks=465195rank=168200 of ranks=465195rank=168300 of ranks=465195rank=168400 of ranks=465195rank=168500 of ranks=465195rank=168600 of ranks=465195rank=168700 of ranks=465195rank=168800 of ranks=465195rank=168900 of ranks=465195rank=169000 of ranks=465195rank=169100 of ranks=465195rank=169200 of ranks=465195rank=169300 of ranks=465195rank=169400 of ranks=465195rank=169500 of ranks=465195rank=169600 of ranks=465195rank=169700 of ranks=465195rank=169800 of ranks=465195rank=169900 of ranks=465195rank=170000 of ranks=465195rank=170100 of ranks=465195rank=170200 of ranks=465195rank=170300 of ranks=465195rank=170400 of ranks=465195rank=170500 of ranks=465195rank=170600 of ranks=465195rank=170700 of ranks=465195rank=170800 of ranks=465195rank=170900 of ranks=465195rank=171000 of ranks=465195rank=171100 of ranks=465195rank=171200 of ranks=465195rank=171300 of ranks=465195rank=171400 of ranks=465195rank=171500 of ranks=465195rank=171600 of ranks=465195rank=171700 of ranks=465195rank=171800 of ranks=465195rank=171900 of ranks=465195rank=172000 of ranks=465195rank=172100 of ranks=465195rank=172200 of ranks=465195rank=172300 of ranks=465195rank=172400 of ranks=465195rank=172500 of ranks=465195rank=172600 of ranks=465195rank=172700 of ranks=465195rank=172800 of ranks=465195rank=172900 of ranks=465195rank=173000 of ranks=465195rank=173100 of ranks=465195rank=173200 of ranks=465195rank=173300 of ranks=465195rank=173400 of ranks=465195rank=173500 of ranks=465195rank=173600 of ranks=465195rank=173700 of ranks=465195rank=173800 of ranks=465195rank=173900 of ranks=465195rank=174000 of ranks=465195rank=174100 of ranks=465195rank=174200 of ranks=465195rank=174300 of ranks=465195rank=174400 of ranks=465195rank=174500 of ranks=465195rank=174600 of ranks=465195rank=174700 of ranks=465195rank=174800 of ranks=465195rank=174900 of ranks=465195rank=175000 of ranks=465195rank=175100 of ranks=465195rank=175200 of ranks=465195rank=175300 of ranks=465195rank=175400 of ranks=465195rank=175500 of ranks=465195rank=175600 of ranks=465195rank=175700 of ranks=465195rank=175800 of ranks=465195rank=175900 of ranks=465195rank=176000 of ranks=465195rank=176100 of ranks=465195rank=176200 of ranks=465195rank=176300 of ranks=465195rank=176400 of ranks=465195rank=176500 of ranks=465195rank=176600 of ranks=465195rank=176700 of ranks=465195rank=176800 of ranks=465195rank=176900 of ranks=465195rank=177000 of ranks=465195rank=177100 of ranks=465195rank=177200 of ranks=465195rank=177300 of ranks=465195rank=177400 of ranks=465195rank=177500 of ranks=465195rank=177600 of ranks=465195rank=177700 of ranks=465195rank=177800 of ranks=465195rank=177900 of ranks=465195rank=178000 of ranks=465195rank=178100 of ranks=465195rank=178200 of ranks=465195rank=178300 of ranks=465195rank=178400 of ranks=465195rank=178500 of ranks=465195rank=178600 of ranks=465195rank=178700 of ranks=465195rank=178800 of ranks=465195rank=178900 of ranks=465195rank=179000 of ranks=465195rank=179100 of ranks=465195rank=179200 of ranks=465195rank=179300 of ranks=465195rank=179400 of ranks=465195rank=179500 of ranks=465195rank=179600 of ranks=465195rank=179700 of ranks=465195rank=179800 of ranks=465195rank=179900 of ranks=465195rank=180000 of ranks=465195rank=180100 of ranks=465195rank=180200 of ranks=465195rank=180300 of ranks=465195rank=180400 of ranks=465195rank=180500 of ranks=465195rank=180600 of ranks=465195rank=180700 of ranks=465195rank=180800 of ranks=465195rank=180900 of ranks=465195rank=181000 of ranks=465195rank=181100 of ranks=465195rank=181200 of ranks=465195rank=181300 of ranks=465195rank=181400 of ranks=465195rank=181500 of ranks=465195rank=181600 of ranks=465195rank=181700 of ranks=465195rank=181800 of ranks=465195rank=181900 of ranks=465195rank=182000 of ranks=465195rank=182100 of ranks=465195rank=182200 of ranks=465195rank=182300 of ranks=465195rank=182400 of ranks=465195rank=182500 of ranks=465195rank=182600 of ranks=465195rank=182700 of ranks=465195rank=182800 of ranks=465195rank=182900 of ranks=465195rank=183000 of ranks=465195rank=183100 of ranks=465195rank=183200 of ranks=465195rank=183300 of ranks=465195rank=183400 of ranks=465195rank=183500 of ranks=465195rank=183600 of ranks=465195rank=183700 of ranks=465195rank=183800 of ranks=465195rank=183900 of ranks=465195rank=184000 of ranks=465195rank=184100 of ranks=465195rank=184200 of ranks=465195rank=184300 of ranks=465195rank=184400 of ranks=465195rank=184500 of ranks=465195rank=184600 of ranks=465195rank=184700 of ranks=465195rank=184800 of ranks=465195rank=184900 of ranks=465195rank=185000 of ranks=465195rank=185100 of ranks=465195rank=185200 of ranks=465195rank=185300 of ranks=465195rank=185400 of ranks=465195rank=185500 of ranks=465195rank=185600 of ranks=465195rank=185700 of ranks=465195rank=185800 of ranks=465195rank=185900 of ranks=465195rank=186000 of ranks=465195rank=186100 of ranks=465195rank=186200 of ranks=465195rank=186300 of ranks=465195rank=186400 of ranks=465195rank=186500 of ranks=465195rank=186600 of ranks=465195rank=186700 of ranks=465195rank=186800 of ranks=465195rank=186900 of ranks=465195rank=187000 of ranks=465195rank=187100 of ranks=465195rank=187200 of ranks=465195rank=187300 of ranks=465195rank=187400 of ranks=465195rank=187500 of ranks=465195rank=187600 of ranks=465195rank=187700 of ranks=465195rank=187800 of ranks=465195rank=187900 of ranks=465195rank=188000 of ranks=465195rank=188100 of ranks=465195rank=188200 of ranks=465195rank=188300 of ranks=465195rank=188400 of ranks=465195rank=188500 of ranks=465195rank=188600 of ranks=465195rank=188700 of ranks=465195rank=188800 of ranks=465195rank=188900 of ranks=465195rank=189000 of ranks=465195rank=189100 of ranks=465195rank=189200 of ranks=465195rank=189300 of ranks=465195rank=189400 of ranks=465195rank=189500 of ranks=465195rank=189600 of ranks=465195rank=189700 of ranks=465195rank=189800 of ranks=465195rank=189900 of ranks=465195rank=190000 of ranks=465195rank=190100 of ranks=465195rank=190200 of ranks=465195rank=190300 of ranks=465195rank=190400 of ranks=465195rank=190500 of ranks=465195rank=190600 of ranks=465195rank=190700 of ranks=465195rank=190800 of ranks=465195rank=190900 of ranks=465195rank=191000 of ranks=465195rank=191100 of ranks=465195rank=191200 of ranks=465195rank=191300 of ranks=465195rank=191400 of ranks=465195rank=191500 of ranks=465195rank=191600 of ranks=465195rank=191700 of ranks=465195rank=191800 of ranks=465195rank=191900 of ranks=465195rank=192000 of ranks=465195rank=192100 of ranks=465195rank=192200 of ranks=465195rank=192300 of ranks=465195rank=192400 of ranks=465195rank=192500 of ranks=465195rank=192600 of ranks=465195rank=192700 of ranks=465195rank=192800 of ranks=465195rank=192900 of ranks=465195rank=193000 of ranks=465195rank=193100 of ranks=465195rank=193200 of ranks=465195rank=193300 of ranks=465195rank=193400 of ranks=465195rank=193500 of ranks=465195rank=193600 of ranks=465195rank=193700 of ranks=465195rank=193800 of ranks=465195rank=193900 of ranks=465195rank=194000 of ranks=465195rank=194100 of ranks=465195rank=194200 of ranks=465195rank=194300 of ranks=465195rank=194400 of ranks=465195rank=194500 of ranks=465195rank=194600 of ranks=465195rank=194700 of ranks=465195rank=194800 of ranks=465195rank=194900 of ranks=465195rank=195000 of ranks=465195rank=195100 of ranks=465195rank=195200 of ranks=465195rank=195300 of ranks=465195rank=195400 of ranks=465195rank=195500 of ranks=465195rank=195600 of ranks=465195rank=195700 of ranks=465195rank=195800 of ranks=465195rank=195900 of ranks=465195rank=196000 of ranks=465195rank=196100 of ranks=465195rank=196200 of ranks=465195rank=196300 of ranks=465195rank=196400 of ranks=465195rank=196500 of ranks=465195rank=196600 of ranks=465195rank=196700 of ranks=465195rank=196800 of ranks=465195rank=196900 of ranks=465195rank=197000 of ranks=465195rank=197100 of ranks=465195rank=197200 of ranks=465195rank=197300 of ranks=465195rank=197400 of ranks=465195rank=197500 of ranks=465195rank=197600 of ranks=465195rank=197700 of ranks=465195rank=197800 of ranks=465195rank=197900 of ranks=465195rank=198000 of ranks=465195rank=198100 of ranks=465195rank=198200 of ranks=465195rank=198300 of ranks=465195rank=198400 of ranks=465195rank=198500 of ranks=465195rank=198600 of ranks=465195rank=198700 of ranks=465195rank=198800 of ranks=465195rank=198900 of ranks=465195rank=199000 of ranks=465195rank=199100 of ranks=465195rank=199200 of ranks=465195rank=199300 of ranks=465195rank=199400 of ranks=465195rank=199500 of ranks=465195rank=199600 of ranks=465195rank=199700 of ranks=465195rank=199800 of ranks=465195rank=199900 of ranks=465195rank=200000 of ranks=465195rank=200100 of ranks=465195rank=200200 of ranks=465195rank=200300 of ranks=465195rank=200400 of ranks=465195rank=200500 of ranks=465195rank=200600 of ranks=465195rank=200700 of ranks=465195rank=200800 of ranks=465195rank=200900 of ranks=465195rank=201000 of ranks=465195rank=201100 of ranks=465195rank=201200 of ranks=465195rank=201300 of ranks=465195rank=201400 of ranks=465195rank=201500 of ranks=465195rank=201600 of ranks=465195rank=201700 of ranks=465195rank=201800 of ranks=465195rank=201900 of ranks=465195rank=202000 of ranks=465195rank=202100 of ranks=465195rank=202200 of ranks=465195rank=202300 of ranks=465195rank=202400 of ranks=465195rank=202500 of ranks=465195rank=202600 of ranks=465195rank=202700 of ranks=465195rank=202800 of ranks=465195rank=202900 of ranks=465195rank=203000 of ranks=465195rank=203100 of ranks=465195rank=203200 of ranks=465195rank=203300 of ranks=465195rank=203400 of ranks=465195rank=203500 of ranks=465195rank=203600 of ranks=465195rank=203700 of ranks=465195rank=203800 of ranks=465195rank=203900 of ranks=465195rank=204000 of ranks=465195rank=204100 of ranks=465195rank=204200 of ranks=465195rank=204300 of ranks=465195rank=204400 of ranks=465195rank=204500 of ranks=465195rank=204600 of ranks=465195rank=204700 of ranks=465195rank=204800 of ranks=465195rank=204900 of ranks=465195rank=205000 of ranks=465195rank=205100 of ranks=465195rank=205200 of ranks=465195rank=205300 of ranks=465195rank=205400 of ranks=465195rank=205500 of ranks=465195rank=205600 of ranks=465195rank=205700 of ranks=465195rank=205800 of ranks=465195rank=205900 of ranks=465195rank=206000 of ranks=465195rank=206100 of ranks=465195rank=206200 of ranks=465195rank=206300 of ranks=465195rank=206400 of ranks=465195rank=206500 of ranks=465195rank=206600 of ranks=465195rank=206700 of ranks=465195rank=206800 of ranks=465195rank=206900 of ranks=465195rank=207000 of ranks=465195rank=207100 of ranks=465195rank=207200 of ranks=465195rank=207300 of ranks=465195rank=207400 of ranks=465195rank=207500 of ranks=465195rank=207600 of ranks=465195rank=207700 of ranks=465195rank=207800 of ranks=465195rank=207900 of ranks=465195rank=208000 of ranks=465195rank=208100 of ranks=465195rank=208200 of ranks=465195rank=208300 of ranks=465195rank=208400 of ranks=465195rank=208500 of ranks=465195rank=208600 of ranks=465195rank=208700 of ranks=465195rank=208800 of ranks=465195rank=208900 of ranks=465195rank=209000 of ranks=465195rank=209100 of ranks=465195rank=209200 of ranks=465195rank=209300 of ranks=465195rank=209400 of ranks=465195rank=209500 of ranks=465195rank=209600 of ranks=465195rank=209700 of ranks=465195rank=209800 of ranks=465195rank=209900 of ranks=465195rank=210000 of ranks=465195rank=210100 of ranks=465195rank=210200 of ranks=465195rank=210300 of ranks=465195rank=210400 of ranks=465195rank=210500 of ranks=465195rank=210600 of ranks=465195rank=210700 of ranks=465195rank=210800 of ranks=465195rank=210900 of ranks=465195rank=211000 of ranks=465195rank=211100 of ranks=465195rank=211200 of ranks=465195rank=211300 of ranks=465195rank=211400 of ranks=465195rank=211500 of ranks=465195rank=211600 of ranks=465195rank=211700 of ranks=465195rank=211800 of ranks=465195rank=211900 of ranks=465195rank=212000 of ranks=465195rank=212100 of ranks=465195rank=212200 of ranks=465195rank=212300 of ranks=465195rank=212400 of ranks=465195rank=212500 of ranks=465195rank=212600 of ranks=465195rank=212700 of ranks=465195rank=212800 of ranks=465195rank=212900 of ranks=465195rank=213000 of ranks=465195rank=213100 of ranks=465195rank=213200 of ranks=465195rank=213300 of ranks=465195rank=213400 of ranks=465195rank=213500 of ranks=465195rank=213600 of ranks=465195rank=213700 of ranks=465195rank=213800 of ranks=465195rank=213900 of ranks=465195rank=214000 of ranks=465195rank=214100 of ranks=465195rank=214200 of ranks=465195rank=214300 of ranks=465195rank=214400 of ranks=465195rank=214500 of ranks=465195rank=214600 of ranks=465195rank=214700 of ranks=465195rank=214800 of ranks=465195rank=214900 of ranks=465195rank=215000 of ranks=465195rank=215100 of ranks=465195rank=215200 of ranks=465195rank=215300 of ranks=465195rank=215400 of ranks=465195rank=215500 of ranks=465195rank=215600 of ranks=465195rank=215700 of ranks=465195rank=215800 of ranks=465195rank=215900 of ranks=465195rank=216000 of ranks=465195rank=216100 of ranks=465195rank=216200 of ranks=465195rank=216300 of ranks=465195rank=216400 of ranks=465195rank=216500 of ranks=465195rank=216600 of ranks=465195rank=216700 of ranks=465195rank=216800 of ranks=465195rank=216900 of ranks=465195rank=217000 of ranks=465195rank=217100 of ranks=465195rank=217200 of ranks=465195rank=217300 of ranks=465195rank=217400 of ranks=465195rank=217500 of ranks=465195rank=217600 of ranks=465195rank=217700 of ranks=465195rank=217800 of ranks=465195rank=217900 of ranks=465195rank=218000 of ranks=465195rank=218100 of ranks=465195rank=218200 of ranks=465195rank=218300 of ranks=465195rank=218400 of ranks=465195rank=218500 of ranks=465195rank=218600 of ranks=465195rank=218700 of ranks=465195rank=218800 of ranks=465195rank=218900 of ranks=465195rank=219000 of ranks=465195rank=219100 of ranks=465195rank=219200 of ranks=465195rank=219300 of ranks=465195rank=219400 of ranks=465195rank=219500 of ranks=465195rank=219600 of ranks=465195rank=219700 of ranks=465195rank=219800 of ranks=465195rank=219900 of ranks=465195rank=220000 of ranks=465195rank=220100 of ranks=465195rank=220200 of ranks=465195rank=220300 of ranks=465195rank=220400 of ranks=465195rank=220500 of ranks=465195rank=220600 of ranks=465195rank=220700 of ranks=465195rank=220800 of ranks=465195rank=220900 of ranks=465195rank=221000 of ranks=465195rank=221100 of ranks=465195rank=221200 of ranks=465195rank=221300 of ranks=465195rank=221400 of ranks=465195rank=221500 of ranks=465195rank=221600 of ranks=465195rank=221700 of ranks=465195rank=221800 of ranks=465195rank=221900 of ranks=465195rank=222000 of ranks=465195rank=222100 of ranks=465195rank=222200 of ranks=465195rank=222300 of ranks=465195rank=222400 of ranks=465195rank=222500 of ranks=465195rank=222600 of ranks=465195rank=222700 of ranks=465195rank=222800 of ranks=465195rank=222900 of ranks=465195rank=223000 of ranks=465195rank=223100 of ranks=465195rank=223200 of ranks=465195rank=223300 of ranks=465195rank=223400 of ranks=465195rank=223500 of ranks=465195rank=223600 of ranks=465195rank=223700 of ranks=465195rank=223800 of ranks=465195rank=223900 of ranks=465195rank=224000 of ranks=465195rank=224100 of ranks=465195rank=224200 of ranks=465195rank=224300 of ranks=465195rank=224400 of ranks=465195rank=224500 of ranks=465195rank=224600 of ranks=465195rank=224700 of ranks=465195rank=224800 of ranks=465195rank=224900 of ranks=465195rank=225000 of ranks=465195rank=225100 of ranks=465195rank=225200 of ranks=465195rank=225300 of ranks=465195rank=225400 of ranks=465195rank=225500 of ranks=465195rank=225600 of ranks=465195rank=225700 of ranks=465195rank=225800 of ranks=465195rank=225900 of ranks=465195rank=226000 of ranks=465195rank=226100 of ranks=465195rank=226200 of ranks=465195rank=226300 of ranks=465195rank=226400 of ranks=465195rank=226500 of ranks=465195rank=226600 of ranks=465195rank=226700 of ranks=465195rank=226800 of ranks=465195rank=226900 of ranks=465195rank=227000 of ranks=465195rank=227100 of ranks=465195rank=227200 of ranks=465195rank=227300 of ranks=465195rank=227400 of ranks=465195rank=227500 of ranks=465195rank=227600 of ranks=465195rank=227700 of ranks=465195rank=227800 of ranks=465195rank=227900 of ranks=465195rank=228000 of ranks=465195rank=228100 of ranks=465195rank=228200 of ranks=465195rank=228300 of ranks=465195rank=228400 of ranks=465195rank=228500 of ranks=465195rank=228600 of ranks=465195rank=228700 of ranks=465195rank=228800 of ranks=465195rank=228900 of ranks=465195rank=229000 of ranks=465195rank=229100 of ranks=465195rank=229200 of ranks=465195rank=229300 of ranks=465195rank=229400 of ranks=465195rank=229500 of ranks=465195rank=229600 of ranks=465195rank=229700 of ranks=465195rank=229800 of ranks=465195rank=229900 of ranks=465195rank=230000 of ranks=465195rank=230100 of ranks=465195rank=230200 of ranks=465195rank=230300 of ranks=465195rank=230400 of ranks=465195rank=230500 of ranks=465195rank=230600 of ranks=465195rank=230700 of ranks=465195rank=230800 of ranks=465195rank=230900 of ranks=465195rank=231000 of ranks=465195rank=231100 of ranks=465195rank=231200 of ranks=465195rank=231300 of ranks=465195rank=231400 of ranks=465195rank=231500 of ranks=465195rank=231600 of ranks=465195rank=231700 of ranks=465195rank=231800 of ranks=465195rank=231900 of ranks=465195rank=232000 of ranks=465195rank=232100 of ranks=465195rank=232200 of ranks=465195rank=232300 of ranks=465195rank=232400 of ranks=465195rank=232500 of ranks=465195rank=232600 of ranks=465195rank=232700 of ranks=465195rank=232800 of ranks=465195rank=232900 of ranks=465195rank=233000 of ranks=465195rank=233100 of ranks=465195rank=233200 of ranks=465195rank=233300 of ranks=465195rank=233400 of ranks=465195rank=233500 of ranks=465195rank=233600 of ranks=465195rank=233700 of ranks=465195rank=233800 of ranks=465195rank=233900 of ranks=465195rank=234000 of ranks=465195rank=234100 of ranks=465195rank=234200 of ranks=465195rank=234300 of ranks=465195rank=234400 of ranks=465195rank=234500 of ranks=465195rank=234600 of ranks=465195rank=234700 of ranks=465195rank=234800 of ranks=465195rank=234900 of ranks=465195rank=235000 of ranks=465195rank=235100 of ranks=465195rank=235200 of ranks=465195rank=235300 of ranks=465195rank=235400 of ranks=465195rank=235500 of ranks=465195rank=235600 of ranks=465195rank=235700 of ranks=465195rank=235800 of ranks=465195rank=235900 of ranks=465195rank=236000 of ranks=465195rank=236100 of ranks=465195rank=236200 of ranks=465195rank=236300 of ranks=465195rank=236400 of ranks=465195rank=236500 of ranks=465195rank=236600 of ranks=465195rank=236700 of ranks=465195rank=236800 of ranks=465195rank=236900 of ranks=465195rank=237000 of ranks=465195rank=237100 of ranks=465195rank=237200 of ranks=465195rank=237300 of ranks=465195rank=237400 of ranks=465195rank=237500 of ranks=465195rank=237600 of ranks=465195rank=237700 of ranks=465195rank=237800 of ranks=465195rank=237900 of ranks=465195rank=238000 of ranks=465195rank=238100 of ranks=465195rank=238200 of ranks=465195rank=238300 of ranks=465195rank=238400 of ranks=465195rank=238500 of ranks=465195rank=238600 of ranks=465195rank=238700 of ranks=465195rank=238800 of ranks=465195rank=238900 of ranks=465195rank=239000 of ranks=465195rank=239100 of ranks=465195rank=239200 of ranks=465195rank=239300 of ranks=465195rank=239400 of ranks=465195rank=239500 of ranks=465195rank=239600 of ranks=465195rank=239700 of ranks=465195rank=239800 of ranks=465195rank=239900 of ranks=465195rank=240000 of ranks=465195rank=240100 of ranks=465195rank=240200 of ranks=465195rank=240300 of ranks=465195rank=240400 of ranks=465195rank=240500 of ranks=465195rank=240600 of ranks=465195rank=240700 of ranks=465195rank=240800 of ranks=465195rank=240900 of ranks=465195rank=241000 of ranks=465195rank=241100 of ranks=465195rank=241200 of ranks=465195rank=241300 of ranks=465195rank=241400 of ranks=465195rank=241500 of ranks=465195rank=241600 of ranks=465195rank=241700 of ranks=465195rank=241800 of ranks=465195rank=241900 of ranks=465195rank=242000 of ranks=465195rank=242100 of ranks=465195rank=242200 of ranks=465195rank=242300 of ranks=465195rank=242400 of ranks=465195rank=242500 of ranks=465195rank=242600 of ranks=465195rank=242700 of ranks=465195rank=242800 of ranks=465195rank=242900 of ranks=465195rank=243000 of ranks=465195rank=243100 of ranks=465195rank=243200 of ranks=465195rank=243300 of ranks=465195rank=243400 of ranks=465195rank=243500 of ranks=465195rank=243600 of ranks=465195rank=243700 of ranks=465195rank=243800 of ranks=465195rank=243900 of ranks=465195rank=244000 of ranks=465195rank=244100 of ranks=465195rank=244200 of ranks=465195rank=244300 of ranks=465195rank=244400 of ranks=465195rank=244500 of ranks=465195rank=244600 of ranks=465195rank=244700 of ranks=465195rank=244800 of ranks=465195rank=244900 of ranks=465195rank=245000 of ranks=465195rank=245100 of ranks=465195rank=245200 of ranks=465195rank=245300 of ranks=465195rank=245400 of ranks=465195rank=245500 of ranks=465195rank=245600 of ranks=465195rank=245700 of ranks=465195rank=245800 of ranks=465195rank=245900 of ranks=465195rank=246000 of ranks=465195rank=246100 of ranks=465195rank=246200 of ranks=465195rank=246300 of ranks=465195rank=246400 of ranks=465195rank=246500 of ranks=465195rank=246600 of ranks=465195rank=246700 of ranks=465195rank=246800 of ranks=465195rank=246900 of ranks=465195rank=247000 of ranks=465195rank=247100 of ranks=465195rank=247200 of ranks=465195rank=247300 of ranks=465195rank=247400 of ranks=465195rank=247500 of ranks=465195rank=247600 of ranks=465195rank=247700 of ranks=465195rank=247800 of ranks=465195rank=247900 of ranks=465195rank=248000 of ranks=465195rank=248100 of ranks=465195rank=248200 of ranks=465195rank=248300 of ranks=465195rank=248400 of ranks=465195rank=248500 of ranks=465195rank=248600 of ranks=465195rank=248700 of ranks=465195rank=248800 of ranks=465195rank=248900 of ranks=465195rank=249000 of ranks=465195rank=249100 of ranks=465195rank=249200 of ranks=465195rank=249300 of ranks=465195rank=249400 of ranks=465195rank=249500 of ranks=465195rank=249600 of ranks=465195rank=249700 of ranks=465195rank=249800 of ranks=465195rank=249900 of ranks=465195rank=250000 of ranks=465195rank=250100 of ranks=465195rank=250200 of ranks=465195rank=250300 of ranks=465195rank=250400 of ranks=465195rank=250500 of ranks=465195rank=250600 of ranks=465195rank=250700 of ranks=465195rank=250800 of ranks=465195rank=250900 of ranks=465195rank=251000 of ranks=465195rank=251100 of ranks=465195rank=251200 of ranks=465195rank=251300 of ranks=465195rank=251400 of ranks=465195rank=251500 of ranks=465195rank=251600 of ranks=465195rank=251700 of ranks=465195rank=251800 of ranks=465195rank=251900 of ranks=465195rank=252000 of ranks=465195rank=252100 of ranks=465195rank=252200 of ranks=465195rank=252300 of ranks=465195rank=252400 of ranks=465195rank=252500 of ranks=465195rank=252600 of ranks=465195rank=252700 of ranks=465195rank=252800 of ranks=465195rank=252900 of ranks=465195rank=253000 of ranks=465195rank=253100 of ranks=465195rank=253200 of ranks=465195rank=253300 of ranks=465195rank=253400 of ranks=465195rank=253500 of ranks=465195rank=253600 of ranks=465195rank=253700 of ranks=465195rank=253800 of ranks=465195rank=253900 of ranks=465195rank=254000 of ranks=465195rank=254100 of ranks=465195rank=254200 of ranks=465195rank=254300 of ranks=465195rank=254400 of ranks=465195rank=254500 of ranks=465195rank=254600 of ranks=465195rank=254700 of ranks=465195rank=254800 of ranks=465195rank=254900 of ranks=465195rank=255000 of ranks=465195rank=255100 of ranks=465195rank=255200 of ranks=465195rank=255300 of ranks=465195rank=255400 of ranks=465195rank=255500 of ranks=465195rank=255600 of ranks=465195rank=255700 of ranks=465195rank=255800 of ranks=465195rank=255900 of ranks=465195rank=256000 of ranks=465195rank=256100 of ranks=465195rank=256200 of ranks=465195rank=256300 of ranks=465195rank=256400 of ranks=465195rank=256500 of ranks=465195rank=256600 of ranks=465195rank=256700 of ranks=465195rank=256800 of ranks=465195rank=256900 of ranks=465195rank=257000 of ranks=465195rank=257100 of ranks=465195rank=257200 of ranks=465195rank=257300 of ranks=465195rank=257400 of ranks=465195rank=257500 of ranks=465195rank=257600 of ranks=465195rank=257700 of ranks=465195rank=257800 of ranks=465195rank=257900 of ranks=465195rank=258000 of ranks=465195rank=258100 of ranks=465195rank=258200 of ranks=465195rank=258300 of ranks=465195rank=258400 of ranks=465195rank=258500 of ranks=465195rank=258600 of ranks=465195rank=258700 of ranks=465195rank=258800 of ranks=465195rank=258900 of ranks=465195rank=259000 of ranks=465195rank=259100 of ranks=465195rank=259200 of ranks=465195rank=259300 of ranks=465195rank=259400 of ranks=465195rank=259500 of ranks=465195rank=259600 of ranks=465195rank=259700 of ranks=465195rank=259800 of ranks=465195rank=259900 of ranks=465195rank=260000 of ranks=465195rank=260100 of ranks=465195rank=260200 of ranks=465195rank=260300 of ranks=465195rank=260400 of ranks=465195rank=260500 of ranks=465195rank=260600 of ranks=465195rank=260700 of ranks=465195rank=260800 of ranks=465195rank=260900 of ranks=465195rank=261000 of ranks=465195rank=261100 of ranks=465195rank=261200 of ranks=465195rank=261300 of ranks=465195rank=261400 of ranks=465195rank=261500 of ranks=465195rank=261600 of ranks=465195rank=261700 of ranks=465195rank=261800 of ranks=465195rank=261900 of ranks=465195rank=262000 of ranks=465195rank=262100 of ranks=465195rank=262200 of ranks=465195rank=262300 of ranks=465195rank=262400 of ranks=465195rank=262500 of ranks=465195rank=262600 of ranks=465195rank=262700 of ranks=465195rank=262800 of ranks=465195rank=262900 of ranks=465195rank=263000 of ranks=465195rank=263100 of ranks=465195rank=263200 of ranks=465195rank=263300 of ranks=465195rank=263400 of ranks=465195rank=263500 of ranks=465195rank=263600 of ranks=465195rank=263700 of ranks=465195rank=263800 of ranks=465195rank=263900 of ranks=465195rank=264000 of ranks=465195rank=264100 of ranks=465195rank=264200 of ranks=465195rank=264300 of ranks=465195rank=264400 of ranks=465195rank=264500 of ranks=465195rank=264600 of ranks=465195rank=264700 of ranks=465195rank=264800 of ranks=465195rank=264900 of ranks=465195rank=265000 of ranks=465195rank=265100 of ranks=465195rank=265200 of ranks=465195rank=265300 of ranks=465195rank=265400 of ranks=465195rank=265500 of ranks=465195rank=265600 of ranks=465195rank=265700 of ranks=465195rank=265800 of ranks=465195rank=265900 of ranks=465195rank=266000 of ranks=465195rank=266100 of ranks=465195rank=266200 of ranks=465195rank=266300 of ranks=465195rank=266400 of ranks=465195rank=266500 of ranks=465195rank=266600 of ranks=465195rank=266700 of ranks=465195rank=266800 of ranks=465195rank=266900 of ranks=465195rank=267000 of ranks=465195rank=267100 of ranks=465195rank=267200 of ranks=465195rank=267300 of ranks=465195rank=267400 of ranks=465195rank=267500 of ranks=465195rank=267600 of ranks=465195rank=267700 of ranks=465195rank=267800 of ranks=465195rank=267900 of ranks=465195rank=268000 of ranks=465195rank=268100 of ranks=465195rank=268200 of ranks=465195rank=268300 of ranks=465195rank=268400 of ranks=465195rank=268500 of ranks=465195rank=268600 of ranks=465195rank=268700 of ranks=465195rank=268800 of ranks=465195rank=268900 of ranks=465195rank=269000 of ranks=465195rank=269100 of ranks=465195rank=269200 of ranks=465195rank=269300 of ranks=465195rank=269400 of ranks=465195rank=269500 of ranks=465195rank=269600 of ranks=465195rank=269700 of ranks=465195rank=269800 of ranks=465195rank=269900 of ranks=465195rank=270000 of ranks=465195rank=270100 of ranks=465195rank=270200 of ranks=465195rank=270300 of ranks=465195rank=270400 of ranks=465195rank=270500 of ranks=465195rank=270600 of ranks=465195rank=270700 of ranks=465195rank=270800 of ranks=465195rank=270900 of ranks=465195rank=271000 of ranks=465195rank=271100 of ranks=465195rank=271200 of ranks=465195rank=271300 of ranks=465195rank=271400 of ranks=465195rank=271500 of ranks=465195rank=271600 of ranks=465195rank=271700 of ranks=465195rank=271800 of ranks=465195rank=271900 of ranks=465195rank=272000 of ranks=465195rank=272100 of ranks=465195rank=272200 of ranks=465195rank=272300 of ranks=465195rank=272400 of ranks=465195rank=272500 of ranks=465195rank=272600 of ranks=465195rank=272700 of ranks=465195rank=272800 of ranks=465195rank=272900 of ranks=465195rank=273000 of ranks=465195rank=273100 of ranks=465195rank=273200 of ranks=465195rank=273300 of ranks=465195rank=273400 of ranks=465195rank=273500 of ranks=465195rank=273600 of ranks=465195rank=273700 of ranks=465195rank=273800 of ranks=465195rank=273900 of ranks=465195rank=274000 of ranks=465195rank=274100 of ranks=465195rank=274200 of ranks=465195rank=274300 of ranks=465195rank=274400 of ranks=465195rank=274500 of ranks=465195rank=274600 of ranks=465195rank=274700 of ranks=465195rank=274800 of ranks=465195rank=274900 of ranks=465195rank=275000 of ranks=465195rank=275100 of ranks=465195rank=275200 of ranks=465195rank=275300 of ranks=465195rank=275400 of ranks=465195rank=275500 of ranks=465195rank=275600 of ranks=465195rank=275700 of ranks=465195rank=275800 of ranks=465195rank=275900 of ranks=465195rank=276000 of ranks=465195rank=276100 of ranks=465195rank=276200 of ranks=465195rank=276300 of ranks=465195rank=276400 of ranks=465195rank=276500 of ranks=465195rank=276600 of ranks=465195rank=276700 of ranks=465195rank=276800 of ranks=465195rank=276900 of ranks=465195rank=277000 of ranks=465195rank=277100 of ranks=465195rank=277200 of ranks=465195rank=277300 of ranks=465195rank=277400 of ranks=465195rank=277500 of ranks=465195rank=277600 of ranks=465195rank=277700 of ranks=465195rank=277800 of ranks=465195rank=277900 of ranks=465195rank=278000 of ranks=465195rank=278100 of ranks=465195rank=278200 of ranks=465195rank=278300 of ranks=465195rank=278400 of ranks=465195rank=278500 of ranks=465195rank=278600 of ranks=465195rank=278700 of ranks=465195rank=278800 of ranks=465195rank=278900 of ranks=465195rank=279000 of ranks=465195rank=279100 of ranks=465195rank=279200 of ranks=465195rank=279300 of ranks=465195rank=279400 of ranks=465195rank=279500 of ranks=465195rank=279600 of ranks=465195rank=279700 of ranks=465195rank=279800 of ranks=465195rank=279900 of ranks=465195rank=280000 of ranks=465195rank=280100 of ranks=465195rank=280200 of ranks=465195rank=280300 of ranks=465195rank=280400 of ranks=465195rank=280500 of ranks=465195rank=280600 of ranks=465195rank=280700 of ranks=465195rank=280800 of ranks=465195rank=280900 of ranks=465195rank=281000 of ranks=465195rank=281100 of ranks=465195rank=281200 of ranks=465195rank=281300 of ranks=465195rank=281400 of ranks=465195rank=281500 of ranks=465195rank=281600 of ranks=465195rank=281700 of ranks=465195rank=281800 of ranks=465195rank=281900 of ranks=465195rank=282000 of ranks=465195rank=282100 of ranks=465195rank=282200 of ranks=465195rank=282300 of ranks=465195rank=282400 of ranks=465195rank=282500 of ranks=465195rank=282600 of ranks=465195rank=282700 of ranks=465195rank=282800 of ranks=465195rank=282900 of ranks=465195rank=283000 of ranks=465195rank=283100 of ranks=465195rank=283200 of ranks=465195rank=283300 of ranks=465195rank=283400 of ranks=465195rank=283500 of ranks=465195rank=283600 of ranks=465195rank=283700 of ranks=465195rank=283800 of ranks=465195rank=283900 of ranks=465195rank=284000 of ranks=465195rank=284100 of ranks=465195rank=284200 of ranks=465195rank=284300 of ranks=465195rank=284400 of ranks=465195rank=284500 of ranks=465195rank=284600 of ranks=465195rank=284700 of ranks=465195rank=284800 of ranks=465195rank=284900 of ranks=465195rank=285000 of ranks=465195rank=285100 of ranks=465195rank=285200 of ranks=465195rank=285300 of ranks=465195rank=285400 of ranks=465195rank=285500 of ranks=465195rank=285600 of ranks=465195rank=285700 of ranks=465195rank=285800 of ranks=465195rank=285900 of ranks=465195rank=286000 of ranks=465195rank=286100 of ranks=465195rank=286200 of ranks=465195rank=286300 of ranks=465195rank=286400 of ranks=465195rank=286500 of ranks=465195rank=286600 of ranks=465195rank=286700 of ranks=465195rank=286800 of ranks=465195rank=286900 of ranks=465195rank=287000 of ranks=465195rank=287100 of ranks=465195rank=287200 of ranks=465195rank=287300 of ranks=465195rank=287400 of ranks=465195rank=287500 of ranks=465195rank=287600 of ranks=465195rank=287700 of ranks=465195rank=287800 of ranks=465195rank=287900 of ranks=465195rank=288000 of ranks=465195rank=288100 of ranks=465195rank=288200 of ranks=465195rank=288300 of ranks=465195rank=288400 of ranks=465195rank=288500 of ranks=465195rank=288600 of ranks=465195rank=288700 of ranks=465195rank=288800 of ranks=465195rank=288900 of ranks=465195rank=289000 of ranks=465195rank=289100 of ranks=465195rank=289200 of ranks=465195rank=289300 of ranks=465195rank=289400 of ranks=465195rank=289500 of ranks=465195rank=289600 of ranks=465195rank=289700 of ranks=465195rank=289800 of ranks=465195rank=289900 of ranks=465195rank=290000 of ranks=465195rank=290100 of ranks=465195rank=290200 of ranks=465195rank=290300 of ranks=465195rank=290400 of ranks=465195rank=290500 of ranks=465195rank=290600 of ranks=465195rank=290700 of ranks=465195rank=290800 of ranks=465195rank=290900 of ranks=465195rank=291000 of ranks=465195rank=291100 of ranks=465195rank=291200 of ranks=465195rank=291300 of ranks=465195rank=291400 of ranks=465195rank=291500 of ranks=465195rank=291600 of ranks=465195rank=291700 of ranks=465195rank=291800 of ranks=465195rank=291900 of ranks=465195rank=292000 of ranks=465195rank=292100 of ranks=465195rank=292200 of ranks=465195rank=292300 of ranks=465195rank=292400 of ranks=465195rank=292500 of ranks=465195rank=292600 of ranks=465195rank=292700 of ranks=465195rank=292800 of ranks=465195rank=292900 of ranks=465195rank=293000 of ranks=465195rank=293100 of ranks=465195rank=293200 of ranks=465195rank=293300 of ranks=465195rank=293400 of ranks=465195rank=293500 of ranks=465195rank=293600 of ranks=465195rank=293700 of ranks=465195rank=293800 of ranks=465195rank=293900 of ranks=465195rank=294000 of ranks=465195rank=294100 of ranks=465195rank=294200 of ranks=465195rank=294300 of ranks=465195rank=294400 of ranks=465195rank=294500 of ranks=465195rank=294600 of ranks=465195rank=294700 of ranks=465195rank=294800 of ranks=465195rank=294900 of ranks=465195rank=295000 of ranks=465195rank=295100 of ranks=465195rank=295200 of ranks=465195rank=295300 of ranks=465195rank=295400 of ranks=465195rank=295500 of ranks=465195rank=295600 of ranks=465195rank=295700 of ranks=465195rank=295800 of ranks=465195rank=295900 of ranks=465195rank=296000 of ranks=465195rank=296100 of ranks=465195rank=296200 of ranks=465195rank=296300 of ranks=465195rank=296400 of ranks=465195rank=296500 of ranks=465195rank=296600 of ranks=465195rank=296700 of ranks=465195rank=296800 of ranks=465195rank=296900 of ranks=465195rank=297000 of ranks=465195rank=297100 of ranks=465195rank=297200 of ranks=465195rank=297300 of ranks=465195rank=297400 of ranks=465195rank=297500 of ranks=465195rank=297600 of ranks=465195rank=297700 of ranks=465195rank=297800 of ranks=465195rank=297900 of ranks=465195rank=298000 of ranks=465195rank=298100 of ranks=465195rank=298200 of ranks=465195rank=298300 of ranks=465195rank=298400 of ranks=465195rank=298500 of ranks=465195rank=298600 of ranks=465195rank=298700 of ranks=465195rank=298800 of ranks=465195rank=298900 of ranks=465195rank=299000 of ranks=465195rank=299100 of ranks=465195rank=299200 of ranks=465195rank=299300 of ranks=465195rank=299400 of ranks=465195rank=299500 of ranks=465195rank=299600 of ranks=465195rank=299700 of ranks=465195rank=299800 of ranks=465195rank=299900 of ranks=465195rank=300000 of ranks=465195rank=300100 of ranks=465195rank=300200 of ranks=465195rank=300300 of ranks=465195rank=300400 of ranks=465195rank=300500 of ranks=465195rank=300600 of ranks=465195rank=300700 of ranks=465195rank=300800 of ranks=465195rank=300900 of ranks=465195rank=301000 of ranks=465195rank=301100 of ranks=465195rank=301200 of ranks=465195rank=301300 of ranks=465195rank=301400 of ranks=465195rank=301500 of ranks=465195rank=301600 of ranks=465195rank=301700 of ranks=465195rank=301800 of ranks=465195rank=301900 of ranks=465195rank=302000 of ranks=465195rank=302100 of ranks=465195rank=302200 of ranks=465195rank=302300 of ranks=465195rank=302400 of ranks=465195rank=302500 of ranks=465195rank=302600 of ranks=465195rank=302700 of ranks=465195rank=302800 of ranks=465195rank=302900 of ranks=465195rank=303000 of ranks=465195rank=303100 of ranks=465195rank=303200 of ranks=465195rank=303300 of ranks=465195rank=303400 of ranks=465195rank=303500 of ranks=465195rank=303600 of ranks=465195rank=303700 of ranks=465195rank=303800 of ranks=465195rank=303900 of ranks=465195rank=304000 of ranks=465195rank=304100 of ranks=465195rank=304200 of ranks=465195rank=304300 of ranks=465195rank=304400 of ranks=465195rank=304500 of ranks=465195rank=304600 of ranks=465195rank=304700 of ranks=465195rank=304800 of ranks=465195rank=304900 of ranks=465195rank=305000 of ranks=465195rank=305100 of ranks=465195rank=305200 of ranks=465195rank=305300 of ranks=465195rank=305400 of ranks=465195rank=305500 of ranks=465195rank=305600 of ranks=465195rank=305700 of ranks=465195rank=305800 of ranks=465195rank=305900 of ranks=465195rank=306000 of ranks=465195rank=306100 of ranks=465195rank=306200 of ranks=465195rank=306300 of ranks=465195rank=306400 of ranks=465195rank=306500 of ranks=465195rank=306600 of ranks=465195rank=306700 of ranks=465195rank=306800 of ranks=465195rank=306900 of ranks=465195rank=307000 of ranks=465195rank=307100 of ranks=465195rank=307200 of ranks=465195rank=307300 of ranks=465195rank=307400 of ranks=465195rank=307500 of ranks=465195rank=307600 of ranks=465195rank=307700 of ranks=465195rank=307800 of ranks=465195rank=307900 of ranks=465195rank=308000 of ranks=465195rank=308100 of ranks=465195rank=308200 of ranks=465195rank=308300 of ranks=465195rank=308400 of ranks=465195rank=308500 of ranks=465195rank=308600 of ranks=465195rank=308700 of ranks=465195rank=308800 of ranks=465195rank=308900 of ranks=465195rank=309000 of ranks=465195rank=309100 of ranks=465195rank=309200 of ranks=465195rank=309300 of ranks=465195rank=309400 of ranks=465195rank=309500 of ranks=465195rank=309600 of ranks=465195rank=309700 of ranks=465195rank=309800 of ranks=465195rank=309900 of ranks=465195rank=310000 of ranks=465195rank=310100 of ranks=465195rank=310200 of ranks=465195rank=310300 of ranks=465195rank=310400 of ranks=465195rank=310500 of ranks=465195rank=310600 of ranks=465195rank=310700 of ranks=465195rank=310800 of ranks=465195rank=310900 of ranks=465195rank=311000 of ranks=465195rank=311100 of ranks=465195rank=311200 of ranks=465195rank=311300 of ranks=465195rank=311400 of ranks=465195rank=311500 of ranks=465195rank=311600 of ranks=465195rank=311700 of ranks=465195rank=311800 of ranks=465195rank=311900 of ranks=465195rank=312000 of ranks=465195rank=312100 of ranks=465195rank=312200 of ranks=465195rank=312300 of ranks=465195rank=312400 of ranks=465195rank=312500 of ranks=465195rank=312600 of ranks=465195rank=312700 of ranks=465195rank=312800 of ranks=465195rank=312900 of ranks=465195rank=313000 of ranks=465195rank=313100 of ranks=465195rank=313200 of ranks=465195rank=313300 of ranks=465195rank=313400 of ranks=465195rank=313500 of ranks=465195rank=313600 of ranks=465195rank=313700 of ranks=465195rank=313800 of ranks=465195rank=313900 of ranks=465195rank=314000 of ranks=465195rank=314100 of ranks=465195rank=314200 of ranks=465195rank=314300 of ranks=465195rank=314400 of ranks=465195rank=314500 of ranks=465195rank=314600 of ranks=465195rank=314700 of ranks=465195rank=314800 of ranks=465195rank=314900 of ranks=465195rank=315000 of ranks=465195rank=315100 of ranks=465195rank=315200 of ranks=465195rank=315300 of ranks=465195rank=315400 of ranks=465195rank=315500 of ranks=465195rank=315600 of ranks=465195rank=315700 of ranks=465195rank=315800 of ranks=465195rank=315900 of ranks=465195rank=316000 of ranks=465195rank=316100 of ranks=465195rank=316200 of ranks=465195rank=316300 of ranks=465195rank=316400 of ranks=465195rank=316500 of ranks=465195rank=316600 of ranks=465195rank=316700 of ranks=465195rank=316800 of ranks=465195rank=316900 of ranks=465195rank=317000 of ranks=465195rank=317100 of ranks=465195rank=317200 of ranks=465195rank=317300 of ranks=465195rank=317400 of ranks=465195rank=317500 of ranks=465195rank=317600 of ranks=465195rank=317700 of ranks=465195rank=317800 of ranks=465195rank=317900 of ranks=465195rank=318000 of ranks=465195rank=318100 of ranks=465195rank=318200 of ranks=465195rank=318300 of ranks=465195rank=318400 of ranks=465195rank=318500 of ranks=465195rank=318600 of ranks=465195rank=318700 of ranks=465195rank=318800 of ranks=465195rank=318900 of ranks=465195rank=319000 of ranks=465195rank=319100 of ranks=465195rank=319200 of ranks=465195rank=319300 of ranks=465195rank=319400 of ranks=465195rank=319500 of ranks=465195rank=319600 of ranks=465195rank=319700 of ranks=465195rank=319800 of ranks=465195rank=319900 of ranks=465195rank=320000 of ranks=465195rank=320100 of ranks=465195rank=320200 of ranks=465195rank=320300 of ranks=465195rank=320400 of ranks=465195rank=320500 of ranks=465195rank=320600 of ranks=465195rank=320700 of ranks=465195rank=320800 of ranks=465195rank=320900 of ranks=465195rank=321000 of ranks=465195rank=321100 of ranks=465195rank=321200 of ranks=465195rank=321300 of ranks=465195rank=321400 of ranks=465195rank=321500 of ranks=465195rank=321600 of ranks=465195rank=321700 of ranks=465195rank=321800 of ranks=465195rank=321900 of ranks=465195rank=322000 of ranks=465195rank=322100 of ranks=465195rank=322200 of ranks=465195rank=322300 of ranks=465195rank=322400 of ranks=465195rank=322500 of ranks=465195rank=322600 of ranks=465195rank=322700 of ranks=465195rank=322800 of ranks=465195rank=322900 of ranks=465195rank=323000 of ranks=465195rank=323100 of ranks=465195rank=323200 of ranks=465195rank=323300 of ranks=465195rank=323400 of ranks=465195rank=323500 of ranks=465195rank=323600 of ranks=465195rank=323700 of ranks=465195rank=323800 of ranks=465195rank=323900 of ranks=465195rank=324000 of ranks=465195rank=324100 of ranks=465195rank=324200 of ranks=465195rank=324300 of ranks=465195rank=324400 of ranks=465195rank=324500 of ranks=465195rank=324600 of ranks=465195rank=324700 of ranks=465195rank=324800 of ranks=465195rank=324900 of ranks=465195rank=325000 of ranks=465195rank=325100 of ranks=465195rank=325200 of ranks=465195rank=325300 of ranks=465195rank=325400 of ranks=465195rank=325500 of ranks=465195rank=325600 of ranks=465195rank=325700 of ranks=465195rank=325800 of ranks=465195rank=325900 of ranks=465195rank=326000 of ranks=465195rank=326100 of ranks=465195rank=326200 of ranks=465195rank=326300 of ranks=465195rank=326400 of ranks=465195rank=326500 of ranks=465195rank=326600 of ranks=465195rank=326700 of ranks=465195rank=326800 of ranks=465195rank=326900 of ranks=465195rank=327000 of ranks=465195rank=327100 of ranks=465195rank=327200 of ranks=465195rank=327300 of ranks=465195rank=327400 of ranks=465195rank=327500 of ranks=465195rank=327600 of ranks=465195rank=327700 of ranks=465195rank=327800 of ranks=465195rank=327900 of ranks=465195rank=328000 of ranks=465195rank=328100 of ranks=465195rank=328200 of ranks=465195rank=328300 of ranks=465195rank=328400 of ranks=465195rank=328500 of ranks=465195rank=328600 of ranks=465195rank=328700 of ranks=465195rank=328800 of ranks=465195rank=328900 of ranks=465195rank=329000 of ranks=465195rank=329100 of ranks=465195rank=329200 of ranks=465195rank=329300 of ranks=465195rank=329400 of ranks=465195rank=329500 of ranks=465195rank=329600 of ranks=465195rank=329700 of ranks=465195rank=329800 of ranks=465195rank=329900 of ranks=465195rank=330000 of ranks=465195rank=330100 of ranks=465195rank=330200 of ranks=465195rank=330300 of ranks=465195rank=330400 of ranks=465195rank=330500 of ranks=465195rank=330600 of ranks=465195rank=330700 of ranks=465195rank=330800 of ranks=465195rank=330900 of ranks=465195rank=331000 of ranks=465195rank=331100 of ranks=465195rank=331200 of ranks=465195rank=331300 of ranks=465195rank=331400 of ranks=465195rank=331500 of ranks=465195rank=331600 of ranks=465195rank=331700 of ranks=465195rank=331800 of ranks=465195rank=331900 of ranks=465195rank=332000 of ranks=465195rank=332100 of ranks=465195rank=332200 of ranks=465195rank=332300 of ranks=465195rank=332400 of ranks=465195rank=332500 of ranks=465195rank=332600 of ranks=465195rank=332700 of ranks=465195rank=332800 of ranks=465195rank=332900 of ranks=465195rank=333000 of ranks=465195rank=333100 of ranks=465195rank=333200 of ranks=465195rank=333300 of ranks=465195rank=333400 of ranks=465195rank=333500 of ranks=465195rank=333600 of ranks=465195rank=333700 of ranks=465195rank=333800 of ranks=465195rank=333900 of ranks=465195rank=334000 of ranks=465195rank=334100 of ranks=465195rank=334200 of ranks=465195rank=334300 of ranks=465195rank=334400 of ranks=465195rank=334500 of ranks=465195rank=334600 of ranks=465195rank=334700 of ranks=465195rank=334800 of ranks=465195rank=334900 of ranks=465195rank=335000 of ranks=465195rank=335100 of ranks=465195rank=335200 of ranks=465195rank=335300 of ranks=465195rank=335400 of ranks=465195rank=335500 of ranks=465195rank=335600 of ranks=465195rank=335700 of ranks=465195rank=335800 of ranks=465195rank=335900 of ranks=465195rank=336000 of ranks=465195rank=336100 of ranks=465195rank=336200 of ranks=465195rank=336300 of ranks=465195rank=336400 of ranks=465195rank=336500 of ranks=465195rank=336600 of ranks=465195rank=336700 of ranks=465195rank=336800 of ranks=465195rank=336900 of ranks=465195rank=337000 of ranks=465195rank=337100 of ranks=465195rank=337200 of ranks=465195rank=337300 of ranks=465195rank=337400 of ranks=465195rank=337500 of ranks=465195rank=337600 of ranks=465195rank=337700 of ranks=465195rank=337800 of ranks=465195rank=337900 of ranks=465195rank=338000 of ranks=465195rank=338100 of ranks=465195rank=338200 of ranks=465195rank=338300 of ranks=465195rank=338400 of ranks=465195rank=338500 of ranks=465195rank=338600 of ranks=465195rank=338700 of ranks=465195rank=338800 of ranks=465195rank=338900 of ranks=465195rank=339000 of ranks=465195rank=339100 of ranks=465195rank=339200 of ranks=465195rank=339300 of ranks=465195rank=339400 of ranks=465195rank=339500 of ranks=465195rank=339600 of ranks=465195rank=339700 of ranks=465195rank=339800 of ranks=465195rank=339900 of ranks=465195rank=340000 of ranks=465195rank=340100 of ranks=465195rank=340200 of ranks=465195rank=340300 of ranks=465195rank=340400 of ranks=465195rank=340500 of ranks=465195rank=340600 of ranks=465195rank=340700 of ranks=465195rank=340800 of ranks=465195rank=340900 of ranks=465195rank=341000 of ranks=465195rank=341100 of ranks=465195rank=341200 of ranks=465195rank=341300 of ranks=465195rank=341400 of ranks=465195rank=341500 of ranks=465195rank=341600 of ranks=465195rank=341700 of ranks=465195rank=341800 of ranks=465195rank=341900 of ranks=465195rank=342000 of ranks=465195rank=342100 of ranks=465195rank=342200 of ranks=465195rank=342300 of ranks=465195rank=342400 of ranks=465195rank=342500 of ranks=465195rank=342600 of ranks=465195rank=342700 of ranks=465195rank=342800 of ranks=465195rank=342900 of ranks=465195rank=343000 of ranks=465195rank=343100 of ranks=465195rank=343200 of ranks=465195rank=343300 of ranks=465195rank=343400 of ranks=465195rank=343500 of ranks=465195rank=343600 of ranks=465195rank=343700 of ranks=465195rank=343800 of ranks=465195rank=343900 of ranks=465195rank=344000 of ranks=465195rank=344100 of ranks=465195rank=344200 of ranks=465195rank=344300 of ranks=465195rank=344400 of ranks=465195rank=344500 of ranks=465195rank=344600 of ranks=465195rank=344700 of ranks=465195rank=344800 of ranks=465195rank=344900 of ranks=465195rank=345000 of ranks=465195rank=345100 of ranks=465195rank=345200 of ranks=465195rank=345300 of ranks=465195rank=345400 of ranks=465195rank=345500 of ranks=465195rank=345600 of ranks=465195rank=345700 of ranks=465195rank=345800 of ranks=465195rank=345900 of ranks=465195rank=346000 of ranks=465195rank=346100 of ranks=465195rank=346200 of ranks=465195rank=346300 of ranks=465195rank=346400 of ranks=465195rank=346500 of ranks=465195rank=346600 of ranks=465195rank=346700 of ranks=465195rank=346800 of ranks=465195rank=346900 of ranks=465195rank=347000 of ranks=465195rank=347100 of ranks=465195rank=347200 of ranks=465195rank=347300 of ranks=465195rank=347400 of ranks=465195rank=347500 of ranks=465195rank=347600 of ranks=465195rank=347700 of ranks=465195rank=347800 of ranks=465195rank=347900 of ranks=465195rank=348000 of ranks=465195rank=348100 of ranks=465195rank=348200 of ranks=465195rank=348300 of ranks=465195rank=348400 of ranks=465195rank=348500 of ranks=465195rank=348600 of ranks=465195rank=348700 of ranks=465195rank=348800 of ranks=465195rank=348900 of ranks=465195rank=349000 of ranks=465195rank=349100 of ranks=465195rank=349200 of ranks=465195rank=349300 of ranks=465195rank=349400 of ranks=465195rank=349500 of ranks=465195rank=349600 of ranks=465195rank=349700 of ranks=465195rank=349800 of ranks=465195rank=349900 of ranks=465195rank=350000 of ranks=465195rank=350100 of ranks=465195rank=350200 of ranks=465195rank=350300 of ranks=465195rank=350400 of ranks=465195rank=350500 of ranks=465195rank=350600 of ranks=465195rank=350700 of ranks=465195rank=350800 of ranks=465195rank=350900 of ranks=465195rank=351000 of ranks=465195rank=351100 of ranks=465195rank=351200 of ranks=465195rank=351300 of ranks=465195rank=351400 of ranks=465195rank=351500 of ranks=465195rank=351600 of ranks=465195rank=351700 of ranks=465195rank=351800 of ranks=465195rank=351900 of ranks=465195rank=352000 of ranks=465195rank=352100 of ranks=465195rank=352200 of ranks=465195rank=352300 of ranks=465195rank=352400 of ranks=465195rank=352500 of ranks=465195rank=352600 of ranks=465195rank=352700 of ranks=465195rank=352800 of ranks=465195rank=352900 of ranks=465195rank=353000 of ranks=465195rank=353100 of ranks=465195rank=353200 of ranks=465195rank=353300 of ranks=465195rank=353400 of ranks=465195rank=353500 of ranks=465195rank=353600 of ranks=465195rank=353700 of ranks=465195rank=353800 of ranks=465195rank=353900 of ranks=465195rank=354000 of ranks=465195rank=354100 of ranks=465195rank=354200 of ranks=465195rank=354300 of ranks=465195rank=354400 of ranks=465195rank=354500 of ranks=465195rank=354600 of ranks=465195rank=354700 of ranks=465195rank=354800 of ranks=465195rank=354900 of ranks=465195rank=355000 of ranks=465195rank=355100 of ranks=465195rank=355200 of ranks=465195rank=355300 of ranks=465195rank=355400 of ranks=465195rank=355500 of ranks=465195rank=355600 of ranks=465195rank=355700 of ranks=465195rank=355800 of ranks=465195rank=355900 of ranks=465195rank=356000 of ranks=465195rank=356100 of ranks=465195rank=356200 of ranks=465195rank=356300 of ranks=465195rank=356400 of ranks=465195rank=356500 of ranks=465195rank=356600 of ranks=465195rank=356700 of ranks=465195rank=356800 of ranks=465195rank=356900 of ranks=465195rank=357000 of ranks=465195rank=357100 of ranks=465195rank=357200 of ranks=465195rank=357300 of ranks=465195rank=357400 of ranks=465195rank=357500 of ranks=465195rank=357600 of ranks=465195rank=357700 of ranks=465195rank=357800 of ranks=465195rank=357900 of ranks=465195rank=358000 of ranks=465195rank=358100 of ranks=465195rank=358200 of ranks=465195rank=358300 of ranks=465195rank=358400 of ranks=465195rank=358500 of ranks=465195rank=358600 of ranks=465195rank=358700 of ranks=465195rank=358800 of ranks=465195rank=358900 of ranks=465195rank=359000 of ranks=465195rank=359100 of ranks=465195rank=359200 of ranks=465195rank=359300 of ranks=465195rank=359400 of ranks=465195rank=359500 of ranks=465195rank=359600 of ranks=465195rank=359700 of ranks=465195rank=359800 of ranks=465195rank=359900 of ranks=465195rank=360000 of ranks=465195rank=360100 of ranks=465195rank=360200 of ranks=465195rank=360300 of ranks=465195rank=360400 of ranks=465195rank=360500 of ranks=465195rank=360600 of ranks=465195rank=360700 of ranks=465195rank=360800 of ranks=465195rank=360900 of ranks=465195rank=361000 of ranks=465195rank=361100 of ranks=465195rank=361200 of ranks=465195rank=361300 of ranks=465195rank=361400 of ranks=465195rank=361500 of ranks=465195rank=361600 of ranks=465195rank=361700 of ranks=465195rank=361800 of ranks=465195rank=361900 of ranks=465195rank=362000 of ranks=465195rank=362100 of ranks=465195rank=362200 of ranks=465195rank=362300 of ranks=465195rank=362400 of ranks=465195rank=362500 of ranks=465195rank=362600 of ranks=465195rank=362700 of ranks=465195rank=362800 of ranks=465195rank=362900 of ranks=465195rank=363000 of ranks=465195rank=363100 of ranks=465195rank=363200 of ranks=465195rank=363300 of ranks=465195rank=363400 of ranks=465195rank=363500 of ranks=465195rank=363600 of ranks=465195rank=363700 of ranks=465195rank=363800 of ranks=465195rank=363900 of ranks=465195rank=364000 of ranks=465195rank=364100 of ranks=465195rank=364200 of ranks=465195rank=364300 of ranks=465195rank=364400 of ranks=465195rank=364500 of ranks=465195rank=364600 of ranks=465195rank=364700 of ranks=465195rank=364800 of ranks=465195rank=364900 of ranks=465195rank=365000 of ranks=465195rank=365100 of ranks=465195rank=365200 of ranks=465195rank=365300 of ranks=465195rank=365400 of ranks=465195rank=365500 of ranks=465195rank=365600 of ranks=465195rank=365700 of ranks=465195rank=365800 of ranks=465195rank=365900 of ranks=465195rank=366000 of ranks=465195rank=366100 of ranks=465195rank=366200 of ranks=465195rank=366300 of ranks=465195rank=366400 of ranks=465195rank=366500 of ranks=465195rank=366600 of ranks=465195rank=366700 of ranks=465195rank=366800 of ranks=465195rank=366900 of ranks=465195rank=367000 of ranks=465195rank=367100 of ranks=465195rank=367200 of ranks=465195rank=367300 of ranks=465195rank=367400 of ranks=465195rank=367500 of ranks=465195rank=367600 of ranks=465195rank=367700 of ranks=465195rank=367800 of ranks=465195rank=367900 of ranks=465195rank=368000 of ranks=465195rank=368100 of ranks=465195rank=368200 of ranks=465195rank=368300 of ranks=465195rank=368400 of ranks=465195rank=368500 of ranks=465195rank=368600 of ranks=465195rank=368700 of ranks=465195rank=368800 of ranks=465195rank=368900 of ranks=465195rank=369000 of ranks=465195rank=369100 of ranks=465195rank=369200 of ranks=465195rank=369300 of ranks=465195rank=369400 of ranks=465195rank=369500 of ranks=465195rank=369600 of ranks=465195rank=369700 of ranks=465195rank=369800 of ranks=465195rank=369900 of ranks=465195rank=370000 of ranks=465195rank=370100 of ranks=465195rank=370200 of ranks=465195rank=370300 of ranks=465195rank=370400 of ranks=465195rank=370500 of ranks=465195rank=370600 of ranks=465195rank=370700 of ranks=465195rank=370800 of ranks=465195rank=370900 of ranks=465195rank=371000 of ranks=465195rank=371100 of ranks=465195rank=371200 of ranks=465195rank=371300 of ranks=465195rank=371400 of ranks=465195rank=371500 of ranks=465195rank=371600 of ranks=465195rank=371700 of ranks=465195rank=371800 of ranks=465195rank=371900 of ranks=465195rank=372000 of ranks=465195rank=372100 of ranks=465195rank=372200 of ranks=465195rank=372300 of ranks=465195rank=372400 of ranks=465195rank=372500 of ranks=465195rank=372600 of ranks=465195rank=372700 of ranks=465195rank=372800 of ranks=465195rank=372900 of ranks=465195rank=373000 of ranks=465195rank=373100 of ranks=465195rank=373200 of ranks=465195rank=373300 of ranks=465195rank=373400 of ranks=465195rank=373500 of ranks=465195rank=373600 of ranks=465195rank=373700 of ranks=465195rank=373800 of ranks=465195rank=373900 of ranks=465195rank=374000 of ranks=465195rank=374100 of ranks=465195rank=374200 of ranks=465195rank=374300 of ranks=465195rank=374400 of ranks=465195rank=374500 of ranks=465195rank=374600 of ranks=465195rank=374700 of ranks=465195rank=374800 of ranks=465195rank=374900 of ranks=465195rank=375000 of ranks=465195rank=375100 of ranks=465195rank=375200 of ranks=465195rank=375300 of ranks=465195rank=375400 of ranks=465195rank=375500 of ranks=465195rank=375600 of ranks=465195rank=375700 of ranks=465195rank=375800 of ranks=465195rank=375900 of ranks=465195rank=376000 of ranks=465195rank=376100 of ranks=465195rank=376200 of ranks=465195rank=376300 of ranks=465195rank=376400 of ranks=465195rank=376500 of ranks=465195rank=376600 of ranks=465195rank=376700 of ranks=465195rank=376800 of ranks=465195rank=376900 of ranks=465195rank=377000 of ranks=465195rank=377100 of ranks=465195rank=377200 of ranks=465195rank=377300 of ranks=465195rank=377400 of ranks=465195rank=377500 of ranks=465195rank=377600 of ranks=465195rank=377700 of ranks=465195rank=377800 of ranks=465195rank=377900 of ranks=465195rank=378000 of ranks=465195rank=378100 of ranks=465195rank=378200 of ranks=465195rank=378300 of ranks=465195rank=378400 of ranks=465195rank=378500 of ranks=465195rank=378600 of ranks=465195rank=378700 of ranks=465195rank=378800 of ranks=465195rank=378900 of ranks=465195rank=379000 of ranks=465195rank=379100 of ranks=465195rank=379200 of ranks=465195rank=379300 of ranks=465195rank=379400 of ranks=465195rank=379500 of ranks=465195rank=379600 of ranks=465195rank=379700 of ranks=465195rank=379800 of ranks=465195rank=379900 of ranks=465195rank=380000 of ranks=465195rank=380100 of ranks=465195rank=380200 of ranks=465195rank=380300 of ranks=465195rank=380400 of ranks=465195rank=380500 of ranks=465195rank=380600 of ranks=465195rank=380700 of ranks=465195rank=380800 of ranks=465195rank=380900 of ranks=465195rank=381000 of ranks=465195rank=381100 of ranks=465195rank=381200 of ranks=465195rank=381300 of ranks=465195rank=381400 of ranks=465195rank=381500 of ranks=465195rank=381600 of ranks=465195rank=381700 of ranks=465195rank=381800 of ranks=465195rank=381900 of ranks=465195rank=382000 of ranks=465195rank=382100 of ranks=465195rank=382200 of ranks=465195rank=382300 of ranks=465195rank=382400 of ranks=465195rank=382500 of ranks=465195rank=382600 of ranks=465195rank=382700 of ranks=465195rank=382800 of ranks=465195rank=382900 of ranks=465195rank=383000 of ranks=465195rank=383100 of ranks=465195rank=383200 of ranks=465195rank=383300 of ranks=465195rank=383400 of ranks=465195rank=383500 of ranks=465195rank=383600 of ranks=465195rank=383700 of ranks=465195rank=383800 of ranks=465195rank=383900 of ranks=465195rank=384000 of ranks=465195rank=384100 of ranks=465195rank=384200 of ranks=465195rank=384300 of ranks=465195rank=384400 of ranks=465195rank=384500 of ranks=465195rank=384600 of ranks=465195rank=384700 of ranks=465195rank=384800 of ranks=465195rank=384900 of ranks=465195rank=385000 of ranks=465195rank=385100 of ranks=465195rank=385200 of ranks=465195rank=385300 of ranks=465195rank=385400 of ranks=465195rank=385500 of ranks=465195rank=385600 of ranks=465195rank=385700 of ranks=465195rank=385800 of ranks=465195rank=385900 of ranks=465195rank=386000 of ranks=465195rank=386100 of ranks=465195rank=386200 of ranks=465195rank=386300 of ranks=465195rank=386400 of ranks=465195rank=386500 of ranks=465195rank=386600 of ranks=465195rank=386700 of ranks=465195rank=386800 of ranks=465195rank=386900 of ranks=465195rank=387000 of ranks=465195rank=387100 of ranks=465195rank=387200 of ranks=465195rank=387300 of ranks=465195rank=387400 of ranks=465195rank=387500 of ranks=465195rank=387600 of ranks=465195rank=387700 of ranks=465195rank=387800 of ranks=465195rank=387900 of ranks=465195rank=388000 of ranks=465195rank=388100 of ranks=465195rank=388200 of ranks=465195rank=388300 of ranks=465195rank=388400 of ranks=465195rank=388500 of ranks=465195rank=388600 of ranks=465195rank=388700 of ranks=465195rank=388800 of ranks=465195rank=388900 of ranks=465195rank=389000 of ranks=465195rank=389100 of ranks=465195rank=389200 of ranks=465195rank=389300 of ranks=465195rank=389400 of ranks=465195rank=389500 of ranks=465195rank=389600 of ranks=465195rank=389700 of ranks=465195rank=389800 of ranks=465195rank=389900 of ranks=465195rank=390000 of ranks=465195rank=390100 of ranks=465195rank=390200 of ranks=465195rank=390300 of ranks=465195rank=390400 of ranks=465195rank=390500 of ranks=465195rank=390600 of ranks=465195rank=390700 of ranks=465195rank=390800 of ranks=465195rank=390900 of ranks=465195rank=391000 of ranks=465195rank=391100 of ranks=465195rank=391200 of ranks=465195rank=391300 of ranks=465195rank=391400 of ranks=465195rank=391500 of ranks=465195rank=391600 of ranks=465195rank=391700 of ranks=465195rank=391800 of ranks=465195rank=391900 of ranks=465195rank=392000 of ranks=465195rank=392100 of ranks=465195rank=392200 of ranks=465195rank=392300 of ranks=465195rank=392400 of ranks=465195rank=392500 of ranks=465195rank=392600 of ranks=465195rank=392700 of ranks=465195rank=392800 of ranks=465195rank=392900 of ranks=465195rank=393000 of ranks=465195rank=393100 of ranks=465195rank=393200 of ranks=465195rank=393300 of ranks=465195rank=393400 of ranks=465195rank=393500 of ranks=465195rank=393600 of ranks=465195rank=393700 of ranks=465195rank=393800 of ranks=465195rank=393900 of ranks=465195rank=394000 of ranks=465195rank=394100 of ranks=465195rank=394200 of ranks=465195rank=394300 of ranks=465195rank=394400 of ranks=465195rank=394500 of ranks=465195rank=394600 of ranks=465195rank=394700 of ranks=465195rank=394800 of ranks=465195rank=394900 of ranks=465195rank=395000 of ranks=465195rank=395100 of ranks=465195rank=395200 of ranks=465195rank=395300 of ranks=465195rank=395400 of ranks=465195rank=395500 of ranks=465195rank=395600 of ranks=465195rank=395700 of ranks=465195rank=395800 of ranks=465195rank=395900 of ranks=465195rank=396000 of ranks=465195rank=396100 of ranks=465195rank=396200 of ranks=465195rank=396300 of ranks=465195rank=396400 of ranks=465195rank=396500 of ranks=465195rank=396600 of ranks=465195rank=396700 of ranks=465195rank=396800 of ranks=465195rank=396900 of ranks=465195rank=397000 of ranks=465195rank=397100 of ranks=465195rank=397200 of ranks=465195rank=397300 of ranks=465195rank=397400 of ranks=465195rank=397500 of ranks=465195rank=397600 of ranks=465195rank=397700 of ranks=465195rank=397800 of ranks=465195rank=397900 of ranks=465195rank=398000 of ranks=465195rank=398100 of ranks=465195rank=398200 of ranks=465195rank=398300 of ranks=465195rank=398400 of ranks=465195rank=398500 of ranks=465195rank=398600 of ranks=465195rank=398700 of ranks=465195rank=398800 of ranks=465195rank=398900 of ranks=465195rank=399000 of ranks=465195rank=399100 of ranks=465195rank=399200 of ranks=465195rank=399300 of ranks=465195rank=399400 of ranks=465195rank=399500 of ranks=465195rank=399600 of ranks=465195rank=399700 of ranks=465195rank=399800 of ranks=465195rank=399900 of ranks=465195rank=400000 of ranks=465195rank=400100 of ranks=465195rank=400200 of ranks=465195rank=400300 of ranks=465195rank=400400 of ranks=465195rank=400500 of ranks=465195rank=400600 of ranks=465195rank=400700 of ranks=465195rank=400800 of ranks=465195rank=400900 of ranks=465195rank=401000 of ranks=465195rank=401100 of ranks=465195rank=401200 of ranks=465195rank=401300 of ranks=465195rank=401400 of ranks=465195rank=401500 of ranks=465195rank=401600 of ranks=465195rank=401700 of ranks=465195rank=401800 of ranks=465195rank=401900 of ranks=465195rank=402000 of ranks=465195rank=402100 of ranks=465195rank=402200 of ranks=465195rank=402300 of ranks=465195rank=402400 of ranks=465195rank=402500 of ranks=465195rank=402600 of ranks=465195rank=402700 of ranks=465195rank=402800 of ranks=465195rank=402900 of ranks=465195rank=403000 of ranks=465195rank=403100 of ranks=465195rank=403200 of ranks=465195rank=403300 of ranks=465195rank=403400 of ranks=465195rank=403500 of ranks=465195rank=403600 of ranks=465195rank=403700 of ranks=465195rank=403800 of ranks=465195rank=403900 of ranks=465195rank=404000 of ranks=465195rank=404100 of ranks=465195rank=404200 of ranks=465195rank=404300 of ranks=465195rank=404400 of ranks=465195rank=404500 of ranks=465195rank=404600 of ranks=465195rank=404700 of ranks=465195rank=404800 of ranks=465195rank=404900 of ranks=465195rank=405000 of ranks=465195rank=405100 of ranks=465195rank=405200 of ranks=465195rank=405300 of ranks=465195rank=405400 of ranks=465195rank=405500 of ranks=465195rank=405600 of ranks=465195rank=405700 of ranks=465195rank=405800 of ranks=465195rank=405900 of ranks=465195rank=406000 of ranks=465195rank=406100 of ranks=465195rank=406200 of ranks=465195rank=406300 of ranks=465195rank=406400 of ranks=465195rank=406500 of ranks=465195rank=406600 of ranks=465195rank=406700 of ranks=465195rank=406800 of ranks=465195rank=406900 of ranks=465195rank=407000 of ranks=465195rank=407100 of ranks=465195rank=407200 of ranks=465195rank=407300 of ranks=465195rank=407400 of ranks=465195rank=407500 of ranks=465195rank=407600 of ranks=465195rank=407700 of ranks=465195rank=407800 of ranks=465195rank=407900 of ranks=465195rank=408000 of ranks=465195rank=408100 of ranks=465195rank=408200 of ranks=465195rank=408300 of ranks=465195rank=408400 of ranks=465195rank=408500 of ranks=465195rank=408600 of ranks=465195rank=408700 of ranks=465195rank=408800 of ranks=465195rank=408900 of ranks=465195rank=409000 of ranks=465195rank=409100 of ranks=465195rank=409200 of ranks=465195rank=409300 of ranks=465195rank=409400 of ranks=465195rank=409500 of ranks=465195rank=409600 of ranks=465195rank=409700 of ranks=465195rank=409800 of ranks=465195rank=409900 of ranks=465195rank=410000 of ranks=465195rank=410100 of ranks=465195rank=410200 of ranks=465195rank=410300 of ranks=465195rank=410400 of ranks=465195rank=410500 of ranks=465195rank=410600 of ranks=465195rank=410700 of ranks=465195rank=410800 of ranks=465195rank=410900 of ranks=465195rank=411000 of ranks=465195rank=411100 of ranks=465195rank=411200 of ranks=465195rank=411300 of ranks=465195rank=411400 of ranks=465195rank=411500 of ranks=465195rank=411600 of ranks=465195rank=411700 of ranks=465195rank=411800 of ranks=465195rank=411900 of ranks=465195rank=412000 of ranks=465195rank=412100 of ranks=465195rank=412200 of ranks=465195rank=412300 of ranks=465195rank=412400 of ranks=465195rank=412500 of ranks=465195rank=412600 of ranks=465195rank=412700 of ranks=465195rank=412800 of ranks=465195rank=412900 of ranks=465195rank=413000 of ranks=465195rank=413100 of ranks=465195rank=413200 of ranks=465195rank=413300 of ranks=465195rank=413400 of ranks=465195rank=413500 of ranks=465195rank=413600 of ranks=465195rank=413700 of ranks=465195rank=413800 of ranks=465195rank=413900 of ranks=465195rank=414000 of ranks=465195rank=414100 of ranks=465195rank=414200 of ranks=465195rank=414300 of ranks=465195rank=414400 of ranks=465195rank=414500 of ranks=465195rank=414600 of ranks=465195rank=414700 of ranks=465195rank=414800 of ranks=465195rank=414900 of ranks=465195rank=415000 of ranks=465195rank=415100 of ranks=465195rank=415200 of ranks=465195rank=415300 of ranks=465195rank=415400 of ranks=465195rank=415500 of ranks=465195rank=415600 of ranks=465195rank=415700 of ranks=465195rank=415800 of ranks=465195rank=415900 of ranks=465195rank=416000 of ranks=465195rank=416100 of ranks=465195rank=416200 of ranks=465195rank=416300 of ranks=465195rank=416400 of ranks=465195rank=416500 of ranks=465195rank=416600 of ranks=465195rank=416700 of ranks=465195rank=416800 of ranks=465195rank=416900 of ranks=465195rank=417000 of ranks=465195rank=417100 of ranks=465195rank=417200 of ranks=465195rank=417300 of ranks=465195rank=417400 of ranks=465195rank=417500 of ranks=465195rank=417600 of ranks=465195rank=417700 of ranks=465195rank=417800 of ranks=465195rank=417900 of ranks=465195rank=418000 of ranks=465195rank=418100 of ranks=465195rank=418200 of ranks=465195rank=418300 of ranks=465195rank=418400 of ranks=465195rank=418500 of ranks=465195rank=418600 of ranks=465195rank=418700 of ranks=465195rank=418800 of ranks=465195rank=418900 of ranks=465195rank=419000 of ranks=465195rank=419100 of ranks=465195rank=419200 of ranks=465195rank=419300 of ranks=465195rank=419400 of ranks=465195rank=419500 of ranks=465195rank=419600 of ranks=465195rank=419700 of ranks=465195rank=419800 of ranks=465195rank=419900 of ranks=465195rank=420000 of ranks=465195rank=420100 of ranks=465195rank=420200 of ranks=465195rank=420300 of ranks=465195rank=420400 of ranks=465195rank=420500 of ranks=465195rank=420600 of ranks=465195rank=420700 of ranks=465195rank=420800 of ranks=465195rank=420900 of ranks=465195rank=421000 of ranks=465195rank=421100 of ranks=465195rank=421200 of ranks=465195rank=421300 of ranks=465195rank=421400 of ranks=465195rank=421500 of ranks=465195rank=421600 of ranks=465195rank=421700 of ranks=465195rank=421800 of ranks=465195rank=421900 of ranks=465195rank=422000 of ranks=465195rank=422100 of ranks=465195rank=422200 of ranks=465195rank=422300 of ranks=465195rank=422400 of ranks=465195rank=422500 of ranks=465195rank=422600 of ranks=465195rank=422700 of ranks=465195rank=422800 of ranks=465195rank=422900 of ranks=465195rank=423000 of ranks=465195rank=423100 of ranks=465195rank=423200 of ranks=465195rank=423300 of ranks=465195rank=423400 of ranks=465195rank=423500 of ranks=465195rank=423600 of ranks=465195rank=423700 of ranks=465195rank=423800 of ranks=465195rank=423900 of ranks=465195rank=424000 of ranks=465195rank=424100 of ranks=465195rank=424200 of ranks=465195rank=424300 of ranks=465195rank=424400 of ranks=465195rank=424500 of ranks=465195rank=424600 of ranks=465195rank=424700 of ranks=465195rank=424800 of ranks=465195rank=424900 of ranks=465195rank=425000 of ranks=465195rank=425100 of ranks=465195rank=425200 of ranks=465195rank=425300 of ranks=465195rank=425400 of ranks=465195rank=425500 of ranks=465195rank=425600 of ranks=465195rank=425700 of ranks=465195rank=425800 of ranks=465195rank=425900 of ranks=465195rank=426000 of ranks=465195rank=426100 of ranks=465195rank=426200 of ranks=465195rank=426300 of ranks=465195rank=426400 of ranks=465195rank=426500 of ranks=465195rank=426600 of ranks=465195rank=426700 of ranks=465195rank=426800 of ranks=465195rank=426900 of ranks=465195rank=427000 of ranks=465195rank=427100 of ranks=465195rank=427200 of ranks=465195rank=427300 of ranks=465195rank=427400 of ranks=465195rank=427500 of ranks=465195rank=427600 of ranks=465195rank=427700 of ranks=465195rank=427800 of ranks=465195rank=427900 of ranks=465195rank=428000 of ranks=465195rank=428100 of ranks=465195rank=428200 of ranks=465195rank=428300 of ranks=465195rank=428400 of ranks=465195rank=428500 of ranks=465195rank=428600 of ranks=465195rank=428700 of ranks=465195rank=428800 of ranks=465195rank=428900 of ranks=465195rank=429000 of ranks=465195rank=429100 of ranks=465195rank=429200 of ranks=465195rank=429300 of ranks=465195rank=429400 of ranks=465195rank=429500 of ranks=465195rank=429600 of ranks=465195rank=429700 of ranks=465195rank=429800 of ranks=465195rank=429900 of ranks=465195rank=430000 of ranks=465195rank=430100 of ranks=465195rank=430200 of ranks=465195rank=430300 of ranks=465195rank=430400 of ranks=465195rank=430500 of ranks=465195rank=430600 of ranks=465195rank=430700 of ranks=465195rank=430800 of ranks=465195rank=430900 of ranks=465195rank=431000 of ranks=465195rank=431100 of ranks=465195rank=431200 of ranks=465195rank=431300 of ranks=465195rank=431400 of ranks=465195rank=431500 of ranks=465195rank=431600 of ranks=465195rank=431700 of ranks=465195rank=431800 of ranks=465195rank=431900 of ranks=465195rank=432000 of ranks=465195rank=432100 of ranks=465195rank=432200 of ranks=465195rank=432300 of ranks=465195rank=432400 of ranks=465195rank=432500 of ranks=465195rank=432600 of ranks=465195rank=432700 of ranks=465195rank=432800 of ranks=465195rank=432900 of ranks=465195rank=433000 of ranks=465195rank=433100 of ranks=465195rank=433200 of ranks=465195rank=433300 of ranks=465195rank=433400 of ranks=465195rank=433500 of ranks=465195rank=433600 of ranks=465195rank=433700 of ranks=465195rank=433800 of ranks=465195rank=433900 of ranks=465195rank=434000 of ranks=465195rank=434100 of ranks=465195rank=434200 of ranks=465195rank=434300 of ranks=465195rank=434400 of ranks=465195rank=434500 of ranks=465195rank=434600 of ranks=465195rank=434700 of ranks=465195rank=434800 of ranks=465195rank=434900 of ranks=465195rank=435000 of ranks=465195rank=435100 of ranks=465195rank=435200 of ranks=465195rank=435300 of ranks=465195rank=435400 of ranks=465195rank=435500 of ranks=465195rank=435600 of ranks=465195rank=435700 of ranks=465195rank=435800 of ranks=465195rank=435900 of ranks=465195rank=436000 of ranks=465195rank=436100 of ranks=465195rank=436200 of ranks=465195rank=436300 of ranks=465195rank=436400 of ranks=465195rank=436500 of ranks=465195rank=436600 of ranks=465195rank=436700 of ranks=465195rank=436800 of ranks=465195rank=436900 of ranks=465195rank=437000 of ranks=465195rank=437100 of ranks=465195rank=437200 of ranks=465195rank=437300 of ranks=465195rank=437400 of ranks=465195rank=437500 of ranks=465195rank=437600 of ranks=465195rank=437700 of ranks=465195rank=437800 of ranks=465195rank=437900 of ranks=465195rank=438000 of ranks=465195rank=438100 of ranks=465195rank=438200 of ranks=465195rank=438300 of ranks=465195rank=438400 of ranks=465195rank=438500 of ranks=465195rank=438600 of ranks=465195rank=438700 of ranks=465195rank=438800 of ranks=465195rank=438900 of ranks=465195rank=439000 of ranks=465195rank=439100 of ranks=465195rank=439200 of ranks=465195rank=439300 of ranks=465195rank=439400 of ranks=465195rank=439500 of ranks=465195rank=439600 of ranks=465195rank=439700 of ranks=465195rank=439800 of ranks=465195rank=439900 of ranks=465195rank=440000 of ranks=465195rank=440100 of ranks=465195rank=440200 of ranks=465195rank=440300 of ranks=465195rank=440400 of ranks=465195rank=440500 of ranks=465195rank=440600 of ranks=465195rank=440700 of ranks=465195rank=440800 of ranks=465195rank=440900 of ranks=465195rank=441000 of ranks=465195rank=441100 of ranks=465195rank=441200 of ranks=465195rank=441300 of ranks=465195rank=441400 of ranks=465195rank=441500 of ranks=465195rank=441600 of ranks=465195rank=441700 of ranks=465195rank=441800 of ranks=465195rank=441900 of ranks=465195rank=442000 of ranks=465195rank=442100 of ranks=465195rank=442200 of ranks=465195rank=442300 of ranks=465195rank=442400 of ranks=465195rank=442500 of ranks=465195rank=442600 of ranks=465195rank=442700 of ranks=465195rank=442800 of ranks=465195rank=442900 of ranks=465195rank=443000 of ranks=465195rank=443100 of ranks=465195rank=443200 of ranks=465195rank=443300 of ranks=465195rank=443400 of ranks=465195rank=443500 of ranks=465195rank=443600 of ranks=465195rank=443700 of ranks=465195rank=443800 of ranks=465195rank=443900 of ranks=465195rank=444000 of ranks=465195rank=444100 of ranks=465195rank=444200 of ranks=465195rank=444300 of ranks=465195rank=444400 of ranks=465195rank=444500 of ranks=465195rank=444600 of ranks=465195rank=444700 of ranks=465195rank=444800 of ranks=465195rank=444900 of ranks=465195rank=445000 of ranks=465195rank=445100 of ranks=465195rank=445200 of ranks=465195rank=445300 of ranks=465195rank=445400 of ranks=465195rank=445500 of ranks=465195rank=445600 of ranks=465195rank=445700 of ranks=465195rank=445800 of ranks=465195rank=445900 of ranks=465195rank=446000 of ranks=465195rank=446100 of ranks=465195rank=446200 of ranks=465195rank=446300 of ranks=465195rank=446400 of ranks=465195rank=446500 of ranks=465195rank=446600 of ranks=465195rank=446700 of ranks=465195rank=446800 of ranks=465195rank=446900 of ranks=465195rank=447000 of ranks=465195rank=447100 of ranks=465195rank=447200 of ranks=465195rank=447300 of ranks=465195rank=447400 of ranks=465195rank=447500 of ranks=465195rank=447600 of ranks=465195rank=447700 of ranks=465195rank=447800 of ranks=465195rank=447900 of ranks=465195rank=448000 of ranks=465195rank=448100 of ranks=465195rank=448200 of ranks=465195rank=448300 of ranks=465195rank=448400 of ranks=465195rank=448500 of ranks=465195rank=448600 of ranks=465195rank=448700 of ranks=465195rank=448800 of ranks=465195rank=448900 of ranks=465195rank=449000 of ranks=465195rank=449100 of ranks=465195rank=449200 of ranks=465195rank=449300 of ranks=465195rank=449400 of ranks=465195rank=449500 of ranks=465195rank=449600 of ranks=465195rank=449700 of ranks=465195rank=449800 of ranks=465195rank=449900 of ranks=465195rank=450000 of ranks=465195rank=450100 of ranks=465195rank=450200 of ranks=465195rank=450300 of ranks=465195rank=450400 of ranks=465195rank=450500 of ranks=465195rank=450600 of ranks=465195rank=450700 of ranks=465195rank=450800 of ranks=465195rank=450900 of ranks=465195rank=451000 of ranks=465195rank=451100 of ranks=465195rank=451200 of ranks=465195rank=451300 of ranks=465195rank=451400 of ranks=465195rank=451500 of ranks=465195rank=451600 of ranks=465195rank=451700 of ranks=465195rank=451800 of ranks=465195rank=451900 of ranks=465195rank=452000 of ranks=465195rank=452100 of ranks=465195rank=452200 of ranks=465195rank=452300 of ranks=465195rank=452400 of ranks=465195rank=452500 of ranks=465195rank=452600 of ranks=465195rank=452700 of ranks=465195rank=452800 of ranks=465195rank=452900 of ranks=465195rank=453000 of ranks=465195rank=453100 of ranks=465195rank=453200 of ranks=465195rank=453300 of ranks=465195rank=453400 of ranks=465195rank=453500 of ranks=465195rank=453600 of ranks=465195rank=453700 of ranks=465195rank=453800 of ranks=465195rank=453900 of ranks=465195rank=454000 of ranks=465195rank=454100 of ranks=465195rank=454200 of ranks=465195rank=454300 of ranks=465195rank=454400 of ranks=465195rank=454500 of ranks=465195rank=454600 of ranks=465195rank=454700 of ranks=465195rank=454800 of ranks=465195rank=454900 of ranks=465195rank=455000 of ranks=465195rank=455100 of ranks=465195rank=455200 of ranks=465195rank=455300 of ranks=465195rank=455400 of ranks=465195rank=455500 of ranks=465195rank=455600 of ranks=465195rank=455700 of ranks=465195rank=455800 of ranks=465195rank=455900 of ranks=465195rank=456000 of ranks=465195rank=456100 of ranks=465195rank=456200 of ranks=465195rank=456300 of ranks=465195rank=456400 of ranks=465195rank=456500 of ranks=465195rank=456600 of ranks=465195rank=456700 of ranks=465195rank=456800 of ranks=465195rank=456900 of ranks=465195rank=457000 of ranks=465195rank=457100 of ranks=465195rank=457200 of ranks=465195rank=457300 of ranks=465195rank=457400 of ranks=465195rank=457500 of ranks=465195rank=457600 of ranks=465195rank=457700 of ranks=465195rank=457800 of ranks=465195rank=457900 of ranks=465195rank=458000 of ranks=465195rank=458100 of ranks=465195rank=458200 of ranks=465195rank=458300 of ranks=465195rank=458400 of ranks=465195rank=458500 of ranks=465195rank=458600 of ranks=465195rank=458700 of ranks=465195rank=458800 of ranks=465195rank=458900 of ranks=465195rank=459000 of ranks=465195rank=459100 of ranks=465195rank=459200 of ranks=465195rank=459300 of ranks=465195rank=459400 of ranks=465195rank=459500 of ranks=465195rank=459600 of ranks=465195rank=459700 of ranks=465195rank=459800 of ranks=465195rank=459900 of ranks=465195rank=460000 of ranks=465195rank=460100 of ranks=465195rank=460200 of ranks=465195rank=460300 of ranks=465195rank=460400 of ranks=465195rank=460500 of ranks=465195rank=460600 of ranks=465195rank=460700 of ranks=465195rank=460800 of ranks=465195rank=460900 of ranks=465195rank=461000 of ranks=465195rank=461100 of ranks=465195rank=461200 of ranks=465195rank=461300 of ranks=465195rank=461400 of ranks=465195rank=461500 of ranks=465195rank=461600 of ranks=465195rank=461700 of ranks=465195rank=461800 of ranks=465195rank=461900 of ranks=465195rank=462000 of ranks=465195rank=462100 of ranks=465195rank=462200 of ranks=465195rank=462300 of ranks=465195rank=462400 of ranks=465195rank=462500 of ranks=465195rank=462600 of ranks=465195rank=462700 of ranks=465195rank=462800 of ranks=465195rank=462900 of ranks=465195rank=463000 of ranks=465195rank=463100 of ranks=465195rank=463200 of ranks=465195rank=463300 of ranks=465195rank=463400 of ranks=465195rank=463500 of ranks=465195rank=463600 of ranks=465195rank=463700 of ranks=465195rank=463800 of ranks=465195rank=463900 of ranks=465195rank=464000 of ranks=465195rank=464100 of ranks=465195rank=464200 of ranks=465195rank=464300 of ranks=465195rank=464400 of ranks=465195rank=464500 of ranks=465195rank=464600 of ranks=465195rank=464700 of ranks=465195rank=464800 of ranks=465195rank=464900 of ranks=465195rank=465000 of ranks=465195rank=465100 of ranks=465195

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              47.2606    397  34753    101    498           66.6823
   1 car                    92.7595  49405 157982    911  50316           56.1564
   2 truck                  54.8818   1650  81031    175   1825           26.3695
   3 bus                    17.0892    312  32475     54    366           14.5776
   4 pedestrian             41.2511   3674 103516    585   4259           24.5238

for conf_thresh=0.25, precision=0.64, recall=0.85, F1 score=0.73
for conf_thresh=0.25, TP=48773, FP=27813, FN=8491, average IoU=51.96%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=50.65%
Total detection time: 162 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1407: loss=8.247, avg loss=8.728, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 90048 images, time remaining=6.3 hours
1408: loss=8.462, avg loss=8.702, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=853.9 milliseconds, train=2.1 seconds, 90112 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1409: loss=7.710, avg loss=8.603, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 90176 images, time remaining=6.3 hours
1410: loss=7.207, avg loss=8.463, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=710.0 milliseconds, train=2.1 seconds, 90240 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1411: loss=7.647, avg loss=8.381, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=1.8 seconds, 90304 images, time remaining=6.3 hours
1412: loss=7.479, avg loss=8.291, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=750.4 milliseconds, train=1.8 seconds, 90368 images, time remaining=6.3 hours
1413: loss=7.936, avg loss=8.256, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=1.8 seconds, 90432 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1414: loss=8.028, avg loss=8.233, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.0 seconds, train=1.8 seconds, 90496 images, time remaining=6.3 hours
1415: loss=7.515, avg loss=8.161, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.8 seconds, 90560 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1416: loss=7.510, avg loss=8.096, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=1.8 seconds, 90624 images, time remaining=6.3 hours
1417: loss=7.786, avg loss=8.065, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=1.8 seconds, 90688 images, time remaining=6.3 hours
1418: loss=7.460, avg loss=8.005, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=948.4 milliseconds, train=1.9 seconds, 90752 images, time remaining=6.3 hours
1419: loss=9.631, avg loss=8.167, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.9 seconds, 90816 images, time remaining=6.3 hours
1420: loss=7.902, avg loss=8.141, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 90880 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1464c2000000
1421: loss=7.871, avg loss=8.114, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 90944 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1422: loss=7.034, avg loss=8.006, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 91008 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1423: loss=7.626, avg loss=7.968, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 91072 images, time remaining=6.3 hours
1424: loss=6.810, avg loss=7.852, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 91136 images, time remaining=6.3 hours
1425: loss=8.610, avg loss=7.928, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=654.0 milliseconds, train=1.6 seconds, 91200 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1426: loss=6.342, avg loss=7.769, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 91264 images, time remaining=6.3 hours
1427: loss=6.170, avg loss=7.609, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=951.9 milliseconds, train=1.6 seconds, 91328 images, time remaining=6.3 hours
1428: loss=7.547, avg loss=7.603, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=825.1 milliseconds, train=1.6 seconds, 91392 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1429: loss=7.346, avg loss=7.577, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 91456 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1430: loss=6.884, avg loss=7.508, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.0 seconds, train=1.6 seconds, 91520 images, time remaining=6.3 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a94000000
1431: loss=7.506, avg loss=7.508, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=658.0 milliseconds, train=1.6 seconds, 91584 images, time remaining=6.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1432: loss=7.273, avg loss=7.484, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 91648 images, time remaining=6.3 hours
1433: loss=9.118, avg loss=7.648, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 91712 images, time remaining=6.3 hours
1434: loss=6.865, avg loss=7.569, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=726.5 milliseconds, train=1.5 seconds, 91776 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1435: loss=6.416, avg loss=7.454, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 91840 images, time remaining=6.2 hours
1436: loss=6.687, avg loss=7.377, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 91904 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1437: loss=7.380, avg loss=7.378, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.5 seconds, train=1.5 seconds, 91968 images, time remaining=6.2 hours
1438: loss=6.438, avg loss=7.284, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 92032 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1439: loss=6.813, avg loss=7.237, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 92096 images, time remaining=6.2 hours
1440: loss=6.440, avg loss=7.157, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=850.4 milliseconds, train=1.5 seconds, 92160 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145ff8e00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1441: loss=7.874, avg loss=7.229, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.9 seconds, train=1.5 seconds, 92224 images, time remaining=6.2 hours
1442: loss=7.112, avg loss=7.217, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=896.6 milliseconds, train=1.4 seconds, 92288 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1443: loss=5.716, avg loss=7.067, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.0 seconds, train=1.4 seconds, 92352 images, time remaining=6.2 hours
1444: loss=5.440, avg loss=6.904, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 92416 images, time remaining=6.2 hours
1445: loss=6.331, avg loss=6.847, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=835.7 milliseconds, train=1.4 seconds, 92480 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1446: loss=7.231, avg loss=6.885, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 92544 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1447: loss=6.864, avg loss=6.883, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 92608 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1448: loss=6.568, avg loss=6.852, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 92672 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1449: loss=7.388, avg loss=6.905, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.4 seconds, 92736 images, time remaining=6.2 hours
1450: loss=5.884, avg loss=6.803, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=921.2 milliseconds, train=1.4 seconds, 92800 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1451: loss=7.131, avg loss=6.836, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 92864 images, time remaining=6.2 hours
1452: loss=6.977, avg loss=6.850, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.9 seconds, 92928 images, time remaining=6.2 hours
1453: loss=7.818, avg loss=6.947, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=1.9 seconds, 92992 images, time remaining=6.2 hours
1454: loss=6.099, avg loss=6.862, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=1.9 seconds, 93056 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1455: loss=5.849, avg loss=6.761, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=1.9 seconds, 93120 images, time remaining=6.2 hours
1456: loss=5.656, avg loss=6.650, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=961.1 milliseconds, train=1.9 seconds, 93184 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1457: loss=7.554, avg loss=6.741, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.7 seconds, train=1.9 seconds, 93248 images, time remaining=6.2 hours
1458: loss=6.206, avg loss=6.687, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.9 seconds, 93312 images, time remaining=6.2 hours
1459: loss=5.641, avg loss=6.583, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=845.7 milliseconds, train=1.9 seconds, 93376 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1460: loss=6.724, avg loss=6.597, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.5 seconds, train=1.9 seconds, 93440 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1461: loss=8.288, avg loss=6.766, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=4.7 seconds, 93504 images, time remaining=6.2 hours
1462: loss=9.310, avg loss=7.020, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=612.5 milliseconds, train=4.7 seconds, 93568 images, time remaining=6.2 hours
1463: loss=8.940, avg loss=7.212, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.9 seconds, train=4.7 seconds, 93632 images, time remaining=6.2 hours
1464: loss=8.628, avg loss=7.354, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=913.0 milliseconds, train=4.7 seconds, 93696 images, time remaining=6.2 hours
1465: loss=8.075, avg loss=7.426, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=4.7 seconds, 93760 images, time remaining=6.2 hours
1466: loss=9.061, avg loss=7.589, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=4.7 seconds, 93824 images, time remaining=6.2 hours
1467: loss=7.985, avg loss=7.629, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=996.0 milliseconds, train=4.7 seconds, 93888 images, time remaining=6.2 hours
1468: loss=7.505, avg loss=7.617, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=999.1 milliseconds, train=4.7 seconds, 93952 images, time remaining=6.2 hours
1469: loss=7.895, avg loss=7.644, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=4.7 seconds, 94016 images, time remaining=6.2 hours
1470: loss=10.316, avg loss=7.912, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=4.8 seconds, 94080 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1471: loss=7.678, avg loss=7.888, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 94144 images, time remaining=6.2 hours
1472: loss=7.970, avg loss=7.896, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=3.4 seconds, 94208 images, time remaining=6.2 hours
1473: loss=7.836, avg loss=7.890, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 94272 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1474: loss=8.064, avg loss=7.908, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=3.4 seconds, 94336 images, time remaining=6.2 hours
1475: loss=7.248, avg loss=7.842, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=738.1 milliseconds, train=3.4 seconds, 94400 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1476: loss=8.470, avg loss=7.905, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=3.4 seconds, 94464 images, time remaining=6.2 hours
1477: loss=7.018, avg loss=7.816, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=864.7 milliseconds, train=3.4 seconds, 94528 images, time remaining=6.2 hours
1478: loss=7.199, avg loss=7.754, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.4 seconds, 94592 images, time remaining=6.2 hours
1479: loss=8.048, avg loss=7.784, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=662.5 milliseconds, train=3.4 seconds, 94656 images, time remaining=6.2 hours
1480: loss=7.001, avg loss=7.705, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.4 seconds, 94720 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1481: loss=7.836, avg loss=7.718, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=3.3 seconds, 94784 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1482: loss=6.604, avg loss=7.607, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=3.3 seconds, 94848 images, time remaining=6.2 hours
1483: loss=8.387, avg loss=7.685, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.4 seconds, 94912 images, time remaining=6.2 hours
1484: loss=8.038, avg loss=7.720, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=842.8 milliseconds, train=3.3 seconds, 94976 images, time remaining=6.2 hours
1485: loss=7.057, avg loss=7.654, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 95040 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1486: loss=6.674, avg loss=7.556, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 95104 images, time remaining=6.2 hours
1487: loss=8.380, avg loss=7.638, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 95168 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1488: loss=6.454, avg loss=7.520, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 95232 images, time remaining=6.2 hours
1489: loss=6.713, avg loss=7.439, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=848.8 milliseconds, train=3.3 seconds, 95296 images, time remaining=6.2 hours
1490: loss=7.250, avg loss=7.420, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 95360 images, time remaining=6.2 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145156000000
1491: loss=7.809, avg loss=7.459, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 95424 images, time remaining=6.2 hours
1492: loss=7.375, avg loss=7.451, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=911.2 milliseconds, train=1.6 seconds, 95488 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1493: loss=6.522, avg loss=7.358, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.1 seconds, train=1.6 seconds, 95552 images, time remaining=6.2 hours
1494: loss=6.439, avg loss=7.266, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=975.6 milliseconds, train=1.6 seconds, 95616 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1495: loss=8.147, avg loss=7.354, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 95680 images, time remaining=6.2 hours
1496: loss=7.010, avg loss=7.320, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 95744 images, time remaining=6.2 hours
1497: loss=8.070, avg loss=7.395, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 95808 images, time remaining=6.2 hours
1498: loss=7.644, avg loss=7.420, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=628.7 milliseconds, train=1.6 seconds, 95872 images, time remaining=6.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1499: loss=6.853, avg loss=7.363, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 95936 images, time remaining=6.2 hours
1500: loss=6.790, avg loss=7.306, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 96000 images, time remaining=6.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1501: loss=6.255, avg loss=7.201, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=554.8 milliseconds, train=2.1 seconds, 96064 images, time remaining=6.2 hours
1502: loss=6.730, avg loss=7.154, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 96128 images, time remaining=6.2 hours
1503: loss=8.424, avg loss=7.281, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=922.2 milliseconds, train=2.1 seconds, 96192 images, time remaining=6.2 hours
1504: loss=5.636, avg loss=7.116, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=852.8 milliseconds, train=2.1 seconds, 96256 images, time remaining=6.2 hours
1505: loss=7.358, avg loss=7.140, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 96320 images, time remaining=6.2 hours
1506: loss=7.270, avg loss=7.153, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=651.1 milliseconds, train=2.1 seconds, 96384 images, time remaining=6.1 hours
1507: loss=7.305, avg loss=7.168, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=820.8 milliseconds, train=2.1 seconds, 96448 images, time remaining=6.1 hours
1508: loss=7.937, avg loss=7.245, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=906.8 milliseconds, train=2.1 seconds, 96512 images, time remaining=6.1 hours
1509: loss=7.147, avg loss=7.236, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 96576 images, time remaining=6.1 hours
1510: loss=6.074, avg loss=7.119, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=977.7 milliseconds, train=2.1 seconds, 96640 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145f8c000000
1511: loss=8.738, avg loss=7.281, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=710.1 milliseconds, train=1.2 seconds, 96704 images, time remaining=6.1 hours
1512: loss=6.776, avg loss=7.231, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=504.7 milliseconds, train=1.2 seconds, 96768 images, time remaining=6.1 hours
1513: loss=6.888, avg loss=7.196, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=676.7 milliseconds, train=1.2 seconds, 96832 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1514: loss=6.535, avg loss=7.130, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 96896 images, time remaining=6.1 hours
1515: loss=7.100, avg loss=7.127, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=791.7 milliseconds, train=1.2 seconds, 96960 images, time remaining=6.1 hours
1516: loss=6.775, avg loss=7.092, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=960.8 milliseconds, train=1.2 seconds, 97024 images, time remaining=6.1 hours
1517: loss=7.342, avg loss=7.117, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=662.8 milliseconds, train=1.2 seconds, 97088 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1518: loss=6.356, avg loss=7.041, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=1.2 seconds, 97152 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1519: loss=7.682, avg loss=7.105, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 97216 images, time remaining=6.1 hours
1520: loss=6.260, avg loss=7.020, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=459.5 milliseconds, train=1.2 seconds, 97280 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1521: loss=8.278, avg loss=7.146, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=732.9 milliseconds, train=3.4 seconds, 97344 images, time remaining=6.1 hours
1522: loss=7.583, avg loss=7.190, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.0 seconds, train=3.4 seconds, 97408 images, time remaining=6.1 hours
1523: loss=7.227, avg loss=7.194, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=626.7 milliseconds, train=3.4 seconds, 97472 images, time remaining=6.1 hours
1524: loss=8.991, avg loss=7.373, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=3.4 seconds, 97536 images, time remaining=6.1 hours
1525: loss=7.161, avg loss=7.352, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=484.9 milliseconds, train=3.4 seconds, 97600 images, time remaining=6.1 hours
1526: loss=7.662, avg loss=7.383, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=3.4 seconds, 97664 images, time remaining=6.1 hours
1527: loss=6.955, avg loss=7.340, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 97728 images, time remaining=6.1 hours
1528: loss=7.611, avg loss=7.367, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=681.9 milliseconds, train=3.4 seconds, 97792 images, time remaining=6.1 hours
1529: loss=7.186, avg loss=7.349, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=3.4 seconds, 97856 images, time remaining=6.1 hours
1530: loss=8.007, avg loss=7.415, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=881.4 milliseconds, train=3.4 seconds, 97920 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1464a6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1531: loss=7.445, avg loss=7.418, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 97984 images, time remaining=6.1 hours
1532: loss=7.162, avg loss=7.392, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=890.8 milliseconds, train=1.5 seconds, 98048 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1533: loss=6.615, avg loss=7.315, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 98112 images, time remaining=6.1 hours
1534: loss=5.527, avg loss=7.136, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 98176 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1535: loss=6.073, avg loss=7.029, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 98240 images, time remaining=6.1 hours
1536: loss=7.154, avg loss=7.042, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=998.6 milliseconds, train=1.5 seconds, 98304 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1537: loss=6.485, avg loss=6.986, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 98368 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1538: loss=6.695, avg loss=6.957, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 98432 images, time remaining=6.1 hours
1539: loss=6.160, avg loss=6.877, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=805.3 milliseconds, train=1.5 seconds, 98496 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1540: loss=6.387, avg loss=6.828, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 98560 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1541: loss=6.219, avg loss=6.767, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=2.0 seconds, 98624 images, time remaining=6.1 hours
1542: loss=5.947, avg loss=6.685, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=629.7 milliseconds, train=2.0 seconds, 98688 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1543: loss=7.587, avg loss=6.776, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 98752 images, time remaining=6.1 hours
1544: loss=6.717, avg loss=6.770, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 98816 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1545: loss=6.472, avg loss=6.740, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=2.0 seconds, 98880 images, time remaining=6.1 hours
1546: loss=6.939, avg loss=6.760, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=793.7 milliseconds, train=2.0 seconds, 98944 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1547: loss=6.366, avg loss=6.720, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=1.9 seconds, 99008 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1548: loss=6.791, avg loss=6.727, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=2.0 seconds, 99072 images, time remaining=6.1 hours
1549: loss=6.317, avg loss=6.686, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=796.2 milliseconds, train=2.0 seconds, 99136 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1550: loss=7.308, avg loss=6.749, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.1 seconds, train=2.0 seconds, 99200 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1551: loss=9.795, avg loss=7.053, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=803.9 milliseconds, train=4.6 seconds, 99264 images, time remaining=6.1 hours
1552: loss=7.704, avg loss=7.118, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 99328 images, time remaining=6.1 hours
1553: loss=7.050, avg loss=7.112, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=747.1 milliseconds, train=4.6 seconds, 99392 images, time remaining=6.1 hours
1554: loss=8.525, avg loss=7.253, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=751.9 milliseconds, train=4.6 seconds, 99456 images, time remaining=6.1 hours
1555: loss=7.159, avg loss=7.244, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=732.1 milliseconds, train=4.6 seconds, 99520 images, time remaining=6.1 hours
1556: loss=8.018, avg loss=7.321, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=808.5 milliseconds, train=4.6 seconds, 99584 images, time remaining=6.1 hours
1557: loss=8.292, avg loss=7.418, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=867.3 milliseconds, train=4.6 seconds, 99648 images, time remaining=6.1 hours
1558: loss=8.750, avg loss=7.551, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=4.6 seconds, 99712 images, time remaining=6.1 hours
1559: loss=8.438, avg loss=7.640, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=752.0 milliseconds, train=4.6 seconds, 99776 images, time remaining=6.1 hours
1560: loss=8.924, avg loss=7.768, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 99840 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1561: loss=8.345, avg loss=7.826, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 99904 images, time remaining=6.1 hours
1562: loss=7.359, avg loss=7.779, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=651.9 milliseconds, train=4.5 seconds, 99968 images, time remaining=6.1 hours
1563: loss=8.089, avg loss=7.810, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 100032 images, time remaining=6.1 hours
1564: loss=7.880, avg loss=7.817, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=4.5 seconds, 100096 images, time remaining=6.1 hours
1565: loss=7.496, avg loss=7.785, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 100160 images, time remaining=6.1 hours
1566: loss=8.185, avg loss=7.825, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 100224 images, time remaining=6.1 hours
1567: loss=6.761, avg loss=7.719, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=4.5 seconds, 100288 images, time remaining=6.1 hours
1568: loss=6.872, avg loss=7.634, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=816.2 milliseconds, train=4.5 seconds, 100352 images, time remaining=6.1 hours
1569: loss=7.474, avg loss=7.618, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 100416 images, time remaining=6.1 hours
1570: loss=6.649, avg loss=7.521, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=999.1 milliseconds, train=4.5 seconds, 100480 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1455c0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1571: loss=8.309, avg loss=7.600, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=1.6 seconds, 100544 images, time remaining=6.1 hours
1572: loss=7.431, avg loss=7.583, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.7 seconds, 100608 images, time remaining=6.1 hours
1573: loss=7.373, avg loss=7.562, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=498.5 milliseconds, train=1.7 seconds, 100672 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1574: loss=7.746, avg loss=7.580, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=1.6 seconds, 100736 images, time remaining=6.1 hours
1575: loss=7.956, avg loss=7.618, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=749.7 milliseconds, train=1.6 seconds, 100800 images, time remaining=6.1 hours
1576: loss=7.875, avg loss=7.644, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=584.9 milliseconds, train=1.7 seconds, 100864 images, time remaining=6.1 hours
1577: loss=7.292, avg loss=7.609, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 100928 images, time remaining=6.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1578: loss=6.361, avg loss=7.484, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 100992 images, time remaining=6.1 hours
1579: loss=7.072, avg loss=7.443, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=648.5 milliseconds, train=1.6 seconds, 101056 images, time remaining=6.1 hours
1580: loss=6.550, avg loss=7.353, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=949.6 milliseconds, train=1.7 seconds, 101120 images, time remaining=6.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1455c0000000
1581: loss=7.595, avg loss=7.378, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 101184 images, time remaining=6.1 hours
1582: loss=7.470, avg loss=7.387, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=850.7 milliseconds, train=1.6 seconds, 101248 images, time remaining=6 hours
1583: loss=7.192, avg loss=7.367, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=716.0 milliseconds, train=1.7 seconds, 101312 images, time remaining=6 hours
1584: loss=7.959, avg loss=7.426, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=694.0 milliseconds, train=1.7 seconds, 101376 images, time remaining=6 hours
1585: loss=7.237, avg loss=7.408, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 101440 images, time remaining=6 hours
1586: loss=6.851, avg loss=7.352, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=967.4 milliseconds, train=1.7 seconds, 101504 images, time remaining=6 hours
1587: loss=6.346, avg loss=7.251, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=858.9 milliseconds, train=1.6 seconds, 101568 images, time remaining=6 hours
1588: loss=6.883, avg loss=7.215, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.7 seconds, 101632 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1589: loss=6.273, avg loss=7.120, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 101696 images, time remaining=6 hours
1590: loss=6.651, avg loss=7.073, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=860.5 milliseconds, train=1.6 seconds, 101760 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1591: loss=8.233, avg loss=7.189, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=790.8 milliseconds, train=4.7 seconds, 101824 images, time remaining=6 hours
1592: loss=8.989, avg loss=7.369, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=4.6 seconds, 101888 images, time remaining=6 hours
1593: loss=7.144, avg loss=7.347, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=729.3 milliseconds, train=4.6 seconds, 101952 images, time remaining=6 hours
1594: loss=8.611, avg loss=7.473, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 102016 images, time remaining=6 hours
1595: loss=8.083, avg loss=7.534, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 102080 images, time remaining=6 hours
1596: loss=7.542, avg loss=7.535, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 102144 images, time remaining=6 hours
1597: loss=6.962, avg loss=7.478, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=765.1 milliseconds, train=4.6 seconds, 102208 images, time remaining=6 hours
1598: loss=7.981, avg loss=7.528, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=690.7 milliseconds, train=4.6 seconds, 102272 images, time remaining=6 hours
1599: loss=7.727, avg loss=7.548, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 102336 images, time remaining=6 hours
1600: loss=7.104, avg loss=7.504, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=744.5 milliseconds, train=4.6 seconds, 102400 images, time remaining=6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1601: loss=6.733, avg loss=7.427, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 102464 images, time remaining=6 hours
1602: loss=6.145, avg loss=7.298, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=713.8 milliseconds, train=3.6 seconds, 102528 images, time remaining=6 hours
1603: loss=6.061, avg loss=7.175, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=602.9 milliseconds, train=3.6 seconds, 102592 images, time remaining=6 hours
1604: loss=7.477, avg loss=7.205, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 102656 images, time remaining=6 hours
1605: loss=7.161, avg loss=7.200, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=690.6 milliseconds, train=3.6 seconds, 102720 images, time remaining=6 hours
1606: loss=6.344, avg loss=7.115, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=3.6 seconds, 102784 images, time remaining=6 hours
1607: loss=7.214, avg loss=7.125, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=990.2 milliseconds, train=3.6 seconds, 102848 images, time remaining=6 hours
1608: loss=7.615, avg loss=7.174, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 102912 images, time remaining=6 hours
1609: loss=6.821, avg loss=7.138, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 102976 images, time remaining=6 hours
1610: loss=7.775, avg loss=7.202, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=957.4 milliseconds, train=3.6 seconds, 103040 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1611: loss=7.089, avg loss=7.191, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 103104 images, time remaining=6 hours
1612: loss=6.936, avg loss=7.165, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=763.7 milliseconds, train=2.0 seconds, 103168 images, time remaining=6 hours
1613: loss=5.934, avg loss=7.042, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=2.0 seconds, 103232 images, time remaining=6 hours
1614: loss=6.848, avg loss=7.023, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 103296 images, time remaining=6 hours
1615: loss=7.391, avg loss=7.060, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=997.5 milliseconds, train=2.0 seconds, 103360 images, time remaining=6 hours
1616: loss=6.921, avg loss=7.046, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=2.0 seconds, 103424 images, time remaining=6 hours
1617: loss=7.053, avg loss=7.047, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=691.8 milliseconds, train=2.0 seconds, 103488 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1618: loss=7.338, avg loss=7.076, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=2.0 seconds, 103552 images, time remaining=6 hours
1619: loss=6.357, avg loss=7.004, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 103616 images, time remaining=6 hours
1620: loss=6.874, avg loss=6.991, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=713.5 milliseconds, train=2.0 seconds, 103680 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463fe000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1621: loss=6.998, avg loss=6.992, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 103744 images, time remaining=6 hours
1622: loss=7.700, avg loss=7.062, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 103808 images, time remaining=6 hours
1623: loss=6.130, avg loss=6.969, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=478.3 milliseconds, train=1.2 seconds, 103872 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1624: loss=7.542, avg loss=7.027, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 103936 images, time remaining=6 hours
1625: loss=6.580, avg loss=6.982, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=860.6 milliseconds, train=1.2 seconds, 104000 images, time remaining=6 hours
1626: loss=7.793, avg loss=7.063, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=458.8 milliseconds, train=1.2 seconds, 104064 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1627: loss=7.211, avg loss=7.078, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=1.2 seconds, 104128 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1628: loss=6.817, avg loss=7.052, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 104192 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1629: loss=6.709, avg loss=7.017, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.5 seconds, train=1.2 seconds, 104256 images, time remaining=6 hours
1630: loss=6.767, avg loss=6.992, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=867.6 milliseconds, train=1.2 seconds, 104320 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1631: loss=8.937, avg loss=7.187, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 104384 images, time remaining=6 hours
1632: loss=6.809, avg loss=7.149, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 104448 images, time remaining=6 hours
1633: loss=6.434, avg loss=7.077, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=3.4 seconds, 104512 images, time remaining=6 hours
1634: loss=6.850, avg loss=7.055, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=952.0 milliseconds, train=3.4 seconds, 104576 images, time remaining=6 hours
1635: loss=8.040, avg loss=7.153, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 104640 images, time remaining=6 hours
1636: loss=10.202, avg loss=7.458, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=745.2 milliseconds, train=3.4 seconds, 104704 images, time remaining=6 hours
1637: loss=7.688, avg loss=7.481, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=948.1 milliseconds, train=3.4 seconds, 104768 images, time remaining=6 hours
1638: loss=7.520, avg loss=7.485, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=3.4 seconds, 104832 images, time remaining=6 hours
1639: loss=8.031, avg loss=7.539, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=846.2 milliseconds, train=3.4 seconds, 104896 images, time remaining=6 hours
1640: loss=7.917, avg loss=7.577, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=3.4 seconds, 104960 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1463ac000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1641: loss=6.627, avg loss=7.482, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 105024 images, time remaining=6 hours
1642: loss=5.773, avg loss=7.311, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 105088 images, time remaining=6 hours
1643: loss=7.267, avg loss=7.307, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 105152 images, time remaining=6 hours
1644: loss=6.371, avg loss=7.213, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=670.9 milliseconds, train=1.5 seconds, 105216 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1645: loss=7.953, avg loss=7.287, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 105280 images, time remaining=6 hours
1646: loss=6.882, avg loss=7.247, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=986.6 milliseconds, train=1.5 seconds, 105344 images, time remaining=6 hours
1647: loss=6.148, avg loss=7.137, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=703.8 milliseconds, train=1.5 seconds, 105408 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1648: loss=5.659, avg loss=6.989, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 105472 images, time remaining=6 hours
1649: loss=5.891, avg loss=6.879, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=890.0 milliseconds, train=1.5 seconds, 105536 images, time remaining=6 hours
1650: loss=7.196, avg loss=6.911, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=577.2 milliseconds, train=1.5 seconds, 105600 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1463ac000000
1651: loss=6.767, avg loss=6.896, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 105664 images, time remaining=6 hours
1652: loss=6.715, avg loss=6.878, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=482.5 milliseconds, train=1.5 seconds, 105728 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1653: loss=7.211, avg loss=6.912, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 105792 images, time remaining=6 hours
1654: loss=6.175, avg loss=6.838, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=733.8 milliseconds, train=1.5 seconds, 105856 images, time remaining=6 hours
1655: loss=6.292, avg loss=6.783, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=962.9 milliseconds, train=1.6 seconds, 105920 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1656: loss=7.576, avg loss=6.863, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 105984 images, time remaining=5.9 hours
1657: loss=5.369, avg loss=6.713, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 106048 images, time remaining=5.9 hours
1658: loss=6.267, avg loss=6.669, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=720.5 milliseconds, train=1.5 seconds, 106112 images, time remaining=5.9 hours
1659: loss=6.403, avg loss=6.642, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=714.5 milliseconds, train=1.5 seconds, 106176 images, time remaining=5.9 hours
1660: loss=6.208, avg loss=6.599, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=770.1 milliseconds, train=1.5 seconds, 106240 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1661: loss=6.946, avg loss=6.633, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=817.1 milliseconds, train=3.3 seconds, 106304 images, time remaining=5.9 hours
1662: loss=9.293, avg loss=6.899, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=644.4 milliseconds, train=3.3 seconds, 106368 images, time remaining=5.9 hours
1663: loss=6.523, avg loss=6.862, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 106432 images, time remaining=5.9 hours
1664: loss=6.518, avg loss=6.827, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 106496 images, time remaining=5.9 hours
1665: loss=5.903, avg loss=6.735, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=770.3 milliseconds, train=3.3 seconds, 106560 images, time remaining=5.9 hours
1666: loss=6.695, avg loss=6.731, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=785.5 milliseconds, train=3.3 seconds, 106624 images, time remaining=5.9 hours
1667: loss=6.840, avg loss=6.742, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 106688 images, time remaining=5.9 hours
1668: loss=6.442, avg loss=6.712, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 106752 images, time remaining=5.9 hours
1669: loss=6.294, avg loss=6.670, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 106816 images, time remaining=5.9 hours
1670: loss=6.952, avg loss=6.698, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 106880 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1671: loss=6.701, avg loss=6.698, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=1.9 seconds, 106944 images, time remaining=5.9 hours
1672: loss=6.201, avg loss=6.649, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 107008 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1673: loss=6.741, avg loss=6.658, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.1 seconds, train=1.9 seconds, 107072 images, time remaining=5.9 hours
1674: loss=7.056, avg loss=6.698, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 107136 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1675: loss=6.416, avg loss=6.670, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.7 seconds, train=1.9 seconds, 107200 images, time remaining=5.9 hours
1676: loss=5.656, avg loss=6.568, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=800.0 milliseconds, train=1.9 seconds, 107264 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1677: loss=6.545, avg loss=6.566, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.5 seconds, train=1.9 seconds, 107328 images, time remaining=5.9 hours
1678: loss=6.274, avg loss=6.537, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=1.9 seconds, 107392 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1679: loss=7.153, avg loss=6.598, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=1.9 seconds, 107456 images, time remaining=5.9 hours
1680: loss=6.619, avg loss=6.600, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=820.8 milliseconds, train=1.9 seconds, 107520 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1464c4000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1681: loss=5.283, avg loss=6.469, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 107584 images, time remaining=5.9 hours
1682: loss=6.307, avg loss=6.453, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=779.8 milliseconds, train=1.5 seconds, 107648 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1683: loss=6.603, avg loss=6.468, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.8 seconds, train=1.5 seconds, 107712 images, time remaining=5.9 hours
1684: loss=6.412, avg loss=6.462, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 107776 images, time remaining=5.9 hours
1685: loss=6.223, avg loss=6.438, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 107840 images, time remaining=5.9 hours
1686: loss=6.740, avg loss=6.468, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=607.1 milliseconds, train=1.5 seconds, 107904 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1687: loss=6.402, avg loss=6.462, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 107968 images, time remaining=5.9 hours
1688: loss=7.420, avg loss=6.557, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=840.2 milliseconds, train=1.5 seconds, 108032 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1689: loss=6.707, avg loss=6.572, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 108096 images, time remaining=5.9 hours
1690: loss=5.770, avg loss=6.492, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=615.1 milliseconds, train=1.5 seconds, 108160 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1464c6000000
1691: loss=6.180, avg loss=6.461, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=850.5 milliseconds, train=1.4 seconds, 108224 images, time remaining=5.9 hours
1692: loss=5.647, avg loss=6.380, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=529.3 milliseconds, train=1.4 seconds, 108288 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1693: loss=5.786, avg loss=6.320, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.2 seconds, train=1.4 seconds, 108352 images, time remaining=5.9 hours
1694: loss=6.219, avg loss=6.310, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=906.5 milliseconds, train=1.4 seconds, 108416 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1695: loss=5.711, avg loss=6.250, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 108480 images, time remaining=5.9 hours
1696: loss=6.066, avg loss=6.232, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=917.1 milliseconds, train=1.4 seconds, 108544 images, time remaining=5.9 hours
1697: loss=5.720, avg loss=6.181, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 108608 images, time remaining=5.9 hours
1698: loss=5.055, avg loss=6.068, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=695.1 milliseconds, train=1.4 seconds, 108672 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1699: loss=5.996, avg loss=6.061, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.8 seconds, train=1.4 seconds, 108736 images, time remaining=5.9 hours
1700: loss=5.482, avg loss=6.003, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=740.1 milliseconds, train=1.4 seconds, 108800 images, time remaining=5.9 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1701: loss=9.768, avg loss=6.380, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 108864 images, time remaining=5.9 hours
1702: loss=7.618, avg loss=6.503, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 108928 images, time remaining=5.9 hours
1703: loss=7.565, avg loss=6.609, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.0 seconds, train=4.5 seconds, 108992 images, time remaining=5.9 hours
1704: loss=7.993, avg loss=6.748, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.4 seconds, train=4.5 seconds, 109056 images, time remaining=5.9 hours
1705: loss=7.487, avg loss=6.822, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=4.5 seconds, 109120 images, time remaining=5.9 hours
1706: loss=8.390, avg loss=6.979, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 109184 images, time remaining=5.9 hours
1707: loss=7.196, avg loss=7.000, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 109248 images, time remaining=5.9 hours
1708: loss=6.965, avg loss=6.997, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.4 seconds, train=4.5 seconds, 109312 images, time remaining=5.9 hours
1709: loss=8.921, avg loss=7.189, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=741.8 milliseconds, train=4.5 seconds, 109376 images, time remaining=5.9 hours
1710: loss=7.209, avg loss=7.191, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 109440 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1711: loss=6.950, avg loss=7.167, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 109504 images, time remaining=5.9 hours
1712: loss=6.706, avg loss=7.121, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 109568 images, time remaining=5.9 hours
1713: loss=7.254, avg loss=7.134, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 109632 images, time remaining=5.9 hours
1714: loss=6.610, avg loss=7.082, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 109696 images, time remaining=5.9 hours
1715: loss=7.456, avg loss=7.119, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 109760 images, time remaining=5.9 hours
1716: loss=6.191, avg loss=7.026, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 109824 images, time remaining=5.9 hours
1717: loss=7.241, avg loss=7.048, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 109888 images, time remaining=5.9 hours
1718: loss=7.136, avg loss=7.057, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 109952 images, time remaining=5.9 hours
1719: loss=6.358, avg loss=6.987, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 110016 images, time remaining=5.9 hours
1720: loss=6.386, avg loss=6.927, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=923.1 milliseconds, train=3.3 seconds, 110080 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1721: loss=8.143, avg loss=7.048, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 110144 images, time remaining=5.9 hours
1722: loss=5.871, avg loss=6.931, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=907.4 milliseconds, train=3.3 seconds, 110208 images, time remaining=5.9 hours
1723: loss=6.133, avg loss=6.851, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 110272 images, time remaining=5.9 hours
1724: loss=7.136, avg loss=6.879, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 110336 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1725: loss=5.886, avg loss=6.780, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.6 seconds, train=3.3 seconds, 110400 images, time remaining=5.9 hours
1726: loss=5.881, avg loss=6.690, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 110464 images, time remaining=5.9 hours
1727: loss=5.891, avg loss=6.610, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 110528 images, time remaining=5.9 hours
1728: loss=6.014, avg loss=6.551, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 110592 images, time remaining=5.9 hours
1729: loss=7.008, avg loss=6.596, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=3.4 seconds, 110656 images, time remaining=5.9 hours
1730: loss=7.424, avg loss=6.679, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=785.0 milliseconds, train=3.3 seconds, 110720 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1731: loss=8.274, avg loss=6.839, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.4 seconds, train=3.9 seconds, 110784 images, time remaining=5.9 hours
1732: loss=6.349, avg loss=6.790, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 110848 images, time remaining=5.9 hours
1733: loss=8.320, avg loss=6.943, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=761.5 milliseconds, train=3.9 seconds, 110912 images, time remaining=5.9 hours
1734: loss=6.884, avg loss=6.937, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 110976 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1735: loss=6.841, avg loss=6.927, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=4.6 seconds, train=3.9 seconds, 111040 images, time remaining=5.9 hours
1736: loss=7.303, avg loss=6.965, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 111104 images, time remaining=5.9 hours
1737: loss=6.003, avg loss=6.869, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 111168 images, time remaining=5.9 hours
1738: loss=7.627, avg loss=6.944, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 111232 images, time remaining=5.9 hours
1739: loss=5.829, avg loss=6.833, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=794.2 milliseconds, train=3.9 seconds, 111296 images, time remaining=5.9 hours
1740: loss=5.849, avg loss=6.735, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 111360 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1741: loss=5.515, avg loss=6.613, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 111424 images, time remaining=5.9 hours
1742: loss=7.039, avg loss=6.655, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=858.2 milliseconds, train=3.6 seconds, 111488 images, time remaining=5.9 hours
1743: loss=6.045, avg loss=6.594, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=977.6 milliseconds, train=3.6 seconds, 111552 images, time remaining=5.9 hours
1744: loss=5.561, avg loss=6.491, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 111616 images, time remaining=5.9 hours
1745: loss=6.315, avg loss=6.473, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 111680 images, time remaining=5.9 hours
1746: loss=7.144, avg loss=6.540, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=860.3 milliseconds, train=3.6 seconds, 111744 images, time remaining=5.9 hours
1747: loss=6.375, avg loss=6.524, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 111808 images, time remaining=5.9 hours
1748: loss=6.506, avg loss=6.522, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.8 seconds, train=3.6 seconds, 111872 images, time remaining=5.9 hours
1749: loss=6.857, avg loss=6.556, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 111936 images, time remaining=5.9 hours
1750: loss=5.666, avg loss=6.467, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=3.6 seconds, 112000 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1751: loss=6.619, avg loss=6.482, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.6 seconds, train=3.6 seconds, 112064 images, time remaining=5.9 hours
1752: loss=5.417, avg loss=6.375, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=995.2 milliseconds, train=3.5 seconds, 112128 images, time remaining=5.9 hours
1753: loss=6.611, avg loss=6.399, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.5 seconds, 112192 images, time remaining=5.9 hours
1754: loss=5.732, avg loss=6.332, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.1 seconds, train=3.5 seconds, 112256 images, time remaining=5.9 hours
1755: loss=5.056, avg loss=6.205, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.5 seconds, train=3.5 seconds, 112320 images, time remaining=5.9 hours
1756: loss=5.756, avg loss=6.160, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=915.1 milliseconds, train=3.5 seconds, 112384 images, time remaining=5.9 hours
1757: loss=7.643, avg loss=6.308, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 112448 images, time remaining=5.9 hours
1758: loss=6.481, avg loss=6.325, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=691.0 milliseconds, train=3.5 seconds, 112512 images, time remaining=5.9 hours
1759: loss=6.570, avg loss=6.350, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.4 seconds, train=3.5 seconds, 112576 images, time remaining=5.9 hours
1760: loss=5.192, avg loss=6.234, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.5 seconds, 112640 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1761: loss=6.637, avg loss=6.274, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 112704 images, time remaining=5.9 hours
1762: loss=6.378, avg loss=6.285, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=504.2 milliseconds, train=3.9 seconds, 112768 images, time remaining=5.9 hours
1763: loss=6.117, avg loss=6.268, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 112832 images, time remaining=5.9 hours
1764: loss=6.398, avg loss=6.281, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 112896 images, time remaining=5.9 hours
1765: loss=6.509, avg loss=6.304, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=994.9 milliseconds, train=3.9 seconds, 112960 images, time remaining=5.9 hours
1766: loss=7.187, avg loss=6.392, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 113024 images, time remaining=5.9 hours
1767: loss=6.930, avg loss=6.446, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 113088 images, time remaining=5.9 hours
1768: loss=5.813, avg loss=6.383, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=959.6 milliseconds, train=3.9 seconds, 113152 images, time remaining=5.9 hours
1769: loss=6.496, avg loss=6.394, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 113216 images, time remaining=5.9 hours
1770: loss=7.209, avg loss=6.475, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 113280 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1771: loss=6.217, avg loss=6.449, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 113344 images, time remaining=5.9 hours
1772: loss=5.495, avg loss=6.354, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 113408 images, time remaining=5.8 hours
1773: loss=6.694, avg loss=6.388, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=567.6 milliseconds, train=3.3 seconds, 113472 images, time remaining=5.8 hours
1774: loss=6.837, avg loss=6.433, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 113536 images, time remaining=5.8 hours
1775: loss=5.681, avg loss=6.358, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=644.3 milliseconds, train=3.3 seconds, 113600 images, time remaining=5.8 hours
1776: loss=7.007, avg loss=6.423, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=956.3 milliseconds, train=3.3 seconds, 113664 images, time remaining=5.8 hours
1777: loss=6.799, avg loss=6.460, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=948.1 milliseconds, train=3.3 seconds, 113728 images, time remaining=5.8 hours
1778: loss=5.800, avg loss=6.394, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=547.4 milliseconds, train=3.3 seconds, 113792 images, time remaining=5.8 hours
1779: loss=6.234, avg loss=6.378, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=691.2 milliseconds, train=3.3 seconds, 113856 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1780: loss=6.549, avg loss=6.395, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 113920 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1781: loss=6.567, avg loss=6.412, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=3.8 seconds, 113984 images, time remaining=5.8 hours
1782: loss=6.551, avg loss=6.426, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=802.5 milliseconds, train=3.8 seconds, 114048 images, time remaining=5.8 hours
1783: loss=6.723, avg loss=6.456, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.8 seconds, 114112 images, time remaining=5.8 hours
1784: loss=6.646, avg loss=6.475, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=719.3 milliseconds, train=3.8 seconds, 114176 images, time remaining=5.8 hours
1785: loss=6.670, avg loss=6.494, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=883.2 milliseconds, train=3.8 seconds, 114240 images, time remaining=5.8 hours
1786: loss=6.091, avg loss=6.454, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.8 seconds, 114304 images, time remaining=5.8 hours
1787: loss=7.149, avg loss=6.523, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=3.8 seconds, 114368 images, time remaining=5.8 hours
1788: loss=6.202, avg loss=6.491, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.8 seconds, 114432 images, time remaining=5.8 hours
1789: loss=6.222, avg loss=6.464, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=3.8 seconds, 114496 images, time remaining=5.8 hours
1790: loss=6.234, avg loss=6.441, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.5 seconds, train=3.8 seconds, 114560 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1791: loss=6.096, avg loss=6.407, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=508.3 milliseconds, train=2.1 seconds, 114624 images, time remaining=5.8 hours
1792: loss=5.272, avg loss=6.293, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=599.3 milliseconds, train=2.1 seconds, 114688 images, time remaining=5.8 hours
1793: loss=5.917, avg loss=6.256, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 114752 images, time remaining=5.8 hours
1794: loss=6.089, avg loss=6.239, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=886.6 milliseconds, train=2.1 seconds, 114816 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1795: loss=6.769, avg loss=6.292, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 114880 images, time remaining=5.8 hours
1796: loss=7.060, avg loss=6.369, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 114944 images, time remaining=5.8 hours
1797: loss=6.260, avg loss=6.358, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=760.3 milliseconds, train=2.1 seconds, 115008 images, time remaining=5.8 hours
1798: loss=5.611, avg loss=6.283, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 115072 images, time remaining=5.8 hours
1799: loss=6.145, avg loss=6.269, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 115136 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1800: loss=7.165, avg loss=6.359, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 115200 images, time remaining=5.8 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1801: loss=6.215, avg loss=6.345, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=869.9 milliseconds, train=3.9 seconds, 115264 images, time remaining=5.8 hours
1802: loss=6.863, avg loss=6.397, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 115328 images, time remaining=5.8 hours
1803: loss=6.004, avg loss=6.357, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 115392 images, time remaining=5.8 hours
1804: loss=7.308, avg loss=6.452, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 115456 images, time remaining=5.8 hours
1805: loss=6.411, avg loss=6.448, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.9 seconds, train=3.9 seconds, 115520 images, time remaining=5.8 hours
1806: loss=6.790, avg loss=6.482, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 115584 images, time remaining=5.8 hours
1807: loss=6.534, avg loss=6.488, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 115648 images, time remaining=5.8 hours
1808: loss=6.660, avg loss=6.505, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 115712 images, time remaining=5.8 hours
1809: loss=6.347, avg loss=6.489, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=3.6 seconds, train=3.9 seconds, 115776 images, time remaining=5.8 hours
1810: loss=6.255, avg loss=6.466, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 115840 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1811: loss=5.975, avg loss=6.417, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=735.3 milliseconds, train=3.2 seconds, 115904 images, time remaining=5.8 hours
1812: loss=6.635, avg loss=6.438, last=50.65%, best=50.65%, next=1812, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 115968 images, time remaining=5.8 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=233373, unique_truth_count=57264
rank=0 of ranks=233373rank=100 of ranks=233373rank=200 of ranks=233373rank=300 of ranks=233373rank=400 of ranks=233373rank=500 of ranks=233373rank=600 of ranks=233373rank=700 of ranks=233373rank=800 of ranks=233373rank=900 of ranks=233373rank=1000 of ranks=233373rank=1100 of ranks=233373rank=1200 of ranks=233373rank=1300 of ranks=233373rank=1400 of ranks=233373rank=1500 of ranks=233373rank=1600 of ranks=233373rank=1700 of ranks=233373rank=1800 of ranks=233373rank=1900 of ranks=233373rank=2000 of ranks=233373rank=2100 of ranks=233373rank=2200 of ranks=233373rank=2300 of ranks=233373rank=2400 of ranks=233373rank=2500 of ranks=233373rank=2600 of ranks=233373rank=2700 of ranks=233373rank=2800 of ranks=233373rank=2900 of ranks=233373rank=3000 of ranks=233373rank=3100 of ranks=233373rank=3200 of ranks=233373rank=3300 of ranks=233373rank=3400 of ranks=233373rank=3500 of ranks=233373rank=3600 of ranks=233373rank=3700 of ranks=233373rank=3800 of ranks=233373rank=3900 of ranks=233373rank=4000 of ranks=233373rank=4100 of ranks=233373rank=4200 of ranks=233373rank=4300 of ranks=233373rank=4400 of ranks=233373rank=4500 of ranks=233373rank=4600 of ranks=233373rank=4700 of ranks=233373rank=4800 of ranks=233373rank=4900 of ranks=233373rank=5000 of ranks=233373rank=5100 of ranks=233373rank=5200 of ranks=233373rank=5300 of ranks=233373rank=5400 of ranks=233373rank=5500 of ranks=233373rank=5600 of ranks=233373rank=5700 of ranks=233373rank=5800 of ranks=233373rank=5900 of ranks=233373rank=6000 of ranks=233373rank=6100 of ranks=233373rank=6200 of ranks=233373rank=6300 of ranks=233373rank=6400 of ranks=233373rank=6500 of ranks=233373rank=6600 of ranks=233373rank=6700 of ranks=233373rank=6800 of ranks=233373rank=6900 of ranks=233373rank=7000 of ranks=233373rank=7100 of ranks=233373rank=7200 of ranks=233373rank=7300 of ranks=233373rank=7400 of ranks=233373rank=7500 of ranks=233373rank=7600 of ranks=233373rank=7700 of ranks=233373rank=7800 of ranks=233373rank=7900 of ranks=233373rank=8000 of ranks=233373rank=8100 of ranks=233373rank=8200 of ranks=233373rank=8300 of ranks=233373rank=8400 of ranks=233373rank=8500 of ranks=233373rank=8600 of ranks=233373rank=8700 of ranks=233373rank=8800 of ranks=233373rank=8900 of ranks=233373rank=9000 of ranks=233373rank=9100 of ranks=233373rank=9200 of ranks=233373rank=9300 of ranks=233373rank=9400 of ranks=233373rank=9500 of ranks=233373rank=9600 of ranks=233373rank=9700 of ranks=233373rank=9800 of ranks=233373rank=9900 of ranks=233373rank=10000 of ranks=233373rank=10100 of ranks=233373rank=10200 of ranks=233373rank=10300 of ranks=233373rank=10400 of ranks=233373rank=10500 of ranks=233373rank=10600 of ranks=233373rank=10700 of ranks=233373rank=10800 of ranks=233373rank=10900 of ranks=233373rank=11000 of ranks=233373rank=11100 of ranks=233373rank=11200 of ranks=233373rank=11300 of ranks=233373rank=11400 of ranks=233373rank=11500 of ranks=233373rank=11600 of ranks=233373rank=11700 of ranks=233373rank=11800 of ranks=233373rank=11900 of ranks=233373rank=12000 of ranks=233373rank=12100 of ranks=233373rank=12200 of ranks=233373rank=12300 of ranks=233373rank=12400 of ranks=233373rank=12500 of ranks=233373rank=12600 of ranks=233373rank=12700 of ranks=233373rank=12800 of ranks=233373rank=12900 of ranks=233373rank=13000 of ranks=233373rank=13100 of ranks=233373rank=13200 of ranks=233373rank=13300 of ranks=233373rank=13400 of ranks=233373rank=13500 of ranks=233373rank=13600 of ranks=233373rank=13700 of ranks=233373rank=13800 of ranks=233373rank=13900 of ranks=233373rank=14000 of ranks=233373rank=14100 of ranks=233373rank=14200 of ranks=233373rank=14300 of ranks=233373rank=14400 of ranks=233373rank=14500 of ranks=233373rank=14600 of ranks=233373rank=14700 of ranks=233373rank=14800 of ranks=233373rank=14900 of ranks=233373rank=15000 of ranks=233373rank=15100 of ranks=233373rank=15200 of ranks=233373rank=15300 of ranks=233373rank=15400 of ranks=233373rank=15500 of ranks=233373rank=15600 of ranks=233373rank=15700 of ranks=233373rank=15800 of ranks=233373rank=15900 of ranks=233373rank=16000 of ranks=233373rank=16100 of ranks=233373rank=16200 of ranks=233373rank=16300 of ranks=233373rank=16400 of ranks=233373rank=16500 of ranks=233373rank=16600 of ranks=233373rank=16700 of ranks=233373rank=16800 of ranks=233373rank=16900 of ranks=233373rank=17000 of ranks=233373rank=17100 of ranks=233373rank=17200 of ranks=233373rank=17300 of ranks=233373rank=17400 of ranks=233373rank=17500 of ranks=233373rank=17600 of ranks=233373rank=17700 of ranks=233373rank=17800 of ranks=233373rank=17900 of ranks=233373rank=18000 of ranks=233373rank=18100 of ranks=233373rank=18200 of ranks=233373rank=18300 of ranks=233373rank=18400 of ranks=233373rank=18500 of ranks=233373rank=18600 of ranks=233373rank=18700 of ranks=233373rank=18800 of ranks=233373rank=18900 of ranks=233373rank=19000 of ranks=233373rank=19100 of ranks=233373rank=19200 of ranks=233373rank=19300 of ranks=233373rank=19400 of ranks=233373rank=19500 of ranks=233373rank=19600 of ranks=233373rank=19700 of ranks=233373rank=19800 of ranks=233373rank=19900 of ranks=233373rank=20000 of ranks=233373rank=20100 of ranks=233373rank=20200 of ranks=233373rank=20300 of ranks=233373rank=20400 of ranks=233373rank=20500 of ranks=233373rank=20600 of ranks=233373rank=20700 of ranks=233373rank=20800 of ranks=233373rank=20900 of ranks=233373rank=21000 of ranks=233373rank=21100 of ranks=233373rank=21200 of ranks=233373rank=21300 of ranks=233373rank=21400 of ranks=233373rank=21500 of ranks=233373rank=21600 of ranks=233373rank=21700 of ranks=233373rank=21800 of ranks=233373rank=21900 of ranks=233373rank=22000 of ranks=233373rank=22100 of ranks=233373rank=22200 of ranks=233373rank=22300 of ranks=233373rank=22400 of ranks=233373rank=22500 of ranks=233373rank=22600 of ranks=233373rank=22700 of ranks=233373rank=22800 of ranks=233373rank=22900 of ranks=233373rank=23000 of ranks=233373rank=23100 of ranks=233373rank=23200 of ranks=233373rank=23300 of ranks=233373rank=23400 of ranks=233373rank=23500 of ranks=233373rank=23600 of ranks=233373rank=23700 of ranks=233373rank=23800 of ranks=233373rank=23900 of ranks=233373rank=24000 of ranks=233373rank=24100 of ranks=233373rank=24200 of ranks=233373rank=24300 of ranks=233373rank=24400 of ranks=233373rank=24500 of ranks=233373rank=24600 of ranks=233373rank=24700 of ranks=233373rank=24800 of ranks=233373rank=24900 of ranks=233373rank=25000 of ranks=233373rank=25100 of ranks=233373rank=25200 of ranks=233373rank=25300 of ranks=233373rank=25400 of ranks=233373rank=25500 of ranks=233373rank=25600 of ranks=233373rank=25700 of ranks=233373rank=25800 of ranks=233373rank=25900 of ranks=233373rank=26000 of ranks=233373rank=26100 of ranks=233373rank=26200 of ranks=233373rank=26300 of ranks=233373rank=26400 of ranks=233373rank=26500 of ranks=233373rank=26600 of ranks=233373rank=26700 of ranks=233373rank=26800 of ranks=233373rank=26900 of ranks=233373rank=27000 of ranks=233373rank=27100 of ranks=233373rank=27200 of ranks=233373rank=27300 of ranks=233373rank=27400 of ranks=233373rank=27500 of ranks=233373rank=27600 of ranks=233373rank=27700 of ranks=233373rank=27800 of ranks=233373rank=27900 of ranks=233373rank=28000 of ranks=233373rank=28100 of ranks=233373rank=28200 of ranks=233373rank=28300 of ranks=233373rank=28400 of ranks=233373rank=28500 of ranks=233373rank=28600 of ranks=233373rank=28700 of ranks=233373rank=28800 of ranks=233373rank=28900 of ranks=233373rank=29000 of ranks=233373rank=29100 of ranks=233373rank=29200 of ranks=233373rank=29300 of ranks=233373rank=29400 of ranks=233373rank=29500 of ranks=233373rank=29600 of ranks=233373rank=29700 of ranks=233373rank=29800 of ranks=233373rank=29900 of ranks=233373rank=30000 of ranks=233373rank=30100 of ranks=233373rank=30200 of ranks=233373rank=30300 of ranks=233373rank=30400 of ranks=233373rank=30500 of ranks=233373rank=30600 of ranks=233373rank=30700 of ranks=233373rank=30800 of ranks=233373rank=30900 of ranks=233373rank=31000 of ranks=233373rank=31100 of ranks=233373rank=31200 of ranks=233373rank=31300 of ranks=233373rank=31400 of ranks=233373rank=31500 of ranks=233373rank=31600 of ranks=233373rank=31700 of ranks=233373rank=31800 of ranks=233373rank=31900 of ranks=233373rank=32000 of ranks=233373rank=32100 of ranks=233373rank=32200 of ranks=233373rank=32300 of ranks=233373rank=32400 of ranks=233373rank=32500 of ranks=233373rank=32600 of ranks=233373rank=32700 of ranks=233373rank=32800 of ranks=233373rank=32900 of ranks=233373rank=33000 of ranks=233373rank=33100 of ranks=233373rank=33200 of ranks=233373rank=33300 of ranks=233373rank=33400 of ranks=233373rank=33500 of ranks=233373rank=33600 of ranks=233373rank=33700 of ranks=233373rank=33800 of ranks=233373rank=33900 of ranks=233373rank=34000 of ranks=233373rank=34100 of ranks=233373rank=34200 of ranks=233373rank=34300 of ranks=233373rank=34400 of ranks=233373rank=34500 of ranks=233373rank=34600 of ranks=233373rank=34700 of ranks=233373rank=34800 of ranks=233373rank=34900 of ranks=233373rank=35000 of ranks=233373rank=35100 of ranks=233373rank=35200 of ranks=233373rank=35300 of ranks=233373rank=35400 of ranks=233373rank=35500 of ranks=233373rank=35600 of ranks=233373rank=35700 of ranks=233373rank=35800 of ranks=233373rank=35900 of ranks=233373rank=36000 of ranks=233373rank=36100 of ranks=233373rank=36200 of ranks=233373rank=36300 of ranks=233373rank=36400 of ranks=233373rank=36500 of ranks=233373rank=36600 of ranks=233373rank=36700 of ranks=233373rank=36800 of ranks=233373rank=36900 of ranks=233373rank=37000 of ranks=233373rank=37100 of ranks=233373rank=37200 of ranks=233373rank=37300 of ranks=233373rank=37400 of ranks=233373rank=37500 of ranks=233373rank=37600 of ranks=233373rank=37700 of ranks=233373rank=37800 of ranks=233373rank=37900 of ranks=233373rank=38000 of ranks=233373rank=38100 of ranks=233373rank=38200 of ranks=233373rank=38300 of ranks=233373rank=38400 of ranks=233373rank=38500 of ranks=233373rank=38600 of ranks=233373rank=38700 of ranks=233373rank=38800 of ranks=233373rank=38900 of ranks=233373rank=39000 of ranks=233373rank=39100 of ranks=233373rank=39200 of ranks=233373rank=39300 of ranks=233373rank=39400 of ranks=233373rank=39500 of ranks=233373rank=39600 of ranks=233373rank=39700 of ranks=233373rank=39800 of ranks=233373rank=39900 of ranks=233373rank=40000 of ranks=233373rank=40100 of ranks=233373rank=40200 of ranks=233373rank=40300 of ranks=233373rank=40400 of ranks=233373rank=40500 of ranks=233373rank=40600 of ranks=233373rank=40700 of ranks=233373rank=40800 of ranks=233373rank=40900 of ranks=233373rank=41000 of ranks=233373rank=41100 of ranks=233373rank=41200 of ranks=233373rank=41300 of ranks=233373rank=41400 of ranks=233373rank=41500 of ranks=233373rank=41600 of ranks=233373rank=41700 of ranks=233373rank=41800 of ranks=233373rank=41900 of ranks=233373rank=42000 of ranks=233373rank=42100 of ranks=233373rank=42200 of ranks=233373rank=42300 of ranks=233373rank=42400 of ranks=233373rank=42500 of ranks=233373rank=42600 of ranks=233373rank=42700 of ranks=233373rank=42800 of ranks=233373rank=42900 of ranks=233373rank=43000 of ranks=233373rank=43100 of ranks=233373rank=43200 of ranks=233373rank=43300 of ranks=233373rank=43400 of ranks=233373rank=43500 of ranks=233373rank=43600 of ranks=233373rank=43700 of ranks=233373rank=43800 of ranks=233373rank=43900 of ranks=233373rank=44000 of ranks=233373rank=44100 of ranks=233373rank=44200 of ranks=233373rank=44300 of ranks=233373rank=44400 of ranks=233373rank=44500 of ranks=233373rank=44600 of ranks=233373rank=44700 of ranks=233373rank=44800 of ranks=233373rank=44900 of ranks=233373rank=45000 of ranks=233373rank=45100 of ranks=233373rank=45200 of ranks=233373rank=45300 of ranks=233373rank=45400 of ranks=233373rank=45500 of ranks=233373rank=45600 of ranks=233373rank=45700 of ranks=233373rank=45800 of ranks=233373rank=45900 of ranks=233373rank=46000 of ranks=233373rank=46100 of ranks=233373rank=46200 of ranks=233373rank=46300 of ranks=233373rank=46400 of ranks=233373rank=46500 of ranks=233373rank=46600 of ranks=233373rank=46700 of ranks=233373rank=46800 of ranks=233373rank=46900 of ranks=233373rank=47000 of ranks=233373rank=47100 of ranks=233373rank=47200 of ranks=233373rank=47300 of ranks=233373rank=47400 of ranks=233373rank=47500 of ranks=233373rank=47600 of ranks=233373rank=47700 of ranks=233373rank=47800 of ranks=233373rank=47900 of ranks=233373rank=48000 of ranks=233373rank=48100 of ranks=233373rank=48200 of ranks=233373rank=48300 of ranks=233373rank=48400 of ranks=233373rank=48500 of ranks=233373rank=48600 of ranks=233373rank=48700 of ranks=233373rank=48800 of ranks=233373rank=48900 of ranks=233373rank=49000 of ranks=233373rank=49100 of ranks=233373rank=49200 of ranks=233373rank=49300 of ranks=233373rank=49400 of ranks=233373rank=49500 of ranks=233373rank=49600 of ranks=233373rank=49700 of ranks=233373rank=49800 of ranks=233373rank=49900 of ranks=233373rank=50000 of ranks=233373rank=50100 of ranks=233373rank=50200 of ranks=233373rank=50300 of ranks=233373rank=50400 of ranks=233373rank=50500 of ranks=233373rank=50600 of ranks=233373rank=50700 of ranks=233373rank=50800 of ranks=233373rank=50900 of ranks=233373rank=51000 of ranks=233373rank=51100 of ranks=233373rank=51200 of ranks=233373rank=51300 of ranks=233373rank=51400 of ranks=233373rank=51500 of ranks=233373rank=51600 of ranks=233373rank=51700 of ranks=233373rank=51800 of ranks=233373rank=51900 of ranks=233373rank=52000 of ranks=233373rank=52100 of ranks=233373rank=52200 of ranks=233373rank=52300 of ranks=233373rank=52400 of ranks=233373rank=52500 of ranks=233373rank=52600 of ranks=233373rank=52700 of ranks=233373rank=52800 of ranks=233373rank=52900 of ranks=233373rank=53000 of ranks=233373rank=53100 of ranks=233373rank=53200 of ranks=233373rank=53300 of ranks=233373rank=53400 of ranks=233373rank=53500 of ranks=233373rank=53600 of ranks=233373rank=53700 of ranks=233373rank=53800 of ranks=233373rank=53900 of ranks=233373rank=54000 of ranks=233373rank=54100 of ranks=233373rank=54200 of ranks=233373rank=54300 of ranks=233373rank=54400 of ranks=233373rank=54500 of ranks=233373rank=54600 of ranks=233373rank=54700 of ranks=233373rank=54800 of ranks=233373rank=54900 of ranks=233373rank=55000 of ranks=233373rank=55100 of ranks=233373rank=55200 of ranks=233373rank=55300 of ranks=233373rank=55400 of ranks=233373rank=55500 of ranks=233373rank=55600 of ranks=233373rank=55700 of ranks=233373rank=55800 of ranks=233373rank=55900 of ranks=233373rank=56000 of ranks=233373rank=56100 of ranks=233373rank=56200 of ranks=233373rank=56300 of ranks=233373rank=56400 of ranks=233373rank=56500 of ranks=233373rank=56600 of ranks=233373rank=56700 of ranks=233373rank=56800 of ranks=233373rank=56900 of ranks=233373rank=57000 of ranks=233373rank=57100 of ranks=233373rank=57200 of ranks=233373rank=57300 of ranks=233373rank=57400 of ranks=233373rank=57500 of ranks=233373rank=57600 of ranks=233373rank=57700 of ranks=233373rank=57800 of ranks=233373rank=57900 of ranks=233373rank=58000 of ranks=233373rank=58100 of ranks=233373rank=58200 of ranks=233373rank=58300 of ranks=233373rank=58400 of ranks=233373rank=58500 of ranks=233373rank=58600 of ranks=233373rank=58700 of ranks=233373rank=58800 of ranks=233373rank=58900 of ranks=233373rank=59000 of ranks=233373rank=59100 of ranks=233373rank=59200 of ranks=233373rank=59300 of ranks=233373rank=59400 of ranks=233373rank=59500 of ranks=233373rank=59600 of ranks=233373rank=59700 of ranks=233373rank=59800 of ranks=233373rank=59900 of ranks=233373rank=60000 of ranks=233373rank=60100 of ranks=233373rank=60200 of ranks=233373rank=60300 of ranks=233373rank=60400 of ranks=233373rank=60500 of ranks=233373rank=60600 of ranks=233373rank=60700 of ranks=233373rank=60800 of ranks=233373rank=60900 of ranks=233373rank=61000 of ranks=233373rank=61100 of ranks=233373rank=61200 of ranks=233373rank=61300 of ranks=233373rank=61400 of ranks=233373rank=61500 of ranks=233373rank=61600 of ranks=233373rank=61700 of ranks=233373rank=61800 of ranks=233373rank=61900 of ranks=233373rank=62000 of ranks=233373rank=62100 of ranks=233373rank=62200 of ranks=233373rank=62300 of ranks=233373rank=62400 of ranks=233373rank=62500 of ranks=233373rank=62600 of ranks=233373rank=62700 of ranks=233373rank=62800 of ranks=233373rank=62900 of ranks=233373rank=63000 of ranks=233373rank=63100 of ranks=233373rank=63200 of ranks=233373rank=63300 of ranks=233373rank=63400 of ranks=233373rank=63500 of ranks=233373rank=63600 of ranks=233373rank=63700 of ranks=233373rank=63800 of ranks=233373rank=63900 of ranks=233373rank=64000 of ranks=233373rank=64100 of ranks=233373rank=64200 of ranks=233373rank=64300 of ranks=233373rank=64400 of ranks=233373rank=64500 of ranks=233373rank=64600 of ranks=233373rank=64700 of ranks=233373rank=64800 of ranks=233373rank=64900 of ranks=233373rank=65000 of ranks=233373rank=65100 of ranks=233373rank=65200 of ranks=233373rank=65300 of ranks=233373rank=65400 of ranks=233373rank=65500 of ranks=233373rank=65600 of ranks=233373rank=65700 of ranks=233373rank=65800 of ranks=233373rank=65900 of ranks=233373rank=66000 of ranks=233373rank=66100 of ranks=233373rank=66200 of ranks=233373rank=66300 of ranks=233373rank=66400 of ranks=233373rank=66500 of ranks=233373rank=66600 of ranks=233373rank=66700 of ranks=233373rank=66800 of ranks=233373rank=66900 of ranks=233373rank=67000 of ranks=233373rank=67100 of ranks=233373rank=67200 of ranks=233373rank=67300 of ranks=233373rank=67400 of ranks=233373rank=67500 of ranks=233373rank=67600 of ranks=233373rank=67700 of ranks=233373rank=67800 of ranks=233373rank=67900 of ranks=233373rank=68000 of ranks=233373rank=68100 of ranks=233373rank=68200 of ranks=233373rank=68300 of ranks=233373rank=68400 of ranks=233373rank=68500 of ranks=233373rank=68600 of ranks=233373rank=68700 of ranks=233373rank=68800 of ranks=233373rank=68900 of ranks=233373rank=69000 of ranks=233373rank=69100 of ranks=233373rank=69200 of ranks=233373rank=69300 of ranks=233373rank=69400 of ranks=233373rank=69500 of ranks=233373rank=69600 of ranks=233373rank=69700 of ranks=233373rank=69800 of ranks=233373rank=69900 of ranks=233373rank=70000 of ranks=233373rank=70100 of ranks=233373rank=70200 of ranks=233373rank=70300 of ranks=233373rank=70400 of ranks=233373rank=70500 of ranks=233373rank=70600 of ranks=233373rank=70700 of ranks=233373rank=70800 of ranks=233373rank=70900 of ranks=233373rank=71000 of ranks=233373rank=71100 of ranks=233373rank=71200 of ranks=233373rank=71300 of ranks=233373rank=71400 of ranks=233373rank=71500 of ranks=233373rank=71600 of ranks=233373rank=71700 of ranks=233373rank=71800 of ranks=233373rank=71900 of ranks=233373rank=72000 of ranks=233373rank=72100 of ranks=233373rank=72200 of ranks=233373rank=72300 of ranks=233373rank=72400 of ranks=233373rank=72500 of ranks=233373rank=72600 of ranks=233373rank=72700 of ranks=233373rank=72800 of ranks=233373rank=72900 of ranks=233373rank=73000 of ranks=233373rank=73100 of ranks=233373rank=73200 of ranks=233373rank=73300 of ranks=233373rank=73400 of ranks=233373rank=73500 of ranks=233373rank=73600 of ranks=233373rank=73700 of ranks=233373rank=73800 of ranks=233373rank=73900 of ranks=233373rank=74000 of ranks=233373rank=74100 of ranks=233373rank=74200 of ranks=233373rank=74300 of ranks=233373rank=74400 of ranks=233373rank=74500 of ranks=233373rank=74600 of ranks=233373rank=74700 of ranks=233373rank=74800 of ranks=233373rank=74900 of ranks=233373rank=75000 of ranks=233373rank=75100 of ranks=233373rank=75200 of ranks=233373rank=75300 of ranks=233373rank=75400 of ranks=233373rank=75500 of ranks=233373rank=75600 of ranks=233373rank=75700 of ranks=233373rank=75800 of ranks=233373rank=75900 of ranks=233373rank=76000 of ranks=233373rank=76100 of ranks=233373rank=76200 of ranks=233373rank=76300 of ranks=233373rank=76400 of ranks=233373rank=76500 of ranks=233373rank=76600 of ranks=233373rank=76700 of ranks=233373rank=76800 of ranks=233373rank=76900 of ranks=233373rank=77000 of ranks=233373rank=77100 of ranks=233373rank=77200 of ranks=233373rank=77300 of ranks=233373rank=77400 of ranks=233373rank=77500 of ranks=233373rank=77600 of ranks=233373rank=77700 of ranks=233373rank=77800 of ranks=233373rank=77900 of ranks=233373rank=78000 of ranks=233373rank=78100 of ranks=233373rank=78200 of ranks=233373rank=78300 of ranks=233373rank=78400 of ranks=233373rank=78500 of ranks=233373rank=78600 of ranks=233373rank=78700 of ranks=233373rank=78800 of ranks=233373rank=78900 of ranks=233373rank=79000 of ranks=233373rank=79100 of ranks=233373rank=79200 of ranks=233373rank=79300 of ranks=233373rank=79400 of ranks=233373rank=79500 of ranks=233373rank=79600 of ranks=233373rank=79700 of ranks=233373rank=79800 of ranks=233373rank=79900 of ranks=233373rank=80000 of ranks=233373rank=80100 of ranks=233373rank=80200 of ranks=233373rank=80300 of ranks=233373rank=80400 of ranks=233373rank=80500 of ranks=233373rank=80600 of ranks=233373rank=80700 of ranks=233373rank=80800 of ranks=233373rank=80900 of ranks=233373rank=81000 of ranks=233373rank=81100 of ranks=233373rank=81200 of ranks=233373rank=81300 of ranks=233373rank=81400 of ranks=233373rank=81500 of ranks=233373rank=81600 of ranks=233373rank=81700 of ranks=233373rank=81800 of ranks=233373rank=81900 of ranks=233373rank=82000 of ranks=233373rank=82100 of ranks=233373rank=82200 of ranks=233373rank=82300 of ranks=233373rank=82400 of ranks=233373rank=82500 of ranks=233373rank=82600 of ranks=233373rank=82700 of ranks=233373rank=82800 of ranks=233373rank=82900 of ranks=233373rank=83000 of ranks=233373rank=83100 of ranks=233373rank=83200 of ranks=233373rank=83300 of ranks=233373rank=83400 of ranks=233373rank=83500 of ranks=233373rank=83600 of ranks=233373rank=83700 of ranks=233373rank=83800 of ranks=233373rank=83900 of ranks=233373rank=84000 of ranks=233373rank=84100 of ranks=233373rank=84200 of ranks=233373rank=84300 of ranks=233373rank=84400 of ranks=233373rank=84500 of ranks=233373rank=84600 of ranks=233373rank=84700 of ranks=233373rank=84800 of ranks=233373rank=84900 of ranks=233373rank=85000 of ranks=233373rank=85100 of ranks=233373rank=85200 of ranks=233373rank=85300 of ranks=233373rank=85400 of ranks=233373rank=85500 of ranks=233373rank=85600 of ranks=233373rank=85700 of ranks=233373rank=85800 of ranks=233373rank=85900 of ranks=233373rank=86000 of ranks=233373rank=86100 of ranks=233373rank=86200 of ranks=233373rank=86300 of ranks=233373rank=86400 of ranks=233373rank=86500 of ranks=233373rank=86600 of ranks=233373rank=86700 of ranks=233373rank=86800 of ranks=233373rank=86900 of ranks=233373rank=87000 of ranks=233373rank=87100 of ranks=233373rank=87200 of ranks=233373rank=87300 of ranks=233373rank=87400 of ranks=233373rank=87500 of ranks=233373rank=87600 of ranks=233373rank=87700 of ranks=233373rank=87800 of ranks=233373rank=87900 of ranks=233373rank=88000 of ranks=233373rank=88100 of ranks=233373rank=88200 of ranks=233373rank=88300 of ranks=233373rank=88400 of ranks=233373rank=88500 of ranks=233373rank=88600 of ranks=233373rank=88700 of ranks=233373rank=88800 of ranks=233373rank=88900 of ranks=233373rank=89000 of ranks=233373rank=89100 of ranks=233373rank=89200 of ranks=233373rank=89300 of ranks=233373rank=89400 of ranks=233373rank=89500 of ranks=233373rank=89600 of ranks=233373rank=89700 of ranks=233373rank=89800 of ranks=233373rank=89900 of ranks=233373rank=90000 of ranks=233373rank=90100 of ranks=233373rank=90200 of ranks=233373rank=90300 of ranks=233373rank=90400 of ranks=233373rank=90500 of ranks=233373rank=90600 of ranks=233373rank=90700 of ranks=233373rank=90800 of ranks=233373rank=90900 of ranks=233373rank=91000 of ranks=233373rank=91100 of ranks=233373rank=91200 of ranks=233373rank=91300 of ranks=233373rank=91400 of ranks=233373rank=91500 of ranks=233373rank=91600 of ranks=233373rank=91700 of ranks=233373rank=91800 of ranks=233373rank=91900 of ranks=233373rank=92000 of ranks=233373rank=92100 of ranks=233373rank=92200 of ranks=233373rank=92300 of ranks=233373rank=92400 of ranks=233373rank=92500 of ranks=233373rank=92600 of ranks=233373rank=92700 of ranks=233373rank=92800 of ranks=233373rank=92900 of ranks=233373rank=93000 of ranks=233373rank=93100 of ranks=233373rank=93200 of ranks=233373rank=93300 of ranks=233373rank=93400 of ranks=233373rank=93500 of ranks=233373rank=93600 of ranks=233373rank=93700 of ranks=233373rank=93800 of ranks=233373rank=93900 of ranks=233373rank=94000 of ranks=233373rank=94100 of ranks=233373rank=94200 of ranks=233373rank=94300 of ranks=233373rank=94400 of ranks=233373rank=94500 of ranks=233373rank=94600 of ranks=233373rank=94700 of ranks=233373rank=94800 of ranks=233373rank=94900 of ranks=233373rank=95000 of ranks=233373rank=95100 of ranks=233373rank=95200 of ranks=233373rank=95300 of ranks=233373rank=95400 of ranks=233373rank=95500 of ranks=233373rank=95600 of ranks=233373rank=95700 of ranks=233373rank=95800 of ranks=233373rank=95900 of ranks=233373rank=96000 of ranks=233373rank=96100 of ranks=233373rank=96200 of ranks=233373rank=96300 of ranks=233373rank=96400 of ranks=233373rank=96500 of ranks=233373rank=96600 of ranks=233373rank=96700 of ranks=233373rank=96800 of ranks=233373rank=96900 of ranks=233373rank=97000 of ranks=233373rank=97100 of ranks=233373rank=97200 of ranks=233373rank=97300 of ranks=233373rank=97400 of ranks=233373rank=97500 of ranks=233373rank=97600 of ranks=233373rank=97700 of ranks=233373rank=97800 of ranks=233373rank=97900 of ranks=233373rank=98000 of ranks=233373rank=98100 of ranks=233373rank=98200 of ranks=233373rank=98300 of ranks=233373rank=98400 of ranks=233373rank=98500 of ranks=233373rank=98600 of ranks=233373rank=98700 of ranks=233373rank=98800 of ranks=233373rank=98900 of ranks=233373rank=99000 of ranks=233373rank=99100 of ranks=233373rank=99200 of ranks=233373rank=99300 of ranks=233373rank=99400 of ranks=233373rank=99500 of ranks=233373rank=99600 of ranks=233373rank=99700 of ranks=233373rank=99800 of ranks=233373rank=99900 of ranks=233373rank=100000 of ranks=233373rank=100100 of ranks=233373rank=100200 of ranks=233373rank=100300 of ranks=233373rank=100400 of ranks=233373rank=100500 of ranks=233373rank=100600 of ranks=233373rank=100700 of ranks=233373rank=100800 of ranks=233373rank=100900 of ranks=233373rank=101000 of ranks=233373rank=101100 of ranks=233373rank=101200 of ranks=233373rank=101300 of ranks=233373rank=101400 of ranks=233373rank=101500 of ranks=233373rank=101600 of ranks=233373rank=101700 of ranks=233373rank=101800 of ranks=233373rank=101900 of ranks=233373rank=102000 of ranks=233373rank=102100 of ranks=233373rank=102200 of ranks=233373rank=102300 of ranks=233373rank=102400 of ranks=233373rank=102500 of ranks=233373rank=102600 of ranks=233373rank=102700 of ranks=233373rank=102800 of ranks=233373rank=102900 of ranks=233373rank=103000 of ranks=233373rank=103100 of ranks=233373rank=103200 of ranks=233373rank=103300 of ranks=233373rank=103400 of ranks=233373rank=103500 of ranks=233373rank=103600 of ranks=233373rank=103700 of ranks=233373rank=103800 of ranks=233373rank=103900 of ranks=233373rank=104000 of ranks=233373rank=104100 of ranks=233373rank=104200 of ranks=233373rank=104300 of ranks=233373rank=104400 of ranks=233373rank=104500 of ranks=233373rank=104600 of ranks=233373rank=104700 of ranks=233373rank=104800 of ranks=233373rank=104900 of ranks=233373rank=105000 of ranks=233373rank=105100 of ranks=233373rank=105200 of ranks=233373rank=105300 of ranks=233373rank=105400 of ranks=233373rank=105500 of ranks=233373rank=105600 of ranks=233373rank=105700 of ranks=233373rank=105800 of ranks=233373rank=105900 of ranks=233373rank=106000 of ranks=233373rank=106100 of ranks=233373rank=106200 of ranks=233373rank=106300 of ranks=233373rank=106400 of ranks=233373rank=106500 of ranks=233373rank=106600 of ranks=233373rank=106700 of ranks=233373rank=106800 of ranks=233373rank=106900 of ranks=233373rank=107000 of ranks=233373rank=107100 of ranks=233373rank=107200 of ranks=233373rank=107300 of ranks=233373rank=107400 of ranks=233373rank=107500 of ranks=233373rank=107600 of ranks=233373rank=107700 of ranks=233373rank=107800 of ranks=233373rank=107900 of ranks=233373rank=108000 of ranks=233373rank=108100 of ranks=233373rank=108200 of ranks=233373rank=108300 of ranks=233373rank=108400 of ranks=233373rank=108500 of ranks=233373rank=108600 of ranks=233373rank=108700 of ranks=233373rank=108800 of ranks=233373rank=108900 of ranks=233373rank=109000 of ranks=233373rank=109100 of ranks=233373rank=109200 of ranks=233373rank=109300 of ranks=233373rank=109400 of ranks=233373rank=109500 of ranks=233373rank=109600 of ranks=233373rank=109700 of ranks=233373rank=109800 of ranks=233373rank=109900 of ranks=233373rank=110000 of ranks=233373rank=110100 of ranks=233373rank=110200 of ranks=233373rank=110300 of ranks=233373rank=110400 of ranks=233373rank=110500 of ranks=233373rank=110600 of ranks=233373rank=110700 of ranks=233373rank=110800 of ranks=233373rank=110900 of ranks=233373rank=111000 of ranks=233373rank=111100 of ranks=233373rank=111200 of ranks=233373rank=111300 of ranks=233373rank=111400 of ranks=233373rank=111500 of ranks=233373rank=111600 of ranks=233373rank=111700 of ranks=233373rank=111800 of ranks=233373rank=111900 of ranks=233373rank=112000 of ranks=233373rank=112100 of ranks=233373rank=112200 of ranks=233373rank=112300 of ranks=233373rank=112400 of ranks=233373rank=112500 of ranks=233373rank=112600 of ranks=233373rank=112700 of ranks=233373rank=112800 of ranks=233373rank=112900 of ranks=233373rank=113000 of ranks=233373rank=113100 of ranks=233373rank=113200 of ranks=233373rank=113300 of ranks=233373rank=113400 of ranks=233373rank=113500 of ranks=233373rank=113600 of ranks=233373rank=113700 of ranks=233373rank=113800 of ranks=233373rank=113900 of ranks=233373rank=114000 of ranks=233373rank=114100 of ranks=233373rank=114200 of ranks=233373rank=114300 of ranks=233373rank=114400 of ranks=233373rank=114500 of ranks=233373rank=114600 of ranks=233373rank=114700 of ranks=233373rank=114800 of ranks=233373rank=114900 of ranks=233373rank=115000 of ranks=233373rank=115100 of ranks=233373rank=115200 of ranks=233373rank=115300 of ranks=233373rank=115400 of ranks=233373rank=115500 of ranks=233373rank=115600 of ranks=233373rank=115700 of ranks=233373rank=115800 of ranks=233373rank=115900 of ranks=233373rank=116000 of ranks=233373rank=116100 of ranks=233373rank=116200 of ranks=233373rank=116300 of ranks=233373rank=116400 of ranks=233373rank=116500 of ranks=233373rank=116600 of ranks=233373rank=116700 of ranks=233373rank=116800 of ranks=233373rank=116900 of ranks=233373rank=117000 of ranks=233373rank=117100 of ranks=233373rank=117200 of ranks=233373rank=117300 of ranks=233373rank=117400 of ranks=233373rank=117500 of ranks=233373rank=117600 of ranks=233373rank=117700 of ranks=233373rank=117800 of ranks=233373rank=117900 of ranks=233373rank=118000 of ranks=233373rank=118100 of ranks=233373rank=118200 of ranks=233373rank=118300 of ranks=233373rank=118400 of ranks=233373rank=118500 of ranks=233373rank=118600 of ranks=233373rank=118700 of ranks=233373rank=118800 of ranks=233373rank=118900 of ranks=233373rank=119000 of ranks=233373rank=119100 of ranks=233373rank=119200 of ranks=233373rank=119300 of ranks=233373rank=119400 of ranks=233373rank=119500 of ranks=233373rank=119600 of ranks=233373rank=119700 of ranks=233373rank=119800 of ranks=233373rank=119900 of ranks=233373rank=120000 of ranks=233373rank=120100 of ranks=233373rank=120200 of ranks=233373rank=120300 of ranks=233373rank=120400 of ranks=233373rank=120500 of ranks=233373rank=120600 of ranks=233373rank=120700 of ranks=233373rank=120800 of ranks=233373rank=120900 of ranks=233373rank=121000 of ranks=233373rank=121100 of ranks=233373rank=121200 of ranks=233373rank=121300 of ranks=233373rank=121400 of ranks=233373rank=121500 of ranks=233373rank=121600 of ranks=233373rank=121700 of ranks=233373rank=121800 of ranks=233373rank=121900 of ranks=233373rank=122000 of ranks=233373rank=122100 of ranks=233373rank=122200 of ranks=233373rank=122300 of ranks=233373rank=122400 of ranks=233373rank=122500 of ranks=233373rank=122600 of ranks=233373rank=122700 of ranks=233373rank=122800 of ranks=233373rank=122900 of ranks=233373rank=123000 of ranks=233373rank=123100 of ranks=233373rank=123200 of ranks=233373rank=123300 of ranks=233373rank=123400 of ranks=233373rank=123500 of ranks=233373rank=123600 of ranks=233373rank=123700 of ranks=233373rank=123800 of ranks=233373rank=123900 of ranks=233373rank=124000 of ranks=233373rank=124100 of ranks=233373rank=124200 of ranks=233373rank=124300 of ranks=233373rank=124400 of ranks=233373rank=124500 of ranks=233373rank=124600 of ranks=233373rank=124700 of ranks=233373rank=124800 of ranks=233373rank=124900 of ranks=233373rank=125000 of ranks=233373rank=125100 of ranks=233373rank=125200 of ranks=233373rank=125300 of ranks=233373rank=125400 of ranks=233373rank=125500 of ranks=233373rank=125600 of ranks=233373rank=125700 of ranks=233373rank=125800 of ranks=233373rank=125900 of ranks=233373rank=126000 of ranks=233373rank=126100 of ranks=233373rank=126200 of ranks=233373rank=126300 of ranks=233373rank=126400 of ranks=233373rank=126500 of ranks=233373rank=126600 of ranks=233373rank=126700 of ranks=233373rank=126800 of ranks=233373rank=126900 of ranks=233373rank=127000 of ranks=233373rank=127100 of ranks=233373rank=127200 of ranks=233373rank=127300 of ranks=233373rank=127400 of ranks=233373rank=127500 of ranks=233373rank=127600 of ranks=233373rank=127700 of ranks=233373rank=127800 of ranks=233373rank=127900 of ranks=233373rank=128000 of ranks=233373rank=128100 of ranks=233373rank=128200 of ranks=233373rank=128300 of ranks=233373rank=128400 of ranks=233373rank=128500 of ranks=233373rank=128600 of ranks=233373rank=128700 of ranks=233373rank=128800 of ranks=233373rank=128900 of ranks=233373rank=129000 of ranks=233373rank=129100 of ranks=233373rank=129200 of ranks=233373rank=129300 of ranks=233373rank=129400 of ranks=233373rank=129500 of ranks=233373rank=129600 of ranks=233373rank=129700 of ranks=233373rank=129800 of ranks=233373rank=129900 of ranks=233373rank=130000 of ranks=233373rank=130100 of ranks=233373rank=130200 of ranks=233373rank=130300 of ranks=233373rank=130400 of ranks=233373rank=130500 of ranks=233373rank=130600 of ranks=233373rank=130700 of ranks=233373rank=130800 of ranks=233373rank=130900 of ranks=233373rank=131000 of ranks=233373rank=131100 of ranks=233373rank=131200 of ranks=233373rank=131300 of ranks=233373rank=131400 of ranks=233373rank=131500 of ranks=233373rank=131600 of ranks=233373rank=131700 of ranks=233373rank=131800 of ranks=233373rank=131900 of ranks=233373rank=132000 of ranks=233373rank=132100 of ranks=233373rank=132200 of ranks=233373rank=132300 of ranks=233373rank=132400 of ranks=233373rank=132500 of ranks=233373rank=132600 of ranks=233373rank=132700 of ranks=233373rank=132800 of ranks=233373rank=132900 of ranks=233373rank=133000 of ranks=233373rank=133100 of ranks=233373rank=133200 of ranks=233373rank=133300 of ranks=233373rank=133400 of ranks=233373rank=133500 of ranks=233373rank=133600 of ranks=233373rank=133700 of ranks=233373rank=133800 of ranks=233373rank=133900 of ranks=233373rank=134000 of ranks=233373rank=134100 of ranks=233373rank=134200 of ranks=233373rank=134300 of ranks=233373rank=134400 of ranks=233373rank=134500 of ranks=233373rank=134600 of ranks=233373rank=134700 of ranks=233373rank=134800 of ranks=233373rank=134900 of ranks=233373rank=135000 of ranks=233373rank=135100 of ranks=233373rank=135200 of ranks=233373rank=135300 of ranks=233373rank=135400 of ranks=233373rank=135500 of ranks=233373rank=135600 of ranks=233373rank=135700 of ranks=233373rank=135800 of ranks=233373rank=135900 of ranks=233373rank=136000 of ranks=233373rank=136100 of ranks=233373rank=136200 of ranks=233373rank=136300 of ranks=233373rank=136400 of ranks=233373rank=136500 of ranks=233373rank=136600 of ranks=233373rank=136700 of ranks=233373rank=136800 of ranks=233373rank=136900 of ranks=233373rank=137000 of ranks=233373rank=137100 of ranks=233373rank=137200 of ranks=233373rank=137300 of ranks=233373rank=137400 of ranks=233373rank=137500 of ranks=233373rank=137600 of ranks=233373rank=137700 of ranks=233373rank=137800 of ranks=233373rank=137900 of ranks=233373rank=138000 of ranks=233373rank=138100 of ranks=233373rank=138200 of ranks=233373rank=138300 of ranks=233373rank=138400 of ranks=233373rank=138500 of ranks=233373rank=138600 of ranks=233373rank=138700 of ranks=233373rank=138800 of ranks=233373rank=138900 of ranks=233373rank=139000 of ranks=233373rank=139100 of ranks=233373rank=139200 of ranks=233373rank=139300 of ranks=233373rank=139400 of ranks=233373rank=139500 of ranks=233373rank=139600 of ranks=233373rank=139700 of ranks=233373rank=139800 of ranks=233373rank=139900 of ranks=233373rank=140000 of ranks=233373rank=140100 of ranks=233373rank=140200 of ranks=233373rank=140300 of ranks=233373rank=140400 of ranks=233373rank=140500 of ranks=233373rank=140600 of ranks=233373rank=140700 of ranks=233373rank=140800 of ranks=233373rank=140900 of ranks=233373rank=141000 of ranks=233373rank=141100 of ranks=233373rank=141200 of ranks=233373rank=141300 of ranks=233373rank=141400 of ranks=233373rank=141500 of ranks=233373rank=141600 of ranks=233373rank=141700 of ranks=233373rank=141800 of ranks=233373rank=141900 of ranks=233373rank=142000 of ranks=233373rank=142100 of ranks=233373rank=142200 of ranks=233373rank=142300 of ranks=233373rank=142400 of ranks=233373rank=142500 of ranks=233373rank=142600 of ranks=233373rank=142700 of ranks=233373rank=142800 of ranks=233373rank=142900 of ranks=233373rank=143000 of ranks=233373rank=143100 of ranks=233373rank=143200 of ranks=233373rank=143300 of ranks=233373rank=143400 of ranks=233373rank=143500 of ranks=233373rank=143600 of ranks=233373rank=143700 of ranks=233373rank=143800 of ranks=233373rank=143900 of ranks=233373rank=144000 of ranks=233373rank=144100 of ranks=233373rank=144200 of ranks=233373rank=144300 of ranks=233373rank=144400 of ranks=233373rank=144500 of ranks=233373rank=144600 of ranks=233373rank=144700 of ranks=233373rank=144800 of ranks=233373rank=144900 of ranks=233373rank=145000 of ranks=233373rank=145100 of ranks=233373rank=145200 of ranks=233373rank=145300 of ranks=233373rank=145400 of ranks=233373rank=145500 of ranks=233373rank=145600 of ranks=233373rank=145700 of ranks=233373rank=145800 of ranks=233373rank=145900 of ranks=233373rank=146000 of ranks=233373rank=146100 of ranks=233373rank=146200 of ranks=233373rank=146300 of ranks=233373rank=146400 of ranks=233373rank=146500 of ranks=233373rank=146600 of ranks=233373rank=146700 of ranks=233373rank=146800 of ranks=233373rank=146900 of ranks=233373rank=147000 of ranks=233373rank=147100 of ranks=233373rank=147200 of ranks=233373rank=147300 of ranks=233373rank=147400 of ranks=233373rank=147500 of ranks=233373rank=147600 of ranks=233373rank=147700 of ranks=233373rank=147800 of ranks=233373rank=147900 of ranks=233373rank=148000 of ranks=233373rank=148100 of ranks=233373rank=148200 of ranks=233373rank=148300 of ranks=233373rank=148400 of ranks=233373rank=148500 of ranks=233373rank=148600 of ranks=233373rank=148700 of ranks=233373rank=148800 of ranks=233373rank=148900 of ranks=233373rank=149000 of ranks=233373rank=149100 of ranks=233373rank=149200 of ranks=233373rank=149300 of ranks=233373rank=149400 of ranks=233373rank=149500 of ranks=233373rank=149600 of ranks=233373rank=149700 of ranks=233373rank=149800 of ranks=233373rank=149900 of ranks=233373rank=150000 of ranks=233373rank=150100 of ranks=233373rank=150200 of ranks=233373rank=150300 of ranks=233373rank=150400 of ranks=233373rank=150500 of ranks=233373rank=150600 of ranks=233373rank=150700 of ranks=233373rank=150800 of ranks=233373rank=150900 of ranks=233373rank=151000 of ranks=233373rank=151100 of ranks=233373rank=151200 of ranks=233373rank=151300 of ranks=233373rank=151400 of ranks=233373rank=151500 of ranks=233373rank=151600 of ranks=233373rank=151700 of ranks=233373rank=151800 of ranks=233373rank=151900 of ranks=233373rank=152000 of ranks=233373rank=152100 of ranks=233373rank=152200 of ranks=233373rank=152300 of ranks=233373rank=152400 of ranks=233373rank=152500 of ranks=233373rank=152600 of ranks=233373rank=152700 of ranks=233373rank=152800 of ranks=233373rank=152900 of ranks=233373rank=153000 of ranks=233373rank=153100 of ranks=233373rank=153200 of ranks=233373rank=153300 of ranks=233373rank=153400 of ranks=233373rank=153500 of ranks=233373rank=153600 of ranks=233373rank=153700 of ranks=233373rank=153800 of ranks=233373rank=153900 of ranks=233373rank=154000 of ranks=233373rank=154100 of ranks=233373rank=154200 of ranks=233373rank=154300 of ranks=233373rank=154400 of ranks=233373rank=154500 of ranks=233373rank=154600 of ranks=233373rank=154700 of ranks=233373rank=154800 of ranks=233373rank=154900 of ranks=233373rank=155000 of ranks=233373rank=155100 of ranks=233373rank=155200 of ranks=233373rank=155300 of ranks=233373rank=155400 of ranks=233373rank=155500 of ranks=233373rank=155600 of ranks=233373rank=155700 of ranks=233373rank=155800 of ranks=233373rank=155900 of ranks=233373rank=156000 of ranks=233373rank=156100 of ranks=233373rank=156200 of ranks=233373rank=156300 of ranks=233373rank=156400 of ranks=233373rank=156500 of ranks=233373rank=156600 of ranks=233373rank=156700 of ranks=233373rank=156800 of ranks=233373rank=156900 of ranks=233373rank=157000 of ranks=233373rank=157100 of ranks=233373rank=157200 of ranks=233373rank=157300 of ranks=233373rank=157400 of ranks=233373rank=157500 of ranks=233373rank=157600 of ranks=233373rank=157700 of ranks=233373rank=157800 of ranks=233373rank=157900 of ranks=233373rank=158000 of ranks=233373rank=158100 of ranks=233373rank=158200 of ranks=233373rank=158300 of ranks=233373rank=158400 of ranks=233373rank=158500 of ranks=233373rank=158600 of ranks=233373rank=158700 of ranks=233373rank=158800 of ranks=233373rank=158900 of ranks=233373rank=159000 of ranks=233373rank=159100 of ranks=233373rank=159200 of ranks=233373rank=159300 of ranks=233373rank=159400 of ranks=233373rank=159500 of ranks=233373rank=159600 of ranks=233373rank=159700 of ranks=233373rank=159800 of ranks=233373rank=159900 of ranks=233373rank=160000 of ranks=233373rank=160100 of ranks=233373rank=160200 of ranks=233373rank=160300 of ranks=233373rank=160400 of ranks=233373rank=160500 of ranks=233373rank=160600 of ranks=233373rank=160700 of ranks=233373rank=160800 of ranks=233373rank=160900 of ranks=233373rank=161000 of ranks=233373rank=161100 of ranks=233373rank=161200 of ranks=233373rank=161300 of ranks=233373rank=161400 of ranks=233373rank=161500 of ranks=233373rank=161600 of ranks=233373rank=161700 of ranks=233373rank=161800 of ranks=233373rank=161900 of ranks=233373rank=162000 of ranks=233373rank=162100 of ranks=233373rank=162200 of ranks=233373rank=162300 of ranks=233373rank=162400 of ranks=233373rank=162500 of ranks=233373rank=162600 of ranks=233373rank=162700 of ranks=233373rank=162800 of ranks=233373rank=162900 of ranks=233373rank=163000 of ranks=233373rank=163100 of ranks=233373rank=163200 of ranks=233373rank=163300 of ranks=233373rank=163400 of ranks=233373rank=163500 of ranks=233373rank=163600 of ranks=233373rank=163700 of ranks=233373rank=163800 of ranks=233373rank=163900 of ranks=233373rank=164000 of ranks=233373rank=164100 of ranks=233373rank=164200 of ranks=233373rank=164300 of ranks=233373rank=164400 of ranks=233373rank=164500 of ranks=233373rank=164600 of ranks=233373rank=164700 of ranks=233373rank=164800 of ranks=233373rank=164900 of ranks=233373rank=165000 of ranks=233373rank=165100 of ranks=233373rank=165200 of ranks=233373rank=165300 of ranks=233373rank=165400 of ranks=233373rank=165500 of ranks=233373rank=165600 of ranks=233373rank=165700 of ranks=233373rank=165800 of ranks=233373rank=165900 of ranks=233373rank=166000 of ranks=233373rank=166100 of ranks=233373rank=166200 of ranks=233373rank=166300 of ranks=233373rank=166400 of ranks=233373rank=166500 of ranks=233373rank=166600 of ranks=233373rank=166700 of ranks=233373rank=166800 of ranks=233373rank=166900 of ranks=233373rank=167000 of ranks=233373rank=167100 of ranks=233373rank=167200 of ranks=233373rank=167300 of ranks=233373rank=167400 of ranks=233373rank=167500 of ranks=233373rank=167600 of ranks=233373rank=167700 of ranks=233373rank=167800 of ranks=233373rank=167900 of ranks=233373rank=168000 of ranks=233373rank=168100 of ranks=233373rank=168200 of ranks=233373rank=168300 of ranks=233373rank=168400 of ranks=233373rank=168500 of ranks=233373rank=168600 of ranks=233373rank=168700 of ranks=233373rank=168800 of ranks=233373rank=168900 of ranks=233373rank=169000 of ranks=233373rank=169100 of ranks=233373rank=169200 of ranks=233373rank=169300 of ranks=233373rank=169400 of ranks=233373rank=169500 of ranks=233373rank=169600 of ranks=233373rank=169700 of ranks=233373rank=169800 of ranks=233373rank=169900 of ranks=233373rank=170000 of ranks=233373rank=170100 of ranks=233373rank=170200 of ranks=233373rank=170300 of ranks=233373rank=170400 of ranks=233373rank=170500 of ranks=233373rank=170600 of ranks=233373rank=170700 of ranks=233373rank=170800 of ranks=233373rank=170900 of ranks=233373rank=171000 of ranks=233373rank=171100 of ranks=233373rank=171200 of ranks=233373rank=171300 of ranks=233373rank=171400 of ranks=233373rank=171500 of ranks=233373rank=171600 of ranks=233373rank=171700 of ranks=233373rank=171800 of ranks=233373rank=171900 of ranks=233373rank=172000 of ranks=233373rank=172100 of ranks=233373rank=172200 of ranks=233373rank=172300 of ranks=233373rank=172400 of ranks=233373rank=172500 of ranks=233373rank=172600 of ranks=233373rank=172700 of ranks=233373rank=172800 of ranks=233373rank=172900 of ranks=233373rank=173000 of ranks=233373rank=173100 of ranks=233373rank=173200 of ranks=233373rank=173300 of ranks=233373rank=173400 of ranks=233373rank=173500 of ranks=233373rank=173600 of ranks=233373rank=173700 of ranks=233373rank=173800 of ranks=233373rank=173900 of ranks=233373rank=174000 of ranks=233373rank=174100 of ranks=233373rank=174200 of ranks=233373rank=174300 of ranks=233373rank=174400 of ranks=233373rank=174500 of ranks=233373rank=174600 of ranks=233373rank=174700 of ranks=233373rank=174800 of ranks=233373rank=174900 of ranks=233373rank=175000 of ranks=233373rank=175100 of ranks=233373rank=175200 of ranks=233373rank=175300 of ranks=233373rank=175400 of ranks=233373rank=175500 of ranks=233373rank=175600 of ranks=233373rank=175700 of ranks=233373rank=175800 of ranks=233373rank=175900 of ranks=233373rank=176000 of ranks=233373rank=176100 of ranks=233373rank=176200 of ranks=233373rank=176300 of ranks=233373rank=176400 of ranks=233373rank=176500 of ranks=233373rank=176600 of ranks=233373rank=176700 of ranks=233373rank=176800 of ranks=233373rank=176900 of ranks=233373rank=177000 of ranks=233373rank=177100 of ranks=233373rank=177200 of ranks=233373rank=177300 of ranks=233373rank=177400 of ranks=233373rank=177500 of ranks=233373rank=177600 of ranks=233373rank=177700 of ranks=233373rank=177800 of ranks=233373rank=177900 of ranks=233373rank=178000 of ranks=233373rank=178100 of ranks=233373rank=178200 of ranks=233373rank=178300 of ranks=233373rank=178400 of ranks=233373rank=178500 of ranks=233373rank=178600 of ranks=233373rank=178700 of ranks=233373rank=178800 of ranks=233373rank=178900 of ranks=233373rank=179000 of ranks=233373rank=179100 of ranks=233373rank=179200 of ranks=233373rank=179300 of ranks=233373rank=179400 of ranks=233373rank=179500 of ranks=233373rank=179600 of ranks=233373rank=179700 of ranks=233373rank=179800 of ranks=233373rank=179900 of ranks=233373rank=180000 of ranks=233373rank=180100 of ranks=233373rank=180200 of ranks=233373rank=180300 of ranks=233373rank=180400 of ranks=233373rank=180500 of ranks=233373rank=180600 of ranks=233373rank=180700 of ranks=233373rank=180800 of ranks=233373rank=180900 of ranks=233373rank=181000 of ranks=233373rank=181100 of ranks=233373rank=181200 of ranks=233373rank=181300 of ranks=233373rank=181400 of ranks=233373rank=181500 of ranks=233373rank=181600 of ranks=233373rank=181700 of ranks=233373rank=181800 of ranks=233373rank=181900 of ranks=233373rank=182000 of ranks=233373rank=182100 of ranks=233373rank=182200 of ranks=233373rank=182300 of ranks=233373rank=182400 of ranks=233373rank=182500 of ranks=233373rank=182600 of ranks=233373rank=182700 of ranks=233373rank=182800 of ranks=233373rank=182900 of ranks=233373rank=183000 of ranks=233373rank=183100 of ranks=233373rank=183200 of ranks=233373rank=183300 of ranks=233373rank=183400 of ranks=233373rank=183500 of ranks=233373rank=183600 of ranks=233373rank=183700 of ranks=233373rank=183800 of ranks=233373rank=183900 of ranks=233373rank=184000 of ranks=233373rank=184100 of ranks=233373rank=184200 of ranks=233373rank=184300 of ranks=233373rank=184400 of ranks=233373rank=184500 of ranks=233373rank=184600 of ranks=233373rank=184700 of ranks=233373rank=184800 of ranks=233373rank=184900 of ranks=233373rank=185000 of ranks=233373rank=185100 of ranks=233373rank=185200 of ranks=233373rank=185300 of ranks=233373rank=185400 of ranks=233373rank=185500 of ranks=233373rank=185600 of ranks=233373rank=185700 of ranks=233373rank=185800 of ranks=233373rank=185900 of ranks=233373rank=186000 of ranks=233373rank=186100 of ranks=233373rank=186200 of ranks=233373rank=186300 of ranks=233373rank=186400 of ranks=233373rank=186500 of ranks=233373rank=186600 of ranks=233373rank=186700 of ranks=233373rank=186800 of ranks=233373rank=186900 of ranks=233373rank=187000 of ranks=233373rank=187100 of ranks=233373rank=187200 of ranks=233373rank=187300 of ranks=233373rank=187400 of ranks=233373rank=187500 of ranks=233373rank=187600 of ranks=233373rank=187700 of ranks=233373rank=187800 of ranks=233373rank=187900 of ranks=233373rank=188000 of ranks=233373rank=188100 of ranks=233373rank=188200 of ranks=233373rank=188300 of ranks=233373rank=188400 of ranks=233373rank=188500 of ranks=233373rank=188600 of ranks=233373rank=188700 of ranks=233373rank=188800 of ranks=233373rank=188900 of ranks=233373rank=189000 of ranks=233373rank=189100 of ranks=233373rank=189200 of ranks=233373rank=189300 of ranks=233373rank=189400 of ranks=233373rank=189500 of ranks=233373rank=189600 of ranks=233373rank=189700 of ranks=233373rank=189800 of ranks=233373rank=189900 of ranks=233373rank=190000 of ranks=233373rank=190100 of ranks=233373rank=190200 of ranks=233373rank=190300 of ranks=233373rank=190400 of ranks=233373rank=190500 of ranks=233373rank=190600 of ranks=233373rank=190700 of ranks=233373rank=190800 of ranks=233373rank=190900 of ranks=233373rank=191000 of ranks=233373rank=191100 of ranks=233373rank=191200 of ranks=233373rank=191300 of ranks=233373rank=191400 of ranks=233373rank=191500 of ranks=233373rank=191600 of ranks=233373rank=191700 of ranks=233373rank=191800 of ranks=233373rank=191900 of ranks=233373rank=192000 of ranks=233373rank=192100 of ranks=233373rank=192200 of ranks=233373rank=192300 of ranks=233373rank=192400 of ranks=233373rank=192500 of ranks=233373rank=192600 of ranks=233373rank=192700 of ranks=233373rank=192800 of ranks=233373rank=192900 of ranks=233373rank=193000 of ranks=233373rank=193100 of ranks=233373rank=193200 of ranks=233373rank=193300 of ranks=233373rank=193400 of ranks=233373rank=193500 of ranks=233373rank=193600 of ranks=233373rank=193700 of ranks=233373rank=193800 of ranks=233373rank=193900 of ranks=233373rank=194000 of ranks=233373rank=194100 of ranks=233373rank=194200 of ranks=233373rank=194300 of ranks=233373rank=194400 of ranks=233373rank=194500 of ranks=233373rank=194600 of ranks=233373rank=194700 of ranks=233373rank=194800 of ranks=233373rank=194900 of ranks=233373rank=195000 of ranks=233373rank=195100 of ranks=233373rank=195200 of ranks=233373rank=195300 of ranks=233373rank=195400 of ranks=233373rank=195500 of ranks=233373rank=195600 of ranks=233373rank=195700 of ranks=233373rank=195800 of ranks=233373rank=195900 of ranks=233373rank=196000 of ranks=233373rank=196100 of ranks=233373rank=196200 of ranks=233373rank=196300 of ranks=233373rank=196400 of ranks=233373rank=196500 of ranks=233373rank=196600 of ranks=233373rank=196700 of ranks=233373rank=196800 of ranks=233373rank=196900 of ranks=233373rank=197000 of ranks=233373rank=197100 of ranks=233373rank=197200 of ranks=233373rank=197300 of ranks=233373rank=197400 of ranks=233373rank=197500 of ranks=233373rank=197600 of ranks=233373rank=197700 of ranks=233373rank=197800 of ranks=233373rank=197900 of ranks=233373rank=198000 of ranks=233373rank=198100 of ranks=233373rank=198200 of ranks=233373rank=198300 of ranks=233373rank=198400 of ranks=233373rank=198500 of ranks=233373rank=198600 of ranks=233373rank=198700 of ranks=233373rank=198800 of ranks=233373rank=198900 of ranks=233373rank=199000 of ranks=233373rank=199100 of ranks=233373rank=199200 of ranks=233373rank=199300 of ranks=233373rank=199400 of ranks=233373rank=199500 of ranks=233373rank=199600 of ranks=233373rank=199700 of ranks=233373rank=199800 of ranks=233373rank=199900 of ranks=233373rank=200000 of ranks=233373rank=200100 of ranks=233373rank=200200 of ranks=233373rank=200300 of ranks=233373rank=200400 of ranks=233373rank=200500 of ranks=233373rank=200600 of ranks=233373rank=200700 of ranks=233373rank=200800 of ranks=233373rank=200900 of ranks=233373rank=201000 of ranks=233373rank=201100 of ranks=233373rank=201200 of ranks=233373rank=201300 of ranks=233373rank=201400 of ranks=233373rank=201500 of ranks=233373rank=201600 of ranks=233373rank=201700 of ranks=233373rank=201800 of ranks=233373rank=201900 of ranks=233373rank=202000 of ranks=233373rank=202100 of ranks=233373rank=202200 of ranks=233373rank=202300 of ranks=233373rank=202400 of ranks=233373rank=202500 of ranks=233373rank=202600 of ranks=233373rank=202700 of ranks=233373rank=202800 of ranks=233373rank=202900 of ranks=233373rank=203000 of ranks=233373rank=203100 of ranks=233373rank=203200 of ranks=233373rank=203300 of ranks=233373rank=203400 of ranks=233373rank=203500 of ranks=233373rank=203600 of ranks=233373rank=203700 of ranks=233373rank=203800 of ranks=233373rank=203900 of ranks=233373rank=204000 of ranks=233373rank=204100 of ranks=233373rank=204200 of ranks=233373rank=204300 of ranks=233373rank=204400 of ranks=233373rank=204500 of ranks=233373rank=204600 of ranks=233373rank=204700 of ranks=233373rank=204800 of ranks=233373rank=204900 of ranks=233373rank=205000 of ranks=233373rank=205100 of ranks=233373rank=205200 of ranks=233373rank=205300 of ranks=233373rank=205400 of ranks=233373rank=205500 of ranks=233373rank=205600 of ranks=233373rank=205700 of ranks=233373rank=205800 of ranks=233373rank=205900 of ranks=233373rank=206000 of ranks=233373rank=206100 of ranks=233373rank=206200 of ranks=233373rank=206300 of ranks=233373rank=206400 of ranks=233373rank=206500 of ranks=233373rank=206600 of ranks=233373rank=206700 of ranks=233373rank=206800 of ranks=233373rank=206900 of ranks=233373rank=207000 of ranks=233373rank=207100 of ranks=233373rank=207200 of ranks=233373rank=207300 of ranks=233373rank=207400 of ranks=233373rank=207500 of ranks=233373rank=207600 of ranks=233373rank=207700 of ranks=233373rank=207800 of ranks=233373rank=207900 of ranks=233373rank=208000 of ranks=233373rank=208100 of ranks=233373rank=208200 of ranks=233373rank=208300 of ranks=233373rank=208400 of ranks=233373rank=208500 of ranks=233373rank=208600 of ranks=233373rank=208700 of ranks=233373rank=208800 of ranks=233373rank=208900 of ranks=233373rank=209000 of ranks=233373rank=209100 of ranks=233373rank=209200 of ranks=233373rank=209300 of ranks=233373rank=209400 of ranks=233373rank=209500 of ranks=233373rank=209600 of ranks=233373rank=209700 of ranks=233373rank=209800 of ranks=233373rank=209900 of ranks=233373rank=210000 of ranks=233373rank=210100 of ranks=233373rank=210200 of ranks=233373rank=210300 of ranks=233373rank=210400 of ranks=233373rank=210500 of ranks=233373rank=210600 of ranks=233373rank=210700 of ranks=233373rank=210800 of ranks=233373rank=210900 of ranks=233373rank=211000 of ranks=233373rank=211100 of ranks=233373rank=211200 of ranks=233373rank=211300 of ranks=233373rank=211400 of ranks=233373rank=211500 of ranks=233373rank=211600 of ranks=233373rank=211700 of ranks=233373rank=211800 of ranks=233373rank=211900 of ranks=233373rank=212000 of ranks=233373rank=212100 of ranks=233373rank=212200 of ranks=233373rank=212300 of ranks=233373rank=212400 of ranks=233373rank=212500 of ranks=233373rank=212600 of ranks=233373rank=212700 of ranks=233373rank=212800 of ranks=233373rank=212900 of ranks=233373rank=213000 of ranks=233373rank=213100 of ranks=233373rank=213200 of ranks=233373rank=213300 of ranks=233373rank=213400 of ranks=233373rank=213500 of ranks=233373rank=213600 of ranks=233373rank=213700 of ranks=233373rank=213800 of ranks=233373rank=213900 of ranks=233373rank=214000 of ranks=233373rank=214100 of ranks=233373rank=214200 of ranks=233373rank=214300 of ranks=233373rank=214400 of ranks=233373rank=214500 of ranks=233373rank=214600 of ranks=233373rank=214700 of ranks=233373rank=214800 of ranks=233373rank=214900 of ranks=233373rank=215000 of ranks=233373rank=215100 of ranks=233373rank=215200 of ranks=233373rank=215300 of ranks=233373rank=215400 of ranks=233373rank=215500 of ranks=233373rank=215600 of ranks=233373rank=215700 of ranks=233373rank=215800 of ranks=233373rank=215900 of ranks=233373rank=216000 of ranks=233373rank=216100 of ranks=233373rank=216200 of ranks=233373rank=216300 of ranks=233373rank=216400 of ranks=233373rank=216500 of ranks=233373rank=216600 of ranks=233373rank=216700 of ranks=233373rank=216800 of ranks=233373rank=216900 of ranks=233373rank=217000 of ranks=233373rank=217100 of ranks=233373rank=217200 of ranks=233373rank=217300 of ranks=233373rank=217400 of ranks=233373rank=217500 of ranks=233373rank=217600 of ranks=233373rank=217700 of ranks=233373rank=217800 of ranks=233373rank=217900 of ranks=233373rank=218000 of ranks=233373rank=218100 of ranks=233373rank=218200 of ranks=233373rank=218300 of ranks=233373rank=218400 of ranks=233373rank=218500 of ranks=233373rank=218600 of ranks=233373rank=218700 of ranks=233373rank=218800 of ranks=233373rank=218900 of ranks=233373rank=219000 of ranks=233373rank=219100 of ranks=233373rank=219200 of ranks=233373rank=219300 of ranks=233373rank=219400 of ranks=233373rank=219500 of ranks=233373rank=219600 of ranks=233373rank=219700 of ranks=233373rank=219800 of ranks=233373rank=219900 of ranks=233373rank=220000 of ranks=233373rank=220100 of ranks=233373rank=220200 of ranks=233373rank=220300 of ranks=233373rank=220400 of ranks=233373rank=220500 of ranks=233373rank=220600 of ranks=233373rank=220700 of ranks=233373rank=220800 of ranks=233373rank=220900 of ranks=233373rank=221000 of ranks=233373rank=221100 of ranks=233373rank=221200 of ranks=233373rank=221300 of ranks=233373rank=221400 of ranks=233373rank=221500 of ranks=233373rank=221600 of ranks=233373rank=221700 of ranks=233373rank=221800 of ranks=233373rank=221900 of ranks=233373rank=222000 of ranks=233373rank=222100 of ranks=233373rank=222200 of ranks=233373rank=222300 of ranks=233373rank=222400 of ranks=233373rank=222500 of ranks=233373rank=222600 of ranks=233373rank=222700 of ranks=233373rank=222800 of ranks=233373rank=222900 of ranks=233373rank=223000 of ranks=233373rank=223100 of ranks=233373rank=223200 of ranks=233373rank=223300 of ranks=233373rank=223400 of ranks=233373rank=223500 of ranks=233373rank=223600 of ranks=233373rank=223700 of ranks=233373rank=223800 of ranks=233373rank=223900 of ranks=233373rank=224000 of ranks=233373rank=224100 of ranks=233373rank=224200 of ranks=233373rank=224300 of ranks=233373rank=224400 of ranks=233373rank=224500 of ranks=233373rank=224600 of ranks=233373rank=224700 of ranks=233373rank=224800 of ranks=233373rank=224900 of ranks=233373rank=225000 of ranks=233373rank=225100 of ranks=233373rank=225200 of ranks=233373rank=225300 of ranks=233373rank=225400 of ranks=233373rank=225500 of ranks=233373rank=225600 of ranks=233373rank=225700 of ranks=233373rank=225800 of ranks=233373rank=225900 of ranks=233373rank=226000 of ranks=233373rank=226100 of ranks=233373rank=226200 of ranks=233373rank=226300 of ranks=233373rank=226400 of ranks=233373rank=226500 of ranks=233373rank=226600 of ranks=233373rank=226700 of ranks=233373rank=226800 of ranks=233373rank=226900 of ranks=233373rank=227000 of ranks=233373rank=227100 of ranks=233373rank=227200 of ranks=233373rank=227300 of ranks=233373rank=227400 of ranks=233373rank=227500 of ranks=233373rank=227600 of ranks=233373rank=227700 of ranks=233373rank=227800 of ranks=233373rank=227900 of ranks=233373rank=228000 of ranks=233373rank=228100 of ranks=233373rank=228200 of ranks=233373rank=228300 of ranks=233373rank=228400 of ranks=233373rank=228500 of ranks=233373rank=228600 of ranks=233373rank=228700 of ranks=233373rank=228800 of ranks=233373rank=228900 of ranks=233373rank=229000 of ranks=233373rank=229100 of ranks=233373rank=229200 of ranks=233373rank=229300 of ranks=233373rank=229400 of ranks=233373rank=229500 of ranks=233373rank=229600 of ranks=233373rank=229700 of ranks=233373rank=229800 of ranks=233373rank=229900 of ranks=233373rank=230000 of ranks=233373rank=230100 of ranks=233373rank=230200 of ranks=233373rank=230300 of ranks=233373rank=230400 of ranks=233373rank=230500 of ranks=233373rank=230600 of ranks=233373rank=230700 of ranks=233373rank=230800 of ranks=233373rank=230900 of ranks=233373rank=231000 of ranks=233373rank=231100 of ranks=233373rank=231200 of ranks=233373rank=231300 of ranks=233373rank=231400 of ranks=233373rank=231500 of ranks=233373rank=231600 of ranks=233373rank=231700 of ranks=233373rank=231800 of ranks=233373rank=231900 of ranks=233373rank=232000 of ranks=233373rank=232100 of ranks=233373rank=232200 of ranks=233373rank=232300 of ranks=233373rank=232400 of ranks=233373rank=232500 of ranks=233373rank=232600 of ranks=233373rank=232700 of ranks=233373rank=232800 of ranks=233373rank=232900 of ranks=233373rank=233000 of ranks=233373rank=233100 of ranks=233373rank=233200 of ranks=233373rank=233300 of ranks=233373

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              56.5245    408  25371     90    498           43.9246
   1 car                    92.4102  48691  57031   1625  50316           69.2038
   2 truck                  69.6951   1644  38210    181   1825           45.9111
   3 bus                    25.5987    291  14031     75    366           67.1273
   4 pedestrian             68.7804   3617  44079    642   4259           44.2817

for conf_thresh=0.25, precision=0.81, recall=0.82, F1 score=0.82
for conf_thresh=0.25, TP=46855, FP=10696, FN=10409, average IoU=66.34%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=62.60%
Total detection time: 214 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
1813: loss=6.308, avg loss=6.425, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=816.0 milliseconds, train=2.1 seconds, 116032 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1814: loss=5.652, avg loss=6.348, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=2.1 seconds, 116096 images, time remaining=6 hours
1815: loss=5.793, avg loss=6.293, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 116160 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1816: loss=6.598, avg loss=6.323, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.6 seconds, train=2.1 seconds, 116224 images, time remaining=6 hours
1817: loss=6.575, avg loss=6.348, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 116288 images, time remaining=6 hours
1818: loss=6.564, avg loss=6.370, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 116352 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1819: loss=5.823, avg loss=6.315, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.0 seconds, train=2.1 seconds, 116416 images, time remaining=6 hours
1820: loss=5.341, avg loss=6.218, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=861.3 milliseconds, train=2.1 seconds, 116480 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1821: loss=6.543, avg loss=6.250, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 116544 images, time remaining=6 hours
1822: loss=6.561, avg loss=6.281, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 116608 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1823: loss=5.120, avg loss=6.165, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=1.9 seconds, 116672 images, time remaining=6 hours
1824: loss=5.715, avg loss=6.120, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=2.0 seconds, 116736 images, time remaining=6 hours
1825: loss=6.706, avg loss=6.179, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=911.4 milliseconds, train=2.0 seconds, 116800 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1826: loss=6.065, avg loss=6.167, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 116864 images, time remaining=6 hours
1827: loss=5.956, avg loss=6.146, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 116928 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1828: loss=5.635, avg loss=6.095, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=2.0 seconds, 116992 images, time remaining=6 hours
1829: loss=6.065, avg loss=6.092, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 117056 images, time remaining=6 hours
1830: loss=5.498, avg loss=6.033, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=754.7 milliseconds, train=2.0 seconds, 117120 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1831: loss=5.801, avg loss=6.009, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=3.2 seconds, 117184 images, time remaining=6 hours
1832: loss=7.078, avg loss=6.116, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=826.9 milliseconds, train=3.2 seconds, 117248 images, time remaining=6 hours
1833: loss=5.392, avg loss=6.044, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 117312 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1834: loss=5.293, avg loss=5.969, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.3 seconds, train=3.2 seconds, 117376 images, time remaining=6 hours
1835: loss=6.305, avg loss=6.002, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=3.2 seconds, 117440 images, time remaining=6 hours
1836: loss=6.144, avg loss=6.017, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.6 seconds, train=3.2 seconds, 117504 images, time remaining=6 hours
1837: loss=4.904, avg loss=5.905, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 117568 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1838: loss=5.238, avg loss=5.839, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=3.2 seconds, 117632 images, time remaining=6 hours
1839: loss=5.737, avg loss=5.829, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 117696 images, time remaining=6 hours
1840: loss=4.907, avg loss=5.736, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=543.1 milliseconds, train=3.2 seconds, 117760 images, time remaining=6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1464ada00000
1841: loss=7.017, avg loss=5.864, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=997.3 milliseconds, train=1.2 seconds, 117824 images, time remaining=6 hours
1842: loss=5.529, avg loss=5.831, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=754.2 milliseconds, train=1.2 seconds, 117888 images, time remaining=6 hours
1843: loss=6.584, avg loss=5.906, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 117952 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1844: loss=6.056, avg loss=5.921, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 118016 images, time remaining=6 hours
1845: loss=5.701, avg loss=5.899, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 118080 images, time remaining=6 hours
1846: loss=5.693, avg loss=5.879, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=955.8 milliseconds, train=1.2 seconds, 118144 images, time remaining=6 hours
1847: loss=6.022, avg loss=5.893, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 118208 images, time remaining=6 hours
1848: loss=6.189, avg loss=5.922, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=724.2 milliseconds, train=1.2 seconds, 118272 images, time remaining=6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1849: loss=5.713, avg loss=5.901, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 118336 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1850: loss=6.573, avg loss=5.969, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 118400 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1464ada00000
1851: loss=6.221, avg loss=5.994, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=812.2 milliseconds, train=1.2 seconds, 118464 images, time remaining=5.9 hours
1852: loss=6.296, avg loss=6.024, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=990.6 milliseconds, train=1.2 seconds, 118528 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1853: loss=6.552, avg loss=6.077, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.7 seconds, train=1.2 seconds, 118592 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1854: loss=5.433, avg loss=6.012, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 118656 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1855: loss=5.932, avg loss=6.004, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.3 seconds, train=1.2 seconds, 118720 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1856: loss=5.785, avg loss=5.983, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 118784 images, time remaining=5.9 hours
1857: loss=5.211, avg loss=5.905, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=702.0 milliseconds, train=1.2 seconds, 118848 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1858: loss=6.409, avg loss=5.956, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.9 seconds, train=1.2 seconds, 118912 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1859: loss=4.491, avg loss=5.809, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 118976 images, time remaining=5.9 hours
1860: loss=5.964, avg loss=5.825, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=792.0 milliseconds, train=1.2 seconds, 119040 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463ada00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1861: loss=5.755, avg loss=5.818, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.8 seconds, train=1.2 seconds, 119104 images, time remaining=5.9 hours
1862: loss=4.938, avg loss=5.730, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=610.9 milliseconds, train=1.2 seconds, 119168 images, time remaining=5.9 hours
1863: loss=5.340, avg loss=5.691, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=898.9 milliseconds, train=1.2 seconds, 119232 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1864: loss=5.629, avg loss=5.685, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=1.2 seconds, 119296 images, time remaining=5.9 hours
1865: loss=5.714, avg loss=5.688, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=870.9 milliseconds, train=1.2 seconds, 119360 images, time remaining=5.9 hours
1866: loss=6.149, avg loss=5.734, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=852.3 milliseconds, train=1.2 seconds, 119424 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1867: loss=5.147, avg loss=5.675, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.3 seconds, train=1.2 seconds, 119488 images, time remaining=5.9 hours
1868: loss=5.186, avg loss=5.626, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 119552 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1869: loss=5.360, avg loss=5.600, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=1.2 seconds, 119616 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1870: loss=6.059, avg loss=5.646, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.2 seconds, train=1.2 seconds, 119680 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145fdf800000
1871: loss=4.885, avg loss=5.569, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=883.8 milliseconds, train=1.4 seconds, 119744 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1872: loss=4.849, avg loss=5.497, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 119808 images, time remaining=5.9 hours
1873: loss=5.361, avg loss=5.484, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=671.6 milliseconds, train=1.4 seconds, 119872 images, time remaining=5.9 hours
1874: loss=5.061, avg loss=5.441, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=645.9 milliseconds, train=1.4 seconds, 119936 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1875: loss=4.889, avg loss=5.386, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 120000 images, time remaining=5.9 hours
1876: loss=5.411, avg loss=5.389, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=754.0 milliseconds, train=1.4 seconds, 120064 images, time remaining=5.9 hours
1877: loss=4.667, avg loss=5.316, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=535.3 milliseconds, train=1.4 seconds, 120128 images, time remaining=5.9 hours
1878: loss=5.651, avg loss=5.350, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=988.9 milliseconds, train=1.4 seconds, 120192 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1879: loss=5.986, avg loss=5.414, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 120256 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1880: loss=5.114, avg loss=5.384, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 120320 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1881: loss=7.564, avg loss=5.602, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=4.5 seconds, 120384 images, time remaining=5.9 hours
1882: loss=7.930, avg loss=5.834, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=678.5 milliseconds, train=4.5 seconds, 120448 images, time remaining=5.9 hours
1883: loss=7.317, avg loss=5.983, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 120512 images, time remaining=5.9 hours
1884: loss=6.731, avg loss=6.058, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.7 seconds, train=4.5 seconds, 120576 images, time remaining=5.9 hours
1885: loss=8.681, avg loss=6.320, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=4.5 seconds, 120640 images, time remaining=5.9 hours
1886: loss=7.464, avg loss=6.434, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=4.5 seconds, 120704 images, time remaining=5.9 hours
1887: loss=7.988, avg loss=6.590, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=872.6 milliseconds, train=4.5 seconds, 120768 images, time remaining=5.9 hours
1888: loss=6.120, avg loss=6.543, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 120832 images, time remaining=5.9 hours
1889: loss=7.449, avg loss=6.633, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 120896 images, time remaining=5.9 hours
1890: loss=7.679, avg loss=6.738, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=4.5 seconds, 120960 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1891: loss=6.649, avg loss=6.729, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 121024 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1892: loss=7.354, avg loss=6.791, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.9 seconds, train=3.3 seconds, 121088 images, time remaining=5.9 hours
1893: loss=5.888, avg loss=6.701, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 121152 images, time remaining=5.9 hours
1894: loss=6.988, avg loss=6.730, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=711.5 milliseconds, train=3.3 seconds, 121216 images, time remaining=5.9 hours
1895: loss=6.705, avg loss=6.727, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=989.8 milliseconds, train=3.3 seconds, 121280 images, time remaining=5.9 hours
1896: loss=7.029, avg loss=6.757, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 121344 images, time remaining=5.9 hours
1897: loss=6.558, avg loss=6.737, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=516.1 milliseconds, train=3.3 seconds, 121408 images, time remaining=5.9 hours
1898: loss=7.141, avg loss=6.778, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=969.4 milliseconds, train=3.3 seconds, 121472 images, time remaining=5.9 hours
1899: loss=6.944, avg loss=6.794, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=727.9 milliseconds, train=3.3 seconds, 121536 images, time remaining=5.9 hours
1900: loss=7.207, avg loss=6.836, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 121600 images, time remaining=5.9 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1457ac000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1901: loss=6.233, avg loss=6.775, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 121664 images, time remaining=5.9 hours
1902: loss=6.079, avg loss=6.706, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=977.5 milliseconds, train=1.6 seconds, 121728 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1903: loss=5.336, avg loss=6.569, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.2 seconds, train=1.6 seconds, 121792 images, time remaining=5.9 hours
1904: loss=6.057, avg loss=6.518, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 121856 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1905: loss=5.649, avg loss=6.431, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.7 seconds, train=1.6 seconds, 121920 images, time remaining=5.9 hours
1906: loss=5.445, avg loss=6.332, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 121984 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1907: loss=5.573, avg loss=6.256, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.7 seconds, train=1.6 seconds, 122048 images, time remaining=5.9 hours
1908: loss=5.747, avg loss=6.205, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.7 seconds, 122112 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1909: loss=5.305, avg loss=6.115, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=1.6 seconds, 122176 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1910: loss=6.404, avg loss=6.144, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=1.7 seconds, 122240 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1911: loss=6.409, avg loss=6.171, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=4.4 seconds, 122304 images, time remaining=5.9 hours
1912: loss=9.144, avg loss=6.468, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=4.4 seconds, 122368 images, time remaining=5.9 hours
1913: loss=6.165, avg loss=6.438, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.6 seconds, train=4.4 seconds, 122432 images, time remaining=5.9 hours
1914: loss=6.747, avg loss=6.469, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 122496 images, time remaining=5.9 hours
1915: loss=5.391, avg loss=6.361, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=4.4 seconds, 122560 images, time remaining=5.9 hours
1916: loss=5.540, avg loss=6.279, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=4.4 seconds, 122624 images, time remaining=5.9 hours
1917: loss=6.289, avg loss=6.280, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.3 seconds, train=4.4 seconds, 122688 images, time remaining=5.9 hours
1918: loss=6.815, avg loss=6.333, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=4.4 seconds, 122752 images, time remaining=5.9 hours
1919: loss=7.209, avg loss=6.421, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.2 seconds, train=4.4 seconds, 122816 images, time remaining=5.9 hours
1920: loss=7.576, avg loss=6.536, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=905.0 milliseconds, train=4.5 seconds, 122880 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a8c000000
1921: loss=6.174, avg loss=6.500, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 122944 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1922: loss=6.058, avg loss=6.456, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 123008 images, time remaining=5.9 hours
1923: loss=6.146, avg loss=6.425, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 123072 images, time remaining=5.9 hours
1924: loss=5.624, avg loss=6.345, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 123136 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1925: loss=5.796, avg loss=6.290, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 123200 images, time remaining=5.9 hours
1926: loss=6.185, avg loss=6.279, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=960.3 milliseconds, train=2.1 seconds, 123264 images, time remaining=5.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1927: loss=5.815, avg loss=6.233, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=5.0 seconds, train=2.1 seconds, 123328 images, time remaining=5.9 hours
1928: loss=6.371, avg loss=6.247, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 123392 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1929: loss=5.952, avg loss=6.217, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 123456 images, time remaining=5.8 hours
1930: loss=6.379, avg loss=6.233, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 123520 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x14477a000000
1931: loss=7.116, avg loss=6.322, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=4.5 seconds, 123584 images, time remaining=5.8 hours
1932: loss=7.484, avg loss=6.438, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=4.5 seconds, 123648 images, time remaining=5.8 hours
1933: loss=6.499, avg loss=6.444, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.1 seconds, train=4.5 seconds, 123712 images, time remaining=5.8 hours
1934: loss=6.708, avg loss=6.470, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 123776 images, time remaining=5.8 hours
1935: loss=5.895, avg loss=6.413, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=4.5 seconds, 123840 images, time remaining=5.8 hours
1936: loss=7.079, avg loss=6.480, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=750.6 milliseconds, train=4.5 seconds, 123904 images, time remaining=5.8 hours
1937: loss=5.728, avg loss=6.404, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=4.5 seconds, 123968 images, time remaining=5.8 hours
1938: loss=6.019, avg loss=6.366, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.3 seconds, train=4.5 seconds, 124032 images, time remaining=5.8 hours
1939: loss=7.199, avg loss=6.449, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=4.5 seconds, 124096 images, time remaining=5.8 hours
1940: loss=5.535, avg loss=6.358, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.6 seconds, train=4.5 seconds, 124160 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144648000000
1941: loss=6.844, avg loss=6.406, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.0 seconds, train=5.1 seconds, 124224 images, time remaining=5.9 hours
1942: loss=7.402, avg loss=6.506, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 124288 images, time remaining=5.9 hours
1943: loss=6.931, avg loss=6.548, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=5.1 seconds, 124352 images, time remaining=5.9 hours
1944: loss=6.228, avg loss=6.516, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 124416 images, time remaining=5.9 hours
1945: loss=6.307, avg loss=6.495, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 124480 images, time remaining=5.9 hours
1946: loss=7.115, avg loss=6.557, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 124544 images, time remaining=5.9 hours
1947: loss=7.046, avg loss=6.606, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=5.1 seconds, 124608 images, time remaining=5.9 hours
1948: loss=7.439, avg loss=6.689, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 124672 images, time remaining=5.9 hours
1949: loss=7.363, avg loss=6.757, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=5.1 seconds, 124736 images, time remaining=5.9 hours
1950: loss=7.743, avg loss=6.855, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=856.2 milliseconds, train=5.1 seconds, 124800 images, time remaining=5.9 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144648000000
1951: loss=6.730, avg loss=6.843, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 124864 images, time remaining=5.9 hours
1952: loss=5.233, avg loss=6.682, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=982.5 milliseconds, train=3.2 seconds, 124928 images, time remaining=5.9 hours
1953: loss=6.348, avg loss=6.649, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=848.9 milliseconds, train=3.2 seconds, 124992 images, time remaining=5.9 hours
1954: loss=5.638, avg loss=6.547, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 125056 images, time remaining=5.9 hours
1955: loss=4.975, avg loss=6.390, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=896.6 milliseconds, train=3.2 seconds, 125120 images, time remaining=5.8 hours
1956: loss=4.978, avg loss=6.249, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=508.1 milliseconds, train=3.2 seconds, 125184 images, time remaining=5.8 hours
1957: loss=6.429, avg loss=6.267, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=3.2 seconds, 125248 images, time remaining=5.8 hours
1958: loss=5.978, avg loss=6.238, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=990.9 milliseconds, train=3.2 seconds, 125312 images, time remaining=5.8 hours
1959: loss=5.221, avg loss=6.136, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=3.2 seconds, 125376 images, time remaining=5.8 hours
1960: loss=5.266, avg loss=6.049, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 125440 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144648000000
1961: loss=7.081, avg loss=6.153, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=4.6 seconds, 125504 images, time remaining=5.8 hours
1962: loss=6.571, avg loss=6.194, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=724.9 milliseconds, train=4.6 seconds, 125568 images, time remaining=5.8 hours
1963: loss=6.776, avg loss=6.253, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 125632 images, time remaining=5.8 hours
1964: loss=7.641, avg loss=6.391, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=576.4 milliseconds, train=4.6 seconds, 125696 images, time remaining=5.8 hours
1965: loss=6.304, avg loss=6.383, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=844.6 milliseconds, train=4.6 seconds, 125760 images, time remaining=5.8 hours
1966: loss=6.160, avg loss=6.360, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 125824 images, time remaining=5.8 hours
1967: loss=5.475, avg loss=6.272, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=640.5 milliseconds, train=4.6 seconds, 125888 images, time remaining=5.8 hours
1968: loss=7.786, avg loss=6.423, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 125952 images, time remaining=5.8 hours
1969: loss=7.166, avg loss=6.498, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=732.5 milliseconds, train=4.6 seconds, 126016 images, time remaining=5.8 hours
1970: loss=5.606, avg loss=6.408, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=750.2 milliseconds, train=4.6 seconds, 126080 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x14495a000000
1971: loss=6.327, avg loss=6.400, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=735.4 milliseconds, train=1.8 seconds, 126144 images, time remaining=5.8 hours
1972: loss=6.337, avg loss=6.394, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=725.2 milliseconds, train=1.8 seconds, 126208 images, time remaining=5.8 hours
1973: loss=6.147, avg loss=6.369, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 126272 images, time remaining=5.8 hours
1974: loss=6.811, avg loss=6.413, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=651.1 milliseconds, train=1.8 seconds, 126336 images, time remaining=5.8 hours
1975: loss=6.567, avg loss=6.429, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.8 seconds, 126400 images, time remaining=5.8 hours
1976: loss=5.247, avg loss=6.311, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=542.6 milliseconds, train=1.8 seconds, 126464 images, time remaining=5.8 hours
1977: loss=5.491, avg loss=6.229, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=551.3 milliseconds, train=1.8 seconds, 126528 images, time remaining=5.8 hours
1978: loss=6.524, avg loss=6.258, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=563.7 milliseconds, train=1.8 seconds, 126592 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1979: loss=5.558, avg loss=6.188, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=1.8 seconds, 126656 images, time remaining=5.8 hours
1980: loss=6.598, avg loss=6.229, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.8 seconds, 126720 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463e0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1981: loss=6.393, avg loss=6.246, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 126784 images, time remaining=5.8 hours
1982: loss=5.486, avg loss=6.170, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=865.6 milliseconds, train=1.5 seconds, 126848 images, time remaining=5.8 hours
1983: loss=6.221, avg loss=6.175, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=499.1 milliseconds, train=1.5 seconds, 126912 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1984: loss=6.195, avg loss=6.177, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.3 seconds, train=1.5 seconds, 126976 images, time remaining=5.8 hours
1985: loss=6.314, avg loss=6.190, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=438.0 milliseconds, train=1.5 seconds, 127040 images, time remaining=5.8 hours
1986: loss=6.262, avg loss=6.198, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 127104 images, time remaining=5.8 hours
1987: loss=4.638, avg loss=6.042, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=442.9 milliseconds, train=1.5 seconds, 127168 images, time remaining=5.8 hours
1988: loss=5.572, avg loss=5.995, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=873.9 milliseconds, train=1.5 seconds, 127232 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1989: loss=6.569, avg loss=6.052, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 127296 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1990: loss=5.678, avg loss=6.015, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 127360 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463e6000000
1991: loss=5.670, avg loss=5.980, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=525.0 milliseconds, train=1.2 seconds, 127424 images, time remaining=5.8 hours
1992: loss=5.675, avg loss=5.950, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=534.7 milliseconds, train=1.2 seconds, 127488 images, time remaining=5.8 hours
1993: loss=6.178, avg loss=5.973, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=702.8 milliseconds, train=1.2 seconds, 127552 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
1994: loss=5.867, avg loss=5.962, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 127616 images, time remaining=5.8 hours
1995: loss=5.885, avg loss=5.954, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=742.1 milliseconds, train=1.2 seconds, 127680 images, time remaining=5.8 hours
1996: loss=6.352, avg loss=5.994, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=686.7 milliseconds, train=1.2 seconds, 127744 images, time remaining=5.8 hours
1997: loss=5.379, avg loss=5.933, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=706.7 milliseconds, train=1.2 seconds, 127808 images, time remaining=5.8 hours
1998: loss=5.872, avg loss=5.926, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=667.5 milliseconds, train=1.2 seconds, 127872 images, time remaining=5.8 hours
1999: loss=5.056, avg loss=5.839, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=524.6 milliseconds, train=1.2 seconds, 127936 images, time remaining=5.8 hours
2000: loss=4.439, avg loss=5.699, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=854.8 milliseconds, train=1.2 seconds, 128000 images, time remaining=5.8 hours
Saving weights to /workspace/.cache/splits/combined_2000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x1445f0000000
2001: loss=10.413, avg loss=6.171, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=834.0 milliseconds, train=5.0 seconds, 128064 images, time remaining=5.8 hours
2002: loss=8.856, avg loss=6.439, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=5.0 seconds, 128128 images, time remaining=5.8 hours
2003: loss=7.201, avg loss=6.516, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 128192 images, time remaining=5.8 hours
2004: loss=7.432, avg loss=6.607, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=704.9 milliseconds, train=5.0 seconds, 128256 images, time remaining=5.8 hours
2005: loss=8.479, avg loss=6.794, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=784.9 milliseconds, train=5.0 seconds, 128320 images, time remaining=5.8 hours
2006: loss=8.554, avg loss=6.970, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=5.0 seconds, 128384 images, time remaining=5.8 hours
2007: loss=7.085, avg loss=6.982, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=5.0 seconds, 128448 images, time remaining=5.8 hours
2008: loss=8.121, avg loss=7.096, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 128512 images, time remaining=5.8 hours
2009: loss=8.040, avg loss=7.190, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=5.0 seconds, 128576 images, time remaining=5.8 hours
2010: loss=9.519, avg loss=7.423, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=919.2 milliseconds, train=5.1 seconds, 128640 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14628e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2011: loss=8.007, avg loss=7.481, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 128704 images, time remaining=5.8 hours
2012: loss=6.409, avg loss=7.374, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=653.2 milliseconds, train=1.5 seconds, 128768 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2013: loss=7.269, avg loss=7.364, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.3 seconds, train=1.5 seconds, 128832 images, time remaining=5.8 hours
2014: loss=8.158, avg loss=7.443, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 128896 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2015: loss=6.180, avg loss=7.317, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 128960 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2016: loss=6.196, avg loss=7.205, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 129024 images, time remaining=5.8 hours
2017: loss=6.690, avg loss=7.153, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 129088 images, time remaining=5.8 hours
2018: loss=6.980, avg loss=7.136, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=878.6 milliseconds, train=1.5 seconds, 129152 images, time remaining=5.8 hours
2019: loss=5.777, avg loss=7.000, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 129216 images, time remaining=5.8 hours
2020: loss=5.944, avg loss=6.894, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=913.4 milliseconds, train=1.5 seconds, 129280 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x1445f0000000
2021: loss=13.240, avg loss=7.529, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 129344 images, time remaining=5.8 hours
2022: loss=9.568, avg loss=7.733, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.7 seconds, train=5.0 seconds, 129408 images, time remaining=5.8 hours
2023: loss=8.745, avg loss=7.834, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=5.0 seconds, 129472 images, time remaining=5.8 hours
2024: loss=7.225, avg loss=7.773, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=5.0 seconds, 129536 images, time remaining=5.8 hours
2025: loss=6.531, avg loss=7.649, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.4 seconds, train=5.0 seconds, 129600 images, time remaining=5.8 hours
2026: loss=6.760, avg loss=7.560, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=813.0 milliseconds, train=5.0 seconds, 129664 images, time remaining=5.8 hours
2027: loss=7.975, avg loss=7.602, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=5.0 seconds, 129728 images, time remaining=5.8 hours
2028: loss=9.114, avg loss=7.753, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=5.1 seconds, 129792 images, time remaining=5.8 hours
2029: loss=9.510, avg loss=7.929, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=888.0 milliseconds, train=5.1 seconds, 129856 images, time remaining=5.8 hours
2030: loss=8.292, avg loss=7.965, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=5.1 seconds, 129920 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144902000000
2031: loss=7.041, avg loss=7.873, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 129984 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2032: loss=7.001, avg loss=7.785, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.3 seconds, train=2.0 seconds, 130048 images, time remaining=5.8 hours
2033: loss=8.323, avg loss=7.839, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 130112 images, time remaining=5.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2034: loss=7.873, avg loss=7.843, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 130176 images, time remaining=5.8 hours
2035: loss=7.572, avg loss=7.816, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 130240 images, time remaining=5.8 hours
2036: loss=7.231, avg loss=7.757, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=838.5 milliseconds, train=2.0 seconds, 130304 images, time remaining=5.8 hours
2037: loss=6.909, avg loss=7.672, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=785.6 milliseconds, train=2.0 seconds, 130368 images, time remaining=5.8 hours
2038: loss=7.216, avg loss=7.627, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 130432 images, time remaining=5.8 hours
2039: loss=6.140, avg loss=7.478, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=847.1 milliseconds, train=2.0 seconds, 130496 images, time remaining=5.8 hours
2040: loss=7.251, avg loss=7.455, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=869.2 milliseconds, train=2.0 seconds, 130560 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x1445f0000000
2041: loss=9.098, avg loss=7.620, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=982.8 milliseconds, train=5.0 seconds, 130624 images, time remaining=5.8 hours
2042: loss=7.694, avg loss=7.627, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=4.9 seconds, 130688 images, time remaining=5.8 hours
2043: loss=7.552, avg loss=7.619, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=966.3 milliseconds, train=4.9 seconds, 130752 images, time remaining=5.8 hours
2044: loss=8.219, avg loss=7.679, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=866.2 milliseconds, train=5.0 seconds, 130816 images, time remaining=5.8 hours
2045: loss=7.037, avg loss=7.615, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=5.0 seconds, 130880 images, time remaining=5.8 hours
2046: loss=6.896, avg loss=7.543, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 130944 images, time remaining=5.8 hours
2047: loss=7.953, avg loss=7.584, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=4.9 seconds, 131008 images, time remaining=5.8 hours
2048: loss=7.492, avg loss=7.575, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=5.0 seconds, 131072 images, time remaining=5.8 hours
2049: loss=7.934, avg loss=7.611, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=4.9 seconds, 131136 images, time remaining=5.8 hours
2050: loss=7.608, avg loss=7.611, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.0 seconds, train=4.9 seconds, 131200 images, time remaining=5.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145faa000000
2051: loss=9.088, avg loss=7.758, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=828.8 milliseconds, train=1.2 seconds, 131264 images, time remaining=5.8 hours
2052: loss=7.536, avg loss=7.736, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=437.2 milliseconds, train=1.2 seconds, 131328 images, time remaining=5.8 hours
2053: loss=7.797, avg loss=7.742, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 131392 images, time remaining=5.7 hours
2054: loss=7.282, avg loss=7.696, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=897.0 milliseconds, train=1.2 seconds, 131456 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2055: loss=7.173, avg loss=7.644, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 131520 images, time remaining=5.7 hours
2056: loss=7.270, avg loss=7.606, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=780.3 milliseconds, train=1.2 seconds, 131584 images, time remaining=5.7 hours
2057: loss=8.193, avg loss=7.665, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=819.4 milliseconds, train=1.2 seconds, 131648 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2058: loss=6.220, avg loss=7.521, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.7 seconds, train=1.2 seconds, 131712 images, time remaining=5.7 hours
2059: loss=6.067, avg loss=7.375, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=858.2 milliseconds, train=1.2 seconds, 131776 images, time remaining=5.7 hours
2060: loss=6.703, avg loss=7.308, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=770.2 milliseconds, train=1.2 seconds, 131840 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x1445f0000000
2061: loss=10.481, avg loss=7.625, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=952.7 milliseconds, train=4.5 seconds, 131904 images, time remaining=5.7 hours
2062: loss=9.610, avg loss=7.824, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 131968 images, time remaining=5.7 hours
2063: loss=7.720, avg loss=7.813, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 132032 images, time remaining=5.7 hours
2064: loss=7.939, avg loss=7.826, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=915.1 milliseconds, train=4.5 seconds, 132096 images, time remaining=5.7 hours
2065: loss=7.675, avg loss=7.811, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=766.9 milliseconds, train=4.5 seconds, 132160 images, time remaining=5.7 hours
2066: loss=7.441, avg loss=7.774, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 132224 images, time remaining=5.7 hours
2067: loss=8.266, avg loss=7.823, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 132288 images, time remaining=5.7 hours
2068: loss=9.249, avg loss=7.966, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 132352 images, time remaining=5.7 hours
2069: loss=7.139, avg loss=7.883, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=4.5 seconds, 132416 images, time remaining=5.7 hours
2070: loss=7.190, avg loss=7.814, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=971.3 milliseconds, train=4.5 seconds, 132480 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145c64000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2071: loss=8.593, avg loss=7.892, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 132544 images, time remaining=5.7 hours
2072: loss=7.656, avg loss=7.868, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=934.2 milliseconds, train=1.2 seconds, 132608 images, time remaining=5.7 hours
2073: loss=8.149, avg loss=7.896, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=498.7 milliseconds, train=1.2 seconds, 132672 images, time remaining=5.7 hours
2074: loss=6.752, avg loss=7.782, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 132736 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2075: loss=6.650, avg loss=7.669, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 132800 images, time remaining=5.7 hours
2076: loss=7.941, avg loss=7.696, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=772.0 milliseconds, train=1.2 seconds, 132864 images, time remaining=5.7 hours
2077: loss=7.150, avg loss=7.641, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=862.9 milliseconds, train=1.2 seconds, 132928 images, time remaining=5.7 hours
2078: loss=6.587, avg loss=7.536, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=504.3 milliseconds, train=1.2 seconds, 132992 images, time remaining=5.7 hours
2079: loss=7.381, avg loss=7.520, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=754.5 milliseconds, train=1.2 seconds, 133056 images, time remaining=5.7 hours
2080: loss=7.608, avg loss=7.529, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=776.9 milliseconds, train=1.2 seconds, 133120 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x1445f0000000
2081: loss=9.392, avg loss=7.715, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 133184 images, time remaining=5.7 hours
2082: loss=6.814, avg loss=7.625, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=724.6 milliseconds, train=3.6 seconds, 133248 images, time remaining=5.7 hours
2083: loss=8.126, avg loss=7.675, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=977.5 milliseconds, train=3.6 seconds, 133312 images, time remaining=5.7 hours
2084: loss=6.316, avg loss=7.539, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=872.5 milliseconds, train=3.6 seconds, 133376 images, time remaining=5.7 hours
2085: loss=8.261, avg loss=7.612, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 133440 images, time remaining=5.7 hours
2086: loss=7.183, avg loss=7.569, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=673.3 milliseconds, train=3.6 seconds, 133504 images, time remaining=5.7 hours
2087: loss=7.041, avg loss=7.516, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=662.6 milliseconds, train=3.6 seconds, 133568 images, time remaining=5.7 hours
2088: loss=6.504, avg loss=7.415, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=3.6 seconds, 133632 images, time remaining=5.7 hours
2089: loss=7.099, avg loss=7.383, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=912.5 milliseconds, train=3.6 seconds, 133696 images, time remaining=5.7 hours
2090: loss=6.709, avg loss=7.316, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=914.0 milliseconds, train=3.6 seconds, 133760 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x144d8ee00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2091: loss=6.663, avg loss=7.250, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 133824 images, time remaining=5.7 hours
2092: loss=6.758, avg loss=7.201, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=609.7 milliseconds, train=1.5 seconds, 133888 images, time remaining=5.7 hours
2093: loss=6.917, avg loss=7.173, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=840.5 milliseconds, train=1.4 seconds, 133952 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2094: loss=6.737, avg loss=7.129, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=1.4 seconds, 134016 images, time remaining=5.7 hours
2095: loss=7.318, avg loss=7.148, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=729.5 milliseconds, train=1.5 seconds, 134080 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2096: loss=7.680, avg loss=7.201, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 134144 images, time remaining=5.7 hours
2097: loss=6.589, avg loss=7.140, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=794.0 milliseconds, train=1.4 seconds, 134208 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2098: loss=7.066, avg loss=7.133, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=1.4 seconds, 134272 images, time remaining=5.7 hours
2099: loss=6.203, avg loss=7.040, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 134336 images, time remaining=5.7 hours
2100: loss=6.369, avg loss=6.973, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=482.8 milliseconds, train=1.4 seconds, 134400 images, time remaining=5.7 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
2101: loss=6.628, avg loss=6.938, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=957.0 milliseconds, train=2.1 seconds, 134464 images, time remaining=5.7 hours
2102: loss=7.156, avg loss=6.960, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=504.3 milliseconds, train=2.1 seconds, 134528 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2103: loss=6.409, avg loss=6.905, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 134592 images, time remaining=5.7 hours
2104: loss=5.803, avg loss=6.795, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 134656 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2105: loss=6.049, avg loss=6.720, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 134720 images, time remaining=5.7 hours
2106: loss=7.317, avg loss=6.780, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 134784 images, time remaining=5.7 hours
2107: loss=6.162, avg loss=6.718, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 134848 images, time remaining=5.7 hours
2108: loss=6.516, avg loss=6.698, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=657.4 milliseconds, train=2.1 seconds, 134912 images, time remaining=5.7 hours
2109: loss=7.794, avg loss=6.807, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 134976 images, time remaining=5.7 hours
2110: loss=6.824, avg loss=6.809, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 135040 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2111: loss=8.801, avg loss=7.008, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=5.1 seconds, 135104 images, time remaining=5.7 hours
2112: loss=7.211, avg loss=7.029, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=560.6 milliseconds, train=5.1 seconds, 135168 images, time remaining=5.7 hours
2113: loss=6.358, avg loss=6.961, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 135232 images, time remaining=5.7 hours
2114: loss=7.399, avg loss=7.005, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 135296 images, time remaining=5.7 hours
2115: loss=7.972, avg loss=7.102, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 135360 images, time remaining=5.7 hours
2116: loss=6.898, avg loss=7.081, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 135424 images, time remaining=5.7 hours
2117: loss=6.482, avg loss=7.021, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 135488 images, time remaining=5.7 hours
2118: loss=6.297, avg loss=6.949, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 135552 images, time remaining=5.7 hours
2119: loss=6.720, avg loss=6.926, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 135616 images, time remaining=5.7 hours
2120: loss=6.000, avg loss=6.834, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 135680 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2121: loss=5.915, avg loss=6.742, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.8 seconds, train=4.5 seconds, 135744 images, time remaining=5.7 hours
2122: loss=7.479, avg loss=6.815, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 135808 images, time remaining=5.7 hours
2123: loss=6.911, avg loss=6.825, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=778.3 milliseconds, train=4.5 seconds, 135872 images, time remaining=5.7 hours
2124: loss=6.608, avg loss=6.803, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=4.5 seconds, 135936 images, time remaining=5.7 hours
2125: loss=7.191, avg loss=6.842, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.2 seconds, train=4.5 seconds, 136000 images, time remaining=5.7 hours
2126: loss=6.133, avg loss=6.771, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 136064 images, time remaining=5.7 hours
2127: loss=5.190, avg loss=6.613, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 136128 images, time remaining=5.7 hours
2128: loss=6.960, avg loss=6.648, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=4.5 seconds, 136192 images, time remaining=5.7 hours
2129: loss=7.161, avg loss=6.699, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=4.5 seconds, 136256 images, time remaining=5.7 hours
2130: loss=6.302, avg loss=6.659, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=881.0 milliseconds, train=4.5 seconds, 136320 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146492000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2131: loss=7.616, avg loss=6.755, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 136384 images, time remaining=5.7 hours
2132: loss=7.705, avg loss=6.850, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=710.6 milliseconds, train=1.4 seconds, 136448 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2133: loss=6.057, avg loss=6.771, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.4 seconds, train=1.4 seconds, 136512 images, time remaining=5.7 hours
2134: loss=7.115, avg loss=6.805, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=720.8 milliseconds, train=1.4 seconds, 136576 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2135: loss=6.408, avg loss=6.765, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=1.4 seconds, 136640 images, time remaining=5.7 hours
2136: loss=5.710, avg loss=6.660, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 136704 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2137: loss=6.414, avg loss=6.635, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 136768 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2138: loss=5.917, avg loss=6.563, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 136832 images, time remaining=5.7 hours
2139: loss=6.507, avg loss=6.558, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 136896 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2140: loss=6.248, avg loss=6.527, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 136960 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1457a6000000
2141: loss=4.893, avg loss=6.363, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=870.1 milliseconds, train=1.2 seconds, 137024 images, time remaining=5.7 hours
2142: loss=6.057, avg loss=6.333, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 137088 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2143: loss=6.026, avg loss=6.302, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.5 seconds, train=1.2 seconds, 137152 images, time remaining=5.7 hours
2144: loss=5.844, avg loss=6.256, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=622.3 milliseconds, train=1.2 seconds, 137216 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2145: loss=5.955, avg loss=6.226, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 137280 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2146: loss=5.671, avg loss=6.171, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=1.2 seconds, 137344 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2147: loss=5.601, avg loss=6.114, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 137408 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2148: loss=5.871, avg loss=6.089, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.9 seconds, train=1.2 seconds, 137472 images, time remaining=5.6 hours
2149: loss=5.972, avg loss=6.078, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 137536 images, time remaining=5.6 hours
2150: loss=4.953, avg loss=5.965, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=599.3 milliseconds, train=1.2 seconds, 137600 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2151: loss=12.335, avg loss=6.602, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 137664 images, time remaining=5.6 hours
2152: loss=8.671, avg loss=6.809, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=5.1 seconds, 137728 images, time remaining=5.6 hours
2153: loss=7.011, avg loss=6.829, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.8 seconds, train=5.1 seconds, 137792 images, time remaining=5.6 hours
2154: loss=7.419, avg loss=6.888, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.3 seconds, train=5.1 seconds, 137856 images, time remaining=5.6 hours
2155: loss=8.066, avg loss=7.006, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=736.9 milliseconds, train=5.1 seconds, 137920 images, time remaining=5.6 hours
2156: loss=7.544, avg loss=7.060, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=858.9 milliseconds, train=5.1 seconds, 137984 images, time remaining=5.6 hours
2157: loss=8.194, avg loss=7.173, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=679.5 milliseconds, train=5.1 seconds, 138048 images, time remaining=5.6 hours
2158: loss=8.235, avg loss=7.279, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.8 seconds, train=5.1 seconds, 138112 images, time remaining=5.6 hours
2159: loss=7.563, avg loss=7.308, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=954.7 milliseconds, train=5.1 seconds, 138176 images, time remaining=5.6 hours
2160: loss=8.499, avg loss=7.427, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 138240 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2161: loss=6.568, avg loss=7.341, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=4.7 seconds, 138304 images, time remaining=5.6 hours
2162: loss=8.804, avg loss=7.487, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=4.8 seconds, 138368 images, time remaining=5.6 hours
2163: loss=6.943, avg loss=7.433, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.1 seconds, train=4.7 seconds, 138432 images, time remaining=5.6 hours
2164: loss=6.471, avg loss=7.337, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=4.7 seconds, 138496 images, time remaining=5.6 hours
2165: loss=7.850, avg loss=7.388, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=4.7 seconds, 138560 images, time remaining=5.6 hours
2166: loss=7.347, avg loss=7.384, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=4.7 seconds, 138624 images, time remaining=5.6 hours
2167: loss=6.892, avg loss=7.335, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.8 seconds, train=4.7 seconds, 138688 images, time remaining=5.6 hours
2168: loss=7.431, avg loss=7.344, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.8 seconds, train=4.7 seconds, 138752 images, time remaining=5.6 hours
2169: loss=6.000, avg loss=7.210, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.6 seconds, train=4.7 seconds, 138816 images, time remaining=5.6 hours
2170: loss=8.925, avg loss=7.382, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=4.8 seconds, 138880 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145bfe000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2171: loss=8.225, avg loss=7.466, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.0 seconds, train=1.4 seconds, 138944 images, time remaining=5.6 hours
2172: loss=6.199, avg loss=7.339, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 139008 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2173: loss=6.850, avg loss=7.290, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.6 seconds, train=1.4 seconds, 139072 images, time remaining=5.6 hours
2174: loss=6.728, avg loss=7.234, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=869.6 milliseconds, train=1.4 seconds, 139136 images, time remaining=5.6 hours
2175: loss=6.010, avg loss=7.112, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=934.7 milliseconds, train=1.4 seconds, 139200 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2176: loss=5.368, avg loss=6.937, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=4.4 seconds, train=1.4 seconds, 139264 images, time remaining=5.6 hours
2177: loss=5.626, avg loss=6.806, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 139328 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2178: loss=5.710, avg loss=6.697, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.5 seconds, train=1.4 seconds, 139392 images, time remaining=5.6 hours
2179: loss=6.880, avg loss=6.715, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=667.8 milliseconds, train=1.4 seconds, 139456 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2180: loss=6.231, avg loss=6.666, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 139520 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1024x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2181: loss=7.038, avg loss=6.704, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=760.9 milliseconds, train=3.2 seconds, 139584 images, time remaining=5.6 hours
2182: loss=7.485, avg loss=6.782, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 139648 images, time remaining=5.6 hours
2183: loss=5.574, avg loss=6.661, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.0 seconds, train=3.2 seconds, 139712 images, time remaining=5.6 hours
2184: loss=5.369, avg loss=6.532, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=710.2 milliseconds, train=3.2 seconds, 139776 images, time remaining=5.6 hours
2185: loss=6.304, avg loss=6.509, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.8 seconds, train=3.2 seconds, 139840 images, time remaining=5.6 hours
2186: loss=6.517, avg loss=6.510, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=817.3 milliseconds, train=3.2 seconds, 139904 images, time remaining=5.6 hours
2187: loss=5.987, avg loss=6.457, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 139968 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2188: loss=6.767, avg loss=6.488, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=3.8 seconds, train=3.2 seconds, 140032 images, time remaining=5.6 hours
2189: loss=6.014, avg loss=6.441, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=816.2 milliseconds, train=3.2 seconds, 140096 images, time remaining=5.6 hours
2190: loss=5.660, avg loss=6.363, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.6 seconds, train=3.2 seconds, 140160 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2191: loss=6.421, avg loss=6.369, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 140224 images, time remaining=5.6 hours
2192: loss=7.347, avg loss=6.466, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 140288 images, time remaining=5.6 hours
2193: loss=6.499, avg loss=6.470, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=513.2 milliseconds, train=3.9 seconds, 140352 images, time remaining=5.6 hours
2194: loss=6.916, avg loss=6.514, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=652.8 milliseconds, train=3.9 seconds, 140416 images, time remaining=5.6 hours
2195: loss=5.605, avg loss=6.423, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 140480 images, time remaining=5.6 hours
2196: loss=6.531, avg loss=6.434, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 140544 images, time remaining=5.6 hours
2197: loss=7.836, avg loss=6.574, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 140608 images, time remaining=5.6 hours
2198: loss=5.889, avg loss=6.506, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=600.2 milliseconds, train=3.9 seconds, 140672 images, time remaining=5.6 hours
2199: loss=5.956, avg loss=6.451, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 140736 images, time remaining=5.6 hours
2200: loss=5.631, avg loss=6.369, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 140800 images, time remaining=5.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2201: loss=5.567, avg loss=6.289, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 140864 images, time remaining=5.6 hours
2202: loss=6.404, avg loss=6.300, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=720.2 milliseconds, train=3.3 seconds, 140928 images, time remaining=5.6 hours
2203: loss=5.856, avg loss=6.256, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 140992 images, time remaining=5.6 hours
2204: loss=5.878, avg loss=6.218, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=948.7 milliseconds, train=3.3 seconds, 141056 images, time remaining=5.6 hours
2205: loss=7.034, avg loss=6.300, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.4 seconds, train=3.3 seconds, 141120 images, time remaining=5.6 hours
2206: loss=5.321, avg loss=6.202, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=906.6 milliseconds, train=3.3 seconds, 141184 images, time remaining=5.6 hours
2207: loss=6.598, avg loss=6.241, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 141248 images, time remaining=5.6 hours
2208: loss=7.298, avg loss=6.347, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 141312 images, time remaining=5.6 hours
2209: loss=6.115, avg loss=6.324, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 141376 images, time remaining=5.6 hours
2210: loss=5.766, avg loss=6.268, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 141440 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x144faa000000
2211: loss=5.865, avg loss=6.228, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 141504 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2212: loss=6.849, avg loss=6.290, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 141568 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2213: loss=5.564, avg loss=6.217, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 141632 images, time remaining=5.6 hours
2214: loss=6.386, avg loss=6.234, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=742.6 milliseconds, train=1.2 seconds, 141696 images, time remaining=5.6 hours
2215: loss=5.706, avg loss=6.181, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=674.0 milliseconds, train=1.2 seconds, 141760 images, time remaining=5.6 hours
2216: loss=5.337, avg loss=6.097, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=685.0 milliseconds, train=1.2 seconds, 141824 images, time remaining=5.6 hours
2217: loss=6.563, avg loss=6.144, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=971.0 milliseconds, train=1.2 seconds, 141888 images, time remaining=5.6 hours
2218: loss=6.413, avg loss=6.170, last=62.60%, best=62.60%, next=2218, rate=0.00130000, load 64=999.1 milliseconds, train=1.2 seconds, 141952 images, time remaining=5.6 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=296559, unique_truth_count=57264
rank=0 of ranks=296559rank=100 of ranks=296559rank=200 of ranks=296559rank=300 of ranks=296559rank=400 of ranks=296559rank=500 of ranks=296559rank=600 of ranks=296559rank=700 of ranks=296559rank=800 of ranks=296559rank=900 of ranks=296559rank=1000 of ranks=296559rank=1100 of ranks=296559rank=1200 of ranks=296559rank=1300 of ranks=296559rank=1400 of ranks=296559rank=1500 of ranks=296559rank=1600 of ranks=296559rank=1700 of ranks=296559rank=1800 of ranks=296559rank=1900 of ranks=296559rank=2000 of ranks=296559rank=2100 of ranks=296559rank=2200 of ranks=296559rank=2300 of ranks=296559rank=2400 of ranks=296559rank=2500 of ranks=296559rank=2600 of ranks=296559rank=2700 of ranks=296559rank=2800 of ranks=296559rank=2900 of ranks=296559rank=3000 of ranks=296559rank=3100 of ranks=296559rank=3200 of ranks=296559rank=3300 of ranks=296559rank=3400 of ranks=296559rank=3500 of ranks=296559rank=3600 of ranks=296559rank=3700 of ranks=296559rank=3800 of ranks=296559rank=3900 of ranks=296559rank=4000 of ranks=296559rank=4100 of ranks=296559rank=4200 of ranks=296559rank=4300 of ranks=296559rank=4400 of ranks=296559rank=4500 of ranks=296559rank=4600 of ranks=296559rank=4700 of ranks=296559rank=4800 of ranks=296559rank=4900 of ranks=296559rank=5000 of ranks=296559rank=5100 of ranks=296559rank=5200 of ranks=296559rank=5300 of ranks=296559rank=5400 of ranks=296559rank=5500 of ranks=296559rank=5600 of ranks=296559rank=5700 of ranks=296559rank=5800 of ranks=296559rank=5900 of ranks=296559rank=6000 of ranks=296559rank=6100 of ranks=296559rank=6200 of ranks=296559rank=6300 of ranks=296559rank=6400 of ranks=296559rank=6500 of ranks=296559rank=6600 of ranks=296559rank=6700 of ranks=296559rank=6800 of ranks=296559rank=6900 of ranks=296559rank=7000 of ranks=296559rank=7100 of ranks=296559rank=7200 of ranks=296559rank=7300 of ranks=296559rank=7400 of ranks=296559rank=7500 of ranks=296559rank=7600 of ranks=296559rank=7700 of ranks=296559rank=7800 of ranks=296559rank=7900 of ranks=296559rank=8000 of ranks=296559rank=8100 of ranks=296559rank=8200 of ranks=296559rank=8300 of ranks=296559rank=8400 of ranks=296559rank=8500 of ranks=296559rank=8600 of ranks=296559rank=8700 of ranks=296559rank=8800 of ranks=296559rank=8900 of ranks=296559rank=9000 of ranks=296559rank=9100 of ranks=296559rank=9200 of ranks=296559rank=9300 of ranks=296559rank=9400 of ranks=296559rank=9500 of ranks=296559rank=9600 of ranks=296559rank=9700 of ranks=296559rank=9800 of ranks=296559rank=9900 of ranks=296559rank=10000 of ranks=296559rank=10100 of ranks=296559rank=10200 of ranks=296559rank=10300 of ranks=296559rank=10400 of ranks=296559rank=10500 of ranks=296559rank=10600 of ranks=296559rank=10700 of ranks=296559rank=10800 of ranks=296559rank=10900 of ranks=296559rank=11000 of ranks=296559rank=11100 of ranks=296559rank=11200 of ranks=296559rank=11300 of ranks=296559rank=11400 of ranks=296559rank=11500 of ranks=296559rank=11600 of ranks=296559rank=11700 of ranks=296559rank=11800 of ranks=296559rank=11900 of ranks=296559rank=12000 of ranks=296559rank=12100 of ranks=296559rank=12200 of ranks=296559rank=12300 of ranks=296559rank=12400 of ranks=296559rank=12500 of ranks=296559rank=12600 of ranks=296559rank=12700 of ranks=296559rank=12800 of ranks=296559rank=12900 of ranks=296559rank=13000 of ranks=296559rank=13100 of ranks=296559rank=13200 of ranks=296559rank=13300 of ranks=296559rank=13400 of ranks=296559rank=13500 of ranks=296559rank=13600 of ranks=296559rank=13700 of ranks=296559rank=13800 of ranks=296559rank=13900 of ranks=296559rank=14000 of ranks=296559rank=14100 of ranks=296559rank=14200 of ranks=296559rank=14300 of ranks=296559rank=14400 of ranks=296559rank=14500 of ranks=296559rank=14600 of ranks=296559rank=14700 of ranks=296559rank=14800 of ranks=296559rank=14900 of ranks=296559rank=15000 of ranks=296559rank=15100 of ranks=296559rank=15200 of ranks=296559rank=15300 of ranks=296559rank=15400 of ranks=296559rank=15500 of ranks=296559rank=15600 of ranks=296559rank=15700 of ranks=296559rank=15800 of ranks=296559rank=15900 of ranks=296559rank=16000 of ranks=296559rank=16100 of ranks=296559rank=16200 of ranks=296559rank=16300 of ranks=296559rank=16400 of ranks=296559rank=16500 of ranks=296559rank=16600 of ranks=296559rank=16700 of ranks=296559rank=16800 of ranks=296559rank=16900 of ranks=296559rank=17000 of ranks=296559rank=17100 of ranks=296559rank=17200 of ranks=296559rank=17300 of ranks=296559rank=17400 of ranks=296559rank=17500 of ranks=296559rank=17600 of ranks=296559rank=17700 of ranks=296559rank=17800 of ranks=296559rank=17900 of ranks=296559rank=18000 of ranks=296559rank=18100 of ranks=296559rank=18200 of ranks=296559rank=18300 of ranks=296559rank=18400 of ranks=296559rank=18500 of ranks=296559rank=18600 of ranks=296559rank=18700 of ranks=296559rank=18800 of ranks=296559rank=18900 of ranks=296559rank=19000 of ranks=296559rank=19100 of ranks=296559rank=19200 of ranks=296559rank=19300 of ranks=296559rank=19400 of ranks=296559rank=19500 of ranks=296559rank=19600 of ranks=296559rank=19700 of ranks=296559rank=19800 of ranks=296559rank=19900 of ranks=296559rank=20000 of ranks=296559rank=20100 of ranks=296559rank=20200 of ranks=296559rank=20300 of ranks=296559rank=20400 of ranks=296559rank=20500 of ranks=296559rank=20600 of ranks=296559rank=20700 of ranks=296559rank=20800 of ranks=296559rank=20900 of ranks=296559rank=21000 of ranks=296559rank=21100 of ranks=296559rank=21200 of ranks=296559rank=21300 of ranks=296559rank=21400 of ranks=296559rank=21500 of ranks=296559rank=21600 of ranks=296559rank=21700 of ranks=296559rank=21800 of ranks=296559rank=21900 of ranks=296559rank=22000 of ranks=296559rank=22100 of ranks=296559rank=22200 of ranks=296559rank=22300 of ranks=296559rank=22400 of ranks=296559rank=22500 of ranks=296559rank=22600 of ranks=296559rank=22700 of ranks=296559rank=22800 of ranks=296559rank=22900 of ranks=296559rank=23000 of ranks=296559rank=23100 of ranks=296559rank=23200 of ranks=296559rank=23300 of ranks=296559rank=23400 of ranks=296559rank=23500 of ranks=296559rank=23600 of ranks=296559rank=23700 of ranks=296559rank=23800 of ranks=296559rank=23900 of ranks=296559rank=24000 of ranks=296559rank=24100 of ranks=296559rank=24200 of ranks=296559rank=24300 of ranks=296559rank=24400 of ranks=296559rank=24500 of ranks=296559rank=24600 of ranks=296559rank=24700 of ranks=296559rank=24800 of ranks=296559rank=24900 of ranks=296559rank=25000 of ranks=296559rank=25100 of ranks=296559rank=25200 of ranks=296559rank=25300 of ranks=296559rank=25400 of ranks=296559rank=25500 of ranks=296559rank=25600 of ranks=296559rank=25700 of ranks=296559rank=25800 of ranks=296559rank=25900 of ranks=296559rank=26000 of ranks=296559rank=26100 of ranks=296559rank=26200 of ranks=296559rank=26300 of ranks=296559rank=26400 of ranks=296559rank=26500 of ranks=296559rank=26600 of ranks=296559rank=26700 of ranks=296559rank=26800 of ranks=296559rank=26900 of ranks=296559rank=27000 of ranks=296559rank=27100 of ranks=296559rank=27200 of ranks=296559rank=27300 of ranks=296559rank=27400 of ranks=296559rank=27500 of ranks=296559rank=27600 of ranks=296559rank=27700 of ranks=296559rank=27800 of ranks=296559rank=27900 of ranks=296559rank=28000 of ranks=296559rank=28100 of ranks=296559rank=28200 of ranks=296559rank=28300 of ranks=296559rank=28400 of ranks=296559rank=28500 of ranks=296559rank=28600 of ranks=296559rank=28700 of ranks=296559rank=28800 of ranks=296559rank=28900 of ranks=296559rank=29000 of ranks=296559rank=29100 of ranks=296559rank=29200 of ranks=296559rank=29300 of ranks=296559rank=29400 of ranks=296559rank=29500 of ranks=296559rank=29600 of ranks=296559rank=29700 of ranks=296559rank=29800 of ranks=296559rank=29900 of ranks=296559rank=30000 of ranks=296559rank=30100 of ranks=296559rank=30200 of ranks=296559rank=30300 of ranks=296559rank=30400 of ranks=296559rank=30500 of ranks=296559rank=30600 of ranks=296559rank=30700 of ranks=296559rank=30800 of ranks=296559rank=30900 of ranks=296559rank=31000 of ranks=296559rank=31100 of ranks=296559rank=31200 of ranks=296559rank=31300 of ranks=296559rank=31400 of ranks=296559rank=31500 of ranks=296559rank=31600 of ranks=296559rank=31700 of ranks=296559rank=31800 of ranks=296559rank=31900 of ranks=296559rank=32000 of ranks=296559rank=32100 of ranks=296559rank=32200 of ranks=296559rank=32300 of ranks=296559rank=32400 of ranks=296559rank=32500 of ranks=296559rank=32600 of ranks=296559rank=32700 of ranks=296559rank=32800 of ranks=296559rank=32900 of ranks=296559rank=33000 of ranks=296559rank=33100 of ranks=296559rank=33200 of ranks=296559rank=33300 of ranks=296559rank=33400 of ranks=296559rank=33500 of ranks=296559rank=33600 of ranks=296559rank=33700 of ranks=296559rank=33800 of ranks=296559rank=33900 of ranks=296559rank=34000 of ranks=296559rank=34100 of ranks=296559rank=34200 of ranks=296559rank=34300 of ranks=296559rank=34400 of ranks=296559rank=34500 of ranks=296559rank=34600 of ranks=296559rank=34700 of ranks=296559rank=34800 of ranks=296559rank=34900 of ranks=296559rank=35000 of ranks=296559rank=35100 of ranks=296559rank=35200 of ranks=296559rank=35300 of ranks=296559rank=35400 of ranks=296559rank=35500 of ranks=296559rank=35600 of ranks=296559rank=35700 of ranks=296559rank=35800 of ranks=296559rank=35900 of ranks=296559rank=36000 of ranks=296559rank=36100 of ranks=296559rank=36200 of ranks=296559rank=36300 of ranks=296559rank=36400 of ranks=296559rank=36500 of ranks=296559rank=36600 of ranks=296559rank=36700 of ranks=296559rank=36800 of ranks=296559rank=36900 of ranks=296559rank=37000 of ranks=296559rank=37100 of ranks=296559rank=37200 of ranks=296559rank=37300 of ranks=296559rank=37400 of ranks=296559rank=37500 of ranks=296559rank=37600 of ranks=296559rank=37700 of ranks=296559rank=37800 of ranks=296559rank=37900 of ranks=296559rank=38000 of ranks=296559rank=38100 of ranks=296559rank=38200 of ranks=296559rank=38300 of ranks=296559rank=38400 of ranks=296559rank=38500 of ranks=296559rank=38600 of ranks=296559rank=38700 of ranks=296559rank=38800 of ranks=296559rank=38900 of ranks=296559rank=39000 of ranks=296559rank=39100 of ranks=296559rank=39200 of ranks=296559rank=39300 of ranks=296559rank=39400 of ranks=296559rank=39500 of ranks=296559rank=39600 of ranks=296559rank=39700 of ranks=296559rank=39800 of ranks=296559rank=39900 of ranks=296559rank=40000 of ranks=296559rank=40100 of ranks=296559rank=40200 of ranks=296559rank=40300 of ranks=296559rank=40400 of ranks=296559rank=40500 of ranks=296559rank=40600 of ranks=296559rank=40700 of ranks=296559rank=40800 of ranks=296559rank=40900 of ranks=296559rank=41000 of ranks=296559rank=41100 of ranks=296559rank=41200 of ranks=296559rank=41300 of ranks=296559rank=41400 of ranks=296559rank=41500 of ranks=296559rank=41600 of ranks=296559rank=41700 of ranks=296559rank=41800 of ranks=296559rank=41900 of ranks=296559rank=42000 of ranks=296559rank=42100 of ranks=296559rank=42200 of ranks=296559rank=42300 of ranks=296559rank=42400 of ranks=296559rank=42500 of ranks=296559rank=42600 of ranks=296559rank=42700 of ranks=296559rank=42800 of ranks=296559rank=42900 of ranks=296559rank=43000 of ranks=296559rank=43100 of ranks=296559rank=43200 of ranks=296559rank=43300 of ranks=296559rank=43400 of ranks=296559rank=43500 of ranks=296559rank=43600 of ranks=296559rank=43700 of ranks=296559rank=43800 of ranks=296559rank=43900 of ranks=296559rank=44000 of ranks=296559rank=44100 of ranks=296559rank=44200 of ranks=296559rank=44300 of ranks=296559rank=44400 of ranks=296559rank=44500 of ranks=296559rank=44600 of ranks=296559rank=44700 of ranks=296559rank=44800 of ranks=296559rank=44900 of ranks=296559rank=45000 of ranks=296559rank=45100 of ranks=296559rank=45200 of ranks=296559rank=45300 of ranks=296559rank=45400 of ranks=296559rank=45500 of ranks=296559rank=45600 of ranks=296559rank=45700 of ranks=296559rank=45800 of ranks=296559rank=45900 of ranks=296559rank=46000 of ranks=296559rank=46100 of ranks=296559rank=46200 of ranks=296559rank=46300 of ranks=296559rank=46400 of ranks=296559rank=46500 of ranks=296559rank=46600 of ranks=296559rank=46700 of ranks=296559rank=46800 of ranks=296559rank=46900 of ranks=296559rank=47000 of ranks=296559rank=47100 of ranks=296559rank=47200 of ranks=296559rank=47300 of ranks=296559rank=47400 of ranks=296559rank=47500 of ranks=296559rank=47600 of ranks=296559rank=47700 of ranks=296559rank=47800 of ranks=296559rank=47900 of ranks=296559rank=48000 of ranks=296559rank=48100 of ranks=296559rank=48200 of ranks=296559rank=48300 of ranks=296559rank=48400 of ranks=296559rank=48500 of ranks=296559rank=48600 of ranks=296559rank=48700 of ranks=296559rank=48800 of ranks=296559rank=48900 of ranks=296559rank=49000 of ranks=296559rank=49100 of ranks=296559rank=49200 of ranks=296559rank=49300 of ranks=296559rank=49400 of ranks=296559rank=49500 of ranks=296559rank=49600 of ranks=296559rank=49700 of ranks=296559rank=49800 of ranks=296559rank=49900 of ranks=296559rank=50000 of ranks=296559rank=50100 of ranks=296559rank=50200 of ranks=296559rank=50300 of ranks=296559rank=50400 of ranks=296559rank=50500 of ranks=296559rank=50600 of ranks=296559rank=50700 of ranks=296559rank=50800 of ranks=296559rank=50900 of ranks=296559rank=51000 of ranks=296559rank=51100 of ranks=296559rank=51200 of ranks=296559rank=51300 of ranks=296559rank=51400 of ranks=296559rank=51500 of ranks=296559rank=51600 of ranks=296559rank=51700 of ranks=296559rank=51800 of ranks=296559rank=51900 of ranks=296559rank=52000 of ranks=296559rank=52100 of ranks=296559rank=52200 of ranks=296559rank=52300 of ranks=296559rank=52400 of ranks=296559rank=52500 of ranks=296559rank=52600 of ranks=296559rank=52700 of ranks=296559rank=52800 of ranks=296559rank=52900 of ranks=296559rank=53000 of ranks=296559rank=53100 of ranks=296559rank=53200 of ranks=296559rank=53300 of ranks=296559rank=53400 of ranks=296559rank=53500 of ranks=296559rank=53600 of ranks=296559rank=53700 of ranks=296559rank=53800 of ranks=296559rank=53900 of ranks=296559rank=54000 of ranks=296559rank=54100 of ranks=296559rank=54200 of ranks=296559rank=54300 of ranks=296559rank=54400 of ranks=296559rank=54500 of ranks=296559rank=54600 of ranks=296559rank=54700 of ranks=296559rank=54800 of ranks=296559rank=54900 of ranks=296559rank=55000 of ranks=296559rank=55100 of ranks=296559rank=55200 of ranks=296559rank=55300 of ranks=296559rank=55400 of ranks=296559rank=55500 of ranks=296559rank=55600 of ranks=296559rank=55700 of ranks=296559rank=55800 of ranks=296559rank=55900 of ranks=296559rank=56000 of ranks=296559rank=56100 of ranks=296559rank=56200 of ranks=296559rank=56300 of ranks=296559rank=56400 of ranks=296559rank=56500 of ranks=296559rank=56600 of ranks=296559rank=56700 of ranks=296559rank=56800 of ranks=296559rank=56900 of ranks=296559rank=57000 of ranks=296559rank=57100 of ranks=296559rank=57200 of ranks=296559rank=57300 of ranks=296559rank=57400 of ranks=296559rank=57500 of ranks=296559rank=57600 of ranks=296559rank=57700 of ranks=296559rank=57800 of ranks=296559rank=57900 of ranks=296559rank=58000 of ranks=296559rank=58100 of ranks=296559rank=58200 of ranks=296559rank=58300 of ranks=296559rank=58400 of ranks=296559rank=58500 of ranks=296559rank=58600 of ranks=296559rank=58700 of ranks=296559rank=58800 of ranks=296559rank=58900 of ranks=296559rank=59000 of ranks=296559rank=59100 of ranks=296559rank=59200 of ranks=296559rank=59300 of ranks=296559rank=59400 of ranks=296559rank=59500 of ranks=296559rank=59600 of ranks=296559rank=59700 of ranks=296559rank=59800 of ranks=296559rank=59900 of ranks=296559rank=60000 of ranks=296559rank=60100 of ranks=296559rank=60200 of ranks=296559rank=60300 of ranks=296559rank=60400 of ranks=296559rank=60500 of ranks=296559rank=60600 of ranks=296559rank=60700 of ranks=296559rank=60800 of ranks=296559rank=60900 of ranks=296559rank=61000 of ranks=296559rank=61100 of ranks=296559rank=61200 of ranks=296559rank=61300 of ranks=296559rank=61400 of ranks=296559rank=61500 of ranks=296559rank=61600 of ranks=296559rank=61700 of ranks=296559rank=61800 of ranks=296559rank=61900 of ranks=296559rank=62000 of ranks=296559rank=62100 of ranks=296559rank=62200 of ranks=296559rank=62300 of ranks=296559rank=62400 of ranks=296559rank=62500 of ranks=296559rank=62600 of ranks=296559rank=62700 of ranks=296559rank=62800 of ranks=296559rank=62900 of ranks=296559rank=63000 of ranks=296559rank=63100 of ranks=296559rank=63200 of ranks=296559rank=63300 of ranks=296559rank=63400 of ranks=296559rank=63500 of ranks=296559rank=63600 of ranks=296559rank=63700 of ranks=296559rank=63800 of ranks=296559rank=63900 of ranks=296559rank=64000 of ranks=296559rank=64100 of ranks=296559rank=64200 of ranks=296559rank=64300 of ranks=296559rank=64400 of ranks=296559rank=64500 of ranks=296559rank=64600 of ranks=296559rank=64700 of ranks=296559rank=64800 of ranks=296559rank=64900 of ranks=296559rank=65000 of ranks=296559rank=65100 of ranks=296559rank=65200 of ranks=296559rank=65300 of ranks=296559rank=65400 of ranks=296559rank=65500 of ranks=296559rank=65600 of ranks=296559rank=65700 of ranks=296559rank=65800 of ranks=296559rank=65900 of ranks=296559rank=66000 of ranks=296559rank=66100 of ranks=296559rank=66200 of ranks=296559rank=66300 of ranks=296559rank=66400 of ranks=296559rank=66500 of ranks=296559rank=66600 of ranks=296559rank=66700 of ranks=296559rank=66800 of ranks=296559rank=66900 of ranks=296559rank=67000 of ranks=296559rank=67100 of ranks=296559rank=67200 of ranks=296559rank=67300 of ranks=296559rank=67400 of ranks=296559rank=67500 of ranks=296559rank=67600 of ranks=296559rank=67700 of ranks=296559rank=67800 of ranks=296559rank=67900 of ranks=296559rank=68000 of ranks=296559rank=68100 of ranks=296559rank=68200 of ranks=296559rank=68300 of ranks=296559rank=68400 of ranks=296559rank=68500 of ranks=296559rank=68600 of ranks=296559rank=68700 of ranks=296559rank=68800 of ranks=296559rank=68900 of ranks=296559rank=69000 of ranks=296559rank=69100 of ranks=296559rank=69200 of ranks=296559rank=69300 of ranks=296559rank=69400 of ranks=296559rank=69500 of ranks=296559rank=69600 of ranks=296559rank=69700 of ranks=296559rank=69800 of ranks=296559rank=69900 of ranks=296559rank=70000 of ranks=296559rank=70100 of ranks=296559rank=70200 of ranks=296559rank=70300 of ranks=296559rank=70400 of ranks=296559rank=70500 of ranks=296559rank=70600 of ranks=296559rank=70700 of ranks=296559rank=70800 of ranks=296559rank=70900 of ranks=296559rank=71000 of ranks=296559rank=71100 of ranks=296559rank=71200 of ranks=296559rank=71300 of ranks=296559rank=71400 of ranks=296559rank=71500 of ranks=296559rank=71600 of ranks=296559rank=71700 of ranks=296559rank=71800 of ranks=296559rank=71900 of ranks=296559rank=72000 of ranks=296559rank=72100 of ranks=296559rank=72200 of ranks=296559rank=72300 of ranks=296559rank=72400 of ranks=296559rank=72500 of ranks=296559rank=72600 of ranks=296559rank=72700 of ranks=296559rank=72800 of ranks=296559rank=72900 of ranks=296559rank=73000 of ranks=296559rank=73100 of ranks=296559rank=73200 of ranks=296559rank=73300 of ranks=296559rank=73400 of ranks=296559rank=73500 of ranks=296559rank=73600 of ranks=296559rank=73700 of ranks=296559rank=73800 of ranks=296559rank=73900 of ranks=296559rank=74000 of ranks=296559rank=74100 of ranks=296559rank=74200 of ranks=296559rank=74300 of ranks=296559rank=74400 of ranks=296559rank=74500 of ranks=296559rank=74600 of ranks=296559rank=74700 of ranks=296559rank=74800 of ranks=296559rank=74900 of ranks=296559rank=75000 of ranks=296559rank=75100 of ranks=296559rank=75200 of ranks=296559rank=75300 of ranks=296559rank=75400 of ranks=296559rank=75500 of ranks=296559rank=75600 of ranks=296559rank=75700 of ranks=296559rank=75800 of ranks=296559rank=75900 of ranks=296559rank=76000 of ranks=296559rank=76100 of ranks=296559rank=76200 of ranks=296559rank=76300 of ranks=296559rank=76400 of ranks=296559rank=76500 of ranks=296559rank=76600 of ranks=296559rank=76700 of ranks=296559rank=76800 of ranks=296559rank=76900 of ranks=296559rank=77000 of ranks=296559rank=77100 of ranks=296559rank=77200 of ranks=296559rank=77300 of ranks=296559rank=77400 of ranks=296559rank=77500 of ranks=296559rank=77600 of ranks=296559rank=77700 of ranks=296559rank=77800 of ranks=296559rank=77900 of ranks=296559rank=78000 of ranks=296559rank=78100 of ranks=296559rank=78200 of ranks=296559rank=78300 of ranks=296559rank=78400 of ranks=296559rank=78500 of ranks=296559rank=78600 of ranks=296559rank=78700 of ranks=296559rank=78800 of ranks=296559rank=78900 of ranks=296559rank=79000 of ranks=296559rank=79100 of ranks=296559rank=79200 of ranks=296559rank=79300 of ranks=296559rank=79400 of ranks=296559rank=79500 of ranks=296559rank=79600 of ranks=296559rank=79700 of ranks=296559rank=79800 of ranks=296559rank=79900 of ranks=296559rank=80000 of ranks=296559rank=80100 of ranks=296559rank=80200 of ranks=296559rank=80300 of ranks=296559rank=80400 of ranks=296559rank=80500 of ranks=296559rank=80600 of ranks=296559rank=80700 of ranks=296559rank=80800 of ranks=296559rank=80900 of ranks=296559rank=81000 of ranks=296559rank=81100 of ranks=296559rank=81200 of ranks=296559rank=81300 of ranks=296559rank=81400 of ranks=296559rank=81500 of ranks=296559rank=81600 of ranks=296559rank=81700 of ranks=296559rank=81800 of ranks=296559rank=81900 of ranks=296559rank=82000 of ranks=296559rank=82100 of ranks=296559rank=82200 of ranks=296559rank=82300 of ranks=296559rank=82400 of ranks=296559rank=82500 of ranks=296559rank=82600 of ranks=296559rank=82700 of ranks=296559rank=82800 of ranks=296559rank=82900 of ranks=296559rank=83000 of ranks=296559rank=83100 of ranks=296559rank=83200 of ranks=296559rank=83300 of ranks=296559rank=83400 of ranks=296559rank=83500 of ranks=296559rank=83600 of ranks=296559rank=83700 of ranks=296559rank=83800 of ranks=296559rank=83900 of ranks=296559rank=84000 of ranks=296559rank=84100 of ranks=296559rank=84200 of ranks=296559rank=84300 of ranks=296559rank=84400 of ranks=296559rank=84500 of ranks=296559rank=84600 of ranks=296559rank=84700 of ranks=296559rank=84800 of ranks=296559rank=84900 of ranks=296559rank=85000 of ranks=296559rank=85100 of ranks=296559rank=85200 of ranks=296559rank=85300 of ranks=296559rank=85400 of ranks=296559rank=85500 of ranks=296559rank=85600 of ranks=296559rank=85700 of ranks=296559rank=85800 of ranks=296559rank=85900 of ranks=296559rank=86000 of ranks=296559rank=86100 of ranks=296559rank=86200 of ranks=296559rank=86300 of ranks=296559rank=86400 of ranks=296559rank=86500 of ranks=296559rank=86600 of ranks=296559rank=86700 of ranks=296559rank=86800 of ranks=296559rank=86900 of ranks=296559rank=87000 of ranks=296559rank=87100 of ranks=296559rank=87200 of ranks=296559rank=87300 of ranks=296559rank=87400 of ranks=296559rank=87500 of ranks=296559rank=87600 of ranks=296559rank=87700 of ranks=296559rank=87800 of ranks=296559rank=87900 of ranks=296559rank=88000 of ranks=296559rank=88100 of ranks=296559rank=88200 of ranks=296559rank=88300 of ranks=296559rank=88400 of ranks=296559rank=88500 of ranks=296559rank=88600 of ranks=296559rank=88700 of ranks=296559rank=88800 of ranks=296559rank=88900 of ranks=296559rank=89000 of ranks=296559rank=89100 of ranks=296559rank=89200 of ranks=296559rank=89300 of ranks=296559rank=89400 of ranks=296559rank=89500 of ranks=296559rank=89600 of ranks=296559rank=89700 of ranks=296559rank=89800 of ranks=296559rank=89900 of ranks=296559rank=90000 of ranks=296559rank=90100 of ranks=296559rank=90200 of ranks=296559rank=90300 of ranks=296559rank=90400 of ranks=296559rank=90500 of ranks=296559rank=90600 of ranks=296559rank=90700 of ranks=296559rank=90800 of ranks=296559rank=90900 of ranks=296559rank=91000 of ranks=296559rank=91100 of ranks=296559rank=91200 of ranks=296559rank=91300 of ranks=296559rank=91400 of ranks=296559rank=91500 of ranks=296559rank=91600 of ranks=296559rank=91700 of ranks=296559rank=91800 of ranks=296559rank=91900 of ranks=296559rank=92000 of ranks=296559rank=92100 of ranks=296559rank=92200 of ranks=296559rank=92300 of ranks=296559rank=92400 of ranks=296559rank=92500 of ranks=296559rank=92600 of ranks=296559rank=92700 of ranks=296559rank=92800 of ranks=296559rank=92900 of ranks=296559rank=93000 of ranks=296559rank=93100 of ranks=296559rank=93200 of ranks=296559rank=93300 of ranks=296559rank=93400 of ranks=296559rank=93500 of ranks=296559rank=93600 of ranks=296559rank=93700 of ranks=296559rank=93800 of ranks=296559rank=93900 of ranks=296559rank=94000 of ranks=296559rank=94100 of ranks=296559rank=94200 of ranks=296559rank=94300 of ranks=296559rank=94400 of ranks=296559rank=94500 of ranks=296559rank=94600 of ranks=296559rank=94700 of ranks=296559rank=94800 of ranks=296559rank=94900 of ranks=296559rank=95000 of ranks=296559rank=95100 of ranks=296559rank=95200 of ranks=296559rank=95300 of ranks=296559rank=95400 of ranks=296559rank=95500 of ranks=296559rank=95600 of ranks=296559rank=95700 of ranks=296559rank=95800 of ranks=296559rank=95900 of ranks=296559rank=96000 of ranks=296559rank=96100 of ranks=296559rank=96200 of ranks=296559rank=96300 of ranks=296559rank=96400 of ranks=296559rank=96500 of ranks=296559rank=96600 of ranks=296559rank=96700 of ranks=296559rank=96800 of ranks=296559rank=96900 of ranks=296559rank=97000 of ranks=296559rank=97100 of ranks=296559rank=97200 of ranks=296559rank=97300 of ranks=296559rank=97400 of ranks=296559rank=97500 of ranks=296559rank=97600 of ranks=296559rank=97700 of ranks=296559rank=97800 of ranks=296559rank=97900 of ranks=296559rank=98000 of ranks=296559rank=98100 of ranks=296559rank=98200 of ranks=296559rank=98300 of ranks=296559rank=98400 of ranks=296559rank=98500 of ranks=296559rank=98600 of ranks=296559rank=98700 of ranks=296559rank=98800 of ranks=296559rank=98900 of ranks=296559rank=99000 of ranks=296559rank=99100 of ranks=296559rank=99200 of ranks=296559rank=99300 of ranks=296559rank=99400 of ranks=296559rank=99500 of ranks=296559rank=99600 of ranks=296559rank=99700 of ranks=296559rank=99800 of ranks=296559rank=99900 of ranks=296559rank=100000 of ranks=296559rank=100100 of ranks=296559rank=100200 of ranks=296559rank=100300 of ranks=296559rank=100400 of ranks=296559rank=100500 of ranks=296559rank=100600 of ranks=296559rank=100700 of ranks=296559rank=100800 of ranks=296559rank=100900 of ranks=296559rank=101000 of ranks=296559rank=101100 of ranks=296559rank=101200 of ranks=296559rank=101300 of ranks=296559rank=101400 of ranks=296559rank=101500 of ranks=296559rank=101600 of ranks=296559rank=101700 of ranks=296559rank=101800 of ranks=296559rank=101900 of ranks=296559rank=102000 of ranks=296559rank=102100 of ranks=296559rank=102200 of ranks=296559rank=102300 of ranks=296559rank=102400 of ranks=296559rank=102500 of ranks=296559rank=102600 of ranks=296559rank=102700 of ranks=296559rank=102800 of ranks=296559rank=102900 of ranks=296559rank=103000 of ranks=296559rank=103100 of ranks=296559rank=103200 of ranks=296559rank=103300 of ranks=296559rank=103400 of ranks=296559rank=103500 of ranks=296559rank=103600 of ranks=296559rank=103700 of ranks=296559rank=103800 of ranks=296559rank=103900 of ranks=296559rank=104000 of ranks=296559rank=104100 of ranks=296559rank=104200 of ranks=296559rank=104300 of ranks=296559rank=104400 of ranks=296559rank=104500 of ranks=296559rank=104600 of ranks=296559rank=104700 of ranks=296559rank=104800 of ranks=296559rank=104900 of ranks=296559rank=105000 of ranks=296559rank=105100 of ranks=296559rank=105200 of ranks=296559rank=105300 of ranks=296559rank=105400 of ranks=296559rank=105500 of ranks=296559rank=105600 of ranks=296559rank=105700 of ranks=296559rank=105800 of ranks=296559rank=105900 of ranks=296559rank=106000 of ranks=296559rank=106100 of ranks=296559rank=106200 of ranks=296559rank=106300 of ranks=296559rank=106400 of ranks=296559rank=106500 of ranks=296559rank=106600 of ranks=296559rank=106700 of ranks=296559rank=106800 of ranks=296559rank=106900 of ranks=296559rank=107000 of ranks=296559rank=107100 of ranks=296559rank=107200 of ranks=296559rank=107300 of ranks=296559rank=107400 of ranks=296559rank=107500 of ranks=296559rank=107600 of ranks=296559rank=107700 of ranks=296559rank=107800 of ranks=296559rank=107900 of ranks=296559rank=108000 of ranks=296559rank=108100 of ranks=296559rank=108200 of ranks=296559rank=108300 of ranks=296559rank=108400 of ranks=296559rank=108500 of ranks=296559rank=108600 of ranks=296559rank=108700 of ranks=296559rank=108800 of ranks=296559rank=108900 of ranks=296559rank=109000 of ranks=296559rank=109100 of ranks=296559rank=109200 of ranks=296559rank=109300 of ranks=296559rank=109400 of ranks=296559rank=109500 of ranks=296559rank=109600 of ranks=296559rank=109700 of ranks=296559rank=109800 of ranks=296559rank=109900 of ranks=296559rank=110000 of ranks=296559rank=110100 of ranks=296559rank=110200 of ranks=296559rank=110300 of ranks=296559rank=110400 of ranks=296559rank=110500 of ranks=296559rank=110600 of ranks=296559rank=110700 of ranks=296559rank=110800 of ranks=296559rank=110900 of ranks=296559rank=111000 of ranks=296559rank=111100 of ranks=296559rank=111200 of ranks=296559rank=111300 of ranks=296559rank=111400 of ranks=296559rank=111500 of ranks=296559rank=111600 of ranks=296559rank=111700 of ranks=296559rank=111800 of ranks=296559rank=111900 of ranks=296559rank=112000 of ranks=296559rank=112100 of ranks=296559rank=112200 of ranks=296559rank=112300 of ranks=296559rank=112400 of ranks=296559rank=112500 of ranks=296559rank=112600 of ranks=296559rank=112700 of ranks=296559rank=112800 of ranks=296559rank=112900 of ranks=296559rank=113000 of ranks=296559rank=113100 of ranks=296559rank=113200 of ranks=296559rank=113300 of ranks=296559rank=113400 of ranks=296559rank=113500 of ranks=296559rank=113600 of ranks=296559rank=113700 of ranks=296559rank=113800 of ranks=296559rank=113900 of ranks=296559rank=114000 of ranks=296559rank=114100 of ranks=296559rank=114200 of ranks=296559rank=114300 of ranks=296559rank=114400 of ranks=296559rank=114500 of ranks=296559rank=114600 of ranks=296559rank=114700 of ranks=296559rank=114800 of ranks=296559rank=114900 of ranks=296559rank=115000 of ranks=296559rank=115100 of ranks=296559rank=115200 of ranks=296559rank=115300 of ranks=296559rank=115400 of ranks=296559rank=115500 of ranks=296559rank=115600 of ranks=296559rank=115700 of ranks=296559rank=115800 of ranks=296559rank=115900 of ranks=296559rank=116000 of ranks=296559rank=116100 of ranks=296559rank=116200 of ranks=296559rank=116300 of ranks=296559rank=116400 of ranks=296559rank=116500 of ranks=296559rank=116600 of ranks=296559rank=116700 of ranks=296559rank=116800 of ranks=296559rank=116900 of ranks=296559rank=117000 of ranks=296559rank=117100 of ranks=296559rank=117200 of ranks=296559rank=117300 of ranks=296559rank=117400 of ranks=296559rank=117500 of ranks=296559rank=117600 of ranks=296559rank=117700 of ranks=296559rank=117800 of ranks=296559rank=117900 of ranks=296559rank=118000 of ranks=296559rank=118100 of ranks=296559rank=118200 of ranks=296559rank=118300 of ranks=296559rank=118400 of ranks=296559rank=118500 of ranks=296559rank=118600 of ranks=296559rank=118700 of ranks=296559rank=118800 of ranks=296559rank=118900 of ranks=296559rank=119000 of ranks=296559rank=119100 of ranks=296559rank=119200 of ranks=296559rank=119300 of ranks=296559rank=119400 of ranks=296559rank=119500 of ranks=296559rank=119600 of ranks=296559rank=119700 of ranks=296559rank=119800 of ranks=296559rank=119900 of ranks=296559rank=120000 of ranks=296559rank=120100 of ranks=296559rank=120200 of ranks=296559rank=120300 of ranks=296559rank=120400 of ranks=296559rank=120500 of ranks=296559rank=120600 of ranks=296559rank=120700 of ranks=296559rank=120800 of ranks=296559rank=120900 of ranks=296559rank=121000 of ranks=296559rank=121100 of ranks=296559rank=121200 of ranks=296559rank=121300 of ranks=296559rank=121400 of ranks=296559rank=121500 of ranks=296559rank=121600 of ranks=296559rank=121700 of ranks=296559rank=121800 of ranks=296559rank=121900 of ranks=296559rank=122000 of ranks=296559rank=122100 of ranks=296559rank=122200 of ranks=296559rank=122300 of ranks=296559rank=122400 of ranks=296559rank=122500 of ranks=296559rank=122600 of ranks=296559rank=122700 of ranks=296559rank=122800 of ranks=296559rank=122900 of ranks=296559rank=123000 of ranks=296559rank=123100 of ranks=296559rank=123200 of ranks=296559rank=123300 of ranks=296559rank=123400 of ranks=296559rank=123500 of ranks=296559rank=123600 of ranks=296559rank=123700 of ranks=296559rank=123800 of ranks=296559rank=123900 of ranks=296559rank=124000 of ranks=296559rank=124100 of ranks=296559rank=124200 of ranks=296559rank=124300 of ranks=296559rank=124400 of ranks=296559rank=124500 of ranks=296559rank=124600 of ranks=296559rank=124700 of ranks=296559rank=124800 of ranks=296559rank=124900 of ranks=296559rank=125000 of ranks=296559rank=125100 of ranks=296559rank=125200 of ranks=296559rank=125300 of ranks=296559rank=125400 of ranks=296559rank=125500 of ranks=296559rank=125600 of ranks=296559rank=125700 of ranks=296559rank=125800 of ranks=296559rank=125900 of ranks=296559rank=126000 of ranks=296559rank=126100 of ranks=296559rank=126200 of ranks=296559rank=126300 of ranks=296559rank=126400 of ranks=296559rank=126500 of ranks=296559rank=126600 of ranks=296559rank=126700 of ranks=296559rank=126800 of ranks=296559rank=126900 of ranks=296559rank=127000 of ranks=296559rank=127100 of ranks=296559rank=127200 of ranks=296559rank=127300 of ranks=296559rank=127400 of ranks=296559rank=127500 of ranks=296559rank=127600 of ranks=296559rank=127700 of ranks=296559rank=127800 of ranks=296559rank=127900 of ranks=296559rank=128000 of ranks=296559rank=128100 of ranks=296559rank=128200 of ranks=296559rank=128300 of ranks=296559rank=128400 of ranks=296559rank=128500 of ranks=296559rank=128600 of ranks=296559rank=128700 of ranks=296559rank=128800 of ranks=296559rank=128900 of ranks=296559rank=129000 of ranks=296559rank=129100 of ranks=296559rank=129200 of ranks=296559rank=129300 of ranks=296559rank=129400 of ranks=296559rank=129500 of ranks=296559rank=129600 of ranks=296559rank=129700 of ranks=296559rank=129800 of ranks=296559rank=129900 of ranks=296559rank=130000 of ranks=296559rank=130100 of ranks=296559rank=130200 of ranks=296559rank=130300 of ranks=296559rank=130400 of ranks=296559rank=130500 of ranks=296559rank=130600 of ranks=296559rank=130700 of ranks=296559rank=130800 of ranks=296559rank=130900 of ranks=296559rank=131000 of ranks=296559rank=131100 of ranks=296559rank=131200 of ranks=296559rank=131300 of ranks=296559rank=131400 of ranks=296559rank=131500 of ranks=296559rank=131600 of ranks=296559rank=131700 of ranks=296559rank=131800 of ranks=296559rank=131900 of ranks=296559rank=132000 of ranks=296559rank=132100 of ranks=296559rank=132200 of ranks=296559rank=132300 of ranks=296559rank=132400 of ranks=296559rank=132500 of ranks=296559rank=132600 of ranks=296559rank=132700 of ranks=296559rank=132800 of ranks=296559rank=132900 of ranks=296559rank=133000 of ranks=296559rank=133100 of ranks=296559rank=133200 of ranks=296559rank=133300 of ranks=296559rank=133400 of ranks=296559rank=133500 of ranks=296559rank=133600 of ranks=296559rank=133700 of ranks=296559rank=133800 of ranks=296559rank=133900 of ranks=296559rank=134000 of ranks=296559rank=134100 of ranks=296559rank=134200 of ranks=296559rank=134300 of ranks=296559rank=134400 of ranks=296559rank=134500 of ranks=296559rank=134600 of ranks=296559rank=134700 of ranks=296559rank=134800 of ranks=296559rank=134900 of ranks=296559rank=135000 of ranks=296559rank=135100 of ranks=296559rank=135200 of ranks=296559rank=135300 of ranks=296559rank=135400 of ranks=296559rank=135500 of ranks=296559rank=135600 of ranks=296559rank=135700 of ranks=296559rank=135800 of ranks=296559rank=135900 of ranks=296559rank=136000 of ranks=296559rank=136100 of ranks=296559rank=136200 of ranks=296559rank=136300 of ranks=296559rank=136400 of ranks=296559rank=136500 of ranks=296559rank=136600 of ranks=296559rank=136700 of ranks=296559rank=136800 of ranks=296559rank=136900 of ranks=296559rank=137000 of ranks=296559rank=137100 of ranks=296559rank=137200 of ranks=296559rank=137300 of ranks=296559rank=137400 of ranks=296559rank=137500 of ranks=296559rank=137600 of ranks=296559rank=137700 of ranks=296559rank=137800 of ranks=296559rank=137900 of ranks=296559rank=138000 of ranks=296559rank=138100 of ranks=296559rank=138200 of ranks=296559rank=138300 of ranks=296559rank=138400 of ranks=296559rank=138500 of ranks=296559rank=138600 of ranks=296559rank=138700 of ranks=296559rank=138800 of ranks=296559rank=138900 of ranks=296559rank=139000 of ranks=296559rank=139100 of ranks=296559rank=139200 of ranks=296559rank=139300 of ranks=296559rank=139400 of ranks=296559rank=139500 of ranks=296559rank=139600 of ranks=296559rank=139700 of ranks=296559rank=139800 of ranks=296559rank=139900 of ranks=296559rank=140000 of ranks=296559rank=140100 of ranks=296559rank=140200 of ranks=296559rank=140300 of ranks=296559rank=140400 of ranks=296559rank=140500 of ranks=296559rank=140600 of ranks=296559rank=140700 of ranks=296559rank=140800 of ranks=296559rank=140900 of ranks=296559rank=141000 of ranks=296559rank=141100 of ranks=296559rank=141200 of ranks=296559rank=141300 of ranks=296559rank=141400 of ranks=296559rank=141500 of ranks=296559rank=141600 of ranks=296559rank=141700 of ranks=296559rank=141800 of ranks=296559rank=141900 of ranks=296559rank=142000 of ranks=296559rank=142100 of ranks=296559rank=142200 of ranks=296559rank=142300 of ranks=296559rank=142400 of ranks=296559rank=142500 of ranks=296559rank=142600 of ranks=296559rank=142700 of ranks=296559rank=142800 of ranks=296559rank=142900 of ranks=296559rank=143000 of ranks=296559rank=143100 of ranks=296559rank=143200 of ranks=296559rank=143300 of ranks=296559rank=143400 of ranks=296559rank=143500 of ranks=296559rank=143600 of ranks=296559rank=143700 of ranks=296559rank=143800 of ranks=296559rank=143900 of ranks=296559rank=144000 of ranks=296559rank=144100 of ranks=296559rank=144200 of ranks=296559rank=144300 of ranks=296559rank=144400 of ranks=296559rank=144500 of ranks=296559rank=144600 of ranks=296559rank=144700 of ranks=296559rank=144800 of ranks=296559rank=144900 of ranks=296559rank=145000 of ranks=296559rank=145100 of ranks=296559rank=145200 of ranks=296559rank=145300 of ranks=296559rank=145400 of ranks=296559rank=145500 of ranks=296559rank=145600 of ranks=296559rank=145700 of ranks=296559rank=145800 of ranks=296559rank=145900 of ranks=296559rank=146000 of ranks=296559rank=146100 of ranks=296559rank=146200 of ranks=296559rank=146300 of ranks=296559rank=146400 of ranks=296559rank=146500 of ranks=296559rank=146600 of ranks=296559rank=146700 of ranks=296559rank=146800 of ranks=296559rank=146900 of ranks=296559rank=147000 of ranks=296559rank=147100 of ranks=296559rank=147200 of ranks=296559rank=147300 of ranks=296559rank=147400 of ranks=296559rank=147500 of ranks=296559rank=147600 of ranks=296559rank=147700 of ranks=296559rank=147800 of ranks=296559rank=147900 of ranks=296559rank=148000 of ranks=296559rank=148100 of ranks=296559rank=148200 of ranks=296559rank=148300 of ranks=296559rank=148400 of ranks=296559rank=148500 of ranks=296559rank=148600 of ranks=296559rank=148700 of ranks=296559rank=148800 of ranks=296559rank=148900 of ranks=296559rank=149000 of ranks=296559rank=149100 of ranks=296559rank=149200 of ranks=296559rank=149300 of ranks=296559rank=149400 of ranks=296559rank=149500 of ranks=296559rank=149600 of ranks=296559rank=149700 of ranks=296559rank=149800 of ranks=296559rank=149900 of ranks=296559rank=150000 of ranks=296559rank=150100 of ranks=296559rank=150200 of ranks=296559rank=150300 of ranks=296559rank=150400 of ranks=296559rank=150500 of ranks=296559rank=150600 of ranks=296559rank=150700 of ranks=296559rank=150800 of ranks=296559rank=150900 of ranks=296559rank=151000 of ranks=296559rank=151100 of ranks=296559rank=151200 of ranks=296559rank=151300 of ranks=296559rank=151400 of ranks=296559rank=151500 of ranks=296559rank=151600 of ranks=296559rank=151700 of ranks=296559rank=151800 of ranks=296559rank=151900 of ranks=296559rank=152000 of ranks=296559rank=152100 of ranks=296559rank=152200 of ranks=296559rank=152300 of ranks=296559rank=152400 of ranks=296559rank=152500 of ranks=296559rank=152600 of ranks=296559rank=152700 of ranks=296559rank=152800 of ranks=296559rank=152900 of ranks=296559rank=153000 of ranks=296559rank=153100 of ranks=296559rank=153200 of ranks=296559rank=153300 of ranks=296559rank=153400 of ranks=296559rank=153500 of ranks=296559rank=153600 of ranks=296559rank=153700 of ranks=296559rank=153800 of ranks=296559rank=153900 of ranks=296559rank=154000 of ranks=296559rank=154100 of ranks=296559rank=154200 of ranks=296559rank=154300 of ranks=296559rank=154400 of ranks=296559rank=154500 of ranks=296559rank=154600 of ranks=296559rank=154700 of ranks=296559rank=154800 of ranks=296559rank=154900 of ranks=296559rank=155000 of ranks=296559rank=155100 of ranks=296559rank=155200 of ranks=296559rank=155300 of ranks=296559rank=155400 of ranks=296559rank=155500 of ranks=296559rank=155600 of ranks=296559rank=155700 of ranks=296559rank=155800 of ranks=296559rank=155900 of ranks=296559rank=156000 of ranks=296559rank=156100 of ranks=296559rank=156200 of ranks=296559rank=156300 of ranks=296559rank=156400 of ranks=296559rank=156500 of ranks=296559rank=156600 of ranks=296559rank=156700 of ranks=296559rank=156800 of ranks=296559rank=156900 of ranks=296559rank=157000 of ranks=296559rank=157100 of ranks=296559rank=157200 of ranks=296559rank=157300 of ranks=296559rank=157400 of ranks=296559rank=157500 of ranks=296559rank=157600 of ranks=296559rank=157700 of ranks=296559rank=157800 of ranks=296559rank=157900 of ranks=296559rank=158000 of ranks=296559rank=158100 of ranks=296559rank=158200 of ranks=296559rank=158300 of ranks=296559rank=158400 of ranks=296559rank=158500 of ranks=296559rank=158600 of ranks=296559rank=158700 of ranks=296559rank=158800 of ranks=296559rank=158900 of ranks=296559rank=159000 of ranks=296559rank=159100 of ranks=296559rank=159200 of ranks=296559rank=159300 of ranks=296559rank=159400 of ranks=296559rank=159500 of ranks=296559rank=159600 of ranks=296559rank=159700 of ranks=296559rank=159800 of ranks=296559rank=159900 of ranks=296559rank=160000 of ranks=296559rank=160100 of ranks=296559rank=160200 of ranks=296559rank=160300 of ranks=296559rank=160400 of ranks=296559rank=160500 of ranks=296559rank=160600 of ranks=296559rank=160700 of ranks=296559rank=160800 of ranks=296559rank=160900 of ranks=296559rank=161000 of ranks=296559rank=161100 of ranks=296559rank=161200 of ranks=296559rank=161300 of ranks=296559rank=161400 of ranks=296559rank=161500 of ranks=296559rank=161600 of ranks=296559rank=161700 of ranks=296559rank=161800 of ranks=296559rank=161900 of ranks=296559rank=162000 of ranks=296559rank=162100 of ranks=296559rank=162200 of ranks=296559rank=162300 of ranks=296559rank=162400 of ranks=296559rank=162500 of ranks=296559rank=162600 of ranks=296559rank=162700 of ranks=296559rank=162800 of ranks=296559rank=162900 of ranks=296559rank=163000 of ranks=296559rank=163100 of ranks=296559rank=163200 of ranks=296559rank=163300 of ranks=296559rank=163400 of ranks=296559rank=163500 of ranks=296559rank=163600 of ranks=296559rank=163700 of ranks=296559rank=163800 of ranks=296559rank=163900 of ranks=296559rank=164000 of ranks=296559rank=164100 of ranks=296559rank=164200 of ranks=296559rank=164300 of ranks=296559rank=164400 of ranks=296559rank=164500 of ranks=296559rank=164600 of ranks=296559rank=164700 of ranks=296559rank=164800 of ranks=296559rank=164900 of ranks=296559rank=165000 of ranks=296559rank=165100 of ranks=296559rank=165200 of ranks=296559rank=165300 of ranks=296559rank=165400 of ranks=296559rank=165500 of ranks=296559rank=165600 of ranks=296559rank=165700 of ranks=296559rank=165800 of ranks=296559rank=165900 of ranks=296559rank=166000 of ranks=296559rank=166100 of ranks=296559rank=166200 of ranks=296559rank=166300 of ranks=296559rank=166400 of ranks=296559rank=166500 of ranks=296559rank=166600 of ranks=296559rank=166700 of ranks=296559rank=166800 of ranks=296559rank=166900 of ranks=296559rank=167000 of ranks=296559rank=167100 of ranks=296559rank=167200 of ranks=296559rank=167300 of ranks=296559rank=167400 of ranks=296559rank=167500 of ranks=296559rank=167600 of ranks=296559rank=167700 of ranks=296559rank=167800 of ranks=296559rank=167900 of ranks=296559rank=168000 of ranks=296559rank=168100 of ranks=296559rank=168200 of ranks=296559rank=168300 of ranks=296559rank=168400 of ranks=296559rank=168500 of ranks=296559rank=168600 of ranks=296559rank=168700 of ranks=296559rank=168800 of ranks=296559rank=168900 of ranks=296559rank=169000 of ranks=296559rank=169100 of ranks=296559rank=169200 of ranks=296559rank=169300 of ranks=296559rank=169400 of ranks=296559rank=169500 of ranks=296559rank=169600 of ranks=296559rank=169700 of ranks=296559rank=169800 of ranks=296559rank=169900 of ranks=296559rank=170000 of ranks=296559rank=170100 of ranks=296559rank=170200 of ranks=296559rank=170300 of ranks=296559rank=170400 of ranks=296559rank=170500 of ranks=296559rank=170600 of ranks=296559rank=170700 of ranks=296559rank=170800 of ranks=296559rank=170900 of ranks=296559rank=171000 of ranks=296559rank=171100 of ranks=296559rank=171200 of ranks=296559rank=171300 of ranks=296559rank=171400 of ranks=296559rank=171500 of ranks=296559rank=171600 of ranks=296559rank=171700 of ranks=296559rank=171800 of ranks=296559rank=171900 of ranks=296559rank=172000 of ranks=296559rank=172100 of ranks=296559rank=172200 of ranks=296559rank=172300 of ranks=296559rank=172400 of ranks=296559rank=172500 of ranks=296559rank=172600 of ranks=296559rank=172700 of ranks=296559rank=172800 of ranks=296559rank=172900 of ranks=296559rank=173000 of ranks=296559rank=173100 of ranks=296559rank=173200 of ranks=296559rank=173300 of ranks=296559rank=173400 of ranks=296559rank=173500 of ranks=296559rank=173600 of ranks=296559rank=173700 of ranks=296559rank=173800 of ranks=296559rank=173900 of ranks=296559rank=174000 of ranks=296559rank=174100 of ranks=296559rank=174200 of ranks=296559rank=174300 of ranks=296559rank=174400 of ranks=296559rank=174500 of ranks=296559rank=174600 of ranks=296559rank=174700 of ranks=296559rank=174800 of ranks=296559rank=174900 of ranks=296559rank=175000 of ranks=296559rank=175100 of ranks=296559rank=175200 of ranks=296559rank=175300 of ranks=296559rank=175400 of ranks=296559rank=175500 of ranks=296559rank=175600 of ranks=296559rank=175700 of ranks=296559rank=175800 of ranks=296559rank=175900 of ranks=296559rank=176000 of ranks=296559rank=176100 of ranks=296559rank=176200 of ranks=296559rank=176300 of ranks=296559rank=176400 of ranks=296559rank=176500 of ranks=296559rank=176600 of ranks=296559rank=176700 of ranks=296559rank=176800 of ranks=296559rank=176900 of ranks=296559rank=177000 of ranks=296559rank=177100 of ranks=296559rank=177200 of ranks=296559rank=177300 of ranks=296559rank=177400 of ranks=296559rank=177500 of ranks=296559rank=177600 of ranks=296559rank=177700 of ranks=296559rank=177800 of ranks=296559rank=177900 of ranks=296559rank=178000 of ranks=296559rank=178100 of ranks=296559rank=178200 of ranks=296559rank=178300 of ranks=296559rank=178400 of ranks=296559rank=178500 of ranks=296559rank=178600 of ranks=296559rank=178700 of ranks=296559rank=178800 of ranks=296559rank=178900 of ranks=296559rank=179000 of ranks=296559rank=179100 of ranks=296559rank=179200 of ranks=296559rank=179300 of ranks=296559rank=179400 of ranks=296559rank=179500 of ranks=296559rank=179600 of ranks=296559rank=179700 of ranks=296559rank=179800 of ranks=296559rank=179900 of ranks=296559rank=180000 of ranks=296559rank=180100 of ranks=296559rank=180200 of ranks=296559rank=180300 of ranks=296559rank=180400 of ranks=296559rank=180500 of ranks=296559rank=180600 of ranks=296559rank=180700 of ranks=296559rank=180800 of ranks=296559rank=180900 of ranks=296559rank=181000 of ranks=296559rank=181100 of ranks=296559rank=181200 of ranks=296559rank=181300 of ranks=296559rank=181400 of ranks=296559rank=181500 of ranks=296559rank=181600 of ranks=296559rank=181700 of ranks=296559rank=181800 of ranks=296559rank=181900 of ranks=296559rank=182000 of ranks=296559rank=182100 of ranks=296559rank=182200 of ranks=296559rank=182300 of ranks=296559rank=182400 of ranks=296559rank=182500 of ranks=296559rank=182600 of ranks=296559rank=182700 of ranks=296559rank=182800 of ranks=296559rank=182900 of ranks=296559rank=183000 of ranks=296559rank=183100 of ranks=296559rank=183200 of ranks=296559rank=183300 of ranks=296559rank=183400 of ranks=296559rank=183500 of ranks=296559rank=183600 of ranks=296559rank=183700 of ranks=296559rank=183800 of ranks=296559rank=183900 of ranks=296559rank=184000 of ranks=296559rank=184100 of ranks=296559rank=184200 of ranks=296559rank=184300 of ranks=296559rank=184400 of ranks=296559rank=184500 of ranks=296559rank=184600 of ranks=296559rank=184700 of ranks=296559rank=184800 of ranks=296559rank=184900 of ranks=296559rank=185000 of ranks=296559rank=185100 of ranks=296559rank=185200 of ranks=296559rank=185300 of ranks=296559rank=185400 of ranks=296559rank=185500 of ranks=296559rank=185600 of ranks=296559rank=185700 of ranks=296559rank=185800 of ranks=296559rank=185900 of ranks=296559rank=186000 of ranks=296559rank=186100 of ranks=296559rank=186200 of ranks=296559rank=186300 of ranks=296559rank=186400 of ranks=296559rank=186500 of ranks=296559rank=186600 of ranks=296559rank=186700 of ranks=296559rank=186800 of ranks=296559rank=186900 of ranks=296559rank=187000 of ranks=296559rank=187100 of ranks=296559rank=187200 of ranks=296559rank=187300 of ranks=296559rank=187400 of ranks=296559rank=187500 of ranks=296559rank=187600 of ranks=296559rank=187700 of ranks=296559rank=187800 of ranks=296559rank=187900 of ranks=296559rank=188000 of ranks=296559rank=188100 of ranks=296559rank=188200 of ranks=296559rank=188300 of ranks=296559rank=188400 of ranks=296559rank=188500 of ranks=296559rank=188600 of ranks=296559rank=188700 of ranks=296559rank=188800 of ranks=296559rank=188900 of ranks=296559rank=189000 of ranks=296559rank=189100 of ranks=296559rank=189200 of ranks=296559rank=189300 of ranks=296559rank=189400 of ranks=296559rank=189500 of ranks=296559rank=189600 of ranks=296559rank=189700 of ranks=296559rank=189800 of ranks=296559rank=189900 of ranks=296559rank=190000 of ranks=296559rank=190100 of ranks=296559rank=190200 of ranks=296559rank=190300 of ranks=296559rank=190400 of ranks=296559rank=190500 of ranks=296559rank=190600 of ranks=296559rank=190700 of ranks=296559rank=190800 of ranks=296559rank=190900 of ranks=296559rank=191000 of ranks=296559rank=191100 of ranks=296559rank=191200 of ranks=296559rank=191300 of ranks=296559rank=191400 of ranks=296559rank=191500 of ranks=296559rank=191600 of ranks=296559rank=191700 of ranks=296559rank=191800 of ranks=296559rank=191900 of ranks=296559rank=192000 of ranks=296559rank=192100 of ranks=296559rank=192200 of ranks=296559rank=192300 of ranks=296559rank=192400 of ranks=296559rank=192500 of ranks=296559rank=192600 of ranks=296559rank=192700 of ranks=296559rank=192800 of ranks=296559rank=192900 of ranks=296559rank=193000 of ranks=296559rank=193100 of ranks=296559rank=193200 of ranks=296559rank=193300 of ranks=296559rank=193400 of ranks=296559rank=193500 of ranks=296559rank=193600 of ranks=296559rank=193700 of ranks=296559rank=193800 of ranks=296559rank=193900 of ranks=296559rank=194000 of ranks=296559rank=194100 of ranks=296559rank=194200 of ranks=296559rank=194300 of ranks=296559rank=194400 of ranks=296559rank=194500 of ranks=296559rank=194600 of ranks=296559rank=194700 of ranks=296559rank=194800 of ranks=296559rank=194900 of ranks=296559rank=195000 of ranks=296559rank=195100 of ranks=296559rank=195200 of ranks=296559rank=195300 of ranks=296559rank=195400 of ranks=296559rank=195500 of ranks=296559rank=195600 of ranks=296559rank=195700 of ranks=296559rank=195800 of ranks=296559rank=195900 of ranks=296559rank=196000 of ranks=296559rank=196100 of ranks=296559rank=196200 of ranks=296559rank=196300 of ranks=296559rank=196400 of ranks=296559rank=196500 of ranks=296559rank=196600 of ranks=296559rank=196700 of ranks=296559rank=196800 of ranks=296559rank=196900 of ranks=296559rank=197000 of ranks=296559rank=197100 of ranks=296559rank=197200 of ranks=296559rank=197300 of ranks=296559rank=197400 of ranks=296559rank=197500 of ranks=296559rank=197600 of ranks=296559rank=197700 of ranks=296559rank=197800 of ranks=296559rank=197900 of ranks=296559rank=198000 of ranks=296559rank=198100 of ranks=296559rank=198200 of ranks=296559rank=198300 of ranks=296559rank=198400 of ranks=296559rank=198500 of ranks=296559rank=198600 of ranks=296559rank=198700 of ranks=296559rank=198800 of ranks=296559rank=198900 of ranks=296559rank=199000 of ranks=296559rank=199100 of ranks=296559rank=199200 of ranks=296559rank=199300 of ranks=296559rank=199400 of ranks=296559rank=199500 of ranks=296559rank=199600 of ranks=296559rank=199700 of ranks=296559rank=199800 of ranks=296559rank=199900 of ranks=296559rank=200000 of ranks=296559rank=200100 of ranks=296559rank=200200 of ranks=296559rank=200300 of ranks=296559rank=200400 of ranks=296559rank=200500 of ranks=296559rank=200600 of ranks=296559rank=200700 of ranks=296559rank=200800 of ranks=296559rank=200900 of ranks=296559rank=201000 of ranks=296559rank=201100 of ranks=296559rank=201200 of ranks=296559rank=201300 of ranks=296559rank=201400 of ranks=296559rank=201500 of ranks=296559rank=201600 of ranks=296559rank=201700 of ranks=296559rank=201800 of ranks=296559rank=201900 of ranks=296559rank=202000 of ranks=296559rank=202100 of ranks=296559rank=202200 of ranks=296559rank=202300 of ranks=296559rank=202400 of ranks=296559rank=202500 of ranks=296559rank=202600 of ranks=296559rank=202700 of ranks=296559rank=202800 of ranks=296559rank=202900 of ranks=296559rank=203000 of ranks=296559rank=203100 of ranks=296559rank=203200 of ranks=296559rank=203300 of ranks=296559rank=203400 of ranks=296559rank=203500 of ranks=296559rank=203600 of ranks=296559rank=203700 of ranks=296559rank=203800 of ranks=296559rank=203900 of ranks=296559rank=204000 of ranks=296559rank=204100 of ranks=296559rank=204200 of ranks=296559rank=204300 of ranks=296559rank=204400 of ranks=296559rank=204500 of ranks=296559rank=204600 of ranks=296559rank=204700 of ranks=296559rank=204800 of ranks=296559rank=204900 of ranks=296559rank=205000 of ranks=296559rank=205100 of ranks=296559rank=205200 of ranks=296559rank=205300 of ranks=296559rank=205400 of ranks=296559rank=205500 of ranks=296559rank=205600 of ranks=296559rank=205700 of ranks=296559rank=205800 of ranks=296559rank=205900 of ranks=296559rank=206000 of ranks=296559rank=206100 of ranks=296559rank=206200 of ranks=296559rank=206300 of ranks=296559rank=206400 of ranks=296559rank=206500 of ranks=296559rank=206600 of ranks=296559rank=206700 of ranks=296559rank=206800 of ranks=296559rank=206900 of ranks=296559rank=207000 of ranks=296559rank=207100 of ranks=296559rank=207200 of ranks=296559rank=207300 of ranks=296559rank=207400 of ranks=296559rank=207500 of ranks=296559rank=207600 of ranks=296559rank=207700 of ranks=296559rank=207800 of ranks=296559rank=207900 of ranks=296559rank=208000 of ranks=296559rank=208100 of ranks=296559rank=208200 of ranks=296559rank=208300 of ranks=296559rank=208400 of ranks=296559rank=208500 of ranks=296559rank=208600 of ranks=296559rank=208700 of ranks=296559rank=208800 of ranks=296559rank=208900 of ranks=296559rank=209000 of ranks=296559rank=209100 of ranks=296559rank=209200 of ranks=296559rank=209300 of ranks=296559rank=209400 of ranks=296559rank=209500 of ranks=296559rank=209600 of ranks=296559rank=209700 of ranks=296559rank=209800 of ranks=296559rank=209900 of ranks=296559rank=210000 of ranks=296559rank=210100 of ranks=296559rank=210200 of ranks=296559rank=210300 of ranks=296559rank=210400 of ranks=296559rank=210500 of ranks=296559rank=210600 of ranks=296559rank=210700 of ranks=296559rank=210800 of ranks=296559rank=210900 of ranks=296559rank=211000 of ranks=296559rank=211100 of ranks=296559rank=211200 of ranks=296559rank=211300 of ranks=296559rank=211400 of ranks=296559rank=211500 of ranks=296559rank=211600 of ranks=296559rank=211700 of ranks=296559rank=211800 of ranks=296559rank=211900 of ranks=296559rank=212000 of ranks=296559rank=212100 of ranks=296559rank=212200 of ranks=296559rank=212300 of ranks=296559rank=212400 of ranks=296559rank=212500 of ranks=296559rank=212600 of ranks=296559rank=212700 of ranks=296559rank=212800 of ranks=296559rank=212900 of ranks=296559rank=213000 of ranks=296559rank=213100 of ranks=296559rank=213200 of ranks=296559rank=213300 of ranks=296559rank=213400 of ranks=296559rank=213500 of ranks=296559rank=213600 of ranks=296559rank=213700 of ranks=296559rank=213800 of ranks=296559rank=213900 of ranks=296559rank=214000 of ranks=296559rank=214100 of ranks=296559rank=214200 of ranks=296559rank=214300 of ranks=296559rank=214400 of ranks=296559rank=214500 of ranks=296559rank=214600 of ranks=296559rank=214700 of ranks=296559rank=214800 of ranks=296559rank=214900 of ranks=296559rank=215000 of ranks=296559rank=215100 of ranks=296559rank=215200 of ranks=296559rank=215300 of ranks=296559rank=215400 of ranks=296559rank=215500 of ranks=296559rank=215600 of ranks=296559rank=215700 of ranks=296559rank=215800 of ranks=296559rank=215900 of ranks=296559rank=216000 of ranks=296559rank=216100 of ranks=296559rank=216200 of ranks=296559rank=216300 of ranks=296559rank=216400 of ranks=296559rank=216500 of ranks=296559rank=216600 of ranks=296559rank=216700 of ranks=296559rank=216800 of ranks=296559rank=216900 of ranks=296559rank=217000 of ranks=296559rank=217100 of ranks=296559rank=217200 of ranks=296559rank=217300 of ranks=296559rank=217400 of ranks=296559rank=217500 of ranks=296559rank=217600 of ranks=296559rank=217700 of ranks=296559rank=217800 of ranks=296559rank=217900 of ranks=296559rank=218000 of ranks=296559rank=218100 of ranks=296559rank=218200 of ranks=296559rank=218300 of ranks=296559rank=218400 of ranks=296559rank=218500 of ranks=296559rank=218600 of ranks=296559rank=218700 of ranks=296559rank=218800 of ranks=296559rank=218900 of ranks=296559rank=219000 of ranks=296559rank=219100 of ranks=296559rank=219200 of ranks=296559rank=219300 of ranks=296559rank=219400 of ranks=296559rank=219500 of ranks=296559rank=219600 of ranks=296559rank=219700 of ranks=296559rank=219800 of ranks=296559rank=219900 of ranks=296559rank=220000 of ranks=296559rank=220100 of ranks=296559rank=220200 of ranks=296559rank=220300 of ranks=296559rank=220400 of ranks=296559rank=220500 of ranks=296559rank=220600 of ranks=296559rank=220700 of ranks=296559rank=220800 of ranks=296559rank=220900 of ranks=296559rank=221000 of ranks=296559rank=221100 of ranks=296559rank=221200 of ranks=296559rank=221300 of ranks=296559rank=221400 of ranks=296559rank=221500 of ranks=296559rank=221600 of ranks=296559rank=221700 of ranks=296559rank=221800 of ranks=296559rank=221900 of ranks=296559rank=222000 of ranks=296559rank=222100 of ranks=296559rank=222200 of ranks=296559rank=222300 of ranks=296559rank=222400 of ranks=296559rank=222500 of ranks=296559rank=222600 of ranks=296559rank=222700 of ranks=296559rank=222800 of ranks=296559rank=222900 of ranks=296559rank=223000 of ranks=296559rank=223100 of ranks=296559rank=223200 of ranks=296559rank=223300 of ranks=296559rank=223400 of ranks=296559rank=223500 of ranks=296559rank=223600 of ranks=296559rank=223700 of ranks=296559rank=223800 of ranks=296559rank=223900 of ranks=296559rank=224000 of ranks=296559rank=224100 of ranks=296559rank=224200 of ranks=296559rank=224300 of ranks=296559rank=224400 of ranks=296559rank=224500 of ranks=296559rank=224600 of ranks=296559rank=224700 of ranks=296559rank=224800 of ranks=296559rank=224900 of ranks=296559rank=225000 of ranks=296559rank=225100 of ranks=296559rank=225200 of ranks=296559rank=225300 of ranks=296559rank=225400 of ranks=296559rank=225500 of ranks=296559rank=225600 of ranks=296559rank=225700 of ranks=296559rank=225800 of ranks=296559rank=225900 of ranks=296559rank=226000 of ranks=296559rank=226100 of ranks=296559rank=226200 of ranks=296559rank=226300 of ranks=296559rank=226400 of ranks=296559rank=226500 of ranks=296559rank=226600 of ranks=296559rank=226700 of ranks=296559rank=226800 of ranks=296559rank=226900 of ranks=296559rank=227000 of ranks=296559rank=227100 of ranks=296559rank=227200 of ranks=296559rank=227300 of ranks=296559rank=227400 of ranks=296559rank=227500 of ranks=296559rank=227600 of ranks=296559rank=227700 of ranks=296559rank=227800 of ranks=296559rank=227900 of ranks=296559rank=228000 of ranks=296559rank=228100 of ranks=296559rank=228200 of ranks=296559rank=228300 of ranks=296559rank=228400 of ranks=296559rank=228500 of ranks=296559rank=228600 of ranks=296559rank=228700 of ranks=296559rank=228800 of ranks=296559rank=228900 of ranks=296559rank=229000 of ranks=296559rank=229100 of ranks=296559rank=229200 of ranks=296559rank=229300 of ranks=296559rank=229400 of ranks=296559rank=229500 of ranks=296559rank=229600 of ranks=296559rank=229700 of ranks=296559rank=229800 of ranks=296559rank=229900 of ranks=296559rank=230000 of ranks=296559rank=230100 of ranks=296559rank=230200 of ranks=296559rank=230300 of ranks=296559rank=230400 of ranks=296559rank=230500 of ranks=296559rank=230600 of ranks=296559rank=230700 of ranks=296559rank=230800 of ranks=296559rank=230900 of ranks=296559rank=231000 of ranks=296559rank=231100 of ranks=296559rank=231200 of ranks=296559rank=231300 of ranks=296559rank=231400 of ranks=296559rank=231500 of ranks=296559rank=231600 of ranks=296559rank=231700 of ranks=296559rank=231800 of ranks=296559rank=231900 of ranks=296559rank=232000 of ranks=296559rank=232100 of ranks=296559rank=232200 of ranks=296559rank=232300 of ranks=296559rank=232400 of ranks=296559rank=232500 of ranks=296559rank=232600 of ranks=296559rank=232700 of ranks=296559rank=232800 of ranks=296559rank=232900 of ranks=296559rank=233000 of ranks=296559rank=233100 of ranks=296559rank=233200 of ranks=296559rank=233300 of ranks=296559rank=233400 of ranks=296559rank=233500 of ranks=296559rank=233600 of ranks=296559rank=233700 of ranks=296559rank=233800 of ranks=296559rank=233900 of ranks=296559rank=234000 of ranks=296559rank=234100 of ranks=296559rank=234200 of ranks=296559rank=234300 of ranks=296559rank=234400 of ranks=296559rank=234500 of ranks=296559rank=234600 of ranks=296559rank=234700 of ranks=296559rank=234800 of ranks=296559rank=234900 of ranks=296559rank=235000 of ranks=296559rank=235100 of ranks=296559rank=235200 of ranks=296559rank=235300 of ranks=296559rank=235400 of ranks=296559rank=235500 of ranks=296559rank=235600 of ranks=296559rank=235700 of ranks=296559rank=235800 of ranks=296559rank=235900 of ranks=296559rank=236000 of ranks=296559rank=236100 of ranks=296559rank=236200 of ranks=296559rank=236300 of ranks=296559rank=236400 of ranks=296559rank=236500 of ranks=296559rank=236600 of ranks=296559rank=236700 of ranks=296559rank=236800 of ranks=296559rank=236900 of ranks=296559rank=237000 of ranks=296559rank=237100 of ranks=296559rank=237200 of ranks=296559rank=237300 of ranks=296559rank=237400 of ranks=296559rank=237500 of ranks=296559rank=237600 of ranks=296559rank=237700 of ranks=296559rank=237800 of ranks=296559rank=237900 of ranks=296559rank=238000 of ranks=296559rank=238100 of ranks=296559rank=238200 of ranks=296559rank=238300 of ranks=296559rank=238400 of ranks=296559rank=238500 of ranks=296559rank=238600 of ranks=296559rank=238700 of ranks=296559rank=238800 of ranks=296559rank=238900 of ranks=296559rank=239000 of ranks=296559rank=239100 of ranks=296559rank=239200 of ranks=296559rank=239300 of ranks=296559rank=239400 of ranks=296559rank=239500 of ranks=296559rank=239600 of ranks=296559rank=239700 of ranks=296559rank=239800 of ranks=296559rank=239900 of ranks=296559rank=240000 of ranks=296559rank=240100 of ranks=296559rank=240200 of ranks=296559rank=240300 of ranks=296559rank=240400 of ranks=296559rank=240500 of ranks=296559rank=240600 of ranks=296559rank=240700 of ranks=296559rank=240800 of ranks=296559rank=240900 of ranks=296559rank=241000 of ranks=296559rank=241100 of ranks=296559rank=241200 of ranks=296559rank=241300 of ranks=296559rank=241400 of ranks=296559rank=241500 of ranks=296559rank=241600 of ranks=296559rank=241700 of ranks=296559rank=241800 of ranks=296559rank=241900 of ranks=296559rank=242000 of ranks=296559rank=242100 of ranks=296559rank=242200 of ranks=296559rank=242300 of ranks=296559rank=242400 of ranks=296559rank=242500 of ranks=296559rank=242600 of ranks=296559rank=242700 of ranks=296559rank=242800 of ranks=296559rank=242900 of ranks=296559rank=243000 of ranks=296559rank=243100 of ranks=296559rank=243200 of ranks=296559rank=243300 of ranks=296559rank=243400 of ranks=296559rank=243500 of ranks=296559rank=243600 of ranks=296559rank=243700 of ranks=296559rank=243800 of ranks=296559rank=243900 of ranks=296559rank=244000 of ranks=296559rank=244100 of ranks=296559rank=244200 of ranks=296559rank=244300 of ranks=296559rank=244400 of ranks=296559rank=244500 of ranks=296559rank=244600 of ranks=296559rank=244700 of ranks=296559rank=244800 of ranks=296559rank=244900 of ranks=296559rank=245000 of ranks=296559rank=245100 of ranks=296559rank=245200 of ranks=296559rank=245300 of ranks=296559rank=245400 of ranks=296559rank=245500 of ranks=296559rank=245600 of ranks=296559rank=245700 of ranks=296559rank=245800 of ranks=296559rank=245900 of ranks=296559rank=246000 of ranks=296559rank=246100 of ranks=296559rank=246200 of ranks=296559rank=246300 of ranks=296559rank=246400 of ranks=296559rank=246500 of ranks=296559rank=246600 of ranks=296559rank=246700 of ranks=296559rank=246800 of ranks=296559rank=246900 of ranks=296559rank=247000 of ranks=296559rank=247100 of ranks=296559rank=247200 of ranks=296559rank=247300 of ranks=296559rank=247400 of ranks=296559rank=247500 of ranks=296559rank=247600 of ranks=296559rank=247700 of ranks=296559rank=247800 of ranks=296559rank=247900 of ranks=296559rank=248000 of ranks=296559rank=248100 of ranks=296559rank=248200 of ranks=296559rank=248300 of ranks=296559rank=248400 of ranks=296559rank=248500 of ranks=296559rank=248600 of ranks=296559rank=248700 of ranks=296559rank=248800 of ranks=296559rank=248900 of ranks=296559rank=249000 of ranks=296559rank=249100 of ranks=296559rank=249200 of ranks=296559rank=249300 of ranks=296559rank=249400 of ranks=296559rank=249500 of ranks=296559rank=249600 of ranks=296559rank=249700 of ranks=296559rank=249800 of ranks=296559rank=249900 of ranks=296559rank=250000 of ranks=296559rank=250100 of ranks=296559rank=250200 of ranks=296559rank=250300 of ranks=296559rank=250400 of ranks=296559rank=250500 of ranks=296559rank=250600 of ranks=296559rank=250700 of ranks=296559rank=250800 of ranks=296559rank=250900 of ranks=296559rank=251000 of ranks=296559rank=251100 of ranks=296559rank=251200 of ranks=296559rank=251300 of ranks=296559rank=251400 of ranks=296559rank=251500 of ranks=296559rank=251600 of ranks=296559rank=251700 of ranks=296559rank=251800 of ranks=296559rank=251900 of ranks=296559rank=252000 of ranks=296559rank=252100 of ranks=296559rank=252200 of ranks=296559rank=252300 of ranks=296559rank=252400 of ranks=296559rank=252500 of ranks=296559rank=252600 of ranks=296559rank=252700 of ranks=296559rank=252800 of ranks=296559rank=252900 of ranks=296559rank=253000 of ranks=296559rank=253100 of ranks=296559rank=253200 of ranks=296559rank=253300 of ranks=296559rank=253400 of ranks=296559rank=253500 of ranks=296559rank=253600 of ranks=296559rank=253700 of ranks=296559rank=253800 of ranks=296559rank=253900 of ranks=296559rank=254000 of ranks=296559rank=254100 of ranks=296559rank=254200 of ranks=296559rank=254300 of ranks=296559rank=254400 of ranks=296559rank=254500 of ranks=296559rank=254600 of ranks=296559rank=254700 of ranks=296559rank=254800 of ranks=296559rank=254900 of ranks=296559rank=255000 of ranks=296559rank=255100 of ranks=296559rank=255200 of ranks=296559rank=255300 of ranks=296559rank=255400 of ranks=296559rank=255500 of ranks=296559rank=255600 of ranks=296559rank=255700 of ranks=296559rank=255800 of ranks=296559rank=255900 of ranks=296559rank=256000 of ranks=296559rank=256100 of ranks=296559rank=256200 of ranks=296559rank=256300 of ranks=296559rank=256400 of ranks=296559rank=256500 of ranks=296559rank=256600 of ranks=296559rank=256700 of ranks=296559rank=256800 of ranks=296559rank=256900 of ranks=296559rank=257000 of ranks=296559rank=257100 of ranks=296559rank=257200 of ranks=296559rank=257300 of ranks=296559rank=257400 of ranks=296559rank=257500 of ranks=296559rank=257600 of ranks=296559rank=257700 of ranks=296559rank=257800 of ranks=296559rank=257900 of ranks=296559rank=258000 of ranks=296559rank=258100 of ranks=296559rank=258200 of ranks=296559rank=258300 of ranks=296559rank=258400 of ranks=296559rank=258500 of ranks=296559rank=258600 of ranks=296559rank=258700 of ranks=296559rank=258800 of ranks=296559rank=258900 of ranks=296559rank=259000 of ranks=296559rank=259100 of ranks=296559rank=259200 of ranks=296559rank=259300 of ranks=296559rank=259400 of ranks=296559rank=259500 of ranks=296559rank=259600 of ranks=296559rank=259700 of ranks=296559rank=259800 of ranks=296559rank=259900 of ranks=296559rank=260000 of ranks=296559rank=260100 of ranks=296559rank=260200 of ranks=296559rank=260300 of ranks=296559rank=260400 of ranks=296559rank=260500 of ranks=296559rank=260600 of ranks=296559rank=260700 of ranks=296559rank=260800 of ranks=296559rank=260900 of ranks=296559rank=261000 of ranks=296559rank=261100 of ranks=296559rank=261200 of ranks=296559rank=261300 of ranks=296559rank=261400 of ranks=296559rank=261500 of ranks=296559rank=261600 of ranks=296559rank=261700 of ranks=296559rank=261800 of ranks=296559rank=261900 of ranks=296559rank=262000 of ranks=296559rank=262100 of ranks=296559rank=262200 of ranks=296559rank=262300 of ranks=296559rank=262400 of ranks=296559rank=262500 of ranks=296559rank=262600 of ranks=296559rank=262700 of ranks=296559rank=262800 of ranks=296559rank=262900 of ranks=296559rank=263000 of ranks=296559rank=263100 of ranks=296559rank=263200 of ranks=296559rank=263300 of ranks=296559rank=263400 of ranks=296559rank=263500 of ranks=296559rank=263600 of ranks=296559rank=263700 of ranks=296559rank=263800 of ranks=296559rank=263900 of ranks=296559rank=264000 of ranks=296559rank=264100 of ranks=296559rank=264200 of ranks=296559rank=264300 of ranks=296559rank=264400 of ranks=296559rank=264500 of ranks=296559rank=264600 of ranks=296559rank=264700 of ranks=296559rank=264800 of ranks=296559rank=264900 of ranks=296559rank=265000 of ranks=296559rank=265100 of ranks=296559rank=265200 of ranks=296559rank=265300 of ranks=296559rank=265400 of ranks=296559rank=265500 of ranks=296559rank=265600 of ranks=296559rank=265700 of ranks=296559rank=265800 of ranks=296559rank=265900 of ranks=296559rank=266000 of ranks=296559rank=266100 of ranks=296559rank=266200 of ranks=296559rank=266300 of ranks=296559rank=266400 of ranks=296559rank=266500 of ranks=296559rank=266600 of ranks=296559rank=266700 of ranks=296559rank=266800 of ranks=296559rank=266900 of ranks=296559rank=267000 of ranks=296559rank=267100 of ranks=296559rank=267200 of ranks=296559rank=267300 of ranks=296559rank=267400 of ranks=296559rank=267500 of ranks=296559rank=267600 of ranks=296559rank=267700 of ranks=296559rank=267800 of ranks=296559rank=267900 of ranks=296559rank=268000 of ranks=296559rank=268100 of ranks=296559rank=268200 of ranks=296559rank=268300 of ranks=296559rank=268400 of ranks=296559rank=268500 of ranks=296559rank=268600 of ranks=296559rank=268700 of ranks=296559rank=268800 of ranks=296559rank=268900 of ranks=296559rank=269000 of ranks=296559rank=269100 of ranks=296559rank=269200 of ranks=296559rank=269300 of ranks=296559rank=269400 of ranks=296559rank=269500 of ranks=296559rank=269600 of ranks=296559rank=269700 of ranks=296559rank=269800 of ranks=296559rank=269900 of ranks=296559rank=270000 of ranks=296559rank=270100 of ranks=296559rank=270200 of ranks=296559rank=270300 of ranks=296559rank=270400 of ranks=296559rank=270500 of ranks=296559rank=270600 of ranks=296559rank=270700 of ranks=296559rank=270800 of ranks=296559rank=270900 of ranks=296559rank=271000 of ranks=296559rank=271100 of ranks=296559rank=271200 of ranks=296559rank=271300 of ranks=296559rank=271400 of ranks=296559rank=271500 of ranks=296559rank=271600 of ranks=296559rank=271700 of ranks=296559rank=271800 of ranks=296559rank=271900 of ranks=296559rank=272000 of ranks=296559rank=272100 of ranks=296559rank=272200 of ranks=296559rank=272300 of ranks=296559rank=272400 of ranks=296559rank=272500 of ranks=296559rank=272600 of ranks=296559rank=272700 of ranks=296559rank=272800 of ranks=296559rank=272900 of ranks=296559rank=273000 of ranks=296559rank=273100 of ranks=296559rank=273200 of ranks=296559rank=273300 of ranks=296559rank=273400 of ranks=296559rank=273500 of ranks=296559rank=273600 of ranks=296559rank=273700 of ranks=296559rank=273800 of ranks=296559rank=273900 of ranks=296559rank=274000 of ranks=296559rank=274100 of ranks=296559rank=274200 of ranks=296559rank=274300 of ranks=296559rank=274400 of ranks=296559rank=274500 of ranks=296559rank=274600 of ranks=296559rank=274700 of ranks=296559rank=274800 of ranks=296559rank=274900 of ranks=296559rank=275000 of ranks=296559rank=275100 of ranks=296559rank=275200 of ranks=296559rank=275300 of ranks=296559rank=275400 of ranks=296559rank=275500 of ranks=296559rank=275600 of ranks=296559rank=275700 of ranks=296559rank=275800 of ranks=296559rank=275900 of ranks=296559rank=276000 of ranks=296559rank=276100 of ranks=296559rank=276200 of ranks=296559rank=276300 of ranks=296559rank=276400 of ranks=296559rank=276500 of ranks=296559rank=276600 of ranks=296559rank=276700 of ranks=296559rank=276800 of ranks=296559rank=276900 of ranks=296559rank=277000 of ranks=296559rank=277100 of ranks=296559rank=277200 of ranks=296559rank=277300 of ranks=296559rank=277400 of ranks=296559rank=277500 of ranks=296559rank=277600 of ranks=296559rank=277700 of ranks=296559rank=277800 of ranks=296559rank=277900 of ranks=296559rank=278000 of ranks=296559rank=278100 of ranks=296559rank=278200 of ranks=296559rank=278300 of ranks=296559rank=278400 of ranks=296559rank=278500 of ranks=296559rank=278600 of ranks=296559rank=278700 of ranks=296559rank=278800 of ranks=296559rank=278900 of ranks=296559rank=279000 of ranks=296559rank=279100 of ranks=296559rank=279200 of ranks=296559rank=279300 of ranks=296559rank=279400 of ranks=296559rank=279500 of ranks=296559rank=279600 of ranks=296559rank=279700 of ranks=296559rank=279800 of ranks=296559rank=279900 of ranks=296559rank=280000 of ranks=296559rank=280100 of ranks=296559rank=280200 of ranks=296559rank=280300 of ranks=296559rank=280400 of ranks=296559rank=280500 of ranks=296559rank=280600 of ranks=296559rank=280700 of ranks=296559rank=280800 of ranks=296559rank=280900 of ranks=296559rank=281000 of ranks=296559rank=281100 of ranks=296559rank=281200 of ranks=296559rank=281300 of ranks=296559rank=281400 of ranks=296559rank=281500 of ranks=296559rank=281600 of ranks=296559rank=281700 of ranks=296559rank=281800 of ranks=296559rank=281900 of ranks=296559rank=282000 of ranks=296559rank=282100 of ranks=296559rank=282200 of ranks=296559rank=282300 of ranks=296559rank=282400 of ranks=296559rank=282500 of ranks=296559rank=282600 of ranks=296559rank=282700 of ranks=296559rank=282800 of ranks=296559rank=282900 of ranks=296559rank=283000 of ranks=296559rank=283100 of ranks=296559rank=283200 of ranks=296559rank=283300 of ranks=296559rank=283400 of ranks=296559rank=283500 of ranks=296559rank=283600 of ranks=296559rank=283700 of ranks=296559rank=283800 of ranks=296559rank=283900 of ranks=296559rank=284000 of ranks=296559rank=284100 of ranks=296559rank=284200 of ranks=296559rank=284300 of ranks=296559rank=284400 of ranks=296559rank=284500 of ranks=296559rank=284600 of ranks=296559rank=284700 of ranks=296559rank=284800 of ranks=296559rank=284900 of ranks=296559rank=285000 of ranks=296559rank=285100 of ranks=296559rank=285200 of ranks=296559rank=285300 of ranks=296559rank=285400 of ranks=296559rank=285500 of ranks=296559rank=285600 of ranks=296559rank=285700 of ranks=296559rank=285800 of ranks=296559rank=285900 of ranks=296559rank=286000 of ranks=296559rank=286100 of ranks=296559rank=286200 of ranks=296559rank=286300 of ranks=296559rank=286400 of ranks=296559rank=286500 of ranks=296559rank=286600 of ranks=296559rank=286700 of ranks=296559rank=286800 of ranks=296559rank=286900 of ranks=296559rank=287000 of ranks=296559rank=287100 of ranks=296559rank=287200 of ranks=296559rank=287300 of ranks=296559rank=287400 of ranks=296559rank=287500 of ranks=296559rank=287600 of ranks=296559rank=287700 of ranks=296559rank=287800 of ranks=296559rank=287900 of ranks=296559rank=288000 of ranks=296559rank=288100 of ranks=296559rank=288200 of ranks=296559rank=288300 of ranks=296559rank=288400 of ranks=296559rank=288500 of ranks=296559rank=288600 of ranks=296559rank=288700 of ranks=296559rank=288800 of ranks=296559rank=288900 of ranks=296559rank=289000 of ranks=296559rank=289100 of ranks=296559rank=289200 of ranks=296559rank=289300 of ranks=296559rank=289400 of ranks=296559rank=289500 of ranks=296559rank=289600 of ranks=296559rank=289700 of ranks=296559rank=289800 of ranks=296559rank=289900 of ranks=296559rank=290000 of ranks=296559rank=290100 of ranks=296559rank=290200 of ranks=296559rank=290300 of ranks=296559rank=290400 of ranks=296559rank=290500 of ranks=296559rank=290600 of ranks=296559rank=290700 of ranks=296559rank=290800 of ranks=296559rank=290900 of ranks=296559rank=291000 of ranks=296559rank=291100 of ranks=296559rank=291200 of ranks=296559rank=291300 of ranks=296559rank=291400 of ranks=296559rank=291500 of ranks=296559rank=291600 of ranks=296559rank=291700 of ranks=296559rank=291800 of ranks=296559rank=291900 of ranks=296559rank=292000 of ranks=296559rank=292100 of ranks=296559rank=292200 of ranks=296559rank=292300 of ranks=296559rank=292400 of ranks=296559rank=292500 of ranks=296559rank=292600 of ranks=296559rank=292700 of ranks=296559rank=292800 of ranks=296559rank=292900 of ranks=296559rank=293000 of ranks=296559rank=293100 of ranks=296559rank=293200 of ranks=296559rank=293300 of ranks=296559rank=293400 of ranks=296559rank=293500 of ranks=296559rank=293600 of ranks=296559rank=293700 of ranks=296559rank=293800 of ranks=296559rank=293900 of ranks=296559rank=294000 of ranks=296559rank=294100 of ranks=296559rank=294200 of ranks=296559rank=294300 of ranks=296559rank=294400 of ranks=296559rank=294500 of ranks=296559rank=294600 of ranks=296559rank=294700 of ranks=296559rank=294800 of ranks=296559rank=294900 of ranks=296559rank=295000 of ranks=296559rank=295100 of ranks=296559rank=295200 of ranks=296559rank=295300 of ranks=296559rank=295400 of ranks=296559rank=295500 of ranks=296559rank=295600 of ranks=296559rank=295700 of ranks=296559rank=295800 of ranks=296559rank=295900 of ranks=296559rank=296000 of ranks=296559rank=296100 of ranks=296559rank=296200 of ranks=296559rank=296300 of ranks=296559rank=296400 of ranks=296559rank=296500 of ranks=296559

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              75.3228    454  25313     44    498           32.7185
   1 car                    96.0957  49735 120466    581  50316           59.1370
   2 truck                  77.6486   1751  34456     74   1825           37.3760
   3 bus                    43.9513    341   9476     25    366           57.5243
   4 pedestrian             73.7892   3805  50762    454   4259           37.2309

for conf_thresh=0.25, precision=0.68, recall=0.90, F1 score=0.77
for conf_thresh=0.25, TP=51458, FP=24198, FN=5806, average IoU=56.47%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=73.36%
Total detection time: 174 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
2219: loss=7.045, avg loss=6.258, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=804.3 milliseconds, train=2.1 seconds, 142016 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2220: loss=6.436, avg loss=6.276, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.4 seconds, train=2.1 seconds, 142080 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2221: loss=6.981, avg loss=6.346, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=4.1 seconds, train=3.3 seconds, 142144 images, time remaining=5.7 hours
2222: loss=5.751, avg loss=6.287, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 142208 images, time remaining=5.7 hours
2223: loss=7.490, avg loss=6.407, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 142272 images, time remaining=5.7 hours
2224: loss=5.847, avg loss=6.351, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 142336 images, time remaining=5.7 hours
2225: loss=6.429, avg loss=6.359, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 142400 images, time remaining=5.7 hours
2226: loss=5.262, avg loss=6.249, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=832.8 milliseconds, train=3.3 seconds, 142464 images, time remaining=5.7 hours
2227: loss=5.617, avg loss=6.186, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 142528 images, time remaining=5.7 hours
2228: loss=6.125, avg loss=6.180, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=509.9 milliseconds, train=3.3 seconds, 142592 images, time remaining=5.7 hours
2229: loss=5.899, avg loss=6.152, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 142656 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2230: loss=5.982, avg loss=6.135, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=4.6 seconds, train=3.3 seconds, 142720 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2231: loss=8.157, avg loss=6.337, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=4.9 seconds, 142784 images, time remaining=5.7 hours
2232: loss=7.697, avg loss=6.473, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=4.9 seconds, 142848 images, time remaining=5.7 hours
2233: loss=6.383, avg loss=6.464, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=4.8 seconds, 142912 images, time remaining=5.7 hours
2234: loss=6.873, avg loss=6.505, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=4.9 seconds, 142976 images, time remaining=5.7 hours
2235: loss=6.511, avg loss=6.505, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=4.8 seconds, 143040 images, time remaining=5.7 hours
2236: loss=5.663, avg loss=6.421, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=919.6 milliseconds, train=4.8 seconds, 143104 images, time remaining=5.7 hours
2237: loss=5.209, avg loss=6.300, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=4.9 seconds, 143168 images, time remaining=5.7 hours
2238: loss=7.291, avg loss=6.399, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=942.7 milliseconds, train=4.9 seconds, 143232 images, time remaining=5.7 hours
2239: loss=6.687, avg loss=6.428, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=4.9 seconds, 143296 images, time remaining=5.7 hours
2240: loss=6.513, avg loss=6.436, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=4.9 seconds, 143360 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2241: loss=5.969, avg loss=6.390, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 143424 images, time remaining=5.7 hours
2242: loss=6.110, avg loss=6.362, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 143488 images, time remaining=5.7 hours
2243: loss=5.001, avg loss=6.226, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=728.5 milliseconds, train=2.0 seconds, 143552 images, time remaining=5.7 hours
2244: loss=5.646, avg loss=6.168, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=749.7 milliseconds, train=2.0 seconds, 143616 images, time remaining=5.7 hours
2245: loss=4.574, avg loss=6.008, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=790.9 milliseconds, train=2.0 seconds, 143680 images, time remaining=5.7 hours
2246: loss=5.577, avg loss=5.965, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=733.8 milliseconds, train=2.0 seconds, 143744 images, time remaining=5.7 hours
2247: loss=6.517, avg loss=6.020, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 143808 images, time remaining=5.7 hours
2248: loss=5.738, avg loss=5.992, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=717.3 milliseconds, train=2.0 seconds, 143872 images, time remaining=5.7 hours
2249: loss=4.755, avg loss=5.868, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=938.3 milliseconds, train=1.9 seconds, 143936 images, time remaining=5.7 hours
2250: loss=4.574, avg loss=5.739, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=950.4 milliseconds, train=2.0 seconds, 144000 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1458be000000
2251: loss=6.381, avg loss=5.803, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=685.9 milliseconds, train=1.2 seconds, 144064 images, time remaining=5.7 hours
2252: loss=6.076, avg loss=5.830, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=840.7 milliseconds, train=1.2 seconds, 144128 images, time remaining=5.7 hours
2253: loss=5.863, avg loss=5.834, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 144192 images, time remaining=5.7 hours
2254: loss=5.670, avg loss=5.817, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=780.5 milliseconds, train=1.2 seconds, 144256 images, time remaining=5.7 hours
2255: loss=5.910, avg loss=5.827, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=801.6 milliseconds, train=1.2 seconds, 144320 images, time remaining=5.7 hours
2256: loss=4.885, avg loss=5.732, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=924.7 milliseconds, train=1.2 seconds, 144384 images, time remaining=5.7 hours
2257: loss=5.003, avg loss=5.659, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=989.2 milliseconds, train=1.2 seconds, 144448 images, time remaining=5.7 hours
2258: loss=5.782, avg loss=5.672, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=655.9 milliseconds, train=1.2 seconds, 144512 images, time remaining=5.7 hours
2259: loss=5.782, avg loss=5.683, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 144576 images, time remaining=5.7 hours
2260: loss=5.453, avg loss=5.660, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=871.2 milliseconds, train=1.2 seconds, 144640 images, time remaining=5.7 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146490000000
2261: loss=5.920, avg loss=5.686, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 144704 images, time remaining=5.7 hours
2262: loss=5.372, avg loss=5.654, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=847.4 milliseconds, train=1.5 seconds, 144768 images, time remaining=5.7 hours
2263: loss=5.068, avg loss=5.596, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=651.7 milliseconds, train=1.5 seconds, 144832 images, time remaining=5.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2264: loss=5.585, avg loss=5.595, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 144896 images, time remaining=5.7 hours
2265: loss=6.451, avg loss=5.680, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=536.9 milliseconds, train=1.5 seconds, 144960 images, time remaining=5.6 hours
2266: loss=5.761, avg loss=5.688, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 145024 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2267: loss=5.923, avg loss=5.712, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 145088 images, time remaining=5.6 hours
2268: loss=4.356, avg loss=5.576, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=488.9 milliseconds, train=1.5 seconds, 145152 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2269: loss=4.253, avg loss=5.444, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 145216 images, time remaining=5.6 hours
2270: loss=4.954, avg loss=5.395, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 145280 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2271: loss=8.460, avg loss=5.701, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=5.0 seconds, 145344 images, time remaining=5.6 hours
2272: loss=7.104, avg loss=5.842, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=986.8 milliseconds, train=4.9 seconds, 145408 images, time remaining=5.6 hours
2273: loss=6.891, avg loss=5.947, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=757.6 milliseconds, train=4.9 seconds, 145472 images, time remaining=5.6 hours
2274: loss=5.941, avg loss=5.946, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=746.2 milliseconds, train=4.9 seconds, 145536 images, time remaining=5.6 hours
2275: loss=6.747, avg loss=6.026, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=834.1 milliseconds, train=5.0 seconds, 145600 images, time remaining=5.6 hours
2276: loss=5.933, avg loss=6.017, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 145664 images, time remaining=5.6 hours
2277: loss=6.367, avg loss=6.052, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=4.9 seconds, 145728 images, time remaining=5.6 hours
2278: loss=7.176, avg loss=6.164, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=5.0 seconds, 145792 images, time remaining=5.6 hours
2279: loss=7.928, avg loss=6.341, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=5.0 seconds, 145856 images, time remaining=5.6 hours
2280: loss=7.205, avg loss=6.427, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=930.8 milliseconds, train=4.9 seconds, 145920 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145aa6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2281: loss=6.575, avg loss=6.442, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 145984 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2282: loss=7.294, avg loss=6.527, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 146048 images, time remaining=5.6 hours
2283: loss=6.339, avg loss=6.508, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=920.4 milliseconds, train=1.2 seconds, 146112 images, time remaining=5.6 hours
2284: loss=6.744, avg loss=6.532, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=585.8 milliseconds, train=1.2 seconds, 146176 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2285: loss=5.280, avg loss=6.407, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=1.2 seconds, 146240 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2286: loss=6.323, avg loss=6.398, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 146304 images, time remaining=5.6 hours
2287: loss=6.434, avg loss=6.402, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=923.8 milliseconds, train=1.2 seconds, 146368 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2288: loss=6.228, avg loss=6.384, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 146432 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2289: loss=5.696, avg loss=6.316, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 146496 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2290: loss=5.804, avg loss=6.264, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 146560 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2291: loss=8.343, avg loss=6.472, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 146624 images, time remaining=5.6 hours
2292: loss=6.990, avg loss=6.524, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.3 seconds, train=4.1 seconds, 146688 images, time remaining=5.6 hours
2293: loss=6.510, avg loss=6.523, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=795.5 milliseconds, train=4.1 seconds, 146752 images, time remaining=5.6 hours
2294: loss=6.278, avg loss=6.498, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=934.5 milliseconds, train=4.1 seconds, 146816 images, time remaining=5.6 hours
2295: loss=6.011, avg loss=6.449, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=561.9 milliseconds, train=4.1 seconds, 146880 images, time remaining=5.6 hours
2296: loss=9.180, avg loss=6.722, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 146944 images, time remaining=5.6 hours
2297: loss=7.070, avg loss=6.757, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=4.1 seconds, 147008 images, time remaining=5.6 hours
2298: loss=6.937, avg loss=6.775, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 147072 images, time remaining=5.6 hours
2299: loss=6.482, avg loss=6.746, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=674.4 milliseconds, train=4.1 seconds, 147136 images, time remaining=5.6 hours
2300: loss=6.702, avg loss=6.741, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=831.2 milliseconds, train=4.1 seconds, 147200 images, time remaining=5.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x14501e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2301: loss=8.006, avg loss=6.868, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 147264 images, time remaining=5.6 hours
2302: loss=7.976, avg loss=6.979, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=725.8 milliseconds, train=1.5 seconds, 147328 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2303: loss=7.124, avg loss=6.993, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 147392 images, time remaining=5.6 hours
2304: loss=6.121, avg loss=6.906, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=558.8 milliseconds, train=1.5 seconds, 147456 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2305: loss=6.256, avg loss=6.841, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 147520 images, time remaining=5.6 hours
2306: loss=6.594, avg loss=6.816, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 147584 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2307: loss=6.148, avg loss=6.749, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 147648 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2308: loss=7.542, avg loss=6.829, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 147712 images, time remaining=5.6 hours
2309: loss=5.957, avg loss=6.742, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 147776 images, time remaining=5.6 hours
2310: loss=6.152, avg loss=6.683, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=867.8 milliseconds, train=1.5 seconds, 147840 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14647c000000
2311: loss=7.142, avg loss=6.729, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=912.7 milliseconds, train=1.6 seconds, 147904 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2312: loss=6.889, avg loss=6.745, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=1.6 seconds, 147968 images, time remaining=5.6 hours
2313: loss=6.814, avg loss=6.752, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=1.7 seconds, 148032 images, time remaining=5.6 hours
2314: loss=4.909, avg loss=6.567, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=891.0 milliseconds, train=1.6 seconds, 148096 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2315: loss=5.177, avg loss=6.428, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 148160 images, time remaining=5.6 hours
2316: loss=6.268, avg loss=6.412, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=593.0 milliseconds, train=1.6 seconds, 148224 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2317: loss=5.206, avg loss=6.292, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 148288 images, time remaining=5.6 hours
2318: loss=5.854, avg loss=6.248, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 148352 images, time remaining=5.6 hours
2319: loss=5.798, avg loss=6.203, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=527.2 milliseconds, train=1.6 seconds, 148416 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2320: loss=5.945, avg loss=6.177, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=1.6 seconds, 148480 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146480000000
2321: loss=6.475, avg loss=6.207, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 148544 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2322: loss=5.478, avg loss=6.134, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=4.2 seconds, train=1.4 seconds, 148608 images, time remaining=5.6 hours
2323: loss=5.111, avg loss=6.032, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=685.7 milliseconds, train=1.5 seconds, 148672 images, time remaining=5.6 hours
2324: loss=6.176, avg loss=6.046, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=622.1 milliseconds, train=1.4 seconds, 148736 images, time remaining=5.6 hours
2325: loss=5.002, avg loss=5.942, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=983.0 milliseconds, train=1.4 seconds, 148800 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2326: loss=5.909, avg loss=5.938, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 148864 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2327: loss=5.445, avg loss=5.889, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 148928 images, time remaining=5.6 hours
2328: loss=5.377, avg loss=5.838, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=791.6 milliseconds, train=1.4 seconds, 148992 images, time remaining=5.6 hours
2329: loss=5.461, avg loss=5.800, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=996.5 milliseconds, train=1.4 seconds, 149056 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2330: loss=4.909, avg loss=5.711, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.6 seconds, train=1.4 seconds, 149120 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2331: loss=7.816, avg loss=5.922, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=756.3 milliseconds, train=3.3 seconds, 149184 images, time remaining=5.6 hours
2332: loss=5.911, avg loss=5.920, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.4 seconds, train=3.3 seconds, 149248 images, time remaining=5.6 hours
2333: loss=5.419, avg loss=5.870, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 149312 images, time remaining=5.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2334: loss=6.695, avg loss=5.953, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=4.1 seconds, train=3.3 seconds, 149376 images, time remaining=5.6 hours
2335: loss=5.541, avg loss=5.912, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 149440 images, time remaining=5.6 hours
2336: loss=5.276, avg loss=5.848, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 149504 images, time remaining=5.6 hours
2337: loss=5.205, avg loss=5.784, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=581.5 milliseconds, train=3.3 seconds, 149568 images, time remaining=5.6 hours
2338: loss=5.491, avg loss=5.754, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.4 seconds, train=3.2 seconds, 149632 images, time remaining=5.6 hours
2339: loss=5.767, avg loss=5.756, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=969.8 milliseconds, train=3.3 seconds, 149696 images, time remaining=5.6 hours
2340: loss=5.883, avg loss=5.768, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 149760 images, time remaining=5.6 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2341: loss=5.414, avg loss=5.733, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=3.5 seconds, 149824 images, time remaining=5.6 hours
2342: loss=5.861, avg loss=5.746, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=3.5 seconds, 149888 images, time remaining=5.6 hours
2343: loss=4.693, avg loss=5.640, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=3.5 seconds, 149952 images, time remaining=5.6 hours
2344: loss=6.029, avg loss=5.679, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=752.0 milliseconds, train=3.5 seconds, 150016 images, time remaining=5.6 hours
2345: loss=4.252, avg loss=5.537, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=805.4 milliseconds, train=3.5 seconds, 150080 images, time remaining=5.5 hours
2346: loss=5.943, avg loss=5.577, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.8 seconds, train=3.5 seconds, 150144 images, time remaining=5.5 hours
2347: loss=5.438, avg loss=5.563, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=3.5 seconds, 150208 images, time remaining=5.5 hours
2348: loss=5.306, avg loss=5.537, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=585.0 milliseconds, train=3.5 seconds, 150272 images, time remaining=5.5 hours
2349: loss=6.100, avg loss=5.594, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=3.5 seconds, 150336 images, time remaining=5.5 hours
2350: loss=5.985, avg loss=5.633, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=3.5 seconds, 150400 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463d8000000
2351: loss=5.947, avg loss=5.664, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=890.1 milliseconds, train=1.2 seconds, 150464 images, time remaining=5.5 hours
2352: loss=6.344, avg loss=5.732, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 second, train=1.2 seconds, 150528 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2353: loss=5.617, avg loss=5.721, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 150592 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2354: loss=4.256, avg loss=5.574, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 150656 images, time remaining=5.5 hours
2355: loss=5.985, avg loss=5.615, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=793.2 milliseconds, train=1.2 seconds, 150720 images, time remaining=5.5 hours
2356: loss=5.096, avg loss=5.563, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=936.6 milliseconds, train=1.2 seconds, 150784 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2357: loss=5.811, avg loss=5.588, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 150848 images, time remaining=5.5 hours
2358: loss=5.896, avg loss=5.619, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=389.8 milliseconds, train=1.2 seconds, 150912 images, time remaining=5.5 hours
2359: loss=5.465, avg loss=5.604, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=898.1 milliseconds, train=1.2 seconds, 150976 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2360: loss=5.349, avg loss=5.578, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 151040 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2361: loss=6.820, avg loss=5.702, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=985.8 milliseconds, train=3.2 seconds, 151104 images, time remaining=5.5 hours
2362: loss=6.043, avg loss=5.736, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=577.0 milliseconds, train=3.2 seconds, 151168 images, time remaining=5.5 hours
2363: loss=5.033, avg loss=5.666, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=889.3 milliseconds, train=3.2 seconds, 151232 images, time remaining=5.5 hours
2364: loss=4.853, avg loss=5.585, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=3.2 seconds, 151296 images, time remaining=5.5 hours
2365: loss=5.473, avg loss=5.574, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=3.2 seconds, 151360 images, time remaining=5.5 hours
2366: loss=6.116, avg loss=5.628, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=3.2 seconds, 151424 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2367: loss=5.693, avg loss=5.634, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=3.2 seconds, 151488 images, time remaining=5.5 hours
2368: loss=5.988, avg loss=5.670, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=461.9 milliseconds, train=3.2 seconds, 151552 images, time remaining=5.5 hours
2369: loss=5.565, avg loss=5.659, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=3.2 seconds, 151616 images, time remaining=5.5 hours
2370: loss=5.647, avg loss=5.658, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=3.2 seconds, 151680 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2371: loss=5.369, avg loss=5.629, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=3.8 seconds, 151744 images, time remaining=5.5 hours
2372: loss=6.371, avg loss=5.703, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=713.8 milliseconds, train=3.8 seconds, 151808 images, time remaining=5.5 hours
2373: loss=5.830, avg loss=5.716, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=3.7 seconds, 151872 images, time remaining=5.5 hours
2374: loss=5.602, avg loss=5.705, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=655.4 milliseconds, train=3.7 seconds, 151936 images, time remaining=5.5 hours
2375: loss=6.316, avg loss=5.766, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=3.8 seconds, 152000 images, time remaining=5.5 hours
2376: loss=5.492, avg loss=5.738, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=800.5 milliseconds, train=3.7 seconds, 152064 images, time remaining=5.5 hours
2377: loss=5.943, avg loss=5.759, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=756.1 milliseconds, train=3.8 seconds, 152128 images, time remaining=5.5 hours
2378: loss=5.986, avg loss=5.781, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=848.0 milliseconds, train=3.8 seconds, 152192 images, time remaining=5.5 hours
2379: loss=5.311, avg loss=5.734, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=674.2 milliseconds, train=3.7 seconds, 152256 images, time remaining=5.5 hours
2380: loss=6.016, avg loss=5.763, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=3.8 seconds, 152320 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146394000000
2381: loss=6.295, avg loss=5.816, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=795.6 milliseconds, train=1.5 seconds, 152384 images, time remaining=5.5 hours
2382: loss=5.082, avg loss=5.742, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 152448 images, time remaining=5.5 hours
2383: loss=6.542, avg loss=5.822, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=764.6 milliseconds, train=1.5 seconds, 152512 images, time remaining=5.5 hours
2384: loss=5.501, avg loss=5.790, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=582.9 milliseconds, train=1.5 seconds, 152576 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2385: loss=4.871, avg loss=5.698, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 152640 images, time remaining=5.5 hours
2386: loss=5.299, avg loss=5.658, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=803.3 milliseconds, train=1.5 seconds, 152704 images, time remaining=5.5 hours
2387: loss=5.809, avg loss=5.673, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=668.5 milliseconds, train=1.5 seconds, 152768 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2388: loss=5.453, avg loss=5.651, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 152832 images, time remaining=5.5 hours
2389: loss=4.940, avg loss=5.580, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=498.2 milliseconds, train=1.5 seconds, 152896 images, time remaining=5.5 hours
2390: loss=4.718, avg loss=5.494, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=528.6 milliseconds, train=1.5 seconds, 152960 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146394000000
2391: loss=5.003, avg loss=5.445, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=689.8 milliseconds, train=1.5 seconds, 153024 images, time remaining=5.5 hours
2392: loss=5.769, avg loss=5.477, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=460.4 milliseconds, train=1.5 seconds, 153088 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2393: loss=5.081, avg loss=5.438, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 153152 images, time remaining=5.5 hours
2394: loss=4.990, avg loss=5.393, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=755.0 milliseconds, train=1.5 seconds, 153216 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2395: loss=4.766, avg loss=5.330, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 153280 images, time remaining=5.5 hours
2396: loss=5.418, avg loss=5.339, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=680.7 milliseconds, train=1.5 seconds, 153344 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2397: loss=4.832, avg loss=5.288, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 153408 images, time remaining=5.5 hours
2398: loss=5.043, avg loss=5.264, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=561.5 milliseconds, train=1.5 seconds, 153472 images, time remaining=5.5 hours
2399: loss=5.027, avg loss=5.240, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=650.5 milliseconds, train=1.5 seconds, 153536 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2400: loss=4.808, avg loss=5.197, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 153600 images, time remaining=5.5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2401: loss=5.859, avg loss=5.263, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=883.6 milliseconds, train=3.6 seconds, 153664 images, time remaining=5.5 hours
2402: loss=6.397, avg loss=5.377, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 153728 images, time remaining=5.5 hours
2403: loss=5.180, avg loss=5.357, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=603.0 milliseconds, train=3.6 seconds, 153792 images, time remaining=5.5 hours
2404: loss=5.940, avg loss=5.415, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=544.6 milliseconds, train=3.6 seconds, 153856 images, time remaining=5.5 hours
2405: loss=6.223, avg loss=5.496, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=3.6 seconds, 153920 images, time remaining=5.5 hours
2406: loss=5.029, avg loss=5.449, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=3.6 seconds, 153984 images, time remaining=5.5 hours
2407: loss=5.704, avg loss=5.475, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 154048 images, time remaining=5.5 hours
2408: loss=5.931, avg loss=5.520, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=850.1 milliseconds, train=3.6 seconds, 154112 images, time remaining=5.5 hours
2409: loss=5.642, avg loss=5.533, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=3.6 seconds, 154176 images, time remaining=5.5 hours
2410: loss=5.469, avg loss=5.526, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=645.5 milliseconds, train=3.6 seconds, 154240 images, time remaining=5.5 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146366000000
2411: loss=5.988, avg loss=5.572, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=566.4 milliseconds, train=1.5 seconds, 154304 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2412: loss=5.958, avg loss=5.611, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 154368 images, time remaining=5.5 hours
2413: loss=4.868, avg loss=5.537, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=627.2 milliseconds, train=1.4 seconds, 154432 images, time remaining=5.5 hours
2414: loss=5.166, avg loss=5.500, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=447.3 milliseconds, train=1.4 seconds, 154496 images, time remaining=5.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2415: loss=5.003, avg loss=5.450, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 154560 images, time remaining=5.4 hours
2416: loss=5.034, avg loss=5.408, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=876.5 milliseconds, train=1.4 seconds, 154624 images, time remaining=5.4 hours
2417: loss=5.588, avg loss=5.426, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=928.4 milliseconds, train=1.4 seconds, 154688 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2418: loss=4.764, avg loss=5.360, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=1.4 seconds, 154752 images, time remaining=5.4 hours
2419: loss=5.331, avg loss=5.357, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=665.2 milliseconds, train=1.4 seconds, 154816 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2420: loss=5.660, avg loss=5.387, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.7 seconds, train=1.4 seconds, 154880 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2421: loss=5.234, avg loss=5.372, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=1.9 seconds, 154944 images, time remaining=5.4 hours
2422: loss=5.953, avg loss=5.430, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 155008 images, time remaining=5.4 hours
2423: loss=4.927, avg loss=5.380, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=822.8 milliseconds, train=1.9 seconds, 155072 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2424: loss=5.950, avg loss=5.437, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=1.9 seconds, 155136 images, time remaining=5.4 hours
2425: loss=5.082, avg loss=5.401, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 155200 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2426: loss=5.140, avg loss=5.375, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=1.9 seconds, 155264 images, time remaining=5.4 hours
2427: loss=4.901, avg loss=5.328, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=810.2 milliseconds, train=1.9 seconds, 155328 images, time remaining=5.4 hours
2428: loss=5.562, avg loss=5.351, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=559.3 milliseconds, train=1.9 seconds, 155392 images, time remaining=5.4 hours
2429: loss=5.146, avg loss=5.331, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 155456 images, time remaining=5.4 hours
2430: loss=4.818, avg loss=5.279, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=994.2 milliseconds, train=1.9 seconds, 155520 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145fc0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2431: loss=5.377, avg loss=5.289, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 155584 images, time remaining=5.4 hours
2432: loss=5.499, avg loss=5.310, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=1.7 seconds, 155648 images, time remaining=5.4 hours
2433: loss=4.286, avg loss=5.208, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=734.1 milliseconds, train=1.6 seconds, 155712 images, time remaining=5.4 hours
2434: loss=4.563, avg loss=5.143, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 second, train=1.7 seconds, 155776 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2435: loss=4.295, avg loss=5.059, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=4.0 seconds, train=1.6 seconds, 155840 images, time remaining=5.4 hours
2436: loss=5.398, avg loss=5.092, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=544.6 milliseconds, train=1.6 seconds, 155904 images, time remaining=5.4 hours
2437: loss=5.677, avg loss=5.151, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=722.7 milliseconds, train=1.6 seconds, 155968 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2438: loss=4.349, avg loss=5.071, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 156032 images, time remaining=5.4 hours
2439: loss=4.302, avg loss=4.994, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 156096 images, time remaining=5.4 hours
2440: loss=4.262, avg loss=4.921, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=632.6 milliseconds, train=1.6 seconds, 156160 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2441: loss=7.101, avg loss=5.139, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 156224 images, time remaining=5.4 hours
2442: loss=6.015, avg loss=5.226, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=4.1 seconds, 156288 images, time remaining=5.4 hours
2443: loss=5.775, avg loss=5.281, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=655.6 milliseconds, train=4.1 seconds, 156352 images, time remaining=5.4 hours
2444: loss=5.689, avg loss=5.322, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=869.7 milliseconds, train=4.1 seconds, 156416 images, time remaining=5.4 hours
2445: loss=5.483, avg loss=5.338, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.8 seconds, train=4.1 seconds, 156480 images, time remaining=5.4 hours
2446: loss=6.450, avg loss=5.449, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=4.1 seconds, 156544 images, time remaining=5.4 hours
2447: loss=4.441, avg loss=5.349, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=580.0 milliseconds, train=4.1 seconds, 156608 images, time remaining=5.4 hours
2448: loss=6.747, avg loss=5.488, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 156672 images, time remaining=5.4 hours
2449: loss=5.539, avg loss=5.493, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=969.0 milliseconds, train=4.1 seconds, 156736 images, time remaining=5.4 hours
2450: loss=4.905, avg loss=5.435, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 156800 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2451: loss=6.231, avg loss=5.514, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=913.2 milliseconds, train=3.9 seconds, 156864 images, time remaining=5.4 hours
2452: loss=6.518, avg loss=5.615, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 156928 images, time remaining=5.4 hours
2453: loss=4.970, avg loss=5.550, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=594.6 milliseconds, train=3.9 seconds, 156992 images, time remaining=5.4 hours
2454: loss=5.224, avg loss=5.518, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 157056 images, time remaining=5.4 hours
2455: loss=5.300, avg loss=5.496, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 157120 images, time remaining=5.4 hours
2456: loss=5.311, avg loss=5.477, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 157184 images, time remaining=5.4 hours
2457: loss=6.142, avg loss=5.544, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 157248 images, time remaining=5.4 hours
2458: loss=5.234, avg loss=5.513, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 157312 images, time remaining=5.4 hours
2459: loss=5.748, avg loss=5.536, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 157376 images, time remaining=5.4 hours
2460: loss=4.698, avg loss=5.452, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.6 seconds, train=3.9 seconds, 157440 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2461: loss=6.197, avg loss=5.527, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=762.1 milliseconds, train=5.1 seconds, 157504 images, time remaining=5.4 hours
2462: loss=5.957, avg loss=5.570, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=5.1 seconds, 157568 images, time remaining=5.4 hours
2463: loss=7.699, avg loss=5.783, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.3 seconds, train=5.1 seconds, 157632 images, time remaining=5.4 hours
2464: loss=6.037, avg loss=5.808, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=5.1 seconds, 157696 images, time remaining=5.4 hours
2465: loss=5.421, avg loss=5.769, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=5.1 seconds, 157760 images, time remaining=5.4 hours
2466: loss=4.686, avg loss=5.661, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 157824 images, time remaining=5.4 hours
2467: loss=5.348, avg loss=5.630, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 157888 images, time remaining=5.4 hours
2468: loss=5.020, avg loss=5.569, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 157952 images, time remaining=5.4 hours
2469: loss=6.338, avg loss=5.646, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 158016 images, time remaining=5.4 hours
2470: loss=6.161, avg loss=5.697, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=5.1 seconds, 158080 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2471: loss=5.286, avg loss=5.656, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=882.6 milliseconds, train=4.9 seconds, 158144 images, time remaining=5.4 hours
2472: loss=6.404, avg loss=5.731, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.9 seconds, 158208 images, time remaining=5.4 hours
2473: loss=5.893, avg loss=5.747, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=954.5 milliseconds, train=5.0 seconds, 158272 images, time remaining=5.4 hours
2474: loss=5.144, avg loss=5.687, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=5.0 seconds, 158336 images, time remaining=5.4 hours
2475: loss=5.650, avg loss=5.683, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=5.0 seconds, 158400 images, time remaining=5.4 hours
2476: loss=6.434, avg loss=5.758, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=5.0 seconds, 158464 images, time remaining=5.4 hours
2477: loss=5.710, avg loss=5.753, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=981.3 milliseconds, train=5.0 seconds, 158528 images, time remaining=5.4 hours
2478: loss=5.633, avg loss=5.741, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=874.0 milliseconds, train=5.0 seconds, 158592 images, time remaining=5.4 hours
2479: loss=5.875, avg loss=5.755, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=892.0 milliseconds, train=5.0 seconds, 158656 images, time remaining=5.4 hours
2480: loss=5.293, avg loss=5.709, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=623.8 milliseconds, train=4.9 seconds, 158720 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1463d6000000
2481: loss=5.284, avg loss=5.666, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=499.9 milliseconds, train=1.5 seconds, 158784 images, time remaining=5.4 hours
2482: loss=5.824, avg loss=5.682, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=765.1 milliseconds, train=1.5 seconds, 158848 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2483: loss=6.532, avg loss=5.767, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 158912 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2484: loss=5.661, avg loss=5.756, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 158976 images, time remaining=5.4 hours
2485: loss=5.512, avg loss=5.732, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 159040 images, time remaining=5.4 hours
2486: loss=4.657, avg loss=5.624, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 159104 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2487: loss=5.036, avg loss=5.566, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 159168 images, time remaining=5.4 hours
2488: loss=5.655, avg loss=5.575, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=993.4 milliseconds, train=1.5 seconds, 159232 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2489: loss=5.367, avg loss=5.554, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 159296 images, time remaining=5.4 hours
2490: loss=5.708, avg loss=5.569, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 159360 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2491: loss=7.677, avg loss=5.780, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=990.4 milliseconds, train=4.7 seconds, 159424 images, time remaining=5.4 hours
2492: loss=6.836, avg loss=5.886, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=729.8 milliseconds, train=4.7 seconds, 159488 images, time remaining=5.4 hours
2493: loss=7.552, avg loss=6.052, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=4.7 seconds, 159552 images, time remaining=5.4 hours
2494: loss=6.855, avg loss=6.133, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=4.7 seconds, 159616 images, time remaining=5.4 hours
2495: loss=6.371, avg loss=6.156, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=4.7 seconds, 159680 images, time remaining=5.4 hours
2496: loss=8.277, avg loss=6.368, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=4.8 seconds, 159744 images, time remaining=5.4 hours
2497: loss=7.207, avg loss=6.452, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=4.7 seconds, 159808 images, time remaining=5.4 hours
2498: loss=6.154, avg loss=6.422, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=4.7 seconds, 159872 images, time remaining=5.4 hours
2499: loss=7.078, avg loss=6.488, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.4 seconds, train=4.7 seconds, 159936 images, time remaining=5.4 hours
2500: loss=6.734, avg loss=6.513, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=4.7 seconds, 160000 images, time remaining=5.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146486000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2501: loss=8.116, avg loss=6.673, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 160064 images, time remaining=5.4 hours
2502: loss=5.848, avg loss=6.590, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=579.9 milliseconds, train=1.5 seconds, 160128 images, time remaining=5.4 hours
2503: loss=6.758, avg loss=6.607, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=780.7 milliseconds, train=1.5 seconds, 160192 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2504: loss=5.002, avg loss=6.447, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 160256 images, time remaining=5.4 hours
2505: loss=5.448, avg loss=6.347, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 160320 images, time remaining=5.4 hours
2506: loss=5.513, avg loss=6.263, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 160384 images, time remaining=5.4 hours
2507: loss=5.921, avg loss=6.229, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=730.3 milliseconds, train=1.5 seconds, 160448 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2508: loss=5.252, avg loss=6.131, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 160512 images, time remaining=5.4 hours
2509: loss=5.981, avg loss=6.116, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=625.4 milliseconds, train=1.5 seconds, 160576 images, time remaining=5.4 hours
2510: loss=5.766, avg loss=6.081, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=821.0 milliseconds, train=1.5 seconds, 160640 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14470a000000
2511: loss=9.825, avg loss=6.456, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=649.3 milliseconds, train=5.0 seconds, 160704 images, time remaining=5.4 hours
2512: loss=8.337, avg loss=6.644, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=551.1 milliseconds, train=4.9 seconds, 160768 images, time remaining=5.4 hours
2513: loss=5.775, avg loss=6.557, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.9 seconds, 160832 images, time remaining=5.4 hours
2514: loss=6.113, avg loss=6.512, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.0 seconds, train=4.9 seconds, 160896 images, time remaining=5.4 hours
2515: loss=7.925, avg loss=6.654, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=5.0 seconds, 160960 images, time remaining=5.4 hours
2516: loss=9.119, avg loss=6.900, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.9 seconds, 161024 images, time remaining=5.4 hours
2517: loss=6.835, avg loss=6.894, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=4.9 seconds, 161088 images, time remaining=5.4 hours
2518: loss=7.584, avg loss=6.963, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.9 seconds, 161152 images, time remaining=5.4 hours
2519: loss=6.242, avg loss=6.891, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.9 seconds, train=4.9 seconds, 161216 images, time remaining=5.4 hours
2520: loss=6.134, avg loss=6.815, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 161280 images, time remaining=5.4 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146000000000
2521: loss=7.157, avg loss=6.849, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=497.4 milliseconds, train=1.5 seconds, 161344 images, time remaining=5.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2522: loss=6.882, avg loss=6.852, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 161408 images, time remaining=5.4 hours
2523: loss=7.180, avg loss=6.885, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=611.2 milliseconds, train=1.5 seconds, 161472 images, time remaining=5.4 hours
2524: loss=6.004, avg loss=6.797, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=764.3 milliseconds, train=1.5 seconds, 161536 images, time remaining=5.4 hours
2525: loss=5.920, avg loss=6.709, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=932.8 milliseconds, train=1.5 seconds, 161600 images, time remaining=5.3 hours
2526: loss=6.416, avg loss=6.680, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=621.7 milliseconds, train=1.5 seconds, 161664 images, time remaining=5.3 hours
2527: loss=5.977, avg loss=6.610, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=779.3 milliseconds, train=1.5 seconds, 161728 images, time remaining=5.3 hours
2528: loss=6.891, avg loss=6.638, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=600.8 milliseconds, train=1.5 seconds, 161792 images, time remaining=5.3 hours
2529: loss=6.287, avg loss=6.603, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=736.7 milliseconds, train=1.5 seconds, 161856 images, time remaining=5.3 hours
2530: loss=5.745, avg loss=6.517, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=975.0 milliseconds, train=1.5 seconds, 161920 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2531: loss=9.786, avg loss=6.844, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=603.7 milliseconds, train=4.2 seconds, 161984 images, time remaining=5.3 hours
2532: loss=9.026, avg loss=7.062, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=747.2 milliseconds, train=4.1 seconds, 162048 images, time remaining=5.3 hours
2533: loss=6.520, avg loss=7.008, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 162112 images, time remaining=5.3 hours
2534: loss=5.836, avg loss=6.891, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=893.6 milliseconds, train=4.1 seconds, 162176 images, time remaining=5.3 hours
2535: loss=5.631, avg loss=6.765, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=620.5 milliseconds, train=4.1 seconds, 162240 images, time remaining=5.3 hours
2536: loss=5.767, avg loss=6.665, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=721.8 milliseconds, train=4.1 seconds, 162304 images, time remaining=5.3 hours
2537: loss=7.219, avg loss=6.720, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=988.5 milliseconds, train=4.1 seconds, 162368 images, time remaining=5.3 hours
2538: loss=6.820, avg loss=6.730, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 162432 images, time remaining=5.3 hours
2539: loss=6.990, avg loss=6.756, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.2 seconds, train=4.1 seconds, 162496 images, time remaining=5.3 hours
2540: loss=7.101, avg loss=6.791, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=448.1 milliseconds, train=4.1 seconds, 162560 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2541: loss=7.026, avg loss=6.814, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=516.4 milliseconds, train=1.9 seconds, 162624 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2542: loss=6.783, avg loss=6.811, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 162688 images, time remaining=5.3 hours
2543: loss=8.078, avg loss=6.938, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=928.2 milliseconds, train=1.9 seconds, 162752 images, time remaining=5.3 hours
2544: loss=5.941, avg loss=6.838, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=749.7 milliseconds, train=1.9 seconds, 162816 images, time remaining=5.3 hours
2545: loss=6.806, avg loss=6.835, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=1.9 seconds, 162880 images, time remaining=5.3 hours
2546: loss=6.015, avg loss=6.753, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=1.9 seconds, 162944 images, time remaining=5.3 hours
2547: loss=5.583, avg loss=6.636, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 163008 images, time remaining=5.3 hours
2548: loss=6.883, avg loss=6.661, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=651.6 milliseconds, train=1.9 seconds, 163072 images, time remaining=5.3 hours
2549: loss=6.671, avg loss=6.662, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 163136 images, time remaining=5.3 hours
2550: loss=7.083, avg loss=6.704, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=622.8 milliseconds, train=1.9 seconds, 163200 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2551: loss=9.538, avg loss=6.987, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 163264 images, time remaining=5.3 hours
2552: loss=6.513, avg loss=6.940, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=750.2 milliseconds, train=4.5 seconds, 163328 images, time remaining=5.3 hours
2553: loss=6.859, avg loss=6.932, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 163392 images, time remaining=5.3 hours
2554: loss=5.832, avg loss=6.822, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=810.8 milliseconds, train=4.5 seconds, 163456 images, time remaining=5.3 hours
2555: loss=6.494, avg loss=6.789, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=994.7 milliseconds, train=4.5 seconds, 163520 images, time remaining=5.3 hours
2556: loss=7.027, avg loss=6.813, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=4.5 seconds, 163584 images, time remaining=5.3 hours
2557: loss=6.358, avg loss=6.767, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=4.5 seconds, 163648 images, time remaining=5.3 hours
2558: loss=7.327, avg loss=6.823, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 163712 images, time remaining=5.3 hours
2559: loss=5.671, avg loss=6.708, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 163776 images, time remaining=5.3 hours
2560: loss=7.334, avg loss=6.771, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 163840 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2561: loss=6.680, avg loss=6.762, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=546.1 milliseconds, train=4.9 seconds, 163904 images, time remaining=5.3 hours
2562: loss=6.437, avg loss=6.729, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=5.0 seconds, 163968 images, time remaining=5.3 hours
2563: loss=6.042, avg loss=6.660, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=4.9 seconds, 164032 images, time remaining=5.3 hours
2564: loss=6.611, avg loss=6.656, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.1 seconds, train=5.0 seconds, 164096 images, time remaining=5.3 hours
2565: loss=6.032, avg loss=6.593, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.9 seconds, train=4.9 seconds, 164160 images, time remaining=5.3 hours
2566: loss=6.047, avg loss=6.539, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=4.9 seconds, 164224 images, time remaining=5.3 hours
2567: loss=6.137, avg loss=6.498, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.6 seconds, train=4.9 seconds, 164288 images, time remaining=5.3 hours
2568: loss=7.008, avg loss=6.549, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=665.2 milliseconds, train=4.9 seconds, 164352 images, time remaining=5.3 hours
2569: loss=6.153, avg loss=6.510, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=4.9 seconds, 164416 images, time remaining=5.3 hours
2570: loss=6.876, avg loss=6.546, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=603.4 milliseconds, train=5.0 seconds, 164480 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14646c000000
2571: loss=7.676, avg loss=6.659, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=843.1 milliseconds, train=1.2 seconds, 164544 images, time remaining=5.3 hours
2572: loss=6.775, avg loss=6.671, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=622.4 milliseconds, train=1.2 seconds, 164608 images, time remaining=5.3 hours
2573: loss=6.222, avg loss=6.626, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=821.5 milliseconds, train=1.2 seconds, 164672 images, time remaining=5.3 hours
2574: loss=5.626, avg loss=6.526, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=484.2 milliseconds, train=1.2 seconds, 164736 images, time remaining=5.3 hours
2575: loss=6.528, avg loss=6.526, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=996.1 milliseconds, train=1.2 seconds, 164800 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2576: loss=5.372, avg loss=6.411, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 164864 images, time remaining=5.3 hours
2577: loss=6.048, avg loss=6.375, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=823.2 milliseconds, train=1.2 seconds, 164928 images, time remaining=5.3 hours
2578: loss=6.177, avg loss=6.355, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=845.9 milliseconds, train=1.2 seconds, 164992 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2579: loss=6.150, avg loss=6.334, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.9 seconds, train=1.2 seconds, 165056 images, time remaining=5.3 hours
2580: loss=6.649, avg loss=6.366, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=576.0 milliseconds, train=1.2 seconds, 165120 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2581: loss=5.699, avg loss=6.299, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 165184 images, time remaining=5.3 hours
2582: loss=6.047, avg loss=6.274, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=595.3 milliseconds, train=1.5 seconds, 165248 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2583: loss=5.616, avg loss=6.208, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 165312 images, time remaining=5.3 hours
2584: loss=6.520, avg loss=6.239, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=955.3 milliseconds, train=1.5 seconds, 165376 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2585: loss=6.124, avg loss=6.228, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 165440 images, time remaining=5.3 hours
2586: loss=5.460, avg loss=6.151, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=945.5 milliseconds, train=1.5 seconds, 165504 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2587: loss=5.470, avg loss=6.083, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 165568 images, time remaining=5.3 hours
2588: loss=5.970, avg loss=6.072, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=634.2 milliseconds, train=1.6 seconds, 165632 images, time remaining=5.3 hours
2589: loss=5.245, avg loss=5.989, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=453.4 milliseconds, train=1.5 seconds, 165696 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2590: loss=4.988, avg loss=5.889, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 165760 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2591: loss=5.617, avg loss=5.862, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 165824 images, time remaining=5.3 hours
2592: loss=6.459, avg loss=5.921, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 165888 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2593: loss=5.381, avg loss=5.867, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.8 seconds, train=2.0 seconds, 165952 images, time remaining=5.3 hours
2594: loss=5.620, avg loss=5.843, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=962.6 milliseconds, train=2.0 seconds, 166016 images, time remaining=5.3 hours
2595: loss=5.062, avg loss=5.765, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=599.8 milliseconds, train=2.0 seconds, 166080 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2596: loss=5.880, avg loss=5.776, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 166144 images, time remaining=5.3 hours
2597: loss=5.312, avg loss=5.730, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 166208 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2598: loss=4.200, avg loss=5.577, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.7 seconds, train=2.0 seconds, 166272 images, time remaining=5.3 hours
2599: loss=5.097, avg loss=5.529, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=899.5 milliseconds, train=2.0 seconds, 166336 images, time remaining=5.3 hours
2600: loss=4.789, avg loss=5.455, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 166400 images, time remaining=5.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2601: loss=4.968, avg loss=5.406, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=614.1 milliseconds, train=3.3 seconds, 166464 images, time remaining=5.3 hours
2602: loss=5.748, avg loss=5.440, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 166528 images, time remaining=5.3 hours
2603: loss=5.540, avg loss=5.450, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 166592 images, time remaining=5.3 hours
2604: loss=4.574, avg loss=5.363, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 166656 images, time remaining=5.3 hours
2605: loss=4.863, avg loss=5.313, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=797.1 milliseconds, train=3.3 seconds, 166720 images, time remaining=5.3 hours
2606: loss=5.364, avg loss=5.318, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 166784 images, time remaining=5.3 hours
2607: loss=4.756, avg loss=5.262, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=724.8 milliseconds, train=3.3 seconds, 166848 images, time remaining=5.3 hours
2608: loss=4.821, avg loss=5.218, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=868.6 milliseconds, train=3.3 seconds, 166912 images, time remaining=5.3 hours
2609: loss=5.459, avg loss=5.242, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 166976 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2610: loss=5.119, avg loss=5.229, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=3.8 seconds, train=3.3 seconds, 167040 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2611: loss=4.753, avg loss=5.182, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 167104 images, time remaining=5.3 hours
2612: loss=4.564, avg loss=5.120, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=539.0 milliseconds, train=2.1 seconds, 167168 images, time remaining=5.3 hours
2613: loss=5.509, avg loss=5.159, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=821.1 milliseconds, train=2.1 seconds, 167232 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2614: loss=4.857, avg loss=5.129, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 167296 images, time remaining=5.2 hours
2615: loss=4.543, avg loss=5.070, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=867.1 milliseconds, train=2.1 seconds, 167360 images, time remaining=5.2 hours
2616: loss=4.575, avg loss=5.021, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 167424 images, time remaining=5.2 hours
2617: loss=4.991, avg loss=5.018, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=786.6 milliseconds, train=2.1 seconds, 167488 images, time remaining=5.2 hours
2618: loss=4.568, avg loss=4.973, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=802.4 milliseconds, train=2.1 seconds, 167552 images, time remaining=5.2 hours
2619: loss=4.289, avg loss=4.904, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 167616 images, time remaining=5.2 hours
2620: loss=5.193, avg loss=4.933, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=807.6 milliseconds, train=2.1 seconds, 167680 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2621: loss=6.348, avg loss=5.075, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=722.8 milliseconds, train=4.5 seconds, 167744 images, time remaining=5.2 hours
2622: loss=5.995, avg loss=5.167, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.5 seconds, train=4.5 seconds, 167808 images, time remaining=5.2 hours
2623: loss=5.231, avg loss=5.173, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 167872 images, time remaining=5.2 hours
2624: loss=5.770, avg loss=5.233, last=73.36%, best=73.36%, next=2624, rate=0.00130000, load 64=2.4 seconds, train=4.5 seconds, 167936 images, time remaining=5.2 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=130090, unique_truth_count=57264
rank=0 of ranks=130090rank=100 of ranks=130090rank=200 of ranks=130090rank=300 of ranks=130090rank=400 of ranks=130090rank=500 of ranks=130090rank=600 of ranks=130090rank=700 of ranks=130090rank=800 of ranks=130090rank=900 of ranks=130090rank=1000 of ranks=130090rank=1100 of ranks=130090rank=1200 of ranks=130090rank=1300 of ranks=130090rank=1400 of ranks=130090rank=1500 of ranks=130090rank=1600 of ranks=130090rank=1700 of ranks=130090rank=1800 of ranks=130090rank=1900 of ranks=130090rank=2000 of ranks=130090rank=2100 of ranks=130090rank=2200 of ranks=130090rank=2300 of ranks=130090rank=2400 of ranks=130090rank=2500 of ranks=130090rank=2600 of ranks=130090rank=2700 of ranks=130090rank=2800 of ranks=130090rank=2900 of ranks=130090rank=3000 of ranks=130090rank=3100 of ranks=130090rank=3200 of ranks=130090rank=3300 of ranks=130090rank=3400 of ranks=130090rank=3500 of ranks=130090rank=3600 of ranks=130090rank=3700 of ranks=130090rank=3800 of ranks=130090rank=3900 of ranks=130090rank=4000 of ranks=130090rank=4100 of ranks=130090rank=4200 of ranks=130090rank=4300 of ranks=130090rank=4400 of ranks=130090rank=4500 of ranks=130090rank=4600 of ranks=130090rank=4700 of ranks=130090rank=4800 of ranks=130090rank=4900 of ranks=130090rank=5000 of ranks=130090rank=5100 of ranks=130090rank=5200 of ranks=130090rank=5300 of ranks=130090rank=5400 of ranks=130090rank=5500 of ranks=130090rank=5600 of ranks=130090rank=5700 of ranks=130090rank=5800 of ranks=130090rank=5900 of ranks=130090rank=6000 of ranks=130090rank=6100 of ranks=130090rank=6200 of ranks=130090rank=6300 of ranks=130090rank=6400 of ranks=130090rank=6500 of ranks=130090rank=6600 of ranks=130090rank=6700 of ranks=130090rank=6800 of ranks=130090rank=6900 of ranks=130090rank=7000 of ranks=130090rank=7100 of ranks=130090rank=7200 of ranks=130090rank=7300 of ranks=130090rank=7400 of ranks=130090rank=7500 of ranks=130090rank=7600 of ranks=130090rank=7700 of ranks=130090rank=7800 of ranks=130090rank=7900 of ranks=130090rank=8000 of ranks=130090rank=8100 of ranks=130090rank=8200 of ranks=130090rank=8300 of ranks=130090rank=8400 of ranks=130090rank=8500 of ranks=130090rank=8600 of ranks=130090rank=8700 of ranks=130090rank=8800 of ranks=130090rank=8900 of ranks=130090rank=9000 of ranks=130090rank=9100 of ranks=130090rank=9200 of ranks=130090rank=9300 of ranks=130090rank=9400 of ranks=130090rank=9500 of ranks=130090rank=9600 of ranks=130090rank=9700 of ranks=130090rank=9800 of ranks=130090rank=9900 of ranks=130090rank=10000 of ranks=130090rank=10100 of ranks=130090rank=10200 of ranks=130090rank=10300 of ranks=130090rank=10400 of ranks=130090rank=10500 of ranks=130090rank=10600 of ranks=130090rank=10700 of ranks=130090rank=10800 of ranks=130090rank=10900 of ranks=130090rank=11000 of ranks=130090rank=11100 of ranks=130090rank=11200 of ranks=130090rank=11300 of ranks=130090rank=11400 of ranks=130090rank=11500 of ranks=130090rank=11600 of ranks=130090rank=11700 of ranks=130090rank=11800 of ranks=130090rank=11900 of ranks=130090rank=12000 of ranks=130090rank=12100 of ranks=130090rank=12200 of ranks=130090rank=12300 of ranks=130090rank=12400 of ranks=130090rank=12500 of ranks=130090rank=12600 of ranks=130090rank=12700 of ranks=130090rank=12800 of ranks=130090rank=12900 of ranks=130090rank=13000 of ranks=130090rank=13100 of ranks=130090rank=13200 of ranks=130090rank=13300 of ranks=130090rank=13400 of ranks=130090rank=13500 of ranks=130090rank=13600 of ranks=130090rank=13700 of ranks=130090rank=13800 of ranks=130090rank=13900 of ranks=130090rank=14000 of ranks=130090rank=14100 of ranks=130090rank=14200 of ranks=130090rank=14300 of ranks=130090rank=14400 of ranks=130090rank=14500 of ranks=130090rank=14600 of ranks=130090rank=14700 of ranks=130090rank=14800 of ranks=130090rank=14900 of ranks=130090rank=15000 of ranks=130090rank=15100 of ranks=130090rank=15200 of ranks=130090rank=15300 of ranks=130090rank=15400 of ranks=130090rank=15500 of ranks=130090rank=15600 of ranks=130090rank=15700 of ranks=130090rank=15800 of ranks=130090rank=15900 of ranks=130090rank=16000 of ranks=130090rank=16100 of ranks=130090rank=16200 of ranks=130090rank=16300 of ranks=130090rank=16400 of ranks=130090rank=16500 of ranks=130090rank=16600 of ranks=130090rank=16700 of ranks=130090rank=16800 of ranks=130090rank=16900 of ranks=130090rank=17000 of ranks=130090rank=17100 of ranks=130090rank=17200 of ranks=130090rank=17300 of ranks=130090rank=17400 of ranks=130090rank=17500 of ranks=130090rank=17600 of ranks=130090rank=17700 of ranks=130090rank=17800 of ranks=130090rank=17900 of ranks=130090rank=18000 of ranks=130090rank=18100 of ranks=130090rank=18200 of ranks=130090rank=18300 of ranks=130090rank=18400 of ranks=130090rank=18500 of ranks=130090rank=18600 of ranks=130090rank=18700 of ranks=130090rank=18800 of ranks=130090rank=18900 of ranks=130090rank=19000 of ranks=130090rank=19100 of ranks=130090rank=19200 of ranks=130090rank=19300 of ranks=130090rank=19400 of ranks=130090rank=19500 of ranks=130090rank=19600 of ranks=130090rank=19700 of ranks=130090rank=19800 of ranks=130090rank=19900 of ranks=130090rank=20000 of ranks=130090rank=20100 of ranks=130090rank=20200 of ranks=130090rank=20300 of ranks=130090rank=20400 of ranks=130090rank=20500 of ranks=130090rank=20600 of ranks=130090rank=20700 of ranks=130090rank=20800 of ranks=130090rank=20900 of ranks=130090rank=21000 of ranks=130090rank=21100 of ranks=130090rank=21200 of ranks=130090rank=21300 of ranks=130090rank=21400 of ranks=130090rank=21500 of ranks=130090rank=21600 of ranks=130090rank=21700 of ranks=130090rank=21800 of ranks=130090rank=21900 of ranks=130090rank=22000 of ranks=130090rank=22100 of ranks=130090rank=22200 of ranks=130090rank=22300 of ranks=130090rank=22400 of ranks=130090rank=22500 of ranks=130090rank=22600 of ranks=130090rank=22700 of ranks=130090rank=22800 of ranks=130090rank=22900 of ranks=130090rank=23000 of ranks=130090rank=23100 of ranks=130090rank=23200 of ranks=130090rank=23300 of ranks=130090rank=23400 of ranks=130090rank=23500 of ranks=130090rank=23600 of ranks=130090rank=23700 of ranks=130090rank=23800 of ranks=130090rank=23900 of ranks=130090rank=24000 of ranks=130090rank=24100 of ranks=130090rank=24200 of ranks=130090rank=24300 of ranks=130090rank=24400 of ranks=130090rank=24500 of ranks=130090rank=24600 of ranks=130090rank=24700 of ranks=130090rank=24800 of ranks=130090rank=24900 of ranks=130090rank=25000 of ranks=130090rank=25100 of ranks=130090rank=25200 of ranks=130090rank=25300 of ranks=130090rank=25400 of ranks=130090rank=25500 of ranks=130090rank=25600 of ranks=130090rank=25700 of ranks=130090rank=25800 of ranks=130090rank=25900 of ranks=130090rank=26000 of ranks=130090rank=26100 of ranks=130090rank=26200 of ranks=130090rank=26300 of ranks=130090rank=26400 of ranks=130090rank=26500 of ranks=130090rank=26600 of ranks=130090rank=26700 of ranks=130090rank=26800 of ranks=130090rank=26900 of ranks=130090rank=27000 of ranks=130090rank=27100 of ranks=130090rank=27200 of ranks=130090rank=27300 of ranks=130090rank=27400 of ranks=130090rank=27500 of ranks=130090rank=27600 of ranks=130090rank=27700 of ranks=130090rank=27800 of ranks=130090rank=27900 of ranks=130090rank=28000 of ranks=130090rank=28100 of ranks=130090rank=28200 of ranks=130090rank=28300 of ranks=130090rank=28400 of ranks=130090rank=28500 of ranks=130090rank=28600 of ranks=130090rank=28700 of ranks=130090rank=28800 of ranks=130090rank=28900 of ranks=130090rank=29000 of ranks=130090rank=29100 of ranks=130090rank=29200 of ranks=130090rank=29300 of ranks=130090rank=29400 of ranks=130090rank=29500 of ranks=130090rank=29600 of ranks=130090rank=29700 of ranks=130090rank=29800 of ranks=130090rank=29900 of ranks=130090rank=30000 of ranks=130090rank=30100 of ranks=130090rank=30200 of ranks=130090rank=30300 of ranks=130090rank=30400 of ranks=130090rank=30500 of ranks=130090rank=30600 of ranks=130090rank=30700 of ranks=130090rank=30800 of ranks=130090rank=30900 of ranks=130090rank=31000 of ranks=130090rank=31100 of ranks=130090rank=31200 of ranks=130090rank=31300 of ranks=130090rank=31400 of ranks=130090rank=31500 of ranks=130090rank=31600 of ranks=130090rank=31700 of ranks=130090rank=31800 of ranks=130090rank=31900 of ranks=130090rank=32000 of ranks=130090rank=32100 of ranks=130090rank=32200 of ranks=130090rank=32300 of ranks=130090rank=32400 of ranks=130090rank=32500 of ranks=130090rank=32600 of ranks=130090rank=32700 of ranks=130090rank=32800 of ranks=130090rank=32900 of ranks=130090rank=33000 of ranks=130090rank=33100 of ranks=130090rank=33200 of ranks=130090rank=33300 of ranks=130090rank=33400 of ranks=130090rank=33500 of ranks=130090rank=33600 of ranks=130090rank=33700 of ranks=130090rank=33800 of ranks=130090rank=33900 of ranks=130090rank=34000 of ranks=130090rank=34100 of ranks=130090rank=34200 of ranks=130090rank=34300 of ranks=130090rank=34400 of ranks=130090rank=34500 of ranks=130090rank=34600 of ranks=130090rank=34700 of ranks=130090rank=34800 of ranks=130090rank=34900 of ranks=130090rank=35000 of ranks=130090rank=35100 of ranks=130090rank=35200 of ranks=130090rank=35300 of ranks=130090rank=35400 of ranks=130090rank=35500 of ranks=130090rank=35600 of ranks=130090rank=35700 of ranks=130090rank=35800 of ranks=130090rank=35900 of ranks=130090rank=36000 of ranks=130090rank=36100 of ranks=130090rank=36200 of ranks=130090rank=36300 of ranks=130090rank=36400 of ranks=130090rank=36500 of ranks=130090rank=36600 of ranks=130090rank=36700 of ranks=130090rank=36800 of ranks=130090rank=36900 of ranks=130090rank=37000 of ranks=130090rank=37100 of ranks=130090rank=37200 of ranks=130090rank=37300 of ranks=130090rank=37400 of ranks=130090rank=37500 of ranks=130090rank=37600 of ranks=130090rank=37700 of ranks=130090rank=37800 of ranks=130090rank=37900 of ranks=130090rank=38000 of ranks=130090rank=38100 of ranks=130090rank=38200 of ranks=130090rank=38300 of ranks=130090rank=38400 of ranks=130090rank=38500 of ranks=130090rank=38600 of ranks=130090rank=38700 of ranks=130090rank=38800 of ranks=130090rank=38900 of ranks=130090rank=39000 of ranks=130090rank=39100 of ranks=130090rank=39200 of ranks=130090rank=39300 of ranks=130090rank=39400 of ranks=130090rank=39500 of ranks=130090rank=39600 of ranks=130090rank=39700 of ranks=130090rank=39800 of ranks=130090rank=39900 of ranks=130090rank=40000 of ranks=130090rank=40100 of ranks=130090rank=40200 of ranks=130090rank=40300 of ranks=130090rank=40400 of ranks=130090rank=40500 of ranks=130090rank=40600 of ranks=130090rank=40700 of ranks=130090rank=40800 of ranks=130090rank=40900 of ranks=130090rank=41000 of ranks=130090rank=41100 of ranks=130090rank=41200 of ranks=130090rank=41300 of ranks=130090rank=41400 of ranks=130090rank=41500 of ranks=130090rank=41600 of ranks=130090rank=41700 of ranks=130090rank=41800 of ranks=130090rank=41900 of ranks=130090rank=42000 of ranks=130090rank=42100 of ranks=130090rank=42200 of ranks=130090rank=42300 of ranks=130090rank=42400 of ranks=130090rank=42500 of ranks=130090rank=42600 of ranks=130090rank=42700 of ranks=130090rank=42800 of ranks=130090rank=42900 of ranks=130090rank=43000 of ranks=130090rank=43100 of ranks=130090rank=43200 of ranks=130090rank=43300 of ranks=130090rank=43400 of ranks=130090rank=43500 of ranks=130090rank=43600 of ranks=130090rank=43700 of ranks=130090rank=43800 of ranks=130090rank=43900 of ranks=130090rank=44000 of ranks=130090rank=44100 of ranks=130090rank=44200 of ranks=130090rank=44300 of ranks=130090rank=44400 of ranks=130090rank=44500 of ranks=130090rank=44600 of ranks=130090rank=44700 of ranks=130090rank=44800 of ranks=130090rank=44900 of ranks=130090rank=45000 of ranks=130090rank=45100 of ranks=130090rank=45200 of ranks=130090rank=45300 of ranks=130090rank=45400 of ranks=130090rank=45500 of ranks=130090rank=45600 of ranks=130090rank=45700 of ranks=130090rank=45800 of ranks=130090rank=45900 of ranks=130090rank=46000 of ranks=130090rank=46100 of ranks=130090rank=46200 of ranks=130090rank=46300 of ranks=130090rank=46400 of ranks=130090rank=46500 of ranks=130090rank=46600 of ranks=130090rank=46700 of ranks=130090rank=46800 of ranks=130090rank=46900 of ranks=130090rank=47000 of ranks=130090rank=47100 of ranks=130090rank=47200 of ranks=130090rank=47300 of ranks=130090rank=47400 of ranks=130090rank=47500 of ranks=130090rank=47600 of ranks=130090rank=47700 of ranks=130090rank=47800 of ranks=130090rank=47900 of ranks=130090rank=48000 of ranks=130090rank=48100 of ranks=130090rank=48200 of ranks=130090rank=48300 of ranks=130090rank=48400 of ranks=130090rank=48500 of ranks=130090rank=48600 of ranks=130090rank=48700 of ranks=130090rank=48800 of ranks=130090rank=48900 of ranks=130090rank=49000 of ranks=130090rank=49100 of ranks=130090rank=49200 of ranks=130090rank=49300 of ranks=130090rank=49400 of ranks=130090rank=49500 of ranks=130090rank=49600 of ranks=130090rank=49700 of ranks=130090rank=49800 of ranks=130090rank=49900 of ranks=130090rank=50000 of ranks=130090rank=50100 of ranks=130090rank=50200 of ranks=130090rank=50300 of ranks=130090rank=50400 of ranks=130090rank=50500 of ranks=130090rank=50600 of ranks=130090rank=50700 of ranks=130090rank=50800 of ranks=130090rank=50900 of ranks=130090rank=51000 of ranks=130090rank=51100 of ranks=130090rank=51200 of ranks=130090rank=51300 of ranks=130090rank=51400 of ranks=130090rank=51500 of ranks=130090rank=51600 of ranks=130090rank=51700 of ranks=130090rank=51800 of ranks=130090rank=51900 of ranks=130090rank=52000 of ranks=130090rank=52100 of ranks=130090rank=52200 of ranks=130090rank=52300 of ranks=130090rank=52400 of ranks=130090rank=52500 of ranks=130090rank=52600 of ranks=130090rank=52700 of ranks=130090rank=52800 of ranks=130090rank=52900 of ranks=130090rank=53000 of ranks=130090rank=53100 of ranks=130090rank=53200 of ranks=130090rank=53300 of ranks=130090rank=53400 of ranks=130090rank=53500 of ranks=130090rank=53600 of ranks=130090rank=53700 of ranks=130090rank=53800 of ranks=130090rank=53900 of ranks=130090rank=54000 of ranks=130090rank=54100 of ranks=130090rank=54200 of ranks=130090rank=54300 of ranks=130090rank=54400 of ranks=130090rank=54500 of ranks=130090rank=54600 of ranks=130090rank=54700 of ranks=130090rank=54800 of ranks=130090rank=54900 of ranks=130090rank=55000 of ranks=130090rank=55100 of ranks=130090rank=55200 of ranks=130090rank=55300 of ranks=130090rank=55400 of ranks=130090rank=55500 of ranks=130090rank=55600 of ranks=130090rank=55700 of ranks=130090rank=55800 of ranks=130090rank=55900 of ranks=130090rank=56000 of ranks=130090rank=56100 of ranks=130090rank=56200 of ranks=130090rank=56300 of ranks=130090rank=56400 of ranks=130090rank=56500 of ranks=130090rank=56600 of ranks=130090rank=56700 of ranks=130090rank=56800 of ranks=130090rank=56900 of ranks=130090rank=57000 of ranks=130090rank=57100 of ranks=130090rank=57200 of ranks=130090rank=57300 of ranks=130090rank=57400 of ranks=130090rank=57500 of ranks=130090rank=57600 of ranks=130090rank=57700 of ranks=130090rank=57800 of ranks=130090rank=57900 of ranks=130090rank=58000 of ranks=130090rank=58100 of ranks=130090rank=58200 of ranks=130090rank=58300 of ranks=130090rank=58400 of ranks=130090rank=58500 of ranks=130090rank=58600 of ranks=130090rank=58700 of ranks=130090rank=58800 of ranks=130090rank=58900 of ranks=130090rank=59000 of ranks=130090rank=59100 of ranks=130090rank=59200 of ranks=130090rank=59300 of ranks=130090rank=59400 of ranks=130090rank=59500 of ranks=130090rank=59600 of ranks=130090rank=59700 of ranks=130090rank=59800 of ranks=130090rank=59900 of ranks=130090rank=60000 of ranks=130090rank=60100 of ranks=130090rank=60200 of ranks=130090rank=60300 of ranks=130090rank=60400 of ranks=130090rank=60500 of ranks=130090rank=60600 of ranks=130090rank=60700 of ranks=130090rank=60800 of ranks=130090rank=60900 of ranks=130090rank=61000 of ranks=130090rank=61100 of ranks=130090rank=61200 of ranks=130090rank=61300 of ranks=130090rank=61400 of ranks=130090rank=61500 of ranks=130090rank=61600 of ranks=130090rank=61700 of ranks=130090rank=61800 of ranks=130090rank=61900 of ranks=130090rank=62000 of ranks=130090rank=62100 of ranks=130090rank=62200 of ranks=130090rank=62300 of ranks=130090rank=62400 of ranks=130090rank=62500 of ranks=130090rank=62600 of ranks=130090rank=62700 of ranks=130090rank=62800 of ranks=130090rank=62900 of ranks=130090rank=63000 of ranks=130090rank=63100 of ranks=130090rank=63200 of ranks=130090rank=63300 of ranks=130090rank=63400 of ranks=130090rank=63500 of ranks=130090rank=63600 of ranks=130090rank=63700 of ranks=130090rank=63800 of ranks=130090rank=63900 of ranks=130090rank=64000 of ranks=130090rank=64100 of ranks=130090rank=64200 of ranks=130090rank=64300 of ranks=130090rank=64400 of ranks=130090rank=64500 of ranks=130090rank=64600 of ranks=130090rank=64700 of ranks=130090rank=64800 of ranks=130090rank=64900 of ranks=130090rank=65000 of ranks=130090rank=65100 of ranks=130090rank=65200 of ranks=130090rank=65300 of ranks=130090rank=65400 of ranks=130090rank=65500 of ranks=130090rank=65600 of ranks=130090rank=65700 of ranks=130090rank=65800 of ranks=130090rank=65900 of ranks=130090rank=66000 of ranks=130090rank=66100 of ranks=130090rank=66200 of ranks=130090rank=66300 of ranks=130090rank=66400 of ranks=130090rank=66500 of ranks=130090rank=66600 of ranks=130090rank=66700 of ranks=130090rank=66800 of ranks=130090rank=66900 of ranks=130090rank=67000 of ranks=130090rank=67100 of ranks=130090rank=67200 of ranks=130090rank=67300 of ranks=130090rank=67400 of ranks=130090rank=67500 of ranks=130090rank=67600 of ranks=130090rank=67700 of ranks=130090rank=67800 of ranks=130090rank=67900 of ranks=130090rank=68000 of ranks=130090rank=68100 of ranks=130090rank=68200 of ranks=130090rank=68300 of ranks=130090rank=68400 of ranks=130090rank=68500 of ranks=130090rank=68600 of ranks=130090rank=68700 of ranks=130090rank=68800 of ranks=130090rank=68900 of ranks=130090rank=69000 of ranks=130090rank=69100 of ranks=130090rank=69200 of ranks=130090rank=69300 of ranks=130090rank=69400 of ranks=130090rank=69500 of ranks=130090rank=69600 of ranks=130090rank=69700 of ranks=130090rank=69800 of ranks=130090rank=69900 of ranks=130090rank=70000 of ranks=130090rank=70100 of ranks=130090rank=70200 of ranks=130090rank=70300 of ranks=130090rank=70400 of ranks=130090rank=70500 of ranks=130090rank=70600 of ranks=130090rank=70700 of ranks=130090rank=70800 of ranks=130090rank=70900 of ranks=130090rank=71000 of ranks=130090rank=71100 of ranks=130090rank=71200 of ranks=130090rank=71300 of ranks=130090rank=71400 of ranks=130090rank=71500 of ranks=130090rank=71600 of ranks=130090rank=71700 of ranks=130090rank=71800 of ranks=130090rank=71900 of ranks=130090rank=72000 of ranks=130090rank=72100 of ranks=130090rank=72200 of ranks=130090rank=72300 of ranks=130090rank=72400 of ranks=130090rank=72500 of ranks=130090rank=72600 of ranks=130090rank=72700 of ranks=130090rank=72800 of ranks=130090rank=72900 of ranks=130090rank=73000 of ranks=130090rank=73100 of ranks=130090rank=73200 of ranks=130090rank=73300 of ranks=130090rank=73400 of ranks=130090rank=73500 of ranks=130090rank=73600 of ranks=130090rank=73700 of ranks=130090rank=73800 of ranks=130090rank=73900 of ranks=130090rank=74000 of ranks=130090rank=74100 of ranks=130090rank=74200 of ranks=130090rank=74300 of ranks=130090rank=74400 of ranks=130090rank=74500 of ranks=130090rank=74600 of ranks=130090rank=74700 of ranks=130090rank=74800 of ranks=130090rank=74900 of ranks=130090rank=75000 of ranks=130090rank=75100 of ranks=130090rank=75200 of ranks=130090rank=75300 of ranks=130090rank=75400 of ranks=130090rank=75500 of ranks=130090rank=75600 of ranks=130090rank=75700 of ranks=130090rank=75800 of ranks=130090rank=75900 of ranks=130090rank=76000 of ranks=130090rank=76100 of ranks=130090rank=76200 of ranks=130090rank=76300 of ranks=130090rank=76400 of ranks=130090rank=76500 of ranks=130090rank=76600 of ranks=130090rank=76700 of ranks=130090rank=76800 of ranks=130090rank=76900 of ranks=130090rank=77000 of ranks=130090rank=77100 of ranks=130090rank=77200 of ranks=130090rank=77300 of ranks=130090rank=77400 of ranks=130090rank=77500 of ranks=130090rank=77600 of ranks=130090rank=77700 of ranks=130090rank=77800 of ranks=130090rank=77900 of ranks=130090rank=78000 of ranks=130090rank=78100 of ranks=130090rank=78200 of ranks=130090rank=78300 of ranks=130090rank=78400 of ranks=130090rank=78500 of ranks=130090rank=78600 of ranks=130090rank=78700 of ranks=130090rank=78800 of ranks=130090rank=78900 of ranks=130090rank=79000 of ranks=130090rank=79100 of ranks=130090rank=79200 of ranks=130090rank=79300 of ranks=130090rank=79400 of ranks=130090rank=79500 of ranks=130090rank=79600 of ranks=130090rank=79700 of ranks=130090rank=79800 of ranks=130090rank=79900 of ranks=130090rank=80000 of ranks=130090rank=80100 of ranks=130090rank=80200 of ranks=130090rank=80300 of ranks=130090rank=80400 of ranks=130090rank=80500 of ranks=130090rank=80600 of ranks=130090rank=80700 of ranks=130090rank=80800 of ranks=130090rank=80900 of ranks=130090rank=81000 of ranks=130090rank=81100 of ranks=130090rank=81200 of ranks=130090rank=81300 of ranks=130090rank=81400 of ranks=130090rank=81500 of ranks=130090rank=81600 of ranks=130090rank=81700 of ranks=130090rank=81800 of ranks=130090rank=81900 of ranks=130090rank=82000 of ranks=130090rank=82100 of ranks=130090rank=82200 of ranks=130090rank=82300 of ranks=130090rank=82400 of ranks=130090rank=82500 of ranks=130090rank=82600 of ranks=130090rank=82700 of ranks=130090rank=82800 of ranks=130090rank=82900 of ranks=130090rank=83000 of ranks=130090rank=83100 of ranks=130090rank=83200 of ranks=130090rank=83300 of ranks=130090rank=83400 of ranks=130090rank=83500 of ranks=130090rank=83600 of ranks=130090rank=83700 of ranks=130090rank=83800 of ranks=130090rank=83900 of ranks=130090rank=84000 of ranks=130090rank=84100 of ranks=130090rank=84200 of ranks=130090rank=84300 of ranks=130090rank=84400 of ranks=130090rank=84500 of ranks=130090rank=84600 of ranks=130090rank=84700 of ranks=130090rank=84800 of ranks=130090rank=84900 of ranks=130090rank=85000 of ranks=130090rank=85100 of ranks=130090rank=85200 of ranks=130090rank=85300 of ranks=130090rank=85400 of ranks=130090rank=85500 of ranks=130090rank=85600 of ranks=130090rank=85700 of ranks=130090rank=85800 of ranks=130090rank=85900 of ranks=130090rank=86000 of ranks=130090rank=86100 of ranks=130090rank=86200 of ranks=130090rank=86300 of ranks=130090rank=86400 of ranks=130090rank=86500 of ranks=130090rank=86600 of ranks=130090rank=86700 of ranks=130090rank=86800 of ranks=130090rank=86900 of ranks=130090rank=87000 of ranks=130090rank=87100 of ranks=130090rank=87200 of ranks=130090rank=87300 of ranks=130090rank=87400 of ranks=130090rank=87500 of ranks=130090rank=87600 of ranks=130090rank=87700 of ranks=130090rank=87800 of ranks=130090rank=87900 of ranks=130090rank=88000 of ranks=130090rank=88100 of ranks=130090rank=88200 of ranks=130090rank=88300 of ranks=130090rank=88400 of ranks=130090rank=88500 of ranks=130090rank=88600 of ranks=130090rank=88700 of ranks=130090rank=88800 of ranks=130090rank=88900 of ranks=130090rank=89000 of ranks=130090rank=89100 of ranks=130090rank=89200 of ranks=130090rank=89300 of ranks=130090rank=89400 of ranks=130090rank=89500 of ranks=130090rank=89600 of ranks=130090rank=89700 of ranks=130090rank=89800 of ranks=130090rank=89900 of ranks=130090rank=90000 of ranks=130090rank=90100 of ranks=130090rank=90200 of ranks=130090rank=90300 of ranks=130090rank=90400 of ranks=130090rank=90500 of ranks=130090rank=90600 of ranks=130090rank=90700 of ranks=130090rank=90800 of ranks=130090rank=90900 of ranks=130090rank=91000 of ranks=130090rank=91100 of ranks=130090rank=91200 of ranks=130090rank=91300 of ranks=130090rank=91400 of ranks=130090rank=91500 of ranks=130090rank=91600 of ranks=130090rank=91700 of ranks=130090rank=91800 of ranks=130090rank=91900 of ranks=130090rank=92000 of ranks=130090rank=92100 of ranks=130090rank=92200 of ranks=130090rank=92300 of ranks=130090rank=92400 of ranks=130090rank=92500 of ranks=130090rank=92600 of ranks=130090rank=92700 of ranks=130090rank=92800 of ranks=130090rank=92900 of ranks=130090rank=93000 of ranks=130090rank=93100 of ranks=130090rank=93200 of ranks=130090rank=93300 of ranks=130090rank=93400 of ranks=130090rank=93500 of ranks=130090rank=93600 of ranks=130090rank=93700 of ranks=130090rank=93800 of ranks=130090rank=93900 of ranks=130090rank=94000 of ranks=130090rank=94100 of ranks=130090rank=94200 of ranks=130090rank=94300 of ranks=130090rank=94400 of ranks=130090rank=94500 of ranks=130090rank=94600 of ranks=130090rank=94700 of ranks=130090rank=94800 of ranks=130090rank=94900 of ranks=130090rank=95000 of ranks=130090rank=95100 of ranks=130090rank=95200 of ranks=130090rank=95300 of ranks=130090rank=95400 of ranks=130090rank=95500 of ranks=130090rank=95600 of ranks=130090rank=95700 of ranks=130090rank=95800 of ranks=130090rank=95900 of ranks=130090rank=96000 of ranks=130090rank=96100 of ranks=130090rank=96200 of ranks=130090rank=96300 of ranks=130090rank=96400 of ranks=130090rank=96500 of ranks=130090rank=96600 of ranks=130090rank=96700 of ranks=130090rank=96800 of ranks=130090rank=96900 of ranks=130090rank=97000 of ranks=130090rank=97100 of ranks=130090rank=97200 of ranks=130090rank=97300 of ranks=130090rank=97400 of ranks=130090rank=97500 of ranks=130090rank=97600 of ranks=130090rank=97700 of ranks=130090rank=97800 of ranks=130090rank=97900 of ranks=130090rank=98000 of ranks=130090rank=98100 of ranks=130090rank=98200 of ranks=130090rank=98300 of ranks=130090rank=98400 of ranks=130090rank=98500 of ranks=130090rank=98600 of ranks=130090rank=98700 of ranks=130090rank=98800 of ranks=130090rank=98900 of ranks=130090rank=99000 of ranks=130090rank=99100 of ranks=130090rank=99200 of ranks=130090rank=99300 of ranks=130090rank=99400 of ranks=130090rank=99500 of ranks=130090rank=99600 of ranks=130090rank=99700 of ranks=130090rank=99800 of ranks=130090rank=99900 of ranks=130090rank=100000 of ranks=130090rank=100100 of ranks=130090rank=100200 of ranks=130090rank=100300 of ranks=130090rank=100400 of ranks=130090rank=100500 of ranks=130090rank=100600 of ranks=130090rank=100700 of ranks=130090rank=100800 of ranks=130090rank=100900 of ranks=130090rank=101000 of ranks=130090rank=101100 of ranks=130090rank=101200 of ranks=130090rank=101300 of ranks=130090rank=101400 of ranks=130090rank=101500 of ranks=130090rank=101600 of ranks=130090rank=101700 of ranks=130090rank=101800 of ranks=130090rank=101900 of ranks=130090rank=102000 of ranks=130090rank=102100 of ranks=130090rank=102200 of ranks=130090rank=102300 of ranks=130090rank=102400 of ranks=130090rank=102500 of ranks=130090rank=102600 of ranks=130090rank=102700 of ranks=130090rank=102800 of ranks=130090rank=102900 of ranks=130090rank=103000 of ranks=130090rank=103100 of ranks=130090rank=103200 of ranks=130090rank=103300 of ranks=130090rank=103400 of ranks=130090rank=103500 of ranks=130090rank=103600 of ranks=130090rank=103700 of ranks=130090rank=103800 of ranks=130090rank=103900 of ranks=130090rank=104000 of ranks=130090rank=104100 of ranks=130090rank=104200 of ranks=130090rank=104300 of ranks=130090rank=104400 of ranks=130090rank=104500 of ranks=130090rank=104600 of ranks=130090rank=104700 of ranks=130090rank=104800 of ranks=130090rank=104900 of ranks=130090rank=105000 of ranks=130090rank=105100 of ranks=130090rank=105200 of ranks=130090rank=105300 of ranks=130090rank=105400 of ranks=130090rank=105500 of ranks=130090rank=105600 of ranks=130090rank=105700 of ranks=130090rank=105800 of ranks=130090rank=105900 of ranks=130090rank=106000 of ranks=130090rank=106100 of ranks=130090rank=106200 of ranks=130090rank=106300 of ranks=130090rank=106400 of ranks=130090rank=106500 of ranks=130090rank=106600 of ranks=130090rank=106700 of ranks=130090rank=106800 of ranks=130090rank=106900 of ranks=130090rank=107000 of ranks=130090rank=107100 of ranks=130090rank=107200 of ranks=130090rank=107300 of ranks=130090rank=107400 of ranks=130090rank=107500 of ranks=130090rank=107600 of ranks=130090rank=107700 of ranks=130090rank=107800 of ranks=130090rank=107900 of ranks=130090rank=108000 of ranks=130090rank=108100 of ranks=130090rank=108200 of ranks=130090rank=108300 of ranks=130090rank=108400 of ranks=130090rank=108500 of ranks=130090rank=108600 of ranks=130090rank=108700 of ranks=130090rank=108800 of ranks=130090rank=108900 of ranks=130090rank=109000 of ranks=130090rank=109100 of ranks=130090rank=109200 of ranks=130090rank=109300 of ranks=130090rank=109400 of ranks=130090rank=109500 of ranks=130090rank=109600 of ranks=130090rank=109700 of ranks=130090rank=109800 of ranks=130090rank=109900 of ranks=130090rank=110000 of ranks=130090rank=110100 of ranks=130090rank=110200 of ranks=130090rank=110300 of ranks=130090rank=110400 of ranks=130090rank=110500 of ranks=130090rank=110600 of ranks=130090rank=110700 of ranks=130090rank=110800 of ranks=130090rank=110900 of ranks=130090rank=111000 of ranks=130090rank=111100 of ranks=130090rank=111200 of ranks=130090rank=111300 of ranks=130090rank=111400 of ranks=130090rank=111500 of ranks=130090rank=111600 of ranks=130090rank=111700 of ranks=130090rank=111800 of ranks=130090rank=111900 of ranks=130090rank=112000 of ranks=130090rank=112100 of ranks=130090rank=112200 of ranks=130090rank=112300 of ranks=130090rank=112400 of ranks=130090rank=112500 of ranks=130090rank=112600 of ranks=130090rank=112700 of ranks=130090rank=112800 of ranks=130090rank=112900 of ranks=130090rank=113000 of ranks=130090rank=113100 of ranks=130090rank=113200 of ranks=130090rank=113300 of ranks=130090rank=113400 of ranks=130090rank=113500 of ranks=130090rank=113600 of ranks=130090rank=113700 of ranks=130090rank=113800 of ranks=130090rank=113900 of ranks=130090rank=114000 of ranks=130090rank=114100 of ranks=130090rank=114200 of ranks=130090rank=114300 of ranks=130090rank=114400 of ranks=130090rank=114500 of ranks=130090rank=114600 of ranks=130090rank=114700 of ranks=130090rank=114800 of ranks=130090rank=114900 of ranks=130090rank=115000 of ranks=130090rank=115100 of ranks=130090rank=115200 of ranks=130090rank=115300 of ranks=130090rank=115400 of ranks=130090rank=115500 of ranks=130090rank=115600 of ranks=130090rank=115700 of ranks=130090rank=115800 of ranks=130090rank=115900 of ranks=130090rank=116000 of ranks=130090rank=116100 of ranks=130090rank=116200 of ranks=130090rank=116300 of ranks=130090rank=116400 of ranks=130090rank=116500 of ranks=130090rank=116600 of ranks=130090rank=116700 of ranks=130090rank=116800 of ranks=130090rank=116900 of ranks=130090rank=117000 of ranks=130090rank=117100 of ranks=130090rank=117200 of ranks=130090rank=117300 of ranks=130090rank=117400 of ranks=130090rank=117500 of ranks=130090rank=117600 of ranks=130090rank=117700 of ranks=130090rank=117800 of ranks=130090rank=117900 of ranks=130090rank=118000 of ranks=130090rank=118100 of ranks=130090rank=118200 of ranks=130090rank=118300 of ranks=130090rank=118400 of ranks=130090rank=118500 of ranks=130090rank=118600 of ranks=130090rank=118700 of ranks=130090rank=118800 of ranks=130090rank=118900 of ranks=130090rank=119000 of ranks=130090rank=119100 of ranks=130090rank=119200 of ranks=130090rank=119300 of ranks=130090rank=119400 of ranks=130090rank=119500 of ranks=130090rank=119600 of ranks=130090rank=119700 of ranks=130090rank=119800 of ranks=130090rank=119900 of ranks=130090rank=120000 of ranks=130090rank=120100 of ranks=130090rank=120200 of ranks=130090rank=120300 of ranks=130090rank=120400 of ranks=130090rank=120500 of ranks=130090rank=120600 of ranks=130090rank=120700 of ranks=130090rank=120800 of ranks=130090rank=120900 of ranks=130090rank=121000 of ranks=130090rank=121100 of ranks=130090rank=121200 of ranks=130090rank=121300 of ranks=130090rank=121400 of ranks=130090rank=121500 of ranks=130090rank=121600 of ranks=130090rank=121700 of ranks=130090rank=121800 of ranks=130090rank=121900 of ranks=130090rank=122000 of ranks=130090rank=122100 of ranks=130090rank=122200 of ranks=130090rank=122300 of ranks=130090rank=122400 of ranks=130090rank=122500 of ranks=130090rank=122600 of ranks=130090rank=122700 of ranks=130090rank=122800 of ranks=130090rank=122900 of ranks=130090rank=123000 of ranks=130090rank=123100 of ranks=130090rank=123200 of ranks=130090rank=123300 of ranks=130090rank=123400 of ranks=130090rank=123500 of ranks=130090rank=123600 of ranks=130090rank=123700 of ranks=130090rank=123800 of ranks=130090rank=123900 of ranks=130090rank=124000 of ranks=130090rank=124100 of ranks=130090rank=124200 of ranks=130090rank=124300 of ranks=130090rank=124400 of ranks=130090rank=124500 of ranks=130090rank=124600 of ranks=130090rank=124700 of ranks=130090rank=124800 of ranks=130090rank=124900 of ranks=130090rank=125000 of ranks=130090rank=125100 of ranks=130090rank=125200 of ranks=130090rank=125300 of ranks=130090rank=125400 of ranks=130090rank=125500 of ranks=130090rank=125600 of ranks=130090rank=125700 of ranks=130090rank=125800 of ranks=130090rank=125900 of ranks=130090rank=126000 of ranks=130090rank=126100 of ranks=130090rank=126200 of ranks=130090rank=126300 of ranks=130090rank=126400 of ranks=130090rank=126500 of ranks=130090rank=126600 of ranks=130090rank=126700 of ranks=130090rank=126800 of ranks=130090rank=126900 of ranks=130090rank=127000 of ranks=130090rank=127100 of ranks=130090rank=127200 of ranks=130090rank=127300 of ranks=130090rank=127400 of ranks=130090rank=127500 of ranks=130090rank=127600 of ranks=130090rank=127700 of ranks=130090rank=127800 of ranks=130090rank=127900 of ranks=130090rank=128000 of ranks=130090rank=128100 of ranks=130090rank=128200 of ranks=130090rank=128300 of ranks=130090rank=128400 of ranks=130090rank=128500 of ranks=130090rank=128600 of ranks=130090rank=128700 of ranks=130090rank=128800 of ranks=130090rank=128900 of ranks=130090rank=129000 of ranks=130090rank=129100 of ranks=130090rank=129200 of ranks=130090rank=129300 of ranks=130090rank=129400 of ranks=130090rank=129500 of ranks=130090rank=129600 of ranks=130090rank=129700 of ranks=130090rank=129800 of ranks=130090rank=129900 of ranks=130090rank=130000 of ranks=130090

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              87.0833    463   5399     35    498           70.4209
   1 car                    96.7051  49701  37787    615  50316           77.0046
   2 truck                  85.4955   1777  14320     48   1825           46.1741
   3 bus                    72.9094    350   5767     16    366           48.0154
   4 pedestrian             88.1459   3965  10561    294   4259           64.9184

for conf_thresh=0.25, precision=0.88, recall=0.89, F1 score=0.89
for conf_thresh=0.25, TP=50969, FP=6867, FN=6295, average IoU=74.68%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=86.07%
Total detection time: 151 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
2625: loss=5.595, avg loss=5.269, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=590.2 milliseconds, train=2.1 seconds, 168000 images, time remaining=5.3 hours
2626: loss=5.009, avg loss=5.243, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=622.6 milliseconds, train=2.1 seconds, 168064 images, time remaining=5.3 hours
2627: loss=5.369, avg loss=5.256, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=913.2 milliseconds, train=2.1 seconds, 168128 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2628: loss=5.617, avg loss=5.292, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.1 seconds, train=2.1 seconds, 168192 images, time remaining=5.3 hours
2629: loss=4.698, avg loss=5.232, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=959.4 milliseconds, train=2.1 seconds, 168256 images, time remaining=5.3 hours
2630: loss=5.605, avg loss=5.270, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 168320 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2631: loss=7.161, avg loss=5.459, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=713.4 milliseconds, train=4.7 seconds, 168384 images, time remaining=5.3 hours
2632: loss=5.867, avg loss=5.500, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 168448 images, time remaining=5.3 hours
2633: loss=6.955, avg loss=5.645, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=934.7 milliseconds, train=4.6 seconds, 168512 images, time remaining=5.3 hours
2634: loss=5.785, avg loss=5.659, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 168576 images, time remaining=5.3 hours
2635: loss=5.228, avg loss=5.616, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 168640 images, time remaining=5.3 hours
2636: loss=7.001, avg loss=5.755, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=764.3 milliseconds, train=4.6 seconds, 168704 images, time remaining=5.3 hours
2637: loss=6.506, avg loss=5.830, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=973.3 milliseconds, train=4.6 seconds, 168768 images, time remaining=5.3 hours
2638: loss=6.020, avg loss=5.849, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=4.6 seconds, 168832 images, time remaining=5.3 hours
2639: loss=6.366, avg loss=5.900, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=4.6 seconds, 168896 images, time remaining=5.3 hours
2640: loss=5.425, avg loss=5.853, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=735.8 milliseconds, train=4.6 seconds, 168960 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14648c000000
2641: loss=5.388, avg loss=5.806, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 169024 images, time remaining=5.3 hours
2642: loss=4.646, avg loss=5.690, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=903.3 milliseconds, train=1.5 seconds, 169088 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2643: loss=4.698, avg loss=5.591, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 169152 images, time remaining=5.3 hours
2644: loss=4.139, avg loss=5.446, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 169216 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2645: loss=5.137, avg loss=5.415, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 169280 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2646: loss=5.626, avg loss=5.436, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 169344 images, time remaining=5.3 hours
2647: loss=4.525, avg loss=5.345, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 169408 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2648: loss=5.478, avg loss=5.358, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 169472 images, time remaining=5.3 hours
2649: loss=5.131, avg loss=5.336, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 169536 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2650: loss=4.783, avg loss=5.280, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 169600 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2651: loss=6.162, avg loss=5.368, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.5 seconds, train=3.6 seconds, 169664 images, time remaining=5.3 hours
2652: loss=6.111, avg loss=5.443, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=542.5 milliseconds, train=3.6 seconds, 169728 images, time remaining=5.3 hours
2653: loss=5.593, avg loss=5.458, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 169792 images, time remaining=5.3 hours
2654: loss=5.215, avg loss=5.434, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=3.6 seconds, 169856 images, time remaining=5.3 hours
2655: loss=5.680, avg loss=5.458, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.2 seconds, train=3.6 seconds, 169920 images, time remaining=5.3 hours
2656: loss=6.587, avg loss=5.571, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 169984 images, time remaining=5.3 hours
2657: loss=5.565, avg loss=5.570, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=997.8 milliseconds, train=3.6 seconds, 170048 images, time remaining=5.3 hours
2658: loss=6.013, avg loss=5.615, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=950.0 milliseconds, train=3.6 seconds, 170112 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2659: loss=4.248, avg loss=5.478, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.7 seconds, train=3.6 seconds, 170176 images, time remaining=5.3 hours
2660: loss=4.880, avg loss=5.418, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 170240 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2661: loss=5.498, avg loss=5.426, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 170304 images, time remaining=5.3 hours
2662: loss=6.878, avg loss=5.571, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=922.4 milliseconds, train=5.1 seconds, 170368 images, time remaining=5.3 hours
2663: loss=6.267, avg loss=5.641, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=749.8 milliseconds, train=5.0 seconds, 170432 images, time remaining=5.3 hours
2664: loss=6.287, avg loss=5.706, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 170496 images, time remaining=5.3 hours
2665: loss=6.344, avg loss=5.769, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=5.0 seconds, 170560 images, time remaining=5.3 hours
2666: loss=6.251, avg loss=5.818, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.2 seconds, train=5.0 seconds, 170624 images, time remaining=5.3 hours
2667: loss=6.797, avg loss=5.916, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.9 seconds, train=5.0 seconds, 170688 images, time remaining=5.3 hours
2668: loss=4.878, avg loss=5.812, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=942.8 milliseconds, train=5.0 seconds, 170752 images, time remaining=5.3 hours
2669: loss=5.743, avg loss=5.805, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=5.0 seconds, 170816 images, time remaining=5.3 hours
2670: loss=5.305, avg loss=5.755, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=801.4 milliseconds, train=5.0 seconds, 170880 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2671: loss=5.461, avg loss=5.726, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 170944 images, time remaining=5.3 hours
2672: loss=6.296, avg loss=5.783, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=696.0 milliseconds, train=3.3 seconds, 171008 images, time remaining=5.3 hours
2673: loss=4.363, avg loss=5.641, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=912.8 milliseconds, train=3.3 seconds, 171072 images, time remaining=5.3 hours
2674: loss=4.691, avg loss=5.546, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=859.2 milliseconds, train=3.3 seconds, 171136 images, time remaining=5.3 hours
2675: loss=4.616, avg loss=5.453, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=837.5 milliseconds, train=3.3 seconds, 171200 images, time remaining=5.3 hours
2676: loss=4.905, avg loss=5.398, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 171264 images, time remaining=5.3 hours
2677: loss=5.894, avg loss=5.448, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=856.2 milliseconds, train=3.3 seconds, 171328 images, time remaining=5.3 hours
2678: loss=4.934, avg loss=5.396, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 171392 images, time remaining=5.3 hours
2679: loss=4.701, avg loss=5.327, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=3.3 seconds, 171456 images, time remaining=5.3 hours
2680: loss=6.031, avg loss=5.397, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=766.8 milliseconds, train=3.3 seconds, 171520 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2681: loss=5.129, avg loss=5.370, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=716.8 milliseconds, train=4.1 seconds, 171584 images, time remaining=5.3 hours
2682: loss=4.622, avg loss=5.296, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=4.1 seconds, 171648 images, time remaining=5.3 hours
2683: loss=4.530, avg loss=5.219, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=4.1 seconds, 171712 images, time remaining=5.3 hours
2684: loss=5.370, avg loss=5.234, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.2 seconds, train=4.1 seconds, 171776 images, time remaining=5.3 hours
2685: loss=4.815, avg loss=5.192, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=829.0 milliseconds, train=4.1 seconds, 171840 images, time remaining=5.3 hours
2686: loss=5.820, avg loss=5.255, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=938.8 milliseconds, train=4.1 seconds, 171904 images, time remaining=5.3 hours
2687: loss=5.151, avg loss=5.245, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=4.1 seconds, 171968 images, time remaining=5.3 hours
2688: loss=5.005, avg loss=5.221, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 172032 images, time remaining=5.3 hours
2689: loss=4.802, avg loss=5.179, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=721.2 milliseconds, train=4.1 seconds, 172096 images, time remaining=5.3 hours
2690: loss=5.189, avg loss=5.180, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=4.1 seconds, 172160 images, time remaining=5.3 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2691: loss=4.976, avg loss=5.159, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=863.8 milliseconds, train=3.4 seconds, 172224 images, time remaining=5.3 hours
2692: loss=4.431, avg loss=5.087, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=962.5 milliseconds, train=3.4 seconds, 172288 images, time remaining=5.3 hours
2693: loss=6.016, avg loss=5.179, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 172352 images, time remaining=5.3 hours
2694: loss=5.254, avg loss=5.187, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=733.5 milliseconds, train=3.4 seconds, 172416 images, time remaining=5.3 hours
2695: loss=5.272, avg loss=5.195, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=3.4 seconds, 172480 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2696: loss=4.777, avg loss=5.154, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=4.5 seconds, train=3.4 seconds, 172544 images, time remaining=5.3 hours
2697: loss=4.321, avg loss=5.070, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=854.4 milliseconds, train=3.4 seconds, 172608 images, time remaining=5.3 hours
2698: loss=4.481, avg loss=5.011, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 172672 images, time remaining=5.3 hours
2699: loss=5.117, avg loss=5.022, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=3.4 seconds, 172736 images, time remaining=5.3 hours
2700: loss=5.331, avg loss=5.053, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 172800 images, time remaining=5.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1463cc000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2701: loss=4.784, avg loss=5.026, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=4.2 seconds, train=1.5 seconds, 172864 images, time remaining=5.3 hours
2702: loss=5.247, avg loss=5.048, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=866.1 milliseconds, train=1.5 seconds, 172928 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2703: loss=5.225, avg loss=5.066, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 172992 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2704: loss=4.267, avg loss=4.986, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 173056 images, time remaining=5.3 hours
2705: loss=3.976, avg loss=4.885, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 173120 images, time remaining=5.3 hours
2706: loss=4.262, avg loss=4.823, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=976.5 milliseconds, train=1.5 seconds, 173184 images, time remaining=5.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2707: loss=5.002, avg loss=4.841, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 173248 images, time remaining=5.2 hours
2708: loss=5.168, avg loss=4.873, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=807.0 milliseconds, train=1.5 seconds, 173312 images, time remaining=5.2 hours
2709: loss=4.550, avg loss=4.841, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=623.8 milliseconds, train=1.5 seconds, 173376 images, time remaining=5.2 hours
2710: loss=5.143, avg loss=4.871, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=740.1 milliseconds, train=1.5 seconds, 173440 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
2711: loss=4.442, avg loss=4.828, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=786.3 milliseconds, train=1.6 seconds, 173504 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2712: loss=4.570, avg loss=4.803, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.7 seconds, train=1.5 seconds, 173568 images, time remaining=5.2 hours
2713: loss=4.691, avg loss=4.791, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 173632 images, time remaining=5.2 hours
2714: loss=5.723, avg loss=4.885, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=959.1 milliseconds, train=1.5 seconds, 173696 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2715: loss=4.539, avg loss=4.850, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 173760 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2716: loss=5.303, avg loss=4.895, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 173824 images, time remaining=5.2 hours
2717: loss=4.673, avg loss=4.873, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=585.2 milliseconds, train=1.5 seconds, 173888 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2718: loss=5.044, avg loss=4.890, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 173952 images, time remaining=5.2 hours
2719: loss=4.868, avg loss=4.888, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=667.5 milliseconds, train=1.5 seconds, 174016 images, time remaining=5.2 hours
2720: loss=5.140, avg loss=4.913, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 174080 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2721: loss=5.171, avg loss=4.939, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 174144 images, time remaining=5.2 hours
2722: loss=6.189, avg loss=5.064, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=646.7 milliseconds, train=3.5 seconds, 174208 images, time remaining=5.2 hours
2723: loss=5.101, avg loss=5.068, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.1 seconds, train=3.5 seconds, 174272 images, time remaining=5.2 hours
2724: loss=4.798, avg loss=5.041, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=3.5 seconds, 174336 images, time remaining=5.2 hours
2725: loss=5.096, avg loss=5.046, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 174400 images, time remaining=5.2 hours
2726: loss=5.427, avg loss=5.084, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=3.5 seconds, 174464 images, time remaining=5.2 hours
2727: loss=6.042, avg loss=5.180, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=526.0 milliseconds, train=3.6 seconds, 174528 images, time remaining=5.2 hours
2728: loss=5.569, avg loss=5.219, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=923.1 milliseconds, train=3.5 seconds, 174592 images, time remaining=5.2 hours
2729: loss=4.795, avg loss=5.176, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=3.5 seconds, 174656 images, time remaining=5.2 hours
2730: loss=4.286, avg loss=5.087, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=3.5 seconds, 174720 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2731: loss=5.282, avg loss=5.107, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=871.3 milliseconds, train=3.3 seconds, 174784 images, time remaining=5.2 hours
2732: loss=5.241, avg loss=5.120, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=628.2 milliseconds, train=3.3 seconds, 174848 images, time remaining=5.2 hours
2733: loss=4.808, avg loss=5.089, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=762.9 milliseconds, train=3.3 seconds, 174912 images, time remaining=5.2 hours
2734: loss=5.189, avg loss=5.099, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 174976 images, time remaining=5.2 hours
2735: loss=4.439, avg loss=5.033, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=853.5 milliseconds, train=3.3 seconds, 175040 images, time remaining=5.2 hours
2736: loss=4.081, avg loss=4.938, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=563.9 milliseconds, train=3.3 seconds, 175104 images, time remaining=5.2 hours
2737: loss=5.268, avg loss=4.971, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 175168 images, time remaining=5.2 hours
2738: loss=4.452, avg loss=4.919, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=759.3 milliseconds, train=3.3 seconds, 175232 images, time remaining=5.2 hours
2739: loss=5.056, avg loss=4.933, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=924.8 milliseconds, train=3.3 seconds, 175296 images, time remaining=5.2 hours
2740: loss=4.555, avg loss=4.895, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=615.1 milliseconds, train=3.3 seconds, 175360 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2741: loss=5.927, avg loss=4.998, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 175424 images, time remaining=5.2 hours
2742: loss=4.474, avg loss=4.946, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=621.9 milliseconds, train=4.6 seconds, 175488 images, time remaining=5.2 hours
2743: loss=6.098, avg loss=5.061, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 175552 images, time remaining=5.2 hours
2744: loss=4.668, avg loss=5.022, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=590.9 milliseconds, train=4.6 seconds, 175616 images, time remaining=5.2 hours
2745: loss=5.145, avg loss=5.034, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 175680 images, time remaining=5.2 hours
2746: loss=4.811, avg loss=5.012, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=736.7 milliseconds, train=4.6 seconds, 175744 images, time remaining=5.2 hours
2747: loss=4.610, avg loss=4.972, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 175808 images, time remaining=5.2 hours
2748: loss=5.748, avg loss=5.049, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.2 seconds, train=4.6 seconds, 175872 images, time remaining=5.2 hours
2749: loss=5.318, avg loss=5.076, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=834.2 milliseconds, train=4.6 seconds, 175936 images, time remaining=5.2 hours
2750: loss=5.770, avg loss=5.146, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=852.7 milliseconds, train=4.6 seconds, 176000 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2751: loss=5.399, avg loss=5.171, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=712.4 milliseconds, train=2.0 seconds, 176064 images, time remaining=5.2 hours
2752: loss=4.281, avg loss=5.082, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=556.1 milliseconds, train=1.9 seconds, 176128 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2753: loss=4.459, avg loss=5.020, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=1.9 seconds, 176192 images, time remaining=5.2 hours
2754: loss=5.665, avg loss=5.084, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=988.8 milliseconds, train=2.0 seconds, 176256 images, time remaining=5.2 hours
2755: loss=5.616, avg loss=5.137, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 176320 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2756: loss=5.785, avg loss=5.202, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 176384 images, time remaining=5.2 hours
2757: loss=3.963, avg loss=5.078, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=483.4 milliseconds, train=2.0 seconds, 176448 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2758: loss=4.640, avg loss=5.034, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.3 seconds, train=1.9 seconds, 176512 images, time remaining=5.2 hours
2759: loss=4.512, avg loss=4.982, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=861.4 milliseconds, train=2.0 seconds, 176576 images, time remaining=5.2 hours
2760: loss=4.806, avg loss=4.964, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=817.3 milliseconds, train=2.0 seconds, 176640 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145842000000
2761: loss=3.370, avg loss=4.805, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=714.8 milliseconds, train=1.2 seconds, 176704 images, time remaining=5.2 hours
2762: loss=5.432, avg loss=4.868, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=534.4 milliseconds, train=1.2 seconds, 176768 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2763: loss=4.278, avg loss=4.809, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 176832 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2764: loss=3.921, avg loss=4.720, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 176896 images, time remaining=5.2 hours
2765: loss=5.278, avg loss=4.776, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=692.8 milliseconds, train=1.2 seconds, 176960 images, time remaining=5.2 hours
2766: loss=5.612, avg loss=4.859, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=728.2 milliseconds, train=1.2 seconds, 177024 images, time remaining=5.2 hours
2767: loss=4.469, avg loss=4.820, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=907.7 milliseconds, train=1.2 seconds, 177088 images, time remaining=5.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2768: loss=4.126, avg loss=4.751, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 177152 images, time remaining=5.2 hours
2769: loss=5.381, avg loss=4.814, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=689.2 milliseconds, train=1.2 seconds, 177216 images, time remaining=5.2 hours
2770: loss=4.451, avg loss=4.778, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=815.5 milliseconds, train=1.2 seconds, 177280 images, time remaining=5.2 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x14501e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2771: loss=4.875, avg loss=4.787, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=1.5 seconds, 177344 images, time remaining=5.2 hours
2772: loss=5.101, avg loss=4.819, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=594.1 milliseconds, train=1.5 seconds, 177408 images, time remaining=5.2 hours
2773: loss=4.227, avg loss=4.760, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=677.7 milliseconds, train=1.6 seconds, 177472 images, time remaining=5.2 hours
2774: loss=4.524, avg loss=4.736, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=404.7 milliseconds, train=1.5 seconds, 177536 images, time remaining=5.2 hours
2775: loss=4.328, avg loss=4.695, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=829.4 milliseconds, train=1.5 seconds, 177600 images, time remaining=5.2 hours
2776: loss=5.000, avg loss=4.726, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=855.9 milliseconds, train=1.6 seconds, 177664 images, time remaining=5.2 hours
2777: loss=4.905, avg loss=4.744, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=732.6 milliseconds, train=1.5 seconds, 177728 images, time remaining=5.2 hours
2778: loss=5.499, avg loss=4.819, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=896.7 milliseconds, train=1.5 seconds, 177792 images, time remaining=5.2 hours
2779: loss=5.332, avg loss=4.870, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=714.8 milliseconds, train=1.5 seconds, 177856 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2780: loss=4.956, avg loss=4.879, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 177920 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x146390000000
2781: loss=3.763, avg loss=4.767, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 177984 images, time remaining=5.1 hours
2782: loss=4.160, avg loss=4.707, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=921.8 milliseconds, train=1.4 seconds, 178048 images, time remaining=5.1 hours
2783: loss=4.838, avg loss=4.720, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=739.4 milliseconds, train=1.4 seconds, 178112 images, time remaining=5.1 hours
2784: loss=3.615, avg loss=4.609, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=452.8 milliseconds, train=1.4 seconds, 178176 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2785: loss=5.401, avg loss=4.688, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 178240 images, time remaining=5.1 hours
2786: loss=5.088, avg loss=4.728, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=869.8 milliseconds, train=1.4 seconds, 178304 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2787: loss=4.355, avg loss=4.691, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=1.4 seconds, 178368 images, time remaining=5.1 hours
2788: loss=4.603, avg loss=4.682, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=906.9 milliseconds, train=1.4 seconds, 178432 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2789: loss=3.798, avg loss=4.594, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 178496 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2790: loss=4.633, avg loss=4.598, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 178560 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2791: loss=7.142, avg loss=4.852, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=975.6 milliseconds, train=5.1 seconds, 178624 images, time remaining=5.1 hours
2792: loss=7.521, avg loss=5.119, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=5.1 seconds, 178688 images, time remaining=5.1 hours
2793: loss=7.206, avg loss=5.328, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 178752 images, time remaining=5.1 hours
2794: loss=6.704, avg loss=5.465, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.9 seconds, train=5.1 seconds, 178816 images, time remaining=5.1 hours
2795: loss=6.725, avg loss=5.591, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=778.3 milliseconds, train=5.1 seconds, 178880 images, time remaining=5.1 hours
2796: loss=6.744, avg loss=5.706, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 178944 images, time remaining=5.1 hours
2797: loss=6.376, avg loss=5.773, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=833.2 milliseconds, train=5.1 seconds, 179008 images, time remaining=5.1 hours
2798: loss=6.098, avg loss=5.806, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 179072 images, time remaining=5.1 hours
2799: loss=6.104, avg loss=5.836, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 179136 images, time remaining=5.1 hours
2800: loss=5.939, avg loss=5.846, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 179200 images, time remaining=5.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2801: loss=4.668, avg loss=5.728, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=937.1 milliseconds, train=3.3 seconds, 179264 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2802: loss=5.135, avg loss=5.669, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=3.3 seconds, 179328 images, time remaining=5.1 hours
2803: loss=4.569, avg loss=5.559, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=880.3 milliseconds, train=3.3 seconds, 179392 images, time remaining=5.1 hours
2804: loss=4.879, avg loss=5.491, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 179456 images, time remaining=5.1 hours
2805: loss=4.870, avg loss=5.429, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 179520 images, time remaining=5.1 hours
2806: loss=5.078, avg loss=5.394, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=605.0 milliseconds, train=3.3 seconds, 179584 images, time remaining=5.1 hours
2807: loss=5.926, avg loss=5.447, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 179648 images, time remaining=5.1 hours
2808: loss=4.317, avg loss=5.334, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=754.7 milliseconds, train=3.3 seconds, 179712 images, time remaining=5.1 hours
2809: loss=4.429, avg loss=5.244, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=967.4 milliseconds, train=3.2 seconds, 179776 images, time remaining=5.1 hours
2810: loss=4.220, avg loss=5.141, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=732.6 milliseconds, train=3.3 seconds, 179840 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
2811: loss=4.827, avg loss=5.110, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=496.2 milliseconds, train=1.9 seconds, 179904 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2812: loss=5.086, avg loss=5.107, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.9 seconds, train=1.9 seconds, 179968 images, time remaining=5.1 hours
2813: loss=4.475, avg loss=5.044, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=918.8 milliseconds, train=1.9 seconds, 180032 images, time remaining=5.1 hours
2814: loss=4.289, avg loss=4.969, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.9 seconds, 180096 images, time remaining=5.1 hours
2815: loss=5.180, avg loss=4.990, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=805.5 milliseconds, train=1.9 seconds, 180160 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2816: loss=4.610, avg loss=4.952, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 180224 images, time remaining=5.1 hours
2817: loss=5.606, avg loss=5.017, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=739.3 milliseconds, train=1.9 seconds, 180288 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2818: loss=5.202, avg loss=5.036, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 180352 images, time remaining=5.1 hours
2819: loss=4.462, avg loss=4.978, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.9 seconds, 180416 images, time remaining=5.1 hours
2820: loss=4.753, avg loss=4.956, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=1.9 seconds, 180480 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x14638e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2821: loss=4.128, avg loss=4.873, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 180544 images, time remaining=5.1 hours
2822: loss=4.789, avg loss=4.865, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=794.4 milliseconds, train=1.4 seconds, 180608 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2823: loss=4.417, avg loss=4.820, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 180672 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2824: loss=4.310, avg loss=4.769, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 180736 images, time remaining=5.1 hours
2825: loss=4.367, avg loss=4.729, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=953.8 milliseconds, train=1.5 seconds, 180800 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2826: loss=4.288, avg loss=4.685, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.5 seconds, train=1.4 seconds, 180864 images, time remaining=5.1 hours
2827: loss=4.404, avg loss=4.656, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=977.2 milliseconds, train=1.4 seconds, 180928 images, time remaining=5.1 hours
2828: loss=4.523, avg loss=4.643, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=784.7 milliseconds, train=1.5 seconds, 180992 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2829: loss=4.436, avg loss=4.622, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 181056 images, time remaining=5.1 hours
2830: loss=4.327, avg loss=4.593, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=591.7 milliseconds, train=1.4 seconds, 181120 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 1024x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2831: loss=4.518, avg loss=4.585, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=659.4 milliseconds, train=3.2 seconds, 181184 images, time remaining=5.1 hours
2832: loss=5.666, avg loss=4.693, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=3.2 seconds, 181248 images, time remaining=5.1 hours
2833: loss=5.846, avg loss=4.809, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=853.4 milliseconds, train=3.2 seconds, 181312 images, time remaining=5.1 hours
2834: loss=4.490, avg loss=4.777, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=3.2 seconds, 181376 images, time remaining=5.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2835: loss=4.798, avg loss=4.779, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=3.2 seconds, 181440 images, time remaining=5.1 hours
2836: loss=4.713, avg loss=4.772, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=721.4 milliseconds, train=3.2 seconds, 181504 images, time remaining=5.1 hours
2837: loss=4.156, avg loss=4.711, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=3.2 seconds, 181568 images, time remaining=5.1 hours
2838: loss=4.541, avg loss=4.694, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=875.5 milliseconds, train=3.2 seconds, 181632 images, time remaining=5.1 hours
2839: loss=4.068, avg loss=4.631, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=3.2 seconds, 181696 images, time remaining=5.1 hours
2840: loss=4.270, avg loss=4.595, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=3.2 seconds, 181760 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2841: loss=6.533, avg loss=4.789, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.1 seconds, train=4.9 seconds, 181824 images, time remaining=5.1 hours
2842: loss=5.354, avg loss=4.845, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=986.2 milliseconds, train=4.8 seconds, 181888 images, time remaining=5.1 hours
2843: loss=6.861, avg loss=5.047, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=585.9 milliseconds, train=4.9 seconds, 181952 images, time remaining=5.1 hours
2844: loss=5.866, avg loss=5.129, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.3 seconds, train=4.8 seconds, 182016 images, time remaining=5.1 hours
2845: loss=5.538, avg loss=5.170, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=4.9 seconds, 182080 images, time remaining=5.1 hours
2846: loss=4.815, avg loss=5.134, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=4.8 seconds, 182144 images, time remaining=5.1 hours
2847: loss=5.629, avg loss=5.184, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=4.8 seconds, 182208 images, time remaining=5.1 hours
2848: loss=5.329, avg loss=5.198, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.6 seconds, train=4.9 seconds, 182272 images, time remaining=5.1 hours
2849: loss=5.307, avg loss=5.209, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=4.8 seconds, 182336 images, time remaining=5.1 hours
2850: loss=5.239, avg loss=5.212, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=4.8 seconds, 182400 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2851: loss=5.873, avg loss=5.278, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.0 seconds, train=4.8 seconds, 182464 images, time remaining=5.1 hours
2852: loss=6.152, avg loss=5.366, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=972.0 milliseconds, train=4.8 seconds, 182528 images, time remaining=5.1 hours
2853: loss=5.665, avg loss=5.396, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=4.9 seconds, 182592 images, time remaining=5.1 hours
2854: loss=5.481, avg loss=5.404, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=4.8 seconds, 182656 images, time remaining=5.1 hours
2855: loss=4.755, avg loss=5.339, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=4.8 seconds, 182720 images, time remaining=5.1 hours
2856: loss=4.898, avg loss=5.295, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=767.7 milliseconds, train=4.8 seconds, 182784 images, time remaining=5.1 hours
2857: loss=5.635, avg loss=5.329, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=885.7 milliseconds, train=4.8 seconds, 182848 images, time remaining=5.1 hours
2858: loss=6.106, avg loss=5.407, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=4.8 seconds, 182912 images, time remaining=5.1 hours
2859: loss=5.098, avg loss=5.376, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=887.2 milliseconds, train=4.8 seconds, 182976 images, time remaining=5.1 hours
2860: loss=5.105, avg loss=5.349, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=4.8 seconds, 183040 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2861: loss=5.061, avg loss=5.320, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=945.0 milliseconds, train=3.2 seconds, 183104 images, time remaining=5.1 hours
2862: loss=4.578, avg loss=5.246, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=567.4 milliseconds, train=3.2 seconds, 183168 images, time remaining=5.1 hours
2863: loss=4.734, avg loss=5.195, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=658.8 milliseconds, train=3.2 seconds, 183232 images, time remaining=5.1 hours
2864: loss=4.680, avg loss=5.143, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 183296 images, time remaining=5.1 hours
2865: loss=4.687, avg loss=5.098, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=745.3 milliseconds, train=3.2 seconds, 183360 images, time remaining=5.1 hours
2866: loss=4.523, avg loss=5.040, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=663.0 milliseconds, train=3.2 seconds, 183424 images, time remaining=5.1 hours
2867: loss=4.491, avg loss=4.985, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=695.3 milliseconds, train=3.2 seconds, 183488 images, time remaining=5.1 hours
2868: loss=5.057, avg loss=4.992, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=822.4 milliseconds, train=3.2 seconds, 183552 images, time remaining=5.1 hours
2869: loss=4.380, avg loss=4.931, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=873.0 milliseconds, train=3.2 seconds, 183616 images, time remaining=5.1 hours
2870: loss=4.906, avg loss=4.929, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=508.9 milliseconds, train=3.2 seconds, 183680 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2871: loss=5.130, avg loss=4.949, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=679.1 milliseconds, train=3.9 seconds, 183744 images, time remaining=5.1 hours
2872: loss=4.685, avg loss=4.922, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 183808 images, time remaining=5.1 hours
2873: loss=5.422, avg loss=4.972, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.9 seconds, train=3.9 seconds, 183872 images, time remaining=5.1 hours
2874: loss=4.691, avg loss=4.944, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 183936 images, time remaining=5.1 hours
2875: loss=4.570, avg loss=4.907, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 184000 images, time remaining=5.1 hours
2876: loss=5.561, avg loss=4.972, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=800.2 milliseconds, train=3.9 seconds, 184064 images, time remaining=5.1 hours
2877: loss=5.007, avg loss=4.976, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 184128 images, time remaining=5.1 hours
2878: loss=4.746, avg loss=4.953, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 184192 images, time remaining=5.1 hours
2879: loss=4.678, avg loss=4.925, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 184256 images, time remaining=5.1 hours
2880: loss=5.304, avg loss=4.963, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 184320 images, time remaining=5.1 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145f84e00000
2881: loss=6.427, avg loss=5.109, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 184384 images, time remaining=5.1 hours
2882: loss=5.108, avg loss=5.109, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=823.5 milliseconds, train=1.4 seconds, 184448 images, time remaining=5.1 hours
2883: loss=4.932, avg loss=5.092, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=966.1 milliseconds, train=1.5 seconds, 184512 images, time remaining=5 hours
2884: loss=4.450, avg loss=5.027, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=937.7 milliseconds, train=1.5 seconds, 184576 images, time remaining=5 hours
2885: loss=5.401, avg loss=5.065, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=746.4 milliseconds, train=1.4 seconds, 184640 images, time remaining=5 hours
2886: loss=4.525, avg loss=5.011, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=873.7 milliseconds, train=1.4 seconds, 184704 images, time remaining=5 hours
2887: loss=5.433, avg loss=5.053, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 184768 images, time remaining=5 hours
2888: loss=3.907, avg loss=4.938, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=743.8 milliseconds, train=1.4 seconds, 184832 images, time remaining=5 hours
2889: loss=5.323, avg loss=4.977, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 184896 images, time remaining=5 hours
2890: loss=5.154, avg loss=4.995, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 184960 images, time remaining=5 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2891: loss=5.405, avg loss=5.036, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=713.6 milliseconds, train=2.1 seconds, 185024 images, time remaining=5 hours
2892: loss=5.343, avg loss=5.066, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=896.1 milliseconds, train=2.1 seconds, 185088 images, time remaining=5 hours
2893: loss=4.813, avg loss=5.041, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=754.6 milliseconds, train=2.1 seconds, 185152 images, time remaining=5 hours
2894: loss=4.885, avg loss=5.025, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=697.0 milliseconds, train=2.1 seconds, 185216 images, time remaining=5 hours
2895: loss=5.452, avg loss=5.068, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 185280 images, time remaining=5 hours
2896: loss=5.204, avg loss=5.082, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=863.4 milliseconds, train=2.1 seconds, 185344 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2897: loss=4.328, avg loss=5.006, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.8 seconds, train=2.1 seconds, 185408 images, time remaining=5 hours
2898: loss=4.856, avg loss=4.991, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 185472 images, time remaining=5 hours
2899: loss=4.289, avg loss=4.921, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 185536 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2900: loss=4.019, avg loss=4.831, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 185600 images, time remaining=5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14635c000000
2901: loss=5.767, avg loss=4.925, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=709.0 milliseconds, train=1.5 seconds, 185664 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2902: loss=4.621, avg loss=4.894, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 185728 images, time remaining=5 hours
2903: loss=4.004, avg loss=4.805, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=890.6 milliseconds, train=1.5 seconds, 185792 images, time remaining=5 hours
2904: loss=4.126, avg loss=4.737, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=731.7 milliseconds, train=1.5 seconds, 185856 images, time remaining=5 hours
2905: loss=4.133, avg loss=4.677, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=479.6 milliseconds, train=1.5 seconds, 185920 images, time remaining=5 hours
2906: loss=4.567, avg loss=4.666, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=677.1 milliseconds, train=1.5 seconds, 185984 images, time remaining=5 hours
2907: loss=5.272, avg loss=4.726, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=703.9 milliseconds, train=1.5 seconds, 186048 images, time remaining=5 hours
2908: loss=4.984, avg loss=4.752, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=729.9 milliseconds, train=1.5 seconds, 186112 images, time remaining=5 hours
2909: loss=5.060, avg loss=4.783, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=782.5 milliseconds, train=1.5 seconds, 186176 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2910: loss=4.920, avg loss=4.797, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 186240 images, time remaining=5 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2911: loss=4.496, avg loss=4.767, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=525.0 milliseconds, train=1.9 seconds, 186304 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2912: loss=3.879, avg loss=4.678, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=1.9 seconds, 186368 images, time remaining=5 hours
2913: loss=4.087, avg loss=4.619, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=982.5 milliseconds, train=1.9 seconds, 186432 images, time remaining=5 hours
2914: loss=4.765, avg loss=4.633, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.5 seconds, train=1.9 seconds, 186496 images, time remaining=5 hours
2915: loss=4.184, avg loss=4.588, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=549.0 milliseconds, train=1.9 seconds, 186560 images, time remaining=5 hours
2916: loss=4.561, avg loss=4.586, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=538.4 milliseconds, train=1.9 seconds, 186624 images, time remaining=5 hours
2917: loss=4.651, avg loss=4.592, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=639.5 milliseconds, train=1.9 seconds, 186688 images, time remaining=5 hours
2918: loss=5.043, avg loss=4.637, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=699.6 milliseconds, train=1.9 seconds, 186752 images, time remaining=5 hours
2919: loss=4.792, avg loss=4.653, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 186816 images, time remaining=5 hours
2920: loss=4.317, avg loss=4.619, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=601.4 milliseconds, train=1.9 seconds, 186880 images, time remaining=5 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2921: loss=6.004, avg loss=4.758, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=928.1 milliseconds, train=3.3 seconds, 186944 images, time remaining=5 hours
2922: loss=4.979, avg loss=4.780, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 187008 images, time remaining=5 hours
2923: loss=4.821, avg loss=4.784, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=648.9 milliseconds, train=3.3 seconds, 187072 images, time remaining=5 hours
2924: loss=5.667, avg loss=4.872, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=532.9 milliseconds, train=3.3 seconds, 187136 images, time remaining=5 hours
2925: loss=5.018, avg loss=4.887, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 187200 images, time remaining=5 hours
2926: loss=3.868, avg loss=4.785, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=586.9 milliseconds, train=3.2 seconds, 187264 images, time remaining=5 hours
2927: loss=3.986, avg loss=4.705, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=518.7 milliseconds, train=3.3 seconds, 187328 images, time remaining=5 hours
2928: loss=5.380, avg loss=4.772, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 187392 images, time remaining=5 hours
2929: loss=5.442, avg loss=4.839, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 187456 images, time remaining=5 hours
2930: loss=3.497, avg loss=4.705, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=839.2 milliseconds, train=3.2 seconds, 187520 images, time remaining=5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145d12000000
2931: loss=5.746, avg loss=4.809, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=552.6 milliseconds, train=1.2 seconds, 187584 images, time remaining=5 hours
2932: loss=5.306, avg loss=4.859, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=598.0 milliseconds, train=1.2 seconds, 187648 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2933: loss=4.976, avg loss=4.871, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.8 seconds, train=1.2 seconds, 187712 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2934: loss=4.306, avg loss=4.814, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 187776 images, time remaining=5 hours
2935: loss=4.434, avg loss=4.776, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 187840 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2936: loss=5.022, avg loss=4.801, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 187904 images, time remaining=5 hours
2937: loss=4.727, avg loss=4.793, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 187968 images, time remaining=5 hours
2938: loss=5.034, avg loss=4.818, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 188032 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2939: loss=4.491, avg loss=4.785, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.3 seconds, train=1.2 seconds, 188096 images, time remaining=5 hours
2940: loss=4.126, avg loss=4.719, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=838.3 milliseconds, train=1.2 seconds, 188160 images, time remaining=5 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2941: loss=4.881, avg loss=4.735, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 188224 images, time remaining=5 hours
2942: loss=4.613, avg loss=4.723, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=866.8 milliseconds, train=1.5 seconds, 188288 images, time remaining=5 hours
2943: loss=4.453, avg loss=4.696, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=667.4 milliseconds, train=1.6 seconds, 188352 images, time remaining=5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2944: loss=4.651, avg loss=4.691, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 188416 images, time remaining=5 hours
2945: loss=4.700, avg loss=4.692, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=840.4 milliseconds, train=1.5 seconds, 188480 images, time remaining=4.9 hours
2946: loss=4.344, avg loss=4.657, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=730.9 milliseconds, train=1.6 seconds, 188544 images, time remaining=4.9 hours
2947: loss=3.607, avg loss=4.552, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=580.9 milliseconds, train=1.5 seconds, 188608 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2948: loss=4.187, avg loss=4.516, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 188672 images, time remaining=4.9 hours
2949: loss=4.503, avg loss=4.515, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=593.0 milliseconds, train=1.5 seconds, 188736 images, time remaining=4.9 hours
2950: loss=3.828, avg loss=4.446, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=461.9 milliseconds, train=1.5 seconds, 188800 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
2951: loss=4.898, avg loss=4.491, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 188864 images, time remaining=4.9 hours
2952: loss=4.859, avg loss=4.528, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.6 seconds, train=2.0 seconds, 188928 images, time remaining=4.9 hours
2953: loss=5.168, avg loss=4.592, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=880.2 milliseconds, train=2.0 seconds, 188992 images, time remaining=4.9 hours
2954: loss=4.972, avg loss=4.630, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=883.0 milliseconds, train=2.0 seconds, 189056 images, time remaining=4.9 hours
2955: loss=4.025, avg loss=4.569, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 189120 images, time remaining=4.9 hours
2956: loss=4.377, avg loss=4.550, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=738.0 milliseconds, train=2.0 seconds, 189184 images, time remaining=4.9 hours
2957: loss=4.271, avg loss=4.522, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=531.1 milliseconds, train=2.0 seconds, 189248 images, time remaining=4.9 hours
2958: loss=4.127, avg loss=4.483, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=688.9 milliseconds, train=2.0 seconds, 189312 images, time remaining=4.9 hours
2959: loss=4.416, avg loss=4.476, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 189376 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2960: loss=3.697, avg loss=4.398, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.0 seconds, train=2.0 seconds, 189440 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x145ff2e00000
2961: loss=5.345, avg loss=4.493, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=584.5 milliseconds, train=1.1 seconds, 189504 images, time remaining=4.9 hours
2962: loss=4.589, avg loss=4.502, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=586.6 milliseconds, train=1.1 seconds, 189568 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2963: loss=4.298, avg loss=4.482, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=4.3 seconds, train=1.1 seconds, 189632 images, time remaining=4.9 hours
2964: loss=4.844, avg loss=4.518, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.1 seconds, train=1.1 seconds, 189696 images, time remaining=4.9 hours
2965: loss=4.931, avg loss=4.560, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 189760 images, time remaining=4.9 hours
2966: loss=4.128, avg loss=4.516, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=591.5 milliseconds, train=1.1 seconds, 189824 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2967: loss=4.577, avg loss=4.522, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.9 seconds, train=1.1 seconds, 189888 images, time remaining=4.9 hours
2968: loss=5.321, avg loss=4.602, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=1.1 seconds, 189952 images, time remaining=4.9 hours
2969: loss=4.408, avg loss=4.583, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=871.2 milliseconds, train=1.1 seconds, 190016 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2970: loss=4.879, avg loss=4.612, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.3 seconds, train=1.1 seconds, 190080 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146396000000
2971: loss=4.067, avg loss=4.558, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=614.8 milliseconds, train=1.5 seconds, 190144 images, time remaining=4.9 hours
2972: loss=4.159, avg loss=4.518, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 190208 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2973: loss=4.714, avg loss=4.538, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 190272 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2974: loss=3.693, avg loss=4.453, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 190336 images, time remaining=4.9 hours
2975: loss=3.844, avg loss=4.392, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=441.4 milliseconds, train=1.5 seconds, 190400 images, time remaining=4.9 hours
2976: loss=4.811, avg loss=4.434, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 190464 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2977: loss=4.915, avg loss=4.482, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=4.7 seconds, train=1.5 seconds, 190528 images, time remaining=4.9 hours
2978: loss=3.746, avg loss=4.409, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=953.6 milliseconds, train=1.5 seconds, 190592 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2979: loss=4.917, avg loss=4.459, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 190656 images, time remaining=4.9 hours
2980: loss=4.421, avg loss=4.456, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=902.6 milliseconds, train=1.5 seconds, 190720 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
2981: loss=6.343, avg loss=4.644, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.6 seconds, train=4.4 seconds, 190784 images, time remaining=4.9 hours
2982: loss=6.753, avg loss=4.855, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=808.6 milliseconds, train=4.4 seconds, 190848 images, time remaining=4.9 hours
2983: loss=6.230, avg loss=4.993, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 190912 images, time remaining=4.9 hours
2984: loss=5.012, avg loss=4.995, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.7 seconds, train=4.4 seconds, 190976 images, time remaining=4.9 hours
2985: loss=4.807, avg loss=4.976, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=4.5 seconds, 191040 images, time remaining=4.9 hours
2986: loss=4.907, avg loss=4.969, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.5 seconds, train=4.4 seconds, 191104 images, time remaining=4.9 hours
2987: loss=5.979, avg loss=5.070, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=608.7 milliseconds, train=4.4 seconds, 191168 images, time remaining=4.9 hours
2988: loss=5.412, avg loss=5.104, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=962.0 milliseconds, train=4.4 seconds, 191232 images, time remaining=4.9 hours
2989: loss=5.250, avg loss=5.119, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=4.4 seconds, 191296 images, time remaining=4.9 hours
2990: loss=5.065, avg loss=5.113, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 191360 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1459b2000000
2991: loss=5.333, avg loss=5.135, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=923.4 milliseconds, train=1.4 seconds, 191424 images, time remaining=4.9 hours
2992: loss=4.434, avg loss=5.065, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=989.1 milliseconds, train=1.4 seconds, 191488 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2993: loss=4.706, avg loss=5.029, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 191552 images, time remaining=4.9 hours
2994: loss=4.415, avg loss=4.968, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=499.1 milliseconds, train=1.4 seconds, 191616 images, time remaining=4.9 hours
2995: loss=4.458, avg loss=4.917, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=739.1 milliseconds, train=1.4 seconds, 191680 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2996: loss=4.599, avg loss=4.885, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 191744 images, time remaining=4.9 hours
2997: loss=4.782, avg loss=4.875, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 191808 images, time remaining=4.9 hours
2998: loss=5.198, avg loss=4.907, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=661.4 milliseconds, train=1.4 seconds, 191872 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
2999: loss=5.003, avg loss=4.917, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.1 seconds, train=1.4 seconds, 191936 images, time remaining=4.9 hours
3000: loss=4.770, avg loss=4.902, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=630.7 milliseconds, train=1.4 seconds, 192000 images, time remaining=4.9 hours
Saving weights to /workspace/.cache/splits/combined_3000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
3001: loss=5.885, avg loss=5.000, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 192064 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3002: loss=5.067, avg loss=5.007, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=2.0 seconds, 192128 images, time remaining=4.9 hours
3003: loss=4.799, avg loss=4.986, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 192192 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3004: loss=4.034, avg loss=4.891, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 192256 images, time remaining=4.9 hours
3005: loss=5.235, avg loss=4.925, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 192320 images, time remaining=4.9 hours
3006: loss=4.235, avg loss=4.856, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 192384 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3007: loss=5.420, avg loss=4.913, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 192448 images, time remaining=4.9 hours
3008: loss=4.799, avg loss=4.901, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 192512 images, time remaining=4.9 hours
3009: loss=4.271, avg loss=4.838, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=850.8 milliseconds, train=2.0 seconds, 192576 images, time remaining=4.9 hours
3010: loss=4.718, avg loss=4.826, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 192640 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145c74000000
3011: loss=4.324, avg loss=4.776, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=912.5 milliseconds, train=1.6 seconds, 192704 images, time remaining=4.9 hours
3012: loss=4.301, avg loss=4.729, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=887.8 milliseconds, train=1.5 seconds, 192768 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3013: loss=4.514, avg loss=4.707, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 192832 images, time remaining=4.9 hours
3014: loss=4.075, avg loss=4.644, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=915.8 milliseconds, train=1.6 seconds, 192896 images, time remaining=4.9 hours
3015: loss=5.189, avg loss=4.698, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=468.4 milliseconds, train=1.6 seconds, 192960 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3016: loss=4.078, avg loss=4.636, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.8 seconds, train=1.5 seconds, 193024 images, time remaining=4.8 hours
3017: loss=3.962, avg loss=4.569, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 193088 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3018: loss=4.217, avg loss=4.534, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 193152 images, time remaining=4.8 hours
3019: loss=4.265, avg loss=4.507, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=881.2 milliseconds, train=1.5 seconds, 193216 images, time remaining=4.8 hours
3020: loss=4.315, avg loss=4.488, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=974.0 milliseconds, train=1.5 seconds, 193280 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145c74000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3021: loss=4.442, avg loss=4.483, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 193344 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3022: loss=4.137, avg loss=4.448, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 193408 images, time remaining=4.8 hours
3023: loss=4.115, avg loss=4.415, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=583.4 milliseconds, train=1.5 seconds, 193472 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3024: loss=4.625, avg loss=4.436, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 193536 images, time remaining=4.8 hours
3025: loss=4.541, avg loss=4.446, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 193600 images, time remaining=4.8 hours
3026: loss=4.487, avg loss=4.451, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=849.8 milliseconds, train=1.5 seconds, 193664 images, time remaining=4.8 hours
3027: loss=3.820, avg loss=4.387, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=655.9 milliseconds, train=1.5 seconds, 193728 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3028: loss=4.496, avg loss=4.398, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 193792 images, time remaining=4.8 hours
3029: loss=4.383, avg loss=4.397, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=825.7 milliseconds, train=1.5 seconds, 193856 images, time remaining=4.8 hours
3030: loss=5.163, avg loss=4.473, last=86.07%, best=86.07%, next=3030, rate=0.00130000, load 64=898.1 milliseconds, train=1.5 seconds, 193920 images, time remaining=4.8 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=145787, unique_truth_count=57264
rank=0 of ranks=145787rank=100 of ranks=145787rank=200 of ranks=145787rank=300 of ranks=145787rank=400 of ranks=145787rank=500 of ranks=145787rank=600 of ranks=145787rank=700 of ranks=145787rank=800 of ranks=145787rank=900 of ranks=145787rank=1000 of ranks=145787rank=1100 of ranks=145787rank=1200 of ranks=145787rank=1300 of ranks=145787rank=1400 of ranks=145787rank=1500 of ranks=145787rank=1600 of ranks=145787rank=1700 of ranks=145787rank=1800 of ranks=145787rank=1900 of ranks=145787rank=2000 of ranks=145787rank=2100 of ranks=145787rank=2200 of ranks=145787rank=2300 of ranks=145787rank=2400 of ranks=145787rank=2500 of ranks=145787rank=2600 of ranks=145787rank=2700 of ranks=145787rank=2800 of ranks=145787rank=2900 of ranks=145787rank=3000 of ranks=145787rank=3100 of ranks=145787rank=3200 of ranks=145787rank=3300 of ranks=145787rank=3400 of ranks=145787rank=3500 of ranks=145787rank=3600 of ranks=145787rank=3700 of ranks=145787rank=3800 of ranks=145787rank=3900 of ranks=145787rank=4000 of ranks=145787rank=4100 of ranks=145787rank=4200 of ranks=145787rank=4300 of ranks=145787rank=4400 of ranks=145787rank=4500 of ranks=145787rank=4600 of ranks=145787rank=4700 of ranks=145787rank=4800 of ranks=145787rank=4900 of ranks=145787rank=5000 of ranks=145787rank=5100 of ranks=145787rank=5200 of ranks=145787rank=5300 of ranks=145787rank=5400 of ranks=145787rank=5500 of ranks=145787rank=5600 of ranks=145787rank=5700 of ranks=145787rank=5800 of ranks=145787rank=5900 of ranks=145787rank=6000 of ranks=145787rank=6100 of ranks=145787rank=6200 of ranks=145787rank=6300 of ranks=145787rank=6400 of ranks=145787rank=6500 of ranks=145787rank=6600 of ranks=145787rank=6700 of ranks=145787rank=6800 of ranks=145787rank=6900 of ranks=145787rank=7000 of ranks=145787rank=7100 of ranks=145787rank=7200 of ranks=145787rank=7300 of ranks=145787rank=7400 of ranks=145787rank=7500 of ranks=145787rank=7600 of ranks=145787rank=7700 of ranks=145787rank=7800 of ranks=145787rank=7900 of ranks=145787rank=8000 of ranks=145787rank=8100 of ranks=145787rank=8200 of ranks=145787rank=8300 of ranks=145787rank=8400 of ranks=145787rank=8500 of ranks=145787rank=8600 of ranks=145787rank=8700 of ranks=145787rank=8800 of ranks=145787rank=8900 of ranks=145787rank=9000 of ranks=145787rank=9100 of ranks=145787rank=9200 of ranks=145787rank=9300 of ranks=145787rank=9400 of ranks=145787rank=9500 of ranks=145787rank=9600 of ranks=145787rank=9700 of ranks=145787rank=9800 of ranks=145787rank=9900 of ranks=145787rank=10000 of ranks=145787rank=10100 of ranks=145787rank=10200 of ranks=145787rank=10300 of ranks=145787rank=10400 of ranks=145787rank=10500 of ranks=145787rank=10600 of ranks=145787rank=10700 of ranks=145787rank=10800 of ranks=145787rank=10900 of ranks=145787rank=11000 of ranks=145787rank=11100 of ranks=145787rank=11200 of ranks=145787rank=11300 of ranks=145787rank=11400 of ranks=145787rank=11500 of ranks=145787rank=11600 of ranks=145787rank=11700 of ranks=145787rank=11800 of ranks=145787rank=11900 of ranks=145787rank=12000 of ranks=145787rank=12100 of ranks=145787rank=12200 of ranks=145787rank=12300 of ranks=145787rank=12400 of ranks=145787rank=12500 of ranks=145787rank=12600 of ranks=145787rank=12700 of ranks=145787rank=12800 of ranks=145787rank=12900 of ranks=145787rank=13000 of ranks=145787rank=13100 of ranks=145787rank=13200 of ranks=145787rank=13300 of ranks=145787rank=13400 of ranks=145787rank=13500 of ranks=145787rank=13600 of ranks=145787rank=13700 of ranks=145787rank=13800 of ranks=145787rank=13900 of ranks=145787rank=14000 of ranks=145787rank=14100 of ranks=145787rank=14200 of ranks=145787rank=14300 of ranks=145787rank=14400 of ranks=145787rank=14500 of ranks=145787rank=14600 of ranks=145787rank=14700 of ranks=145787rank=14800 of ranks=145787rank=14900 of ranks=145787rank=15000 of ranks=145787rank=15100 of ranks=145787rank=15200 of ranks=145787rank=15300 of ranks=145787rank=15400 of ranks=145787rank=15500 of ranks=145787rank=15600 of ranks=145787rank=15700 of ranks=145787rank=15800 of ranks=145787rank=15900 of ranks=145787rank=16000 of ranks=145787rank=16100 of ranks=145787rank=16200 of ranks=145787rank=16300 of ranks=145787rank=16400 of ranks=145787rank=16500 of ranks=145787rank=16600 of ranks=145787rank=16700 of ranks=145787rank=16800 of ranks=145787rank=16900 of ranks=145787rank=17000 of ranks=145787rank=17100 of ranks=145787rank=17200 of ranks=145787rank=17300 of ranks=145787rank=17400 of ranks=145787rank=17500 of ranks=145787rank=17600 of ranks=145787rank=17700 of ranks=145787rank=17800 of ranks=145787rank=17900 of ranks=145787rank=18000 of ranks=145787rank=18100 of ranks=145787rank=18200 of ranks=145787rank=18300 of ranks=145787rank=18400 of ranks=145787rank=18500 of ranks=145787rank=18600 of ranks=145787rank=18700 of ranks=145787rank=18800 of ranks=145787rank=18900 of ranks=145787rank=19000 of ranks=145787rank=19100 of ranks=145787rank=19200 of ranks=145787rank=19300 of ranks=145787rank=19400 of ranks=145787rank=19500 of ranks=145787rank=19600 of ranks=145787rank=19700 of ranks=145787rank=19800 of ranks=145787rank=19900 of ranks=145787rank=20000 of ranks=145787rank=20100 of ranks=145787rank=20200 of ranks=145787rank=20300 of ranks=145787rank=20400 of ranks=145787rank=20500 of ranks=145787rank=20600 of ranks=145787rank=20700 of ranks=145787rank=20800 of ranks=145787rank=20900 of ranks=145787rank=21000 of ranks=145787rank=21100 of ranks=145787rank=21200 of ranks=145787rank=21300 of ranks=145787rank=21400 of ranks=145787rank=21500 of ranks=145787rank=21600 of ranks=145787rank=21700 of ranks=145787rank=21800 of ranks=145787rank=21900 of ranks=145787rank=22000 of ranks=145787rank=22100 of ranks=145787rank=22200 of ranks=145787rank=22300 of ranks=145787rank=22400 of ranks=145787rank=22500 of ranks=145787rank=22600 of ranks=145787rank=22700 of ranks=145787rank=22800 of ranks=145787rank=22900 of ranks=145787rank=23000 of ranks=145787rank=23100 of ranks=145787rank=23200 of ranks=145787rank=23300 of ranks=145787rank=23400 of ranks=145787rank=23500 of ranks=145787rank=23600 of ranks=145787rank=23700 of ranks=145787rank=23800 of ranks=145787rank=23900 of ranks=145787rank=24000 of ranks=145787rank=24100 of ranks=145787rank=24200 of ranks=145787rank=24300 of ranks=145787rank=24400 of ranks=145787rank=24500 of ranks=145787rank=24600 of ranks=145787rank=24700 of ranks=145787rank=24800 of ranks=145787rank=24900 of ranks=145787rank=25000 of ranks=145787rank=25100 of ranks=145787rank=25200 of ranks=145787rank=25300 of ranks=145787rank=25400 of ranks=145787rank=25500 of ranks=145787rank=25600 of ranks=145787rank=25700 of ranks=145787rank=25800 of ranks=145787rank=25900 of ranks=145787rank=26000 of ranks=145787rank=26100 of ranks=145787rank=26200 of ranks=145787rank=26300 of ranks=145787rank=26400 of ranks=145787rank=26500 of ranks=145787rank=26600 of ranks=145787rank=26700 of ranks=145787rank=26800 of ranks=145787rank=26900 of ranks=145787rank=27000 of ranks=145787rank=27100 of ranks=145787rank=27200 of ranks=145787rank=27300 of ranks=145787rank=27400 of ranks=145787rank=27500 of ranks=145787rank=27600 of ranks=145787rank=27700 of ranks=145787rank=27800 of ranks=145787rank=27900 of ranks=145787rank=28000 of ranks=145787rank=28100 of ranks=145787rank=28200 of ranks=145787rank=28300 of ranks=145787rank=28400 of ranks=145787rank=28500 of ranks=145787rank=28600 of ranks=145787rank=28700 of ranks=145787rank=28800 of ranks=145787rank=28900 of ranks=145787rank=29000 of ranks=145787rank=29100 of ranks=145787rank=29200 of ranks=145787rank=29300 of ranks=145787rank=29400 of ranks=145787rank=29500 of ranks=145787rank=29600 of ranks=145787rank=29700 of ranks=145787rank=29800 of ranks=145787rank=29900 of ranks=145787rank=30000 of ranks=145787rank=30100 of ranks=145787rank=30200 of ranks=145787rank=30300 of ranks=145787rank=30400 of ranks=145787rank=30500 of ranks=145787rank=30600 of ranks=145787rank=30700 of ranks=145787rank=30800 of ranks=145787rank=30900 of ranks=145787rank=31000 of ranks=145787rank=31100 of ranks=145787rank=31200 of ranks=145787rank=31300 of ranks=145787rank=31400 of ranks=145787rank=31500 of ranks=145787rank=31600 of ranks=145787rank=31700 of ranks=145787rank=31800 of ranks=145787rank=31900 of ranks=145787rank=32000 of ranks=145787rank=32100 of ranks=145787rank=32200 of ranks=145787rank=32300 of ranks=145787rank=32400 of ranks=145787rank=32500 of ranks=145787rank=32600 of ranks=145787rank=32700 of ranks=145787rank=32800 of ranks=145787rank=32900 of ranks=145787rank=33000 of ranks=145787rank=33100 of ranks=145787rank=33200 of ranks=145787rank=33300 of ranks=145787rank=33400 of ranks=145787rank=33500 of ranks=145787rank=33600 of ranks=145787rank=33700 of ranks=145787rank=33800 of ranks=145787rank=33900 of ranks=145787rank=34000 of ranks=145787rank=34100 of ranks=145787rank=34200 of ranks=145787rank=34300 of ranks=145787rank=34400 of ranks=145787rank=34500 of ranks=145787rank=34600 of ranks=145787rank=34700 of ranks=145787rank=34800 of ranks=145787rank=34900 of ranks=145787rank=35000 of ranks=145787rank=35100 of ranks=145787rank=35200 of ranks=145787rank=35300 of ranks=145787rank=35400 of ranks=145787rank=35500 of ranks=145787rank=35600 of ranks=145787rank=35700 of ranks=145787rank=35800 of ranks=145787rank=35900 of ranks=145787rank=36000 of ranks=145787rank=36100 of ranks=145787rank=36200 of ranks=145787rank=36300 of ranks=145787rank=36400 of ranks=145787rank=36500 of ranks=145787rank=36600 of ranks=145787rank=36700 of ranks=145787rank=36800 of ranks=145787rank=36900 of ranks=145787rank=37000 of ranks=145787rank=37100 of ranks=145787rank=37200 of ranks=145787rank=37300 of ranks=145787rank=37400 of ranks=145787rank=37500 of ranks=145787rank=37600 of ranks=145787rank=37700 of ranks=145787rank=37800 of ranks=145787rank=37900 of ranks=145787rank=38000 of ranks=145787rank=38100 of ranks=145787rank=38200 of ranks=145787rank=38300 of ranks=145787rank=38400 of ranks=145787rank=38500 of ranks=145787rank=38600 of ranks=145787rank=38700 of ranks=145787rank=38800 of ranks=145787rank=38900 of ranks=145787rank=39000 of ranks=145787rank=39100 of ranks=145787rank=39200 of ranks=145787rank=39300 of ranks=145787rank=39400 of ranks=145787rank=39500 of ranks=145787rank=39600 of ranks=145787rank=39700 of ranks=145787rank=39800 of ranks=145787rank=39900 of ranks=145787rank=40000 of ranks=145787rank=40100 of ranks=145787rank=40200 of ranks=145787rank=40300 of ranks=145787rank=40400 of ranks=145787rank=40500 of ranks=145787rank=40600 of ranks=145787rank=40700 of ranks=145787rank=40800 of ranks=145787rank=40900 of ranks=145787rank=41000 of ranks=145787rank=41100 of ranks=145787rank=41200 of ranks=145787rank=41300 of ranks=145787rank=41400 of ranks=145787rank=41500 of ranks=145787rank=41600 of ranks=145787rank=41700 of ranks=145787rank=41800 of ranks=145787rank=41900 of ranks=145787rank=42000 of ranks=145787rank=42100 of ranks=145787rank=42200 of ranks=145787rank=42300 of ranks=145787rank=42400 of ranks=145787rank=42500 of ranks=145787rank=42600 of ranks=145787rank=42700 of ranks=145787rank=42800 of ranks=145787rank=42900 of ranks=145787rank=43000 of ranks=145787rank=43100 of ranks=145787rank=43200 of ranks=145787rank=43300 of ranks=145787rank=43400 of ranks=145787rank=43500 of ranks=145787rank=43600 of ranks=145787rank=43700 of ranks=145787rank=43800 of ranks=145787rank=43900 of ranks=145787rank=44000 of ranks=145787rank=44100 of ranks=145787rank=44200 of ranks=145787rank=44300 of ranks=145787rank=44400 of ranks=145787rank=44500 of ranks=145787rank=44600 of ranks=145787rank=44700 of ranks=145787rank=44800 of ranks=145787rank=44900 of ranks=145787rank=45000 of ranks=145787rank=45100 of ranks=145787rank=45200 of ranks=145787rank=45300 of ranks=145787rank=45400 of ranks=145787rank=45500 of ranks=145787rank=45600 of ranks=145787rank=45700 of ranks=145787rank=45800 of ranks=145787rank=45900 of ranks=145787rank=46000 of ranks=145787rank=46100 of ranks=145787rank=46200 of ranks=145787rank=46300 of ranks=145787rank=46400 of ranks=145787rank=46500 of ranks=145787rank=46600 of ranks=145787rank=46700 of ranks=145787rank=46800 of ranks=145787rank=46900 of ranks=145787rank=47000 of ranks=145787rank=47100 of ranks=145787rank=47200 of ranks=145787rank=47300 of ranks=145787rank=47400 of ranks=145787rank=47500 of ranks=145787rank=47600 of ranks=145787rank=47700 of ranks=145787rank=47800 of ranks=145787rank=47900 of ranks=145787rank=48000 of ranks=145787rank=48100 of ranks=145787rank=48200 of ranks=145787rank=48300 of ranks=145787rank=48400 of ranks=145787rank=48500 of ranks=145787rank=48600 of ranks=145787rank=48700 of ranks=145787rank=48800 of ranks=145787rank=48900 of ranks=145787rank=49000 of ranks=145787rank=49100 of ranks=145787rank=49200 of ranks=145787rank=49300 of ranks=145787rank=49400 of ranks=145787rank=49500 of ranks=145787rank=49600 of ranks=145787rank=49700 of ranks=145787rank=49800 of ranks=145787rank=49900 of ranks=145787rank=50000 of ranks=145787rank=50100 of ranks=145787rank=50200 of ranks=145787rank=50300 of ranks=145787rank=50400 of ranks=145787rank=50500 of ranks=145787rank=50600 of ranks=145787rank=50700 of ranks=145787rank=50800 of ranks=145787rank=50900 of ranks=145787rank=51000 of ranks=145787rank=51100 of ranks=145787rank=51200 of ranks=145787rank=51300 of ranks=145787rank=51400 of ranks=145787rank=51500 of ranks=145787rank=51600 of ranks=145787rank=51700 of ranks=145787rank=51800 of ranks=145787rank=51900 of ranks=145787rank=52000 of ranks=145787rank=52100 of ranks=145787rank=52200 of ranks=145787rank=52300 of ranks=145787rank=52400 of ranks=145787rank=52500 of ranks=145787rank=52600 of ranks=145787rank=52700 of ranks=145787rank=52800 of ranks=145787rank=52900 of ranks=145787rank=53000 of ranks=145787rank=53100 of ranks=145787rank=53200 of ranks=145787rank=53300 of ranks=145787rank=53400 of ranks=145787rank=53500 of ranks=145787rank=53600 of ranks=145787rank=53700 of ranks=145787rank=53800 of ranks=145787rank=53900 of ranks=145787rank=54000 of ranks=145787rank=54100 of ranks=145787rank=54200 of ranks=145787rank=54300 of ranks=145787rank=54400 of ranks=145787rank=54500 of ranks=145787rank=54600 of ranks=145787rank=54700 of ranks=145787rank=54800 of ranks=145787rank=54900 of ranks=145787rank=55000 of ranks=145787rank=55100 of ranks=145787rank=55200 of ranks=145787rank=55300 of ranks=145787rank=55400 of ranks=145787rank=55500 of ranks=145787rank=55600 of ranks=145787rank=55700 of ranks=145787rank=55800 of ranks=145787rank=55900 of ranks=145787rank=56000 of ranks=145787rank=56100 of ranks=145787rank=56200 of ranks=145787rank=56300 of ranks=145787rank=56400 of ranks=145787rank=56500 of ranks=145787rank=56600 of ranks=145787rank=56700 of ranks=145787rank=56800 of ranks=145787rank=56900 of ranks=145787rank=57000 of ranks=145787rank=57100 of ranks=145787rank=57200 of ranks=145787rank=57300 of ranks=145787rank=57400 of ranks=145787rank=57500 of ranks=145787rank=57600 of ranks=145787rank=57700 of ranks=145787rank=57800 of ranks=145787rank=57900 of ranks=145787rank=58000 of ranks=145787rank=58100 of ranks=145787rank=58200 of ranks=145787rank=58300 of ranks=145787rank=58400 of ranks=145787rank=58500 of ranks=145787rank=58600 of ranks=145787rank=58700 of ranks=145787rank=58800 of ranks=145787rank=58900 of ranks=145787rank=59000 of ranks=145787rank=59100 of ranks=145787rank=59200 of ranks=145787rank=59300 of ranks=145787rank=59400 of ranks=145787rank=59500 of ranks=145787rank=59600 of ranks=145787rank=59700 of ranks=145787rank=59800 of ranks=145787rank=59900 of ranks=145787rank=60000 of ranks=145787rank=60100 of ranks=145787rank=60200 of ranks=145787rank=60300 of ranks=145787rank=60400 of ranks=145787rank=60500 of ranks=145787rank=60600 of ranks=145787rank=60700 of ranks=145787rank=60800 of ranks=145787rank=60900 of ranks=145787rank=61000 of ranks=145787rank=61100 of ranks=145787rank=61200 of ranks=145787rank=61300 of ranks=145787rank=61400 of ranks=145787rank=61500 of ranks=145787rank=61600 of ranks=145787rank=61700 of ranks=145787rank=61800 of ranks=145787rank=61900 of ranks=145787rank=62000 of ranks=145787rank=62100 of ranks=145787rank=62200 of ranks=145787rank=62300 of ranks=145787rank=62400 of ranks=145787rank=62500 of ranks=145787rank=62600 of ranks=145787rank=62700 of ranks=145787rank=62800 of ranks=145787rank=62900 of ranks=145787rank=63000 of ranks=145787rank=63100 of ranks=145787rank=63200 of ranks=145787rank=63300 of ranks=145787rank=63400 of ranks=145787rank=63500 of ranks=145787rank=63600 of ranks=145787rank=63700 of ranks=145787rank=63800 of ranks=145787rank=63900 of ranks=145787rank=64000 of ranks=145787rank=64100 of ranks=145787rank=64200 of ranks=145787rank=64300 of ranks=145787rank=64400 of ranks=145787rank=64500 of ranks=145787rank=64600 of ranks=145787rank=64700 of ranks=145787rank=64800 of ranks=145787rank=64900 of ranks=145787rank=65000 of ranks=145787rank=65100 of ranks=145787rank=65200 of ranks=145787rank=65300 of ranks=145787rank=65400 of ranks=145787rank=65500 of ranks=145787rank=65600 of ranks=145787rank=65700 of ranks=145787rank=65800 of ranks=145787rank=65900 of ranks=145787rank=66000 of ranks=145787rank=66100 of ranks=145787rank=66200 of ranks=145787rank=66300 of ranks=145787rank=66400 of ranks=145787rank=66500 of ranks=145787rank=66600 of ranks=145787rank=66700 of ranks=145787rank=66800 of ranks=145787rank=66900 of ranks=145787rank=67000 of ranks=145787rank=67100 of ranks=145787rank=67200 of ranks=145787rank=67300 of ranks=145787rank=67400 of ranks=145787rank=67500 of ranks=145787rank=67600 of ranks=145787rank=67700 of ranks=145787rank=67800 of ranks=145787rank=67900 of ranks=145787rank=68000 of ranks=145787rank=68100 of ranks=145787rank=68200 of ranks=145787rank=68300 of ranks=145787rank=68400 of ranks=145787rank=68500 of ranks=145787rank=68600 of ranks=145787rank=68700 of ranks=145787rank=68800 of ranks=145787rank=68900 of ranks=145787rank=69000 of ranks=145787rank=69100 of ranks=145787rank=69200 of ranks=145787rank=69300 of ranks=145787rank=69400 of ranks=145787rank=69500 of ranks=145787rank=69600 of ranks=145787rank=69700 of ranks=145787rank=69800 of ranks=145787rank=69900 of ranks=145787rank=70000 of ranks=145787rank=70100 of ranks=145787rank=70200 of ranks=145787rank=70300 of ranks=145787rank=70400 of ranks=145787rank=70500 of ranks=145787rank=70600 of ranks=145787rank=70700 of ranks=145787rank=70800 of ranks=145787rank=70900 of ranks=145787rank=71000 of ranks=145787rank=71100 of ranks=145787rank=71200 of ranks=145787rank=71300 of ranks=145787rank=71400 of ranks=145787rank=71500 of ranks=145787rank=71600 of ranks=145787rank=71700 of ranks=145787rank=71800 of ranks=145787rank=71900 of ranks=145787rank=72000 of ranks=145787rank=72100 of ranks=145787rank=72200 of ranks=145787rank=72300 of ranks=145787rank=72400 of ranks=145787rank=72500 of ranks=145787rank=72600 of ranks=145787rank=72700 of ranks=145787rank=72800 of ranks=145787rank=72900 of ranks=145787rank=73000 of ranks=145787rank=73100 of ranks=145787rank=73200 of ranks=145787rank=73300 of ranks=145787rank=73400 of ranks=145787rank=73500 of ranks=145787rank=73600 of ranks=145787rank=73700 of ranks=145787rank=73800 of ranks=145787rank=73900 of ranks=145787rank=74000 of ranks=145787rank=74100 of ranks=145787rank=74200 of ranks=145787rank=74300 of ranks=145787rank=74400 of ranks=145787rank=74500 of ranks=145787rank=74600 of ranks=145787rank=74700 of ranks=145787rank=74800 of ranks=145787rank=74900 of ranks=145787rank=75000 of ranks=145787rank=75100 of ranks=145787rank=75200 of ranks=145787rank=75300 of ranks=145787rank=75400 of ranks=145787rank=75500 of ranks=145787rank=75600 of ranks=145787rank=75700 of ranks=145787rank=75800 of ranks=145787rank=75900 of ranks=145787rank=76000 of ranks=145787rank=76100 of ranks=145787rank=76200 of ranks=145787rank=76300 of ranks=145787rank=76400 of ranks=145787rank=76500 of ranks=145787rank=76600 of ranks=145787rank=76700 of ranks=145787rank=76800 of ranks=145787rank=76900 of ranks=145787rank=77000 of ranks=145787rank=77100 of ranks=145787rank=77200 of ranks=145787rank=77300 of ranks=145787rank=77400 of ranks=145787rank=77500 of ranks=145787rank=77600 of ranks=145787rank=77700 of ranks=145787rank=77800 of ranks=145787rank=77900 of ranks=145787rank=78000 of ranks=145787rank=78100 of ranks=145787rank=78200 of ranks=145787rank=78300 of ranks=145787rank=78400 of ranks=145787rank=78500 of ranks=145787rank=78600 of ranks=145787rank=78700 of ranks=145787rank=78800 of ranks=145787rank=78900 of ranks=145787rank=79000 of ranks=145787rank=79100 of ranks=145787rank=79200 of ranks=145787rank=79300 of ranks=145787rank=79400 of ranks=145787rank=79500 of ranks=145787rank=79600 of ranks=145787rank=79700 of ranks=145787rank=79800 of ranks=145787rank=79900 of ranks=145787rank=80000 of ranks=145787rank=80100 of ranks=145787rank=80200 of ranks=145787rank=80300 of ranks=145787rank=80400 of ranks=145787rank=80500 of ranks=145787rank=80600 of ranks=145787rank=80700 of ranks=145787rank=80800 of ranks=145787rank=80900 of ranks=145787rank=81000 of ranks=145787rank=81100 of ranks=145787rank=81200 of ranks=145787rank=81300 of ranks=145787rank=81400 of ranks=145787rank=81500 of ranks=145787rank=81600 of ranks=145787rank=81700 of ranks=145787rank=81800 of ranks=145787rank=81900 of ranks=145787rank=82000 of ranks=145787rank=82100 of ranks=145787rank=82200 of ranks=145787rank=82300 of ranks=145787rank=82400 of ranks=145787rank=82500 of ranks=145787rank=82600 of ranks=145787rank=82700 of ranks=145787rank=82800 of ranks=145787rank=82900 of ranks=145787rank=83000 of ranks=145787rank=83100 of ranks=145787rank=83200 of ranks=145787rank=83300 of ranks=145787rank=83400 of ranks=145787rank=83500 of ranks=145787rank=83600 of ranks=145787rank=83700 of ranks=145787rank=83800 of ranks=145787rank=83900 of ranks=145787rank=84000 of ranks=145787rank=84100 of ranks=145787rank=84200 of ranks=145787rank=84300 of ranks=145787rank=84400 of ranks=145787rank=84500 of ranks=145787rank=84600 of ranks=145787rank=84700 of ranks=145787rank=84800 of ranks=145787rank=84900 of ranks=145787rank=85000 of ranks=145787rank=85100 of ranks=145787rank=85200 of ranks=145787rank=85300 of ranks=145787rank=85400 of ranks=145787rank=85500 of ranks=145787rank=85600 of ranks=145787rank=85700 of ranks=145787rank=85800 of ranks=145787rank=85900 of ranks=145787rank=86000 of ranks=145787rank=86100 of ranks=145787rank=86200 of ranks=145787rank=86300 of ranks=145787rank=86400 of ranks=145787rank=86500 of ranks=145787rank=86600 of ranks=145787rank=86700 of ranks=145787rank=86800 of ranks=145787rank=86900 of ranks=145787rank=87000 of ranks=145787rank=87100 of ranks=145787rank=87200 of ranks=145787rank=87300 of ranks=145787rank=87400 of ranks=145787rank=87500 of ranks=145787rank=87600 of ranks=145787rank=87700 of ranks=145787rank=87800 of ranks=145787rank=87900 of ranks=145787rank=88000 of ranks=145787rank=88100 of ranks=145787rank=88200 of ranks=145787rank=88300 of ranks=145787rank=88400 of ranks=145787rank=88500 of ranks=145787rank=88600 of ranks=145787rank=88700 of ranks=145787rank=88800 of ranks=145787rank=88900 of ranks=145787rank=89000 of ranks=145787rank=89100 of ranks=145787rank=89200 of ranks=145787rank=89300 of ranks=145787rank=89400 of ranks=145787rank=89500 of ranks=145787rank=89600 of ranks=145787rank=89700 of ranks=145787rank=89800 of ranks=145787rank=89900 of ranks=145787rank=90000 of ranks=145787rank=90100 of ranks=145787rank=90200 of ranks=145787rank=90300 of ranks=145787rank=90400 of ranks=145787rank=90500 of ranks=145787rank=90600 of ranks=145787rank=90700 of ranks=145787rank=90800 of ranks=145787rank=90900 of ranks=145787rank=91000 of ranks=145787rank=91100 of ranks=145787rank=91200 of ranks=145787rank=91300 of ranks=145787rank=91400 of ranks=145787rank=91500 of ranks=145787rank=91600 of ranks=145787rank=91700 of ranks=145787rank=91800 of ranks=145787rank=91900 of ranks=145787rank=92000 of ranks=145787rank=92100 of ranks=145787rank=92200 of ranks=145787rank=92300 of ranks=145787rank=92400 of ranks=145787rank=92500 of ranks=145787rank=92600 of ranks=145787rank=92700 of ranks=145787rank=92800 of ranks=145787rank=92900 of ranks=145787rank=93000 of ranks=145787rank=93100 of ranks=145787rank=93200 of ranks=145787rank=93300 of ranks=145787rank=93400 of ranks=145787rank=93500 of ranks=145787rank=93600 of ranks=145787rank=93700 of ranks=145787rank=93800 of ranks=145787rank=93900 of ranks=145787rank=94000 of ranks=145787rank=94100 of ranks=145787rank=94200 of ranks=145787rank=94300 of ranks=145787rank=94400 of ranks=145787rank=94500 of ranks=145787rank=94600 of ranks=145787rank=94700 of ranks=145787rank=94800 of ranks=145787rank=94900 of ranks=145787rank=95000 of ranks=145787rank=95100 of ranks=145787rank=95200 of ranks=145787rank=95300 of ranks=145787rank=95400 of ranks=145787rank=95500 of ranks=145787rank=95600 of ranks=145787rank=95700 of ranks=145787rank=95800 of ranks=145787rank=95900 of ranks=145787rank=96000 of ranks=145787rank=96100 of ranks=145787rank=96200 of ranks=145787rank=96300 of ranks=145787rank=96400 of ranks=145787rank=96500 of ranks=145787rank=96600 of ranks=145787rank=96700 of ranks=145787rank=96800 of ranks=145787rank=96900 of ranks=145787rank=97000 of ranks=145787rank=97100 of ranks=145787rank=97200 of ranks=145787rank=97300 of ranks=145787rank=97400 of ranks=145787rank=97500 of ranks=145787rank=97600 of ranks=145787rank=97700 of ranks=145787rank=97800 of ranks=145787rank=97900 of ranks=145787rank=98000 of ranks=145787rank=98100 of ranks=145787rank=98200 of ranks=145787rank=98300 of ranks=145787rank=98400 of ranks=145787rank=98500 of ranks=145787rank=98600 of ranks=145787rank=98700 of ranks=145787rank=98800 of ranks=145787rank=98900 of ranks=145787rank=99000 of ranks=145787rank=99100 of ranks=145787rank=99200 of ranks=145787rank=99300 of ranks=145787rank=99400 of ranks=145787rank=99500 of ranks=145787rank=99600 of ranks=145787rank=99700 of ranks=145787rank=99800 of ranks=145787rank=99900 of ranks=145787rank=100000 of ranks=145787rank=100100 of ranks=145787rank=100200 of ranks=145787rank=100300 of ranks=145787rank=100400 of ranks=145787rank=100500 of ranks=145787rank=100600 of ranks=145787rank=100700 of ranks=145787rank=100800 of ranks=145787rank=100900 of ranks=145787rank=101000 of ranks=145787rank=101100 of ranks=145787rank=101200 of ranks=145787rank=101300 of ranks=145787rank=101400 of ranks=145787rank=101500 of ranks=145787rank=101600 of ranks=145787rank=101700 of ranks=145787rank=101800 of ranks=145787rank=101900 of ranks=145787rank=102000 of ranks=145787rank=102100 of ranks=145787rank=102200 of ranks=145787rank=102300 of ranks=145787rank=102400 of ranks=145787rank=102500 of ranks=145787rank=102600 of ranks=145787rank=102700 of ranks=145787rank=102800 of ranks=145787rank=102900 of ranks=145787rank=103000 of ranks=145787rank=103100 of ranks=145787rank=103200 of ranks=145787rank=103300 of ranks=145787rank=103400 of ranks=145787rank=103500 of ranks=145787rank=103600 of ranks=145787rank=103700 of ranks=145787rank=103800 of ranks=145787rank=103900 of ranks=145787rank=104000 of ranks=145787rank=104100 of ranks=145787rank=104200 of ranks=145787rank=104300 of ranks=145787rank=104400 of ranks=145787rank=104500 of ranks=145787rank=104600 of ranks=145787rank=104700 of ranks=145787rank=104800 of ranks=145787rank=104900 of ranks=145787rank=105000 of ranks=145787rank=105100 of ranks=145787rank=105200 of ranks=145787rank=105300 of ranks=145787rank=105400 of ranks=145787rank=105500 of ranks=145787rank=105600 of ranks=145787rank=105700 of ranks=145787rank=105800 of ranks=145787rank=105900 of ranks=145787rank=106000 of ranks=145787rank=106100 of ranks=145787rank=106200 of ranks=145787rank=106300 of ranks=145787rank=106400 of ranks=145787rank=106500 of ranks=145787rank=106600 of ranks=145787rank=106700 of ranks=145787rank=106800 of ranks=145787rank=106900 of ranks=145787rank=107000 of ranks=145787rank=107100 of ranks=145787rank=107200 of ranks=145787rank=107300 of ranks=145787rank=107400 of ranks=145787rank=107500 of ranks=145787rank=107600 of ranks=145787rank=107700 of ranks=145787rank=107800 of ranks=145787rank=107900 of ranks=145787rank=108000 of ranks=145787rank=108100 of ranks=145787rank=108200 of ranks=145787rank=108300 of ranks=145787rank=108400 of ranks=145787rank=108500 of ranks=145787rank=108600 of ranks=145787rank=108700 of ranks=145787rank=108800 of ranks=145787rank=108900 of ranks=145787rank=109000 of ranks=145787rank=109100 of ranks=145787rank=109200 of ranks=145787rank=109300 of ranks=145787rank=109400 of ranks=145787rank=109500 of ranks=145787rank=109600 of ranks=145787rank=109700 of ranks=145787rank=109800 of ranks=145787rank=109900 of ranks=145787rank=110000 of ranks=145787rank=110100 of ranks=145787rank=110200 of ranks=145787rank=110300 of ranks=145787rank=110400 of ranks=145787rank=110500 of ranks=145787rank=110600 of ranks=145787rank=110700 of ranks=145787rank=110800 of ranks=145787rank=110900 of ranks=145787rank=111000 of ranks=145787rank=111100 of ranks=145787rank=111200 of ranks=145787rank=111300 of ranks=145787rank=111400 of ranks=145787rank=111500 of ranks=145787rank=111600 of ranks=145787rank=111700 of ranks=145787rank=111800 of ranks=145787rank=111900 of ranks=145787rank=112000 of ranks=145787rank=112100 of ranks=145787rank=112200 of ranks=145787rank=112300 of ranks=145787rank=112400 of ranks=145787rank=112500 of ranks=145787rank=112600 of ranks=145787rank=112700 of ranks=145787rank=112800 of ranks=145787rank=112900 of ranks=145787rank=113000 of ranks=145787rank=113100 of ranks=145787rank=113200 of ranks=145787rank=113300 of ranks=145787rank=113400 of ranks=145787rank=113500 of ranks=145787rank=113600 of ranks=145787rank=113700 of ranks=145787rank=113800 of ranks=145787rank=113900 of ranks=145787rank=114000 of ranks=145787rank=114100 of ranks=145787rank=114200 of ranks=145787rank=114300 of ranks=145787rank=114400 of ranks=145787rank=114500 of ranks=145787rank=114600 of ranks=145787rank=114700 of ranks=145787rank=114800 of ranks=145787rank=114900 of ranks=145787rank=115000 of ranks=145787rank=115100 of ranks=145787rank=115200 of ranks=145787rank=115300 of ranks=145787rank=115400 of ranks=145787rank=115500 of ranks=145787rank=115600 of ranks=145787rank=115700 of ranks=145787rank=115800 of ranks=145787rank=115900 of ranks=145787rank=116000 of ranks=145787rank=116100 of ranks=145787rank=116200 of ranks=145787rank=116300 of ranks=145787rank=116400 of ranks=145787rank=116500 of ranks=145787rank=116600 of ranks=145787rank=116700 of ranks=145787rank=116800 of ranks=145787rank=116900 of ranks=145787rank=117000 of ranks=145787rank=117100 of ranks=145787rank=117200 of ranks=145787rank=117300 of ranks=145787rank=117400 of ranks=145787rank=117500 of ranks=145787rank=117600 of ranks=145787rank=117700 of ranks=145787rank=117800 of ranks=145787rank=117900 of ranks=145787rank=118000 of ranks=145787rank=118100 of ranks=145787rank=118200 of ranks=145787rank=118300 of ranks=145787rank=118400 of ranks=145787rank=118500 of ranks=145787rank=118600 of ranks=145787rank=118700 of ranks=145787rank=118800 of ranks=145787rank=118900 of ranks=145787rank=119000 of ranks=145787rank=119100 of ranks=145787rank=119200 of ranks=145787rank=119300 of ranks=145787rank=119400 of ranks=145787rank=119500 of ranks=145787rank=119600 of ranks=145787rank=119700 of ranks=145787rank=119800 of ranks=145787rank=119900 of ranks=145787rank=120000 of ranks=145787rank=120100 of ranks=145787rank=120200 of ranks=145787rank=120300 of ranks=145787rank=120400 of ranks=145787rank=120500 of ranks=145787rank=120600 of ranks=145787rank=120700 of ranks=145787rank=120800 of ranks=145787rank=120900 of ranks=145787rank=121000 of ranks=145787rank=121100 of ranks=145787rank=121200 of ranks=145787rank=121300 of ranks=145787rank=121400 of ranks=145787rank=121500 of ranks=145787rank=121600 of ranks=145787rank=121700 of ranks=145787rank=121800 of ranks=145787rank=121900 of ranks=145787rank=122000 of ranks=145787rank=122100 of ranks=145787rank=122200 of ranks=145787rank=122300 of ranks=145787rank=122400 of ranks=145787rank=122500 of ranks=145787rank=122600 of ranks=145787rank=122700 of ranks=145787rank=122800 of ranks=145787rank=122900 of ranks=145787rank=123000 of ranks=145787rank=123100 of ranks=145787rank=123200 of ranks=145787rank=123300 of ranks=145787rank=123400 of ranks=145787rank=123500 of ranks=145787rank=123600 of ranks=145787rank=123700 of ranks=145787rank=123800 of ranks=145787rank=123900 of ranks=145787rank=124000 of ranks=145787rank=124100 of ranks=145787rank=124200 of ranks=145787rank=124300 of ranks=145787rank=124400 of ranks=145787rank=124500 of ranks=145787rank=124600 of ranks=145787rank=124700 of ranks=145787rank=124800 of ranks=145787rank=124900 of ranks=145787rank=125000 of ranks=145787rank=125100 of ranks=145787rank=125200 of ranks=145787rank=125300 of ranks=145787rank=125400 of ranks=145787rank=125500 of ranks=145787rank=125600 of ranks=145787rank=125700 of ranks=145787rank=125800 of ranks=145787rank=125900 of ranks=145787rank=126000 of ranks=145787rank=126100 of ranks=145787rank=126200 of ranks=145787rank=126300 of ranks=145787rank=126400 of ranks=145787rank=126500 of ranks=145787rank=126600 of ranks=145787rank=126700 of ranks=145787rank=126800 of ranks=145787rank=126900 of ranks=145787rank=127000 of ranks=145787rank=127100 of ranks=145787rank=127200 of ranks=145787rank=127300 of ranks=145787rank=127400 of ranks=145787rank=127500 of ranks=145787rank=127600 of ranks=145787rank=127700 of ranks=145787rank=127800 of ranks=145787rank=127900 of ranks=145787rank=128000 of ranks=145787rank=128100 of ranks=145787rank=128200 of ranks=145787rank=128300 of ranks=145787rank=128400 of ranks=145787rank=128500 of ranks=145787rank=128600 of ranks=145787rank=128700 of ranks=145787rank=128800 of ranks=145787rank=128900 of ranks=145787rank=129000 of ranks=145787rank=129100 of ranks=145787rank=129200 of ranks=145787rank=129300 of ranks=145787rank=129400 of ranks=145787rank=129500 of ranks=145787rank=129600 of ranks=145787rank=129700 of ranks=145787rank=129800 of ranks=145787rank=129900 of ranks=145787rank=130000 of ranks=145787rank=130100 of ranks=145787rank=130200 of ranks=145787rank=130300 of ranks=145787rank=130400 of ranks=145787rank=130500 of ranks=145787rank=130600 of ranks=145787rank=130700 of ranks=145787rank=130800 of ranks=145787rank=130900 of ranks=145787rank=131000 of ranks=145787rank=131100 of ranks=145787rank=131200 of ranks=145787rank=131300 of ranks=145787rank=131400 of ranks=145787rank=131500 of ranks=145787rank=131600 of ranks=145787rank=131700 of ranks=145787rank=131800 of ranks=145787rank=131900 of ranks=145787rank=132000 of ranks=145787rank=132100 of ranks=145787rank=132200 of ranks=145787rank=132300 of ranks=145787rank=132400 of ranks=145787rank=132500 of ranks=145787rank=132600 of ranks=145787rank=132700 of ranks=145787rank=132800 of ranks=145787rank=132900 of ranks=145787rank=133000 of ranks=145787rank=133100 of ranks=145787rank=133200 of ranks=145787rank=133300 of ranks=145787rank=133400 of ranks=145787rank=133500 of ranks=145787rank=133600 of ranks=145787rank=133700 of ranks=145787rank=133800 of ranks=145787rank=133900 of ranks=145787rank=134000 of ranks=145787rank=134100 of ranks=145787rank=134200 of ranks=145787rank=134300 of ranks=145787rank=134400 of ranks=145787rank=134500 of ranks=145787rank=134600 of ranks=145787rank=134700 of ranks=145787rank=134800 of ranks=145787rank=134900 of ranks=145787rank=135000 of ranks=145787rank=135100 of ranks=145787rank=135200 of ranks=145787rank=135300 of ranks=145787rank=135400 of ranks=145787rank=135500 of ranks=145787rank=135600 of ranks=145787rank=135700 of ranks=145787rank=135800 of ranks=145787rank=135900 of ranks=145787rank=136000 of ranks=145787rank=136100 of ranks=145787rank=136200 of ranks=145787rank=136300 of ranks=145787rank=136400 of ranks=145787rank=136500 of ranks=145787rank=136600 of ranks=145787rank=136700 of ranks=145787rank=136800 of ranks=145787rank=136900 of ranks=145787rank=137000 of ranks=145787rank=137100 of ranks=145787rank=137200 of ranks=145787rank=137300 of ranks=145787rank=137400 of ranks=145787rank=137500 of ranks=145787rank=137600 of ranks=145787rank=137700 of ranks=145787rank=137800 of ranks=145787rank=137900 of ranks=145787rank=138000 of ranks=145787rank=138100 of ranks=145787rank=138200 of ranks=145787rank=138300 of ranks=145787rank=138400 of ranks=145787rank=138500 of ranks=145787rank=138600 of ranks=145787rank=138700 of ranks=145787rank=138800 of ranks=145787rank=138900 of ranks=145787rank=139000 of ranks=145787rank=139100 of ranks=145787rank=139200 of ranks=145787rank=139300 of ranks=145787rank=139400 of ranks=145787rank=139500 of ranks=145787rank=139600 of ranks=145787rank=139700 of ranks=145787rank=139800 of ranks=145787rank=139900 of ranks=145787rank=140000 of ranks=145787rank=140100 of ranks=145787rank=140200 of ranks=145787rank=140300 of ranks=145787rank=140400 of ranks=145787rank=140500 of ranks=145787rank=140600 of ranks=145787rank=140700 of ranks=145787rank=140800 of ranks=145787rank=140900 of ranks=145787rank=141000 of ranks=145787rank=141100 of ranks=145787rank=141200 of ranks=145787rank=141300 of ranks=145787rank=141400 of ranks=145787rank=141500 of ranks=145787rank=141600 of ranks=145787rank=141700 of ranks=145787rank=141800 of ranks=145787rank=141900 of ranks=145787rank=142000 of ranks=145787rank=142100 of ranks=145787rank=142200 of ranks=145787rank=142300 of ranks=145787rank=142400 of ranks=145787rank=142500 of ranks=145787rank=142600 of ranks=145787rank=142700 of ranks=145787rank=142800 of ranks=145787rank=142900 of ranks=145787rank=143000 of ranks=145787rank=143100 of ranks=145787rank=143200 of ranks=145787rank=143300 of ranks=145787rank=143400 of ranks=145787rank=143500 of ranks=145787rank=143600 of ranks=145787rank=143700 of ranks=145787rank=143800 of ranks=145787rank=143900 of ranks=145787rank=144000 of ranks=145787rank=144100 of ranks=145787rank=144200 of ranks=145787rank=144300 of ranks=145787rank=144400 of ranks=145787rank=144500 of ranks=145787rank=144600 of ranks=145787rank=144700 of ranks=145787rank=144800 of ranks=145787rank=144900 of ranks=145787rank=145000 of ranks=145787rank=145100 of ranks=145787rank=145200 of ranks=145787rank=145300 of ranks=145787rank=145400 of ranks=145787rank=145500 of ranks=145787rank=145600 of ranks=145787rank=145700 of ranks=145787

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              89.2911    473   5733     25    498           62.6345
   1 car                    97.3880  49907  48898    409  50316           72.5939
   2 truck                  87.5336   1793  11256     32   1825           55.6335
   3 bus                    76.7991    360   5025      6    366           45.6076
   4 pedestrian             91.1695   4072  18270    187   4259           62.5849

for conf_thresh=0.25, precision=0.83, recall=0.93, F1 score=0.88
for conf_thresh=0.25, TP=53516, FP=11164, FN=3748, average IoU=71.03%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=88.44%
Total detection time: 157 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145d4a000000
3031: loss=4.105, avg loss=4.437, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 193984 images, time remaining=4.9 hours
3032: loss=3.256, avg loss=4.319, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 194048 images, time remaining=4.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3033: loss=3.332, avg loss=4.220, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 194112 images, time remaining=4.9 hours
3034: loss=4.267, avg loss=4.225, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=796.4 milliseconds, train=1.5 seconds, 194176 images, time remaining=4.9 hours
3035: loss=3.959, avg loss=4.198, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=509.0 milliseconds, train=1.5 seconds, 194240 images, time remaining=4.9 hours
3036: loss=4.351, avg loss=4.213, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 194304 images, time remaining=4.9 hours
3037: loss=4.163, avg loss=4.208, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=891.7 milliseconds, train=1.5 seconds, 194368 images, time remaining=4.9 hours
3038: loss=4.025, avg loss=4.190, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=792.9 milliseconds, train=1.6 seconds, 194432 images, time remaining=4.9 hours
3039: loss=3.550, avg loss=4.126, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=613.0 milliseconds, train=1.5 seconds, 194496 images, time remaining=4.9 hours
3040: loss=3.874, avg loss=4.101, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=848.1 milliseconds, train=1.5 seconds, 194560 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145d4a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3041: loss=4.240, avg loss=4.115, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 194624 images, time remaining=4.9 hours
3042: loss=3.903, avg loss=4.093, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=949.2 milliseconds, train=1.5 seconds, 194688 images, time remaining=4.9 hours
3043: loss=3.582, avg loss=4.042, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=556.0 milliseconds, train=1.5 seconds, 194752 images, time remaining=4.9 hours
3044: loss=4.017, avg loss=4.040, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=697.5 milliseconds, train=1.5 seconds, 194816 images, time remaining=4.9 hours
3045: loss=4.145, avg loss=4.050, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=723.2 milliseconds, train=1.5 seconds, 194880 images, time remaining=4.9 hours
3046: loss=3.932, avg loss=4.038, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=649.1 milliseconds, train=1.5 seconds, 194944 images, time remaining=4.9 hours
3047: loss=3.798, avg loss=4.014, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=762.4 milliseconds, train=1.5 seconds, 195008 images, time remaining=4.9 hours
3048: loss=3.420, avg loss=3.955, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 195072 images, time remaining=4.9 hours
3049: loss=3.311, avg loss=3.891, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=998.3 milliseconds, train=1.5 seconds, 195136 images, time remaining=4.9 hours
3050: loss=3.926, avg loss=3.894, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 195200 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3051: loss=4.165, avg loss=3.921, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=546.4 milliseconds, train=3.2 seconds, 195264 images, time remaining=4.9 hours
3052: loss=4.466, avg loss=3.976, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=558.9 milliseconds, train=3.2 seconds, 195328 images, time remaining=4.9 hours
3053: loss=4.063, avg loss=3.984, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 195392 images, time remaining=4.9 hours
3054: loss=3.709, avg loss=3.957, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=3.2 seconds, 195456 images, time remaining=4.9 hours
3055: loss=3.987, avg loss=3.960, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=717.5 milliseconds, train=3.2 seconds, 195520 images, time remaining=4.9 hours
3056: loss=3.553, avg loss=3.919, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=3.2 seconds, 195584 images, time remaining=4.9 hours
3057: loss=3.561, avg loss=3.883, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=604.7 milliseconds, train=3.2 seconds, 195648 images, time remaining=4.9 hours
3058: loss=3.495, avg loss=3.845, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=3.2 seconds, 195712 images, time remaining=4.9 hours
3059: loss=4.362, avg loss=3.896, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=793.0 milliseconds, train=3.2 seconds, 195776 images, time remaining=4.9 hours
3060: loss=4.519, avg loss=3.959, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=3.2 seconds, 195840 images, time remaining=4.9 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146456000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3061: loss=3.838, avg loss=3.946, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.6 seconds, 195904 images, time remaining=4.8 hours
3062: loss=3.923, avg loss=3.944, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=937.6 milliseconds, train=1.6 seconds, 195968 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3063: loss=4.316, avg loss=3.981, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=4.1 seconds, train=1.6 seconds, 196032 images, time remaining=4.8 hours
3064: loss=3.923, avg loss=3.976, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=851.3 milliseconds, train=1.6 seconds, 196096 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3065: loss=3.834, avg loss=3.961, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 196160 images, time remaining=4.8 hours
3066: loss=4.341, avg loss=3.999, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 196224 images, time remaining=4.8 hours
3067: loss=4.419, avg loss=4.041, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=886.7 milliseconds, train=1.6 seconds, 196288 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3068: loss=3.730, avg loss=4.010, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=1.6 seconds, 196352 images, time remaining=4.8 hours
3069: loss=3.696, avg loss=3.979, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 196416 images, time remaining=4.8 hours
3070: loss=3.924, avg loss=3.973, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=891.0 milliseconds, train=1.7 seconds, 196480 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3071: loss=6.113, avg loss=4.187, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=4.0 seconds, 196544 images, time remaining=4.8 hours
3072: loss=5.449, avg loss=4.313, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 196608 images, time remaining=4.8 hours
3073: loss=5.023, avg loss=4.384, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 196672 images, time remaining=4.8 hours
3074: loss=4.957, avg loss=4.442, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 196736 images, time remaining=4.8 hours
3075: loss=4.464, avg loss=4.444, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=746.3 milliseconds, train=3.9 seconds, 196800 images, time remaining=4.8 hours
3076: loss=6.043, avg loss=4.604, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 196864 images, time remaining=4.8 hours
3077: loss=4.502, avg loss=4.594, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 196928 images, time remaining=4.8 hours
3078: loss=4.824, avg loss=4.617, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=988.0 milliseconds, train=3.9 seconds, 196992 images, time remaining=4.8 hours
3079: loss=5.510, avg loss=4.706, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=832.7 milliseconds, train=3.9 seconds, 197056 images, time remaining=4.8 hours
3080: loss=4.363, avg loss=4.672, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=3.9 seconds, 197120 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3081: loss=5.426, avg loss=4.747, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=587.5 milliseconds, train=4.5 seconds, 197184 images, time remaining=4.8 hours
3082: loss=5.541, avg loss=4.826, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=639.3 milliseconds, train=4.5 seconds, 197248 images, time remaining=4.8 hours
3083: loss=5.554, avg loss=4.899, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=787.6 milliseconds, train=4.5 seconds, 197312 images, time remaining=4.8 hours
3084: loss=4.853, avg loss=4.895, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=965.8 milliseconds, train=4.5 seconds, 197376 images, time remaining=4.8 hours
3085: loss=5.603, avg loss=4.965, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 197440 images, time remaining=4.8 hours
3086: loss=5.356, avg loss=5.005, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=792.8 milliseconds, train=4.5 seconds, 197504 images, time remaining=4.8 hours
3087: loss=4.569, avg loss=4.961, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 197568 images, time remaining=4.8 hours
3088: loss=3.999, avg loss=4.865, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 197632 images, time remaining=4.8 hours
3089: loss=5.044, avg loss=4.883, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 197696 images, time remaining=4.8 hours
3090: loss=4.394, avg loss=4.834, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 197760 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3091: loss=4.703, avg loss=4.821, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=847.6 milliseconds, train=4.1 seconds, 197824 images, time remaining=4.8 hours
3092: loss=4.542, avg loss=4.793, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=714.8 milliseconds, train=4.1 seconds, 197888 images, time remaining=4.8 hours
3093: loss=4.211, avg loss=4.735, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=693.0 milliseconds, train=4.1 seconds, 197952 images, time remaining=4.8 hours
3094: loss=3.795, avg loss=4.641, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 198016 images, time remaining=4.8 hours
3095: loss=3.914, avg loss=4.568, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 198080 images, time remaining=4.8 hours
3096: loss=4.964, avg loss=4.608, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=866.1 milliseconds, train=4.1 seconds, 198144 images, time remaining=4.8 hours
3097: loss=4.571, avg loss=4.604, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=688.4 milliseconds, train=4.1 seconds, 198208 images, time remaining=4.8 hours
3098: loss=6.089, avg loss=4.752, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=4.1 seconds, 198272 images, time remaining=4.8 hours
3099: loss=5.472, avg loss=4.824, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=4.1 seconds, 198336 images, time remaining=4.8 hours
3100: loss=4.581, avg loss=4.800, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 198400 images, time remaining=4.8 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1463e0000000
3101: loss=4.296, avg loss=4.750, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 198464 images, time remaining=4.8 hours
3102: loss=4.210, avg loss=4.696, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 198528 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3103: loss=5.086, avg loss=4.735, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 198592 images, time remaining=4.8 hours
3104: loss=4.692, avg loss=4.730, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 198656 images, time remaining=4.8 hours
3105: loss=3.878, avg loss=4.645, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=655.9 milliseconds, train=1.4 seconds, 198720 images, time remaining=4.8 hours
3106: loss=4.241, avg loss=4.605, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=858.6 milliseconds, train=1.5 seconds, 198784 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3107: loss=4.053, avg loss=4.550, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 198848 images, time remaining=4.8 hours
3108: loss=4.914, avg loss=4.586, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=883.2 milliseconds, train=1.4 seconds, 198912 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3109: loss=4.210, avg loss=4.548, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.3 seconds, train=1.4 seconds, 198976 images, time remaining=4.8 hours
3110: loss=4.025, avg loss=4.496, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 199040 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14646a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3111: loss=5.029, avg loss=4.549, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 199104 images, time remaining=4.8 hours
3112: loss=4.168, avg loss=4.511, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=836.0 milliseconds, train=1.2 seconds, 199168 images, time remaining=4.8 hours
3113: loss=4.971, avg loss=4.557, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=524.4 milliseconds, train=1.2 seconds, 199232 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3114: loss=3.802, avg loss=4.482, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.2 seconds, 199296 images, time remaining=4.8 hours
3115: loss=4.866, avg loss=4.520, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=657.5 milliseconds, train=1.2 seconds, 199360 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3116: loss=4.032, avg loss=4.471, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 199424 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3117: loss=4.157, avg loss=4.440, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 199488 images, time remaining=4.8 hours
3118: loss=4.131, avg loss=4.409, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=631.6 milliseconds, train=1.2 seconds, 199552 images, time remaining=4.8 hours
3119: loss=4.226, avg loss=4.391, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 199616 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3120: loss=3.718, avg loss=4.323, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=1.2 seconds, 199680 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
3121: loss=4.260, avg loss=4.317, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=915.2 milliseconds, train=2.0 seconds, 199744 images, time remaining=4.8 hours
3122: loss=4.687, avg loss=4.354, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=2.0 seconds, 199808 images, time remaining=4.8 hours
3123: loss=4.410, avg loss=4.360, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=887.0 milliseconds, train=2.0 seconds, 199872 images, time remaining=4.8 hours
3124: loss=3.935, avg loss=4.317, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=512.1 milliseconds, train=2.0 seconds, 199936 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3125: loss=3.793, avg loss=4.265, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.2 seconds, train=2.0 seconds, 200000 images, time remaining=4.8 hours
3126: loss=4.121, avg loss=4.250, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=715.0 milliseconds, train=2.0 seconds, 200064 images, time remaining=4.8 hours
3127: loss=3.006, avg loss=4.126, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=2.0 seconds, 200128 images, time remaining=4.8 hours
3128: loss=3.731, avg loss=4.086, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=839.6 milliseconds, train=2.0 seconds, 200192 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3129: loss=3.733, avg loss=4.051, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.6 seconds, train=2.0 seconds, 200256 images, time remaining=4.8 hours
3130: loss=4.073, avg loss=4.053, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=617.3 milliseconds, train=2.0 seconds, 200320 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3131: loss=3.696, avg loss=4.018, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=597.0 milliseconds, train=3.3 seconds, 200384 images, time remaining=4.8 hours
3132: loss=3.806, avg loss=3.996, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=462.2 milliseconds, train=3.3 seconds, 200448 images, time remaining=4.8 hours
3133: loss=5.103, avg loss=4.107, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=536.1 milliseconds, train=3.3 seconds, 200512 images, time remaining=4.8 hours
3134: loss=4.892, avg loss=4.186, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 200576 images, time remaining=4.8 hours
3135: loss=5.016, avg loss=4.269, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 200640 images, time remaining=4.8 hours
3136: loss=4.524, avg loss=4.294, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=600.0 milliseconds, train=3.3 seconds, 200704 images, time remaining=4.8 hours
3137: loss=3.847, avg loss=4.249, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 200768 images, time remaining=4.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3138: loss=4.133, avg loss=4.238, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 200832 images, time remaining=4.8 hours
3139: loss=4.659, avg loss=4.280, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=574.5 milliseconds, train=3.3 seconds, 200896 images, time remaining=4.8 hours
3140: loss=4.574, avg loss=4.309, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=3.3 seconds, 200960 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3141: loss=4.379, avg loss=4.316, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=3.5 seconds, 201024 images, time remaining=4.8 hours
3142: loss=4.345, avg loss=4.319, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=973.4 milliseconds, train=3.5 seconds, 201088 images, time remaining=4.8 hours
3143: loss=3.930, avg loss=4.280, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=3.5 seconds, 201152 images, time remaining=4.8 hours
3144: loss=4.494, avg loss=4.302, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 201216 images, time remaining=4.8 hours
3145: loss=4.537, avg loss=4.325, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=3.5 seconds, 201280 images, time remaining=4.8 hours
3146: loss=4.036, avg loss=4.296, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.5 seconds, 201344 images, time remaining=4.8 hours
3147: loss=4.696, avg loss=4.336, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=661.9 milliseconds, train=3.5 seconds, 201408 images, time remaining=4.8 hours
3148: loss=4.667, avg loss=4.369, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.5 seconds, 201472 images, time remaining=4.8 hours
3149: loss=4.574, avg loss=4.390, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=3.5 seconds, 201536 images, time remaining=4.8 hours
3150: loss=4.529, avg loss=4.404, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.2 seconds, train=3.5 seconds, 201600 images, time remaining=4.8 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1458d3800000
3151: loss=4.529, avg loss=4.416, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=858.5 milliseconds, train=1.4 seconds, 201664 images, time remaining=4.7 hours
3152: loss=4.504, avg loss=4.425, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 201728 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3153: loss=4.474, avg loss=4.430, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 201792 images, time remaining=4.7 hours
3154: loss=5.089, avg loss=4.496, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=682.5 milliseconds, train=1.4 seconds, 201856 images, time remaining=4.7 hours
3155: loss=3.963, avg loss=4.442, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=570.7 milliseconds, train=1.5 seconds, 201920 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3156: loss=4.072, avg loss=4.405, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 201984 images, time remaining=4.7 hours
3157: loss=4.629, avg loss=4.428, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 202048 images, time remaining=4.7 hours
3158: loss=4.487, avg loss=4.434, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=676.5 milliseconds, train=1.4 seconds, 202112 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3159: loss=3.837, avg loss=4.374, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 202176 images, time remaining=4.7 hours
3160: loss=4.475, avg loss=4.384, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=473.7 milliseconds, train=1.4 seconds, 202240 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1458d3800000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3161: loss=3.554, avg loss=4.301, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 202304 images, time remaining=4.7 hours
3162: loss=4.280, avg loss=4.299, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=671.2 milliseconds, train=1.4 seconds, 202368 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3163: loss=4.455, avg loss=4.315, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 202432 images, time remaining=4.7 hours
3164: loss=3.569, avg loss=4.240, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=799.2 milliseconds, train=1.4 seconds, 202496 images, time remaining=4.7 hours
3165: loss=3.771, avg loss=4.193, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 202560 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3166: loss=3.798, avg loss=4.154, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 202624 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3167: loss=4.045, avg loss=4.143, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 202688 images, time remaining=4.7 hours
3168: loss=4.698, avg loss=4.198, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=805.6 milliseconds, train=1.4 seconds, 202752 images, time remaining=4.7 hours
3169: loss=3.736, avg loss=4.152, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=733.8 milliseconds, train=1.4 seconds, 202816 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3170: loss=3.416, avg loss=4.078, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 202880 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145154e00000
3171: loss=4.626, avg loss=4.133, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=477.1 milliseconds, train=1.7 seconds, 202944 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3172: loss=3.138, avg loss=4.034, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.0 seconds, train=1.6 seconds, 203008 images, time remaining=4.7 hours
3173: loss=3.801, avg loss=4.010, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=851.9 milliseconds, train=1.6 seconds, 203072 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3174: loss=3.953, avg loss=4.005, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=1.6 seconds, 203136 images, time remaining=4.7 hours
3175: loss=3.996, avg loss=4.004, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=674.5 milliseconds, train=1.6 seconds, 203200 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3176: loss=4.565, avg loss=4.060, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=1.6 seconds, 203264 images, time remaining=4.7 hours
3177: loss=3.753, avg loss=4.029, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=808.9 milliseconds, train=1.6 seconds, 203328 images, time remaining=4.7 hours
3178: loss=3.501, avg loss=3.976, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=677.2 milliseconds, train=1.6 seconds, 203392 images, time remaining=4.7 hours
3179: loss=3.764, avg loss=3.955, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 203456 images, time remaining=4.7 hours
3180: loss=4.182, avg loss=3.978, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=630.6 milliseconds, train=1.7 seconds, 203520 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3181: loss=3.949, avg loss=3.975, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 203584 images, time remaining=4.7 hours
3182: loss=4.015, avg loss=3.979, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=549.8 milliseconds, train=3.4 seconds, 203648 images, time remaining=4.7 hours
3183: loss=5.114, avg loss=4.092, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=3.4 seconds, 203712 images, time remaining=4.7 hours
3184: loss=4.173, avg loss=4.100, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=3.4 seconds, 203776 images, time remaining=4.7 hours
3185: loss=3.549, avg loss=4.045, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=860.6 milliseconds, train=3.4 seconds, 203840 images, time remaining=4.7 hours
3186: loss=4.328, avg loss=4.074, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 203904 images, time remaining=4.7 hours
3187: loss=5.954, avg loss=4.262, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=571.4 milliseconds, train=3.4 seconds, 203968 images, time remaining=4.7 hours
3188: loss=4.169, avg loss=4.252, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=3.4 seconds, 204032 images, time remaining=4.7 hours
3189: loss=4.121, avg loss=4.239, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=697.2 milliseconds, train=3.4 seconds, 204096 images, time remaining=4.7 hours
3190: loss=4.098, avg loss=4.225, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=3.4 seconds, 204160 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3191: loss=3.730, avg loss=4.176, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=813.0 milliseconds, train=2.1 seconds, 204224 images, time remaining=4.7 hours
3192: loss=3.941, avg loss=4.152, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 204288 images, time remaining=4.7 hours
3193: loss=3.800, avg loss=4.117, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=525.7 milliseconds, train=2.1 seconds, 204352 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3194: loss=3.588, avg loss=4.064, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.1 seconds, train=2.1 seconds, 204416 images, time remaining=4.7 hours
3195: loss=3.715, avg loss=4.029, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=928.0 milliseconds, train=2.1 seconds, 204480 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3196: loss=4.087, avg loss=4.035, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 204544 images, time remaining=4.7 hours
3197: loss=4.411, avg loss=4.073, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=722.7 milliseconds, train=2.1 seconds, 204608 images, time remaining=4.7 hours
3198: loss=4.670, avg loss=4.132, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 204672 images, time remaining=4.7 hours
3199: loss=4.625, avg loss=4.182, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 204736 images, time remaining=4.7 hours
3200: loss=4.230, avg loss=4.186, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=567.5 milliseconds, train=2.1 seconds, 204800 images, time remaining=4.7 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145f56000000
3201: loss=3.987, avg loss=4.166, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=617.6 milliseconds, train=1.2 seconds, 204864 images, time remaining=4.7 hours
3202: loss=4.074, avg loss=4.157, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=672.8 milliseconds, train=1.2 seconds, 204928 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3203: loss=4.886, avg loss=4.230, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.2 seconds, 204992 images, time remaining=4.7 hours
3204: loss=4.277, avg loss=4.235, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=697.8 milliseconds, train=1.2 seconds, 205056 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3205: loss=4.721, avg loss=4.283, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 205120 images, time remaining=4.7 hours
3206: loss=4.041, avg loss=4.259, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=601.9 milliseconds, train=1.2 seconds, 205184 images, time remaining=4.7 hours
3207: loss=4.470, avg loss=4.280, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=766.6 milliseconds, train=1.2 seconds, 205248 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3208: loss=4.135, avg loss=4.266, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 205312 images, time remaining=4.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3209: loss=3.867, avg loss=4.226, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 205376 images, time remaining=4.7 hours
3210: loss=4.208, avg loss=4.224, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=810.1 milliseconds, train=1.2 seconds, 205440 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3211: loss=7.174, avg loss=4.519, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.0 seconds, train=3.9 seconds, 205504 images, time remaining=4.7 hours
3212: loss=5.497, avg loss=4.617, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=673.8 milliseconds, train=3.9 seconds, 205568 images, time remaining=4.7 hours
3213: loss=4.710, avg loss=4.626, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 205632 images, time remaining=4.7 hours
3214: loss=4.923, avg loss=4.656, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=735.3 milliseconds, train=3.9 seconds, 205696 images, time remaining=4.7 hours
3215: loss=5.364, avg loss=4.727, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 205760 images, time remaining=4.7 hours
3216: loss=4.988, avg loss=4.753, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=614.1 milliseconds, train=3.9 seconds, 205824 images, time remaining=4.7 hours
3217: loss=5.560, avg loss=4.834, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=624.2 milliseconds, train=3.9 seconds, 205888 images, time remaining=4.7 hours
3218: loss=5.089, avg loss=4.859, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=505.3 milliseconds, train=3.9 seconds, 205952 images, time remaining=4.7 hours
3219: loss=5.213, avg loss=4.894, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=730.7 milliseconds, train=3.9 seconds, 206016 images, time remaining=4.7 hours
3220: loss=4.843, avg loss=4.889, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 206080 images, time remaining=4.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1459ca000000
3221: loss=6.037, avg loss=5.004, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=454.4 milliseconds, train=1.2 seconds, 206144 images, time remaining=4.7 hours
3222: loss=4.428, avg loss=4.946, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=649.7 milliseconds, train=1.2 seconds, 206208 images, time remaining=4.7 hours
3223: loss=5.450, avg loss=4.997, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=693.2 milliseconds, train=1.2 seconds, 206272 images, time remaining=4.6 hours
3224: loss=4.691, avg loss=4.966, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=634.7 milliseconds, train=1.2 seconds, 206336 images, time remaining=4.6 hours
3225: loss=5.255, avg loss=4.995, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 206400 images, time remaining=4.6 hours
3226: loss=3.955, avg loss=4.891, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=717.7 milliseconds, train=1.2 seconds, 206464 images, time remaining=4.6 hours
3227: loss=5.016, avg loss=4.904, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=835.5 milliseconds, train=1.2 seconds, 206528 images, time remaining=4.6 hours
3228: loss=5.602, avg loss=4.973, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=669.6 milliseconds, train=1.2 seconds, 206592 images, time remaining=4.6 hours
3229: loss=3.799, avg loss=4.856, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=480.5 milliseconds, train=1.2 seconds, 206656 images, time remaining=4.6 hours
3230: loss=4.883, avg loss=4.859, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=493.1 milliseconds, train=1.2 seconds, 206720 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3231: loss=7.870, avg loss=5.160, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=587.6 milliseconds, train=3.4 seconds, 206784 images, time remaining=4.6 hours
3232: loss=6.535, avg loss=5.297, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=889.0 milliseconds, train=3.3 seconds, 206848 images, time remaining=4.6 hours
3233: loss=4.852, avg loss=5.253, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 206912 images, time remaining=4.6 hours
3234: loss=4.978, avg loss=5.225, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 206976 images, time remaining=4.6 hours
3235: loss=4.941, avg loss=5.197, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=567.3 milliseconds, train=3.3 seconds, 207040 images, time remaining=4.6 hours
3236: loss=5.586, avg loss=5.236, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 207104 images, time remaining=4.6 hours
3237: loss=6.109, avg loss=5.323, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 207168 images, time remaining=4.6 hours
3238: loss=5.671, avg loss=5.358, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 207232 images, time remaining=4.6 hours
3239: loss=5.115, avg loss=5.334, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=682.2 milliseconds, train=3.3 seconds, 207296 images, time remaining=4.6 hours
3240: loss=4.728, avg loss=5.273, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 207360 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x144fbf800000
3241: loss=5.798, avg loss=5.326, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=566.3 milliseconds, train=1.2 seconds, 207424 images, time remaining=4.6 hours
3242: loss=6.237, avg loss=5.417, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=518.4 milliseconds, train=1.2 seconds, 207488 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3243: loss=4.749, avg loss=5.350, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 207552 images, time remaining=4.6 hours
3244: loss=4.417, avg loss=5.257, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=346.3 milliseconds, train=1.2 seconds, 207616 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3245: loss=4.687, avg loss=5.200, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=1.2 seconds, 207680 images, time remaining=4.6 hours
3246: loss=4.860, avg loss=5.166, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=444.2 milliseconds, train=1.2 seconds, 207744 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3247: loss=5.801, avg loss=5.229, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 207808 images, time remaining=4.6 hours
3248: loss=5.776, avg loss=5.284, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=549.6 milliseconds, train=1.2 seconds, 207872 images, time remaining=4.6 hours
3249: loss=5.759, avg loss=5.331, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=657.9 milliseconds, train=1.2 seconds, 207936 images, time remaining=4.6 hours
3250: loss=5.533, avg loss=5.352, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=457.4 milliseconds, train=1.2 seconds, 208000 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3251: loss=5.938, avg loss=5.410, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=376.3 milliseconds, train=2.0 seconds, 208064 images, time remaining=4.6 hours
3252: loss=4.273, avg loss=5.297, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=647.5 milliseconds, train=2.0 seconds, 208128 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3253: loss=4.680, avg loss=5.235, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.0 seconds, train=2.0 seconds, 208192 images, time remaining=4.6 hours
3254: loss=4.836, avg loss=5.195, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=949.5 milliseconds, train=2.0 seconds, 208256 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3255: loss=4.211, avg loss=5.097, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=2.0 seconds, 208320 images, time remaining=4.6 hours
3256: loss=4.851, avg loss=5.072, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=550.2 milliseconds, train=2.0 seconds, 208384 images, time remaining=4.6 hours
3257: loss=4.314, avg loss=4.996, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 208448 images, time remaining=4.6 hours
3258: loss=5.108, avg loss=5.007, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=835.6 milliseconds, train=2.0 seconds, 208512 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3259: loss=4.968, avg loss=5.003, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.3 seconds, train=2.0 seconds, 208576 images, time remaining=4.6 hours
3260: loss=5.314, avg loss=5.034, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=596.6 milliseconds, train=2.0 seconds, 208640 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3261: loss=4.047, avg loss=4.936, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=730.0 milliseconds, train=1.9 seconds, 208704 images, time remaining=4.6 hours
3262: loss=4.068, avg loss=4.849, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=484.2 milliseconds, train=1.9 seconds, 208768 images, time remaining=4.6 hours
3263: loss=4.430, avg loss=4.807, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 208832 images, time remaining=4.6 hours
3264: loss=4.527, avg loss=4.779, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=504.8 milliseconds, train=1.9 seconds, 208896 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3265: loss=4.062, avg loss=4.707, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=1.9 seconds, 208960 images, time remaining=4.6 hours
3266: loss=5.260, avg loss=4.763, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=677.2 milliseconds, train=1.9 seconds, 209024 images, time remaining=4.6 hours
3267: loss=4.923, avg loss=4.779, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=575.7 milliseconds, train=1.9 seconds, 209088 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3268: loss=5.020, avg loss=4.803, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=1.9 seconds, 209152 images, time remaining=4.6 hours
3269: loss=5.241, avg loss=4.847, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=638.2 milliseconds, train=1.9 seconds, 209216 images, time remaining=4.6 hours
3270: loss=4.925, avg loss=4.854, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=1.9 seconds, 209280 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3271: loss=5.802, avg loss=4.949, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=4.1 seconds, 209344 images, time remaining=4.6 hours
3272: loss=5.816, avg loss=5.036, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=847.4 milliseconds, train=4.1 seconds, 209408 images, time remaining=4.6 hours
3273: loss=4.642, avg loss=4.997, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=647.6 milliseconds, train=4.1 seconds, 209472 images, time remaining=4.6 hours
3274: loss=5.664, avg loss=5.063, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=767.8 milliseconds, train=4.1 seconds, 209536 images, time remaining=4.6 hours
3275: loss=4.655, avg loss=5.022, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=4.1 seconds, 209600 images, time remaining=4.6 hours
3276: loss=4.933, avg loss=5.013, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=486.2 milliseconds, train=4.1 seconds, 209664 images, time remaining=4.6 hours
3277: loss=4.247, avg loss=4.937, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=584.1 milliseconds, train=4.1 seconds, 209728 images, time remaining=4.6 hours
3278: loss=4.885, avg loss=4.932, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=4.1 seconds, 209792 images, time remaining=4.6 hours
3279: loss=4.164, avg loss=4.855, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 209856 images, time remaining=4.6 hours
3280: loss=4.771, avg loss=4.846, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=4.1 seconds, 209920 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3281: loss=6.045, avg loss=4.966, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=794.1 milliseconds, train=5.1 seconds, 209984 images, time remaining=4.6 hours
3282: loss=5.672, avg loss=5.037, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=599.1 milliseconds, train=5.1 seconds, 210048 images, time remaining=4.6 hours
3283: loss=6.671, avg loss=5.200, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=550.3 milliseconds, train=5.1 seconds, 210112 images, time remaining=4.6 hours
3284: loss=5.267, avg loss=5.207, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 210176 images, time remaining=4.6 hours
3285: loss=5.736, avg loss=5.260, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=717.8 milliseconds, train=5.2 seconds, 210240 images, time remaining=4.6 hours
3286: loss=5.543, avg loss=5.288, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=433.7 milliseconds, train=5.1 seconds, 210304 images, time remaining=4.6 hours
3287: loss=4.696, avg loss=5.229, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 210368 images, time remaining=4.6 hours
3288: loss=5.711, avg loss=5.277, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=832.4 milliseconds, train=5.1 seconds, 210432 images, time remaining=4.6 hours
3289: loss=5.175, avg loss=5.267, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=626.6 milliseconds, train=5.1 seconds, 210496 images, time remaining=4.6 hours
3290: loss=5.295, avg loss=5.270, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=883.9 milliseconds, train=5.1 seconds, 210560 images, time remaining=4.6 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145710000000
3291: loss=5.370, avg loss=5.280, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=742.6 milliseconds, train=1.4 seconds, 210624 images, time remaining=4.6 hours
3292: loss=5.897, avg loss=5.341, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=566.1 milliseconds, train=1.4 seconds, 210688 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3293: loss=5.641, avg loss=5.371, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=1.4 seconds, 210752 images, time remaining=4.6 hours
3294: loss=4.377, avg loss=5.272, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=672.6 milliseconds, train=1.4 seconds, 210816 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3295: loss=4.377, avg loss=5.182, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 210880 images, time remaining=4.6 hours
3296: loss=4.767, avg loss=5.141, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=855.4 milliseconds, train=1.4 seconds, 210944 images, time remaining=4.6 hours
3297: loss=4.498, avg loss=5.077, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=816.2 milliseconds, train=1.4 seconds, 211008 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3298: loss=4.701, avg loss=5.039, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=4.8 seconds, train=1.4 seconds, 211072 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3299: loss=5.242, avg loss=5.059, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 211136 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3300: loss=4.754, avg loss=5.029, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 211200 images, time remaining=4.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146472000000
3301: loss=5.846, avg loss=5.111, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=320.3 milliseconds, train=1.4 seconds, 211264 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3302: loss=4.706, avg loss=5.070, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 211328 images, time remaining=4.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3303: loss=4.532, avg loss=5.016, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 211392 images, time remaining=4.6 hours
3304: loss=3.942, avg loss=4.909, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=867.3 milliseconds, train=1.4 seconds, 211456 images, time remaining=4.6 hours
3305: loss=4.596, avg loss=4.878, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 211520 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3306: loss=4.767, avg loss=4.867, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.1 seconds, train=1.4 seconds, 211584 images, time remaining=4.5 hours
3307: loss=3.979, avg loss=4.778, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=839.3 milliseconds, train=1.4 seconds, 211648 images, time remaining=4.5 hours
3308: loss=4.219, avg loss=4.722, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=589.8 milliseconds, train=1.4 seconds, 211712 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3309: loss=4.059, avg loss=4.656, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 211776 images, time remaining=4.5 hours
3310: loss=3.784, avg loss=4.568, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=738.2 milliseconds, train=1.4 seconds, 211840 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3311: loss=4.882, avg loss=4.600, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 211904 images, time remaining=4.5 hours
3312: loss=4.406, avg loss=4.580, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=631.3 milliseconds, train=3.2 seconds, 211968 images, time remaining=4.5 hours
3313: loss=5.250, avg loss=4.647, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 212032 images, time remaining=4.5 hours
3314: loss=5.029, avg loss=4.686, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=640.5 milliseconds, train=3.3 seconds, 212096 images, time remaining=4.5 hours
3315: loss=4.434, avg loss=4.660, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 212160 images, time remaining=4.5 hours
3316: loss=4.133, avg loss=4.608, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 212224 images, time remaining=4.5 hours
3317: loss=3.927, avg loss=4.540, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=992.5 milliseconds, train=3.3 seconds, 212288 images, time remaining=4.5 hours
3318: loss=4.214, avg loss=4.507, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=530.9 milliseconds, train=3.3 seconds, 212352 images, time remaining=4.5 hours
3319: loss=5.109, avg loss=4.567, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.2 seconds, train=3.3 seconds, 212416 images, time remaining=4.5 hours
3320: loss=4.028, avg loss=4.513, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=468.7 milliseconds, train=3.2 seconds, 212480 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3321: loss=4.325, avg loss=4.494, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=888.5 milliseconds, train=3.6 seconds, 212544 images, time remaining=4.5 hours
3322: loss=4.625, avg loss=4.507, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=3.6 seconds, 212608 images, time remaining=4.5 hours
3323: loss=4.435, avg loss=4.500, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=874.1 milliseconds, train=3.6 seconds, 212672 images, time remaining=4.5 hours
3324: loss=4.719, avg loss=4.522, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 212736 images, time remaining=4.5 hours
3325: loss=4.376, avg loss=4.507, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=3.6 seconds, 212800 images, time remaining=4.5 hours
3326: loss=4.255, avg loss=4.482, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 212864 images, time remaining=4.5 hours
3327: loss=4.847, avg loss=4.519, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=3.6 seconds, 212928 images, time remaining=4.5 hours
3328: loss=3.870, avg loss=4.454, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=937.4 milliseconds, train=3.6 seconds, 212992 images, time remaining=4.5 hours
3329: loss=4.157, avg loss=4.424, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=722.4 milliseconds, train=3.6 seconds, 213056 images, time remaining=4.5 hours
3330: loss=4.324, avg loss=4.414, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=3.6 seconds, 213120 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3331: loss=4.650, avg loss=4.438, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=814.9 milliseconds, train=4.5 seconds, 213184 images, time remaining=4.5 hours
3332: loss=4.573, avg loss=4.451, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=908.5 milliseconds, train=4.5 seconds, 213248 images, time remaining=4.5 hours
3333: loss=5.229, avg loss=4.529, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=728.1 milliseconds, train=4.5 seconds, 213312 images, time remaining=4.5 hours
3334: loss=4.553, avg loss=4.531, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 213376 images, time remaining=4.5 hours
3335: loss=4.615, avg loss=4.540, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 213440 images, time remaining=4.5 hours
3336: loss=5.941, avg loss=4.680, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 213504 images, time remaining=4.5 hours
3337: loss=4.477, avg loss=4.660, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=4.5 seconds, 213568 images, time remaining=4.5 hours
3338: loss=4.970, avg loss=4.691, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=983.1 milliseconds, train=4.5 seconds, 213632 images, time remaining=4.5 hours
3339: loss=3.765, avg loss=4.598, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=4.5 seconds, 213696 images, time remaining=4.5 hours
3340: loss=5.321, avg loss=4.670, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.4 seconds, train=4.5 seconds, 213760 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
3341: loss=3.826, avg loss=4.586, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=726.8 milliseconds, train=2.1 seconds, 213824 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3342: loss=4.986, avg loss=4.626, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.2 seconds, train=2.0 seconds, 213888 images, time remaining=4.5 hours
3343: loss=5.124, avg loss=4.676, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=940.2 milliseconds, train=2.0 seconds, 213952 images, time remaining=4.5 hours
3344: loss=3.849, avg loss=4.593, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=862.4 milliseconds, train=2.0 seconds, 214016 images, time remaining=4.5 hours
3345: loss=5.673, avg loss=4.701, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 214080 images, time remaining=4.5 hours
3346: loss=3.932, avg loss=4.624, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=630.7 milliseconds, train=2.0 seconds, 214144 images, time remaining=4.5 hours
3347: loss=4.235, avg loss=4.585, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=760.5 milliseconds, train=2.0 seconds, 214208 images, time remaining=4.5 hours
3348: loss=4.633, avg loss=4.590, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 214272 images, time remaining=4.5 hours
3349: loss=4.182, avg loss=4.549, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 214336 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3350: loss=4.249, avg loss=4.519, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.9 seconds, train=2.0 seconds, 214400 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3351: loss=7.495, avg loss=4.817, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=981.2 milliseconds, train=5.0 seconds, 214464 images, time remaining=4.5 hours
3352: loss=5.668, avg loss=4.902, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=4.9 seconds, 214528 images, time remaining=4.5 hours
3353: loss=6.042, avg loss=5.016, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=4.9 seconds, 214592 images, time remaining=4.5 hours
3354: loss=4.994, avg loss=5.014, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=4.9 seconds, 214656 images, time remaining=4.5 hours
3355: loss=5.583, avg loss=5.071, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=5.0 seconds, 214720 images, time remaining=4.5 hours
3356: loss=5.684, avg loss=5.132, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=869.6 milliseconds, train=4.9 seconds, 214784 images, time remaining=4.5 hours
3357: loss=5.696, avg loss=5.188, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=748.1 milliseconds, train=4.9 seconds, 214848 images, time remaining=4.5 hours
3358: loss=4.871, avg loss=5.157, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=778.4 milliseconds, train=4.9 seconds, 214912 images, time remaining=4.5 hours
3359: loss=5.096, avg loss=5.151, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=985.3 milliseconds, train=4.9 seconds, 214976 images, time remaining=4.5 hours
3360: loss=5.087, avg loss=5.144, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=4.9 seconds, 215040 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146498000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3361: loss=4.845, avg loss=5.114, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 215104 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3362: loss=5.662, avg loss=5.169, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.7 seconds, train=1.2 seconds, 215168 images, time remaining=4.5 hours
3363: loss=5.909, avg loss=5.243, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 215232 images, time remaining=4.5 hours
3364: loss=5.079, avg loss=5.227, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=352.6 milliseconds, train=1.2 seconds, 215296 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3365: loss=4.768, avg loss=5.181, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 215360 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3366: loss=4.678, avg loss=5.131, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 215424 images, time remaining=4.5 hours
3367: loss=5.325, avg loss=5.150, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=688.5 milliseconds, train=1.2 seconds, 215488 images, time remaining=4.5 hours
3368: loss=5.331, avg loss=5.168, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=775.2 milliseconds, train=1.2 seconds, 215552 images, time remaining=4.5 hours
3369: loss=5.655, avg loss=5.217, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=667.7 milliseconds, train=1.2 seconds, 215616 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3370: loss=5.573, avg loss=5.252, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 215680 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145cade00000
3371: loss=5.537, avg loss=5.281, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 215744 images, time remaining=4.5 hours
3372: loss=4.926, avg loss=5.245, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=952.7 milliseconds, train=1.5 seconds, 215808 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3373: loss=4.809, avg loss=5.202, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 215872 images, time remaining=4.5 hours
3374: loss=5.348, avg loss=5.216, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 215936 images, time remaining=4.5 hours
3375: loss=4.042, avg loss=5.099, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=912.5 milliseconds, train=1.5 seconds, 216000 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3376: loss=4.435, avg loss=5.033, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 216064 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3377: loss=4.659, avg loss=4.995, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 216128 images, time remaining=4.5 hours
3378: loss=4.032, avg loss=4.899, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=672.9 milliseconds, train=1.5 seconds, 216192 images, time remaining=4.5 hours
3379: loss=5.071, avg loss=4.916, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=807.3 milliseconds, train=1.5 seconds, 216256 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3380: loss=4.811, avg loss=4.906, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 216320 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145cade00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3381: loss=4.487, avg loss=4.864, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 216384 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3382: loss=4.504, avg loss=4.828, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=1.4 seconds, 216448 images, time remaining=4.5 hours
3383: loss=4.975, avg loss=4.842, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 216512 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3384: loss=5.119, avg loss=4.870, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.6 seconds, train=1.4 seconds, 216576 images, time remaining=4.5 hours
3385: loss=3.882, avg loss=4.771, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=985.5 milliseconds, train=1.5 seconds, 216640 images, time remaining=4.5 hours
3386: loss=4.665, avg loss=4.761, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=810.9 milliseconds, train=1.4 seconds, 216704 images, time remaining=4.5 hours
3387: loss=3.940, avg loss=4.679, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=537.0 milliseconds, train=1.4 seconds, 216768 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3388: loss=3.990, avg loss=4.610, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 216832 images, time remaining=4.5 hours
3389: loss=4.452, avg loss=4.594, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 216896 images, time remaining=4.5 hours
3390: loss=4.263, avg loss=4.561, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=635.6 milliseconds, train=1.4 seconds, 216960 images, time remaining=4.5 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145a4bc00000
3391: loss=4.179, avg loss=4.523, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=612.3 milliseconds, train=1.4 seconds, 217024 images, time remaining=4.5 hours
3392: loss=4.951, avg loss=4.566, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=810.5 milliseconds, train=1.4 seconds, 217088 images, time remaining=4.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3393: loss=3.975, avg loss=4.507, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 217152 images, time remaining=4.5 hours
3394: loss=4.173, avg loss=4.473, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=646.5 milliseconds, train=1.4 seconds, 217216 images, time remaining=4.4 hours
3395: loss=4.386, avg loss=4.464, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=560.1 milliseconds, train=1.4 seconds, 217280 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3396: loss=3.413, avg loss=4.359, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 217344 images, time remaining=4.4 hours
3397: loss=4.093, avg loss=4.333, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=620.9 milliseconds, train=1.4 seconds, 217408 images, time remaining=4.4 hours
3398: loss=4.216, avg loss=4.321, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=621.5 milliseconds, train=1.4 seconds, 217472 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3399: loss=4.253, avg loss=4.314, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 217536 images, time remaining=4.4 hours
3400: loss=4.202, avg loss=4.303, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 217600 images, time remaining=4.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146464000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3401: loss=4.628, avg loss=4.336, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 217664 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3402: loss=3.711, avg loss=4.273, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 217728 images, time remaining=4.4 hours
3403: loss=3.381, avg loss=4.184, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 217792 images, time remaining=4.4 hours
3404: loss=3.670, avg loss=4.132, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=521.8 milliseconds, train=1.4 seconds, 217856 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3405: loss=3.550, avg loss=4.074, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 217920 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3406: loss=3.817, avg loss=4.048, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 217984 images, time remaining=4.4 hours
3407: loss=3.264, avg loss=3.970, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=927.6 milliseconds, train=1.4 seconds, 218048 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3408: loss=4.647, avg loss=4.038, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 218112 images, time remaining=4.4 hours
3409: loss=3.925, avg loss=4.026, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=816.0 milliseconds, train=1.4 seconds, 218176 images, time remaining=4.4 hours
3410: loss=3.520, avg loss=3.976, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=862.2 milliseconds, train=1.4 seconds, 218240 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3411: loss=5.708, avg loss=4.149, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=900.3 milliseconds, train=3.6 seconds, 218304 images, time remaining=4.4 hours
3412: loss=5.088, avg loss=4.243, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 218368 images, time remaining=4.4 hours
3413: loss=5.408, avg loss=4.359, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=644.0 milliseconds, train=3.6 seconds, 218432 images, time remaining=4.4 hours
3414: loss=5.287, avg loss=4.452, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=426.0 milliseconds, train=3.6 seconds, 218496 images, time remaining=4.4 hours
3415: loss=4.683, avg loss=4.475, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 218560 images, time remaining=4.4 hours
3416: loss=4.444, avg loss=4.472, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.4 seconds, train=3.6 seconds, 218624 images, time remaining=4.4 hours
3417: loss=4.622, avg loss=4.487, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=774.6 milliseconds, train=3.6 seconds, 218688 images, time remaining=4.4 hours
3418: loss=4.303, avg loss=4.469, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.6 seconds, train=3.6 seconds, 218752 images, time remaining=4.4 hours
3419: loss=4.608, avg loss=4.483, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.4 seconds, train=3.6 seconds, 218816 images, time remaining=4.4 hours
3420: loss=4.537, avg loss=4.488, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=641.7 milliseconds, train=3.6 seconds, 218880 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145ce2000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3421: loss=4.300, avg loss=4.469, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 218944 images, time remaining=4.4 hours
3422: loss=4.579, avg loss=4.480, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=447.9 milliseconds, train=1.6 seconds, 219008 images, time remaining=4.4 hours
3423: loss=4.684, avg loss=4.501, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 219072 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3424: loss=4.585, avg loss=4.509, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=1.6 seconds, 219136 images, time remaining=4.4 hours
3425: loss=4.615, avg loss=4.520, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 219200 images, time remaining=4.4 hours
3426: loss=3.892, avg loss=4.457, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=616.8 milliseconds, train=1.6 seconds, 219264 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3427: loss=3.887, avg loss=4.400, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 219328 images, time remaining=4.4 hours
3428: loss=4.945, avg loss=4.454, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=698.5 milliseconds, train=1.6 seconds, 219392 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3429: loss=4.016, avg loss=4.411, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.1 seconds, train=1.6 seconds, 219456 images, time remaining=4.4 hours
3430: loss=4.688, avg loss=4.438, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 219520 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3431: loss=7.648, avg loss=4.759, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.3 seconds, train=4.2 seconds, 219584 images, time remaining=4.4 hours
3432: loss=6.320, avg loss=4.915, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=2.3 seconds, train=4.1 seconds, 219648 images, time remaining=4.4 hours
3433: loss=5.376, avg loss=4.961, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 219712 images, time remaining=4.4 hours
3434: loss=4.703, avg loss=4.935, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=856.0 milliseconds, train=4.1 seconds, 219776 images, time remaining=4.4 hours
3435: loss=4.935, avg loss=4.935, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=600.3 milliseconds, train=4.1 seconds, 219840 images, time remaining=4.4 hours
3436: loss=6.121, avg loss=5.054, last=88.44%, best=88.44%, next=3436, rate=0.00130000, load 64=532.9 milliseconds, train=4.1 seconds, 219904 images, time remaining=4.4 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=142463, unique_truth_count=57264
rank=0 of ranks=142463rank=100 of ranks=142463rank=200 of ranks=142463rank=300 of ranks=142463rank=400 of ranks=142463rank=500 of ranks=142463rank=600 of ranks=142463rank=700 of ranks=142463rank=800 of ranks=142463rank=900 of ranks=142463rank=1000 of ranks=142463rank=1100 of ranks=142463rank=1200 of ranks=142463rank=1300 of ranks=142463rank=1400 of ranks=142463rank=1500 of ranks=142463rank=1600 of ranks=142463rank=1700 of ranks=142463rank=1800 of ranks=142463rank=1900 of ranks=142463rank=2000 of ranks=142463rank=2100 of ranks=142463rank=2200 of ranks=142463rank=2300 of ranks=142463rank=2400 of ranks=142463rank=2500 of ranks=142463rank=2600 of ranks=142463rank=2700 of ranks=142463rank=2800 of ranks=142463rank=2900 of ranks=142463rank=3000 of ranks=142463rank=3100 of ranks=142463rank=3200 of ranks=142463rank=3300 of ranks=142463rank=3400 of ranks=142463rank=3500 of ranks=142463rank=3600 of ranks=142463rank=3700 of ranks=142463rank=3800 of ranks=142463rank=3900 of ranks=142463rank=4000 of ranks=142463rank=4100 of ranks=142463rank=4200 of ranks=142463rank=4300 of ranks=142463rank=4400 of ranks=142463rank=4500 of ranks=142463rank=4600 of ranks=142463rank=4700 of ranks=142463rank=4800 of ranks=142463rank=4900 of ranks=142463rank=5000 of ranks=142463rank=5100 of ranks=142463rank=5200 of ranks=142463rank=5300 of ranks=142463rank=5400 of ranks=142463rank=5500 of ranks=142463rank=5600 of ranks=142463rank=5700 of ranks=142463rank=5800 of ranks=142463rank=5900 of ranks=142463rank=6000 of ranks=142463rank=6100 of ranks=142463rank=6200 of ranks=142463rank=6300 of ranks=142463rank=6400 of ranks=142463rank=6500 of ranks=142463rank=6600 of ranks=142463rank=6700 of ranks=142463rank=6800 of ranks=142463rank=6900 of ranks=142463rank=7000 of ranks=142463rank=7100 of ranks=142463rank=7200 of ranks=142463rank=7300 of ranks=142463rank=7400 of ranks=142463rank=7500 of ranks=142463rank=7600 of ranks=142463rank=7700 of ranks=142463rank=7800 of ranks=142463rank=7900 of ranks=142463rank=8000 of ranks=142463rank=8100 of ranks=142463rank=8200 of ranks=142463rank=8300 of ranks=142463rank=8400 of ranks=142463rank=8500 of ranks=142463rank=8600 of ranks=142463rank=8700 of ranks=142463rank=8800 of ranks=142463rank=8900 of ranks=142463rank=9000 of ranks=142463rank=9100 of ranks=142463rank=9200 of ranks=142463rank=9300 of ranks=142463rank=9400 of ranks=142463rank=9500 of ranks=142463rank=9600 of ranks=142463rank=9700 of ranks=142463rank=9800 of ranks=142463rank=9900 of ranks=142463rank=10000 of ranks=142463rank=10100 of ranks=142463rank=10200 of ranks=142463rank=10300 of ranks=142463rank=10400 of ranks=142463rank=10500 of ranks=142463rank=10600 of ranks=142463rank=10700 of ranks=142463rank=10800 of ranks=142463rank=10900 of ranks=142463rank=11000 of ranks=142463rank=11100 of ranks=142463rank=11200 of ranks=142463rank=11300 of ranks=142463rank=11400 of ranks=142463rank=11500 of ranks=142463rank=11600 of ranks=142463rank=11700 of ranks=142463rank=11800 of ranks=142463rank=11900 of ranks=142463rank=12000 of ranks=142463rank=12100 of ranks=142463rank=12200 of ranks=142463rank=12300 of ranks=142463rank=12400 of ranks=142463rank=12500 of ranks=142463rank=12600 of ranks=142463rank=12700 of ranks=142463rank=12800 of ranks=142463rank=12900 of ranks=142463rank=13000 of ranks=142463rank=13100 of ranks=142463rank=13200 of ranks=142463rank=13300 of ranks=142463rank=13400 of ranks=142463rank=13500 of ranks=142463rank=13600 of ranks=142463rank=13700 of ranks=142463rank=13800 of ranks=142463rank=13900 of ranks=142463rank=14000 of ranks=142463rank=14100 of ranks=142463rank=14200 of ranks=142463rank=14300 of ranks=142463rank=14400 of ranks=142463rank=14500 of ranks=142463rank=14600 of ranks=142463rank=14700 of ranks=142463rank=14800 of ranks=142463rank=14900 of ranks=142463rank=15000 of ranks=142463rank=15100 of ranks=142463rank=15200 of ranks=142463rank=15300 of ranks=142463rank=15400 of ranks=142463rank=15500 of ranks=142463rank=15600 of ranks=142463rank=15700 of ranks=142463rank=15800 of ranks=142463rank=15900 of ranks=142463rank=16000 of ranks=142463rank=16100 of ranks=142463rank=16200 of ranks=142463rank=16300 of ranks=142463rank=16400 of ranks=142463rank=16500 of ranks=142463rank=16600 of ranks=142463rank=16700 of ranks=142463rank=16800 of ranks=142463rank=16900 of ranks=142463rank=17000 of ranks=142463rank=17100 of ranks=142463rank=17200 of ranks=142463rank=17300 of ranks=142463rank=17400 of ranks=142463rank=17500 of ranks=142463rank=17600 of ranks=142463rank=17700 of ranks=142463rank=17800 of ranks=142463rank=17900 of ranks=142463rank=18000 of ranks=142463rank=18100 of ranks=142463rank=18200 of ranks=142463rank=18300 of ranks=142463rank=18400 of ranks=142463rank=18500 of ranks=142463rank=18600 of ranks=142463rank=18700 of ranks=142463rank=18800 of ranks=142463rank=18900 of ranks=142463rank=19000 of ranks=142463rank=19100 of ranks=142463rank=19200 of ranks=142463rank=19300 of ranks=142463rank=19400 of ranks=142463rank=19500 of ranks=142463rank=19600 of ranks=142463rank=19700 of ranks=142463rank=19800 of ranks=142463rank=19900 of ranks=142463rank=20000 of ranks=142463rank=20100 of ranks=142463rank=20200 of ranks=142463rank=20300 of ranks=142463rank=20400 of ranks=142463rank=20500 of ranks=142463rank=20600 of ranks=142463rank=20700 of ranks=142463rank=20800 of ranks=142463rank=20900 of ranks=142463rank=21000 of ranks=142463rank=21100 of ranks=142463rank=21200 of ranks=142463rank=21300 of ranks=142463rank=21400 of ranks=142463rank=21500 of ranks=142463rank=21600 of ranks=142463rank=21700 of ranks=142463rank=21800 of ranks=142463rank=21900 of ranks=142463rank=22000 of ranks=142463rank=22100 of ranks=142463rank=22200 of ranks=142463rank=22300 of ranks=142463rank=22400 of ranks=142463rank=22500 of ranks=142463rank=22600 of ranks=142463rank=22700 of ranks=142463rank=22800 of ranks=142463rank=22900 of ranks=142463rank=23000 of ranks=142463rank=23100 of ranks=142463rank=23200 of ranks=142463rank=23300 of ranks=142463rank=23400 of ranks=142463rank=23500 of ranks=142463rank=23600 of ranks=142463rank=23700 of ranks=142463rank=23800 of ranks=142463rank=23900 of ranks=142463rank=24000 of ranks=142463rank=24100 of ranks=142463rank=24200 of ranks=142463rank=24300 of ranks=142463rank=24400 of ranks=142463rank=24500 of ranks=142463rank=24600 of ranks=142463rank=24700 of ranks=142463rank=24800 of ranks=142463rank=24900 of ranks=142463rank=25000 of ranks=142463rank=25100 of ranks=142463rank=25200 of ranks=142463rank=25300 of ranks=142463rank=25400 of ranks=142463rank=25500 of ranks=142463rank=25600 of ranks=142463rank=25700 of ranks=142463rank=25800 of ranks=142463rank=25900 of ranks=142463rank=26000 of ranks=142463rank=26100 of ranks=142463rank=26200 of ranks=142463rank=26300 of ranks=142463rank=26400 of ranks=142463rank=26500 of ranks=142463rank=26600 of ranks=142463rank=26700 of ranks=142463rank=26800 of ranks=142463rank=26900 of ranks=142463rank=27000 of ranks=142463rank=27100 of ranks=142463rank=27200 of ranks=142463rank=27300 of ranks=142463rank=27400 of ranks=142463rank=27500 of ranks=142463rank=27600 of ranks=142463rank=27700 of ranks=142463rank=27800 of ranks=142463rank=27900 of ranks=142463rank=28000 of ranks=142463rank=28100 of ranks=142463rank=28200 of ranks=142463rank=28300 of ranks=142463rank=28400 of ranks=142463rank=28500 of ranks=142463rank=28600 of ranks=142463rank=28700 of ranks=142463rank=28800 of ranks=142463rank=28900 of ranks=142463rank=29000 of ranks=142463rank=29100 of ranks=142463rank=29200 of ranks=142463rank=29300 of ranks=142463rank=29400 of ranks=142463rank=29500 of ranks=142463rank=29600 of ranks=142463rank=29700 of ranks=142463rank=29800 of ranks=142463rank=29900 of ranks=142463rank=30000 of ranks=142463rank=30100 of ranks=142463rank=30200 of ranks=142463rank=30300 of ranks=142463rank=30400 of ranks=142463rank=30500 of ranks=142463rank=30600 of ranks=142463rank=30700 of ranks=142463rank=30800 of ranks=142463rank=30900 of ranks=142463rank=31000 of ranks=142463rank=31100 of ranks=142463rank=31200 of ranks=142463rank=31300 of ranks=142463rank=31400 of ranks=142463rank=31500 of ranks=142463rank=31600 of ranks=142463rank=31700 of ranks=142463rank=31800 of ranks=142463rank=31900 of ranks=142463rank=32000 of ranks=142463rank=32100 of ranks=142463rank=32200 of ranks=142463rank=32300 of ranks=142463rank=32400 of ranks=142463rank=32500 of ranks=142463rank=32600 of ranks=142463rank=32700 of ranks=142463rank=32800 of ranks=142463rank=32900 of ranks=142463rank=33000 of ranks=142463rank=33100 of ranks=142463rank=33200 of ranks=142463rank=33300 of ranks=142463rank=33400 of ranks=142463rank=33500 of ranks=142463rank=33600 of ranks=142463rank=33700 of ranks=142463rank=33800 of ranks=142463rank=33900 of ranks=142463rank=34000 of ranks=142463rank=34100 of ranks=142463rank=34200 of ranks=142463rank=34300 of ranks=142463rank=34400 of ranks=142463rank=34500 of ranks=142463rank=34600 of ranks=142463rank=34700 of ranks=142463rank=34800 of ranks=142463rank=34900 of ranks=142463rank=35000 of ranks=142463rank=35100 of ranks=142463rank=35200 of ranks=142463rank=35300 of ranks=142463rank=35400 of ranks=142463rank=35500 of ranks=142463rank=35600 of ranks=142463rank=35700 of ranks=142463rank=35800 of ranks=142463rank=35900 of ranks=142463rank=36000 of ranks=142463rank=36100 of ranks=142463rank=36200 of ranks=142463rank=36300 of ranks=142463rank=36400 of ranks=142463rank=36500 of ranks=142463rank=36600 of ranks=142463rank=36700 of ranks=142463rank=36800 of ranks=142463rank=36900 of ranks=142463rank=37000 of ranks=142463rank=37100 of ranks=142463rank=37200 of ranks=142463rank=37300 of ranks=142463rank=37400 of ranks=142463rank=37500 of ranks=142463rank=37600 of ranks=142463rank=37700 of ranks=142463rank=37800 of ranks=142463rank=37900 of ranks=142463rank=38000 of ranks=142463rank=38100 of ranks=142463rank=38200 of ranks=142463rank=38300 of ranks=142463rank=38400 of ranks=142463rank=38500 of ranks=142463rank=38600 of ranks=142463rank=38700 of ranks=142463rank=38800 of ranks=142463rank=38900 of ranks=142463rank=39000 of ranks=142463rank=39100 of ranks=142463rank=39200 of ranks=142463rank=39300 of ranks=142463rank=39400 of ranks=142463rank=39500 of ranks=142463rank=39600 of ranks=142463rank=39700 of ranks=142463rank=39800 of ranks=142463rank=39900 of ranks=142463rank=40000 of ranks=142463rank=40100 of ranks=142463rank=40200 of ranks=142463rank=40300 of ranks=142463rank=40400 of ranks=142463rank=40500 of ranks=142463rank=40600 of ranks=142463rank=40700 of ranks=142463rank=40800 of ranks=142463rank=40900 of ranks=142463rank=41000 of ranks=142463rank=41100 of ranks=142463rank=41200 of ranks=142463rank=41300 of ranks=142463rank=41400 of ranks=142463rank=41500 of ranks=142463rank=41600 of ranks=142463rank=41700 of ranks=142463rank=41800 of ranks=142463rank=41900 of ranks=142463rank=42000 of ranks=142463rank=42100 of ranks=142463rank=42200 of ranks=142463rank=42300 of ranks=142463rank=42400 of ranks=142463rank=42500 of ranks=142463rank=42600 of ranks=142463rank=42700 of ranks=142463rank=42800 of ranks=142463rank=42900 of ranks=142463rank=43000 of ranks=142463rank=43100 of ranks=142463rank=43200 of ranks=142463rank=43300 of ranks=142463rank=43400 of ranks=142463rank=43500 of ranks=142463rank=43600 of ranks=142463rank=43700 of ranks=142463rank=43800 of ranks=142463rank=43900 of ranks=142463rank=44000 of ranks=142463rank=44100 of ranks=142463rank=44200 of ranks=142463rank=44300 of ranks=142463rank=44400 of ranks=142463rank=44500 of ranks=142463rank=44600 of ranks=142463rank=44700 of ranks=142463rank=44800 of ranks=142463rank=44900 of ranks=142463rank=45000 of ranks=142463rank=45100 of ranks=142463rank=45200 of ranks=142463rank=45300 of ranks=142463rank=45400 of ranks=142463rank=45500 of ranks=142463rank=45600 of ranks=142463rank=45700 of ranks=142463rank=45800 of ranks=142463rank=45900 of ranks=142463rank=46000 of ranks=142463rank=46100 of ranks=142463rank=46200 of ranks=142463rank=46300 of ranks=142463rank=46400 of ranks=142463rank=46500 of ranks=142463rank=46600 of ranks=142463rank=46700 of ranks=142463rank=46800 of ranks=142463rank=46900 of ranks=142463rank=47000 of ranks=142463rank=47100 of ranks=142463rank=47200 of ranks=142463rank=47300 of ranks=142463rank=47400 of ranks=142463rank=47500 of ranks=142463rank=47600 of ranks=142463rank=47700 of ranks=142463rank=47800 of ranks=142463rank=47900 of ranks=142463rank=48000 of ranks=142463rank=48100 of ranks=142463rank=48200 of ranks=142463rank=48300 of ranks=142463rank=48400 of ranks=142463rank=48500 of ranks=142463rank=48600 of ranks=142463rank=48700 of ranks=142463rank=48800 of ranks=142463rank=48900 of ranks=142463rank=49000 of ranks=142463rank=49100 of ranks=142463rank=49200 of ranks=142463rank=49300 of ranks=142463rank=49400 of ranks=142463rank=49500 of ranks=142463rank=49600 of ranks=142463rank=49700 of ranks=142463rank=49800 of ranks=142463rank=49900 of ranks=142463rank=50000 of ranks=142463rank=50100 of ranks=142463rank=50200 of ranks=142463rank=50300 of ranks=142463rank=50400 of ranks=142463rank=50500 of ranks=142463rank=50600 of ranks=142463rank=50700 of ranks=142463rank=50800 of ranks=142463rank=50900 of ranks=142463rank=51000 of ranks=142463rank=51100 of ranks=142463rank=51200 of ranks=142463rank=51300 of ranks=142463rank=51400 of ranks=142463rank=51500 of ranks=142463rank=51600 of ranks=142463rank=51700 of ranks=142463rank=51800 of ranks=142463rank=51900 of ranks=142463rank=52000 of ranks=142463rank=52100 of ranks=142463rank=52200 of ranks=142463rank=52300 of ranks=142463rank=52400 of ranks=142463rank=52500 of ranks=142463rank=52600 of ranks=142463rank=52700 of ranks=142463rank=52800 of ranks=142463rank=52900 of ranks=142463rank=53000 of ranks=142463rank=53100 of ranks=142463rank=53200 of ranks=142463rank=53300 of ranks=142463rank=53400 of ranks=142463rank=53500 of ranks=142463rank=53600 of ranks=142463rank=53700 of ranks=142463rank=53800 of ranks=142463rank=53900 of ranks=142463rank=54000 of ranks=142463rank=54100 of ranks=142463rank=54200 of ranks=142463rank=54300 of ranks=142463rank=54400 of ranks=142463rank=54500 of ranks=142463rank=54600 of ranks=142463rank=54700 of ranks=142463rank=54800 of ranks=142463rank=54900 of ranks=142463rank=55000 of ranks=142463rank=55100 of ranks=142463rank=55200 of ranks=142463rank=55300 of ranks=142463rank=55400 of ranks=142463rank=55500 of ranks=142463rank=55600 of ranks=142463rank=55700 of ranks=142463rank=55800 of ranks=142463rank=55900 of ranks=142463rank=56000 of ranks=142463rank=56100 of ranks=142463rank=56200 of ranks=142463rank=56300 of ranks=142463rank=56400 of ranks=142463rank=56500 of ranks=142463rank=56600 of ranks=142463rank=56700 of ranks=142463rank=56800 of ranks=142463rank=56900 of ranks=142463rank=57000 of ranks=142463rank=57100 of ranks=142463rank=57200 of ranks=142463rank=57300 of ranks=142463rank=57400 of ranks=142463rank=57500 of ranks=142463rank=57600 of ranks=142463rank=57700 of ranks=142463rank=57800 of ranks=142463rank=57900 of ranks=142463rank=58000 of ranks=142463rank=58100 of ranks=142463rank=58200 of ranks=142463rank=58300 of ranks=142463rank=58400 of ranks=142463rank=58500 of ranks=142463rank=58600 of ranks=142463rank=58700 of ranks=142463rank=58800 of ranks=142463rank=58900 of ranks=142463rank=59000 of ranks=142463rank=59100 of ranks=142463rank=59200 of ranks=142463rank=59300 of ranks=142463rank=59400 of ranks=142463rank=59500 of ranks=142463rank=59600 of ranks=142463rank=59700 of ranks=142463rank=59800 of ranks=142463rank=59900 of ranks=142463rank=60000 of ranks=142463rank=60100 of ranks=142463rank=60200 of ranks=142463rank=60300 of ranks=142463rank=60400 of ranks=142463rank=60500 of ranks=142463rank=60600 of ranks=142463rank=60700 of ranks=142463rank=60800 of ranks=142463rank=60900 of ranks=142463rank=61000 of ranks=142463rank=61100 of ranks=142463rank=61200 of ranks=142463rank=61300 of ranks=142463rank=61400 of ranks=142463rank=61500 of ranks=142463rank=61600 of ranks=142463rank=61700 of ranks=142463rank=61800 of ranks=142463rank=61900 of ranks=142463rank=62000 of ranks=142463rank=62100 of ranks=142463rank=62200 of ranks=142463rank=62300 of ranks=142463rank=62400 of ranks=142463rank=62500 of ranks=142463rank=62600 of ranks=142463rank=62700 of ranks=142463rank=62800 of ranks=142463rank=62900 of ranks=142463rank=63000 of ranks=142463rank=63100 of ranks=142463rank=63200 of ranks=142463rank=63300 of ranks=142463rank=63400 of ranks=142463rank=63500 of ranks=142463rank=63600 of ranks=142463rank=63700 of ranks=142463rank=63800 of ranks=142463rank=63900 of ranks=142463rank=64000 of ranks=142463rank=64100 of ranks=142463rank=64200 of ranks=142463rank=64300 of ranks=142463rank=64400 of ranks=142463rank=64500 of ranks=142463rank=64600 of ranks=142463rank=64700 of ranks=142463rank=64800 of ranks=142463rank=64900 of ranks=142463rank=65000 of ranks=142463rank=65100 of ranks=142463rank=65200 of ranks=142463rank=65300 of ranks=142463rank=65400 of ranks=142463rank=65500 of ranks=142463rank=65600 of ranks=142463rank=65700 of ranks=142463rank=65800 of ranks=142463rank=65900 of ranks=142463rank=66000 of ranks=142463rank=66100 of ranks=142463rank=66200 of ranks=142463rank=66300 of ranks=142463rank=66400 of ranks=142463rank=66500 of ranks=142463rank=66600 of ranks=142463rank=66700 of ranks=142463rank=66800 of ranks=142463rank=66900 of ranks=142463rank=67000 of ranks=142463rank=67100 of ranks=142463rank=67200 of ranks=142463rank=67300 of ranks=142463rank=67400 of ranks=142463rank=67500 of ranks=142463rank=67600 of ranks=142463rank=67700 of ranks=142463rank=67800 of ranks=142463rank=67900 of ranks=142463rank=68000 of ranks=142463rank=68100 of ranks=142463rank=68200 of ranks=142463rank=68300 of ranks=142463rank=68400 of ranks=142463rank=68500 of ranks=142463rank=68600 of ranks=142463rank=68700 of ranks=142463rank=68800 of ranks=142463rank=68900 of ranks=142463rank=69000 of ranks=142463rank=69100 of ranks=142463rank=69200 of ranks=142463rank=69300 of ranks=142463rank=69400 of ranks=142463rank=69500 of ranks=142463rank=69600 of ranks=142463rank=69700 of ranks=142463rank=69800 of ranks=142463rank=69900 of ranks=142463rank=70000 of ranks=142463rank=70100 of ranks=142463rank=70200 of ranks=142463rank=70300 of ranks=142463rank=70400 of ranks=142463rank=70500 of ranks=142463rank=70600 of ranks=142463rank=70700 of ranks=142463rank=70800 of ranks=142463rank=70900 of ranks=142463rank=71000 of ranks=142463rank=71100 of ranks=142463rank=71200 of ranks=142463rank=71300 of ranks=142463rank=71400 of ranks=142463rank=71500 of ranks=142463rank=71600 of ranks=142463rank=71700 of ranks=142463rank=71800 of ranks=142463rank=71900 of ranks=142463rank=72000 of ranks=142463rank=72100 of ranks=142463rank=72200 of ranks=142463rank=72300 of ranks=142463rank=72400 of ranks=142463rank=72500 of ranks=142463rank=72600 of ranks=142463rank=72700 of ranks=142463rank=72800 of ranks=142463rank=72900 of ranks=142463rank=73000 of ranks=142463rank=73100 of ranks=142463rank=73200 of ranks=142463rank=73300 of ranks=142463rank=73400 of ranks=142463rank=73500 of ranks=142463rank=73600 of ranks=142463rank=73700 of ranks=142463rank=73800 of ranks=142463rank=73900 of ranks=142463rank=74000 of ranks=142463rank=74100 of ranks=142463rank=74200 of ranks=142463rank=74300 of ranks=142463rank=74400 of ranks=142463rank=74500 of ranks=142463rank=74600 of ranks=142463rank=74700 of ranks=142463rank=74800 of ranks=142463rank=74900 of ranks=142463rank=75000 of ranks=142463rank=75100 of ranks=142463rank=75200 of ranks=142463rank=75300 of ranks=142463rank=75400 of ranks=142463rank=75500 of ranks=142463rank=75600 of ranks=142463rank=75700 of ranks=142463rank=75800 of ranks=142463rank=75900 of ranks=142463rank=76000 of ranks=142463rank=76100 of ranks=142463rank=76200 of ranks=142463rank=76300 of ranks=142463rank=76400 of ranks=142463rank=76500 of ranks=142463rank=76600 of ranks=142463rank=76700 of ranks=142463rank=76800 of ranks=142463rank=76900 of ranks=142463rank=77000 of ranks=142463rank=77100 of ranks=142463rank=77200 of ranks=142463rank=77300 of ranks=142463rank=77400 of ranks=142463rank=77500 of ranks=142463rank=77600 of ranks=142463rank=77700 of ranks=142463rank=77800 of ranks=142463rank=77900 of ranks=142463rank=78000 of ranks=142463rank=78100 of ranks=142463rank=78200 of ranks=142463rank=78300 of ranks=142463rank=78400 of ranks=142463rank=78500 of ranks=142463rank=78600 of ranks=142463rank=78700 of ranks=142463rank=78800 of ranks=142463rank=78900 of ranks=142463rank=79000 of ranks=142463rank=79100 of ranks=142463rank=79200 of ranks=142463rank=79300 of ranks=142463rank=79400 of ranks=142463rank=79500 of ranks=142463rank=79600 of ranks=142463rank=79700 of ranks=142463rank=79800 of ranks=142463rank=79900 of ranks=142463rank=80000 of ranks=142463rank=80100 of ranks=142463rank=80200 of ranks=142463rank=80300 of ranks=142463rank=80400 of ranks=142463rank=80500 of ranks=142463rank=80600 of ranks=142463rank=80700 of ranks=142463rank=80800 of ranks=142463rank=80900 of ranks=142463rank=81000 of ranks=142463rank=81100 of ranks=142463rank=81200 of ranks=142463rank=81300 of ranks=142463rank=81400 of ranks=142463rank=81500 of ranks=142463rank=81600 of ranks=142463rank=81700 of ranks=142463rank=81800 of ranks=142463rank=81900 of ranks=142463rank=82000 of ranks=142463rank=82100 of ranks=142463rank=82200 of ranks=142463rank=82300 of ranks=142463rank=82400 of ranks=142463rank=82500 of ranks=142463rank=82600 of ranks=142463rank=82700 of ranks=142463rank=82800 of ranks=142463rank=82900 of ranks=142463rank=83000 of ranks=142463rank=83100 of ranks=142463rank=83200 of ranks=142463rank=83300 of ranks=142463rank=83400 of ranks=142463rank=83500 of ranks=142463rank=83600 of ranks=142463rank=83700 of ranks=142463rank=83800 of ranks=142463rank=83900 of ranks=142463rank=84000 of ranks=142463rank=84100 of ranks=142463rank=84200 of ranks=142463rank=84300 of ranks=142463rank=84400 of ranks=142463rank=84500 of ranks=142463rank=84600 of ranks=142463rank=84700 of ranks=142463rank=84800 of ranks=142463rank=84900 of ranks=142463rank=85000 of ranks=142463rank=85100 of ranks=142463rank=85200 of ranks=142463rank=85300 of ranks=142463rank=85400 of ranks=142463rank=85500 of ranks=142463rank=85600 of ranks=142463rank=85700 of ranks=142463rank=85800 of ranks=142463rank=85900 of ranks=142463rank=86000 of ranks=142463rank=86100 of ranks=142463rank=86200 of ranks=142463rank=86300 of ranks=142463rank=86400 of ranks=142463rank=86500 of ranks=142463rank=86600 of ranks=142463rank=86700 of ranks=142463rank=86800 of ranks=142463rank=86900 of ranks=142463rank=87000 of ranks=142463rank=87100 of ranks=142463rank=87200 of ranks=142463rank=87300 of ranks=142463rank=87400 of ranks=142463rank=87500 of ranks=142463rank=87600 of ranks=142463rank=87700 of ranks=142463rank=87800 of ranks=142463rank=87900 of ranks=142463rank=88000 of ranks=142463rank=88100 of ranks=142463rank=88200 of ranks=142463rank=88300 of ranks=142463rank=88400 of ranks=142463rank=88500 of ranks=142463rank=88600 of ranks=142463rank=88700 of ranks=142463rank=88800 of ranks=142463rank=88900 of ranks=142463rank=89000 of ranks=142463rank=89100 of ranks=142463rank=89200 of ranks=142463rank=89300 of ranks=142463rank=89400 of ranks=142463rank=89500 of ranks=142463rank=89600 of ranks=142463rank=89700 of ranks=142463rank=89800 of ranks=142463rank=89900 of ranks=142463rank=90000 of ranks=142463rank=90100 of ranks=142463rank=90200 of ranks=142463rank=90300 of ranks=142463rank=90400 of ranks=142463rank=90500 of ranks=142463rank=90600 of ranks=142463rank=90700 of ranks=142463rank=90800 of ranks=142463rank=90900 of ranks=142463rank=91000 of ranks=142463rank=91100 of ranks=142463rank=91200 of ranks=142463rank=91300 of ranks=142463rank=91400 of ranks=142463rank=91500 of ranks=142463rank=91600 of ranks=142463rank=91700 of ranks=142463rank=91800 of ranks=142463rank=91900 of ranks=142463rank=92000 of ranks=142463rank=92100 of ranks=142463rank=92200 of ranks=142463rank=92300 of ranks=142463rank=92400 of ranks=142463rank=92500 of ranks=142463rank=92600 of ranks=142463rank=92700 of ranks=142463rank=92800 of ranks=142463rank=92900 of ranks=142463rank=93000 of ranks=142463rank=93100 of ranks=142463rank=93200 of ranks=142463rank=93300 of ranks=142463rank=93400 of ranks=142463rank=93500 of ranks=142463rank=93600 of ranks=142463rank=93700 of ranks=142463rank=93800 of ranks=142463rank=93900 of ranks=142463rank=94000 of ranks=142463rank=94100 of ranks=142463rank=94200 of ranks=142463rank=94300 of ranks=142463rank=94400 of ranks=142463rank=94500 of ranks=142463rank=94600 of ranks=142463rank=94700 of ranks=142463rank=94800 of ranks=142463rank=94900 of ranks=142463rank=95000 of ranks=142463rank=95100 of ranks=142463rank=95200 of ranks=142463rank=95300 of ranks=142463rank=95400 of ranks=142463rank=95500 of ranks=142463rank=95600 of ranks=142463rank=95700 of ranks=142463rank=95800 of ranks=142463rank=95900 of ranks=142463rank=96000 of ranks=142463rank=96100 of ranks=142463rank=96200 of ranks=142463rank=96300 of ranks=142463rank=96400 of ranks=142463rank=96500 of ranks=142463rank=96600 of ranks=142463rank=96700 of ranks=142463rank=96800 of ranks=142463rank=96900 of ranks=142463rank=97000 of ranks=142463rank=97100 of ranks=142463rank=97200 of ranks=142463rank=97300 of ranks=142463rank=97400 of ranks=142463rank=97500 of ranks=142463rank=97600 of ranks=142463rank=97700 of ranks=142463rank=97800 of ranks=142463rank=97900 of ranks=142463rank=98000 of ranks=142463rank=98100 of ranks=142463rank=98200 of ranks=142463rank=98300 of ranks=142463rank=98400 of ranks=142463rank=98500 of ranks=142463rank=98600 of ranks=142463rank=98700 of ranks=142463rank=98800 of ranks=142463rank=98900 of ranks=142463rank=99000 of ranks=142463rank=99100 of ranks=142463rank=99200 of ranks=142463rank=99300 of ranks=142463rank=99400 of ranks=142463rank=99500 of ranks=142463rank=99600 of ranks=142463rank=99700 of ranks=142463rank=99800 of ranks=142463rank=99900 of ranks=142463rank=100000 of ranks=142463rank=100100 of ranks=142463rank=100200 of ranks=142463rank=100300 of ranks=142463rank=100400 of ranks=142463rank=100500 of ranks=142463rank=100600 of ranks=142463rank=100700 of ranks=142463rank=100800 of ranks=142463rank=100900 of ranks=142463rank=101000 of ranks=142463rank=101100 of ranks=142463rank=101200 of ranks=142463rank=101300 of ranks=142463rank=101400 of ranks=142463rank=101500 of ranks=142463rank=101600 of ranks=142463rank=101700 of ranks=142463rank=101800 of ranks=142463rank=101900 of ranks=142463rank=102000 of ranks=142463rank=102100 of ranks=142463rank=102200 of ranks=142463rank=102300 of ranks=142463rank=102400 of ranks=142463rank=102500 of ranks=142463rank=102600 of ranks=142463rank=102700 of ranks=142463rank=102800 of ranks=142463rank=102900 of ranks=142463rank=103000 of ranks=142463rank=103100 of ranks=142463rank=103200 of ranks=142463rank=103300 of ranks=142463rank=103400 of ranks=142463rank=103500 of ranks=142463rank=103600 of ranks=142463rank=103700 of ranks=142463rank=103800 of ranks=142463rank=103900 of ranks=142463rank=104000 of ranks=142463rank=104100 of ranks=142463rank=104200 of ranks=142463rank=104300 of ranks=142463rank=104400 of ranks=142463rank=104500 of ranks=142463rank=104600 of ranks=142463rank=104700 of ranks=142463rank=104800 of ranks=142463rank=104900 of ranks=142463rank=105000 of ranks=142463rank=105100 of ranks=142463rank=105200 of ranks=142463rank=105300 of ranks=142463rank=105400 of ranks=142463rank=105500 of ranks=142463rank=105600 of ranks=142463rank=105700 of ranks=142463rank=105800 of ranks=142463rank=105900 of ranks=142463rank=106000 of ranks=142463rank=106100 of ranks=142463rank=106200 of ranks=142463rank=106300 of ranks=142463rank=106400 of ranks=142463rank=106500 of ranks=142463rank=106600 of ranks=142463rank=106700 of ranks=142463rank=106800 of ranks=142463rank=106900 of ranks=142463rank=107000 of ranks=142463rank=107100 of ranks=142463rank=107200 of ranks=142463rank=107300 of ranks=142463rank=107400 of ranks=142463rank=107500 of ranks=142463rank=107600 of ranks=142463rank=107700 of ranks=142463rank=107800 of ranks=142463rank=107900 of ranks=142463rank=108000 of ranks=142463rank=108100 of ranks=142463rank=108200 of ranks=142463rank=108300 of ranks=142463rank=108400 of ranks=142463rank=108500 of ranks=142463rank=108600 of ranks=142463rank=108700 of ranks=142463rank=108800 of ranks=142463rank=108900 of ranks=142463rank=109000 of ranks=142463rank=109100 of ranks=142463rank=109200 of ranks=142463rank=109300 of ranks=142463rank=109400 of ranks=142463rank=109500 of ranks=142463rank=109600 of ranks=142463rank=109700 of ranks=142463rank=109800 of ranks=142463rank=109900 of ranks=142463rank=110000 of ranks=142463rank=110100 of ranks=142463rank=110200 of ranks=142463rank=110300 of ranks=142463rank=110400 of ranks=142463rank=110500 of ranks=142463rank=110600 of ranks=142463rank=110700 of ranks=142463rank=110800 of ranks=142463rank=110900 of ranks=142463rank=111000 of ranks=142463rank=111100 of ranks=142463rank=111200 of ranks=142463rank=111300 of ranks=142463rank=111400 of ranks=142463rank=111500 of ranks=142463rank=111600 of ranks=142463rank=111700 of ranks=142463rank=111800 of ranks=142463rank=111900 of ranks=142463rank=112000 of ranks=142463rank=112100 of ranks=142463rank=112200 of ranks=142463rank=112300 of ranks=142463rank=112400 of ranks=142463rank=112500 of ranks=142463rank=112600 of ranks=142463rank=112700 of ranks=142463rank=112800 of ranks=142463rank=112900 of ranks=142463rank=113000 of ranks=142463rank=113100 of ranks=142463rank=113200 of ranks=142463rank=113300 of ranks=142463rank=113400 of ranks=142463rank=113500 of ranks=142463rank=113600 of ranks=142463rank=113700 of ranks=142463rank=113800 of ranks=142463rank=113900 of ranks=142463rank=114000 of ranks=142463rank=114100 of ranks=142463rank=114200 of ranks=142463rank=114300 of ranks=142463rank=114400 of ranks=142463rank=114500 of ranks=142463rank=114600 of ranks=142463rank=114700 of ranks=142463rank=114800 of ranks=142463rank=114900 of ranks=142463rank=115000 of ranks=142463rank=115100 of ranks=142463rank=115200 of ranks=142463rank=115300 of ranks=142463rank=115400 of ranks=142463rank=115500 of ranks=142463rank=115600 of ranks=142463rank=115700 of ranks=142463rank=115800 of ranks=142463rank=115900 of ranks=142463rank=116000 of ranks=142463rank=116100 of ranks=142463rank=116200 of ranks=142463rank=116300 of ranks=142463rank=116400 of ranks=142463rank=116500 of ranks=142463rank=116600 of ranks=142463rank=116700 of ranks=142463rank=116800 of ranks=142463rank=116900 of ranks=142463rank=117000 of ranks=142463rank=117100 of ranks=142463rank=117200 of ranks=142463rank=117300 of ranks=142463rank=117400 of ranks=142463rank=117500 of ranks=142463rank=117600 of ranks=142463rank=117700 of ranks=142463rank=117800 of ranks=142463rank=117900 of ranks=142463rank=118000 of ranks=142463rank=118100 of ranks=142463rank=118200 of ranks=142463rank=118300 of ranks=142463rank=118400 of ranks=142463rank=118500 of ranks=142463rank=118600 of ranks=142463rank=118700 of ranks=142463rank=118800 of ranks=142463rank=118900 of ranks=142463rank=119000 of ranks=142463rank=119100 of ranks=142463rank=119200 of ranks=142463rank=119300 of ranks=142463rank=119400 of ranks=142463rank=119500 of ranks=142463rank=119600 of ranks=142463rank=119700 of ranks=142463rank=119800 of ranks=142463rank=119900 of ranks=142463rank=120000 of ranks=142463rank=120100 of ranks=142463rank=120200 of ranks=142463rank=120300 of ranks=142463rank=120400 of ranks=142463rank=120500 of ranks=142463rank=120600 of ranks=142463rank=120700 of ranks=142463rank=120800 of ranks=142463rank=120900 of ranks=142463rank=121000 of ranks=142463rank=121100 of ranks=142463rank=121200 of ranks=142463rank=121300 of ranks=142463rank=121400 of ranks=142463rank=121500 of ranks=142463rank=121600 of ranks=142463rank=121700 of ranks=142463rank=121800 of ranks=142463rank=121900 of ranks=142463rank=122000 of ranks=142463rank=122100 of ranks=142463rank=122200 of ranks=142463rank=122300 of ranks=142463rank=122400 of ranks=142463rank=122500 of ranks=142463rank=122600 of ranks=142463rank=122700 of ranks=142463rank=122800 of ranks=142463rank=122900 of ranks=142463rank=123000 of ranks=142463rank=123100 of ranks=142463rank=123200 of ranks=142463rank=123300 of ranks=142463rank=123400 of ranks=142463rank=123500 of ranks=142463rank=123600 of ranks=142463rank=123700 of ranks=142463rank=123800 of ranks=142463rank=123900 of ranks=142463rank=124000 of ranks=142463rank=124100 of ranks=142463rank=124200 of ranks=142463rank=124300 of ranks=142463rank=124400 of ranks=142463rank=124500 of ranks=142463rank=124600 of ranks=142463rank=124700 of ranks=142463rank=124800 of ranks=142463rank=124900 of ranks=142463rank=125000 of ranks=142463rank=125100 of ranks=142463rank=125200 of ranks=142463rank=125300 of ranks=142463rank=125400 of ranks=142463rank=125500 of ranks=142463rank=125600 of ranks=142463rank=125700 of ranks=142463rank=125800 of ranks=142463rank=125900 of ranks=142463rank=126000 of ranks=142463rank=126100 of ranks=142463rank=126200 of ranks=142463rank=126300 of ranks=142463rank=126400 of ranks=142463rank=126500 of ranks=142463rank=126600 of ranks=142463rank=126700 of ranks=142463rank=126800 of ranks=142463rank=126900 of ranks=142463rank=127000 of ranks=142463rank=127100 of ranks=142463rank=127200 of ranks=142463rank=127300 of ranks=142463rank=127400 of ranks=142463rank=127500 of ranks=142463rank=127600 of ranks=142463rank=127700 of ranks=142463rank=127800 of ranks=142463rank=127900 of ranks=142463rank=128000 of ranks=142463rank=128100 of ranks=142463rank=128200 of ranks=142463rank=128300 of ranks=142463rank=128400 of ranks=142463rank=128500 of ranks=142463rank=128600 of ranks=142463rank=128700 of ranks=142463rank=128800 of ranks=142463rank=128900 of ranks=142463rank=129000 of ranks=142463rank=129100 of ranks=142463rank=129200 of ranks=142463rank=129300 of ranks=142463rank=129400 of ranks=142463rank=129500 of ranks=142463rank=129600 of ranks=142463rank=129700 of ranks=142463rank=129800 of ranks=142463rank=129900 of ranks=142463rank=130000 of ranks=142463rank=130100 of ranks=142463rank=130200 of ranks=142463rank=130300 of ranks=142463rank=130400 of ranks=142463rank=130500 of ranks=142463rank=130600 of ranks=142463rank=130700 of ranks=142463rank=130800 of ranks=142463rank=130900 of ranks=142463rank=131000 of ranks=142463rank=131100 of ranks=142463rank=131200 of ranks=142463rank=131300 of ranks=142463rank=131400 of ranks=142463rank=131500 of ranks=142463rank=131600 of ranks=142463rank=131700 of ranks=142463rank=131800 of ranks=142463rank=131900 of ranks=142463rank=132000 of ranks=142463rank=132100 of ranks=142463rank=132200 of ranks=142463rank=132300 of ranks=142463rank=132400 of ranks=142463rank=132500 of ranks=142463rank=132600 of ranks=142463rank=132700 of ranks=142463rank=132800 of ranks=142463rank=132900 of ranks=142463rank=133000 of ranks=142463rank=133100 of ranks=142463rank=133200 of ranks=142463rank=133300 of ranks=142463rank=133400 of ranks=142463rank=133500 of ranks=142463rank=133600 of ranks=142463rank=133700 of ranks=142463rank=133800 of ranks=142463rank=133900 of ranks=142463rank=134000 of ranks=142463rank=134100 of ranks=142463rank=134200 of ranks=142463rank=134300 of ranks=142463rank=134400 of ranks=142463rank=134500 of ranks=142463rank=134600 of ranks=142463rank=134700 of ranks=142463rank=134800 of ranks=142463rank=134900 of ranks=142463rank=135000 of ranks=142463rank=135100 of ranks=142463rank=135200 of ranks=142463rank=135300 of ranks=142463rank=135400 of ranks=142463rank=135500 of ranks=142463rank=135600 of ranks=142463rank=135700 of ranks=142463rank=135800 of ranks=142463rank=135900 of ranks=142463rank=136000 of ranks=142463rank=136100 of ranks=142463rank=136200 of ranks=142463rank=136300 of ranks=142463rank=136400 of ranks=142463rank=136500 of ranks=142463rank=136600 of ranks=142463rank=136700 of ranks=142463rank=136800 of ranks=142463rank=136900 of ranks=142463rank=137000 of ranks=142463rank=137100 of ranks=142463rank=137200 of ranks=142463rank=137300 of ranks=142463rank=137400 of ranks=142463rank=137500 of ranks=142463rank=137600 of ranks=142463rank=137700 of ranks=142463rank=137800 of ranks=142463rank=137900 of ranks=142463rank=138000 of ranks=142463rank=138100 of ranks=142463rank=138200 of ranks=142463rank=138300 of ranks=142463rank=138400 of ranks=142463rank=138500 of ranks=142463rank=138600 of ranks=142463rank=138700 of ranks=142463rank=138800 of ranks=142463rank=138900 of ranks=142463rank=139000 of ranks=142463rank=139100 of ranks=142463rank=139200 of ranks=142463rank=139300 of ranks=142463rank=139400 of ranks=142463rank=139500 of ranks=142463rank=139600 of ranks=142463rank=139700 of ranks=142463rank=139800 of ranks=142463rank=139900 of ranks=142463rank=140000 of ranks=142463rank=140100 of ranks=142463rank=140200 of ranks=142463rank=140300 of ranks=142463rank=140400 of ranks=142463rank=140500 of ranks=142463rank=140600 of ranks=142463rank=140700 of ranks=142463rank=140800 of ranks=142463rank=140900 of ranks=142463rank=141000 of ranks=142463rank=141100 of ranks=142463rank=141200 of ranks=142463rank=141300 of ranks=142463rank=141400 of ranks=142463rank=141500 of ranks=142463rank=141600 of ranks=142463rank=141700 of ranks=142463rank=141800 of ranks=142463rank=141900 of ranks=142463rank=142000 of ranks=142463rank=142100 of ranks=142463rank=142200 of ranks=142463rank=142300 of ranks=142463rank=142400 of ranks=142463

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              87.8835    472  11181     26    498           63.5108
   1 car                    97.1581  49728  28885    588  50316           82.1303
   2 truck                  89.5094   1799  24970     26   1825           51.3747
   3 bus                    78.4919    356   8228     10    366           51.7465
   4 pedestrian             90.8866   3994  12850    265   4259           69.5759

for conf_thresh=0.25, precision=0.92, recall=0.89, F1 score=0.90
for conf_thresh=0.25, TP=51026, FP=4579, FN=6238, average IoU=79.47%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=88.79%
Total detection time: 126 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
3437: loss=6.303, avg loss=5.179, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 219968 images, time remaining=4.4 hours
3438: loss=5.346, avg loss=5.196, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=786.3 milliseconds, train=2.1 seconds, 220032 images, time remaining=4.4 hours
3439: loss=4.451, avg loss=5.121, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=683.0 milliseconds, train=2.1 seconds, 220096 images, time remaining=4.4 hours
3440: loss=4.702, avg loss=5.079, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=526.0 milliseconds, train=2.1 seconds, 220160 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3441: loss=4.787, avg loss=5.050, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=536.7 milliseconds, train=3.4 seconds, 220224 images, time remaining=4.4 hours
3442: loss=3.742, avg loss=4.919, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=675.3 milliseconds, train=3.3 seconds, 220288 images, time remaining=4.4 hours
3443: loss=4.756, avg loss=4.903, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=542.9 milliseconds, train=3.3 seconds, 220352 images, time remaining=4.4 hours
3444: loss=4.960, avg loss=4.909, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=473.8 milliseconds, train=3.4 seconds, 220416 images, time remaining=4.4 hours
3445: loss=4.643, avg loss=4.882, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=643.5 milliseconds, train=3.3 seconds, 220480 images, time remaining=4.4 hours
3446: loss=5.114, avg loss=4.905, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=601.2 milliseconds, train=3.3 seconds, 220544 images, time remaining=4.4 hours
3447: loss=4.063, avg loss=4.821, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=539.2 milliseconds, train=3.3 seconds, 220608 images, time remaining=4.4 hours
3448: loss=4.267, avg loss=4.766, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=682.2 milliseconds, train=3.3 seconds, 220672 images, time remaining=4.4 hours
3449: loss=3.837, avg loss=4.673, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=764.5 milliseconds, train=3.4 seconds, 220736 images, time remaining=4.4 hours
3450: loss=4.516, avg loss=4.657, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=844.5 milliseconds, train=3.3 seconds, 220800 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
3451: loss=5.582, avg loss=4.750, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=483.7 milliseconds, train=1.6 seconds, 220864 images, time remaining=4.4 hours
3452: loss=5.772, avg loss=4.852, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=800.3 milliseconds, train=1.6 seconds, 220928 images, time remaining=4.4 hours
3453: loss=5.242, avg loss=4.891, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=535.5 milliseconds, train=1.5 seconds, 220992 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3454: loss=4.797, avg loss=4.881, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 221056 images, time remaining=4.4 hours
3455: loss=4.408, avg loss=4.834, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=593.8 milliseconds, train=1.5 seconds, 221120 images, time remaining=4.4 hours
3456: loss=4.387, avg loss=4.789, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=724.3 milliseconds, train=1.5 seconds, 221184 images, time remaining=4.4 hours
3457: loss=3.689, avg loss=4.679, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 221248 images, time remaining=4.4 hours
3458: loss=4.164, avg loss=4.628, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=958.5 milliseconds, train=1.5 seconds, 221312 images, time remaining=4.4 hours
3459: loss=4.112, avg loss=4.576, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=719.1 milliseconds, train=1.5 seconds, 221376 images, time remaining=4.4 hours
3460: loss=3.956, avg loss=4.514, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=824.4 milliseconds, train=1.6 seconds, 221440 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3461: loss=5.321, avg loss=4.595, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 221504 images, time remaining=4.4 hours
3462: loss=4.311, avg loss=4.566, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=936.5 milliseconds, train=1.9 seconds, 221568 images, time remaining=4.4 hours
3463: loss=4.028, avg loss=4.513, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=463.2 milliseconds, train=1.9 seconds, 221632 images, time remaining=4.4 hours
3464: loss=4.487, avg loss=4.510, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=585.5 milliseconds, train=1.9 seconds, 221696 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3465: loss=4.778, avg loss=4.537, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 221760 images, time remaining=4.4 hours
3466: loss=4.557, avg loss=4.539, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=522.5 milliseconds, train=1.9 seconds, 221824 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3467: loss=4.788, avg loss=4.564, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.9 seconds, train=1.9 seconds, 221888 images, time remaining=4.4 hours
3468: loss=4.043, avg loss=4.512, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=1.9 seconds, 221952 images, time remaining=4.4 hours
3469: loss=4.438, avg loss=4.504, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=653.9 milliseconds, train=1.9 seconds, 222016 images, time remaining=4.4 hours
3470: loss=4.662, avg loss=4.520, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=927.3 milliseconds, train=1.9 seconds, 222080 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3471: loss=3.962, avg loss=4.464, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 222144 images, time remaining=4.4 hours
3472: loss=5.043, avg loss=4.522, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 222208 images, time remaining=4.4 hours
3473: loss=4.449, avg loss=4.515, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=718.1 milliseconds, train=2.1 seconds, 222272 images, time remaining=4.4 hours
3474: loss=4.388, avg loss=4.502, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=609.6 milliseconds, train=2.1 seconds, 222336 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3475: loss=4.128, avg loss=4.465, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 222400 images, time remaining=4.4 hours
3476: loss=4.701, avg loss=4.488, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=752.9 milliseconds, train=2.1 seconds, 222464 images, time remaining=4.4 hours
3477: loss=4.662, avg loss=4.506, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 222528 images, time remaining=4.4 hours
3478: loss=3.860, avg loss=4.441, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=740.8 milliseconds, train=2.1 seconds, 222592 images, time remaining=4.4 hours
3479: loss=4.204, avg loss=4.417, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 222656 images, time remaining=4.4 hours
3480: loss=4.266, avg loss=4.402, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=483.1 milliseconds, train=2.1 seconds, 222720 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3481: loss=3.572, avg loss=4.319, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=446.0 milliseconds, train=2.1 seconds, 222784 images, time remaining=4.4 hours
3482: loss=4.069, avg loss=4.294, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=524.9 milliseconds, train=2.1 seconds, 222848 images, time remaining=4.4 hours
3483: loss=4.349, avg loss=4.300, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 222912 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3484: loss=4.179, avg loss=4.288, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.3 seconds, train=2.1 seconds, 222976 images, time remaining=4.4 hours
3485: loss=4.236, avg loss=4.283, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 223040 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3486: loss=3.086, avg loss=4.163, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 223104 images, time remaining=4.4 hours
3487: loss=3.639, avg loss=4.110, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=595.7 milliseconds, train=2.1 seconds, 223168 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3488: loss=3.247, avg loss=4.024, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 223232 images, time remaining=4.4 hours
3489: loss=4.248, avg loss=4.046, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=878.3 milliseconds, train=2.1 seconds, 223296 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3490: loss=4.019, avg loss=4.044, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 223360 images, time remaining=4.4 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3491: loss=4.702, avg loss=4.110, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=3.6 seconds, 223424 images, time remaining=4.4 hours
3492: loss=3.947, avg loss=4.093, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 223488 images, time remaining=4.4 hours
3493: loss=4.743, avg loss=4.158, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.8 seconds, train=3.6 seconds, 223552 images, time remaining=4.4 hours
3494: loss=3.671, avg loss=4.109, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=950.1 milliseconds, train=3.6 seconds, 223616 images, time remaining=4.4 hours
3495: loss=3.586, avg loss=4.057, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=3.6 seconds, 223680 images, time remaining=4.4 hours
3496: loss=5.913, avg loss=4.243, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=563.3 milliseconds, train=3.6 seconds, 223744 images, time remaining=4.4 hours
3497: loss=3.132, avg loss=4.132, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=673.6 milliseconds, train=3.6 seconds, 223808 images, time remaining=4.4 hours
3498: loss=4.473, avg loss=4.166, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=638.5 milliseconds, train=3.6 seconds, 223872 images, time remaining=4.4 hours
3499: loss=3.698, avg loss=4.119, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=603.4 milliseconds, train=3.6 seconds, 223936 images, time remaining=4.4 hours
3500: loss=3.977, avg loss=4.105, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=684.3 milliseconds, train=3.6 seconds, 224000 images, time remaining=4.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x14593a000000
3501: loss=4.057, avg loss=4.100, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 224064 images, time remaining=4.4 hours
3502: loss=3.624, avg loss=4.052, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=876.3 milliseconds, train=1.5 seconds, 224128 images, time remaining=4.4 hours
3503: loss=3.971, avg loss=4.044, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=820.3 milliseconds, train=1.5 seconds, 224192 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3504: loss=3.896, avg loss=4.030, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 224256 images, time remaining=4.4 hours
3505: loss=3.798, avg loss=4.006, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=717.6 milliseconds, train=1.5 seconds, 224320 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3506: loss=3.860, avg loss=3.992, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 224384 images, time remaining=4.4 hours
3507: loss=4.287, avg loss=4.021, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=721.9 milliseconds, train=1.5 seconds, 224448 images, time remaining=4.4 hours
3508: loss=3.779, avg loss=3.997, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=815.3 milliseconds, train=1.5 seconds, 224512 images, time remaining=4.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3509: loss=4.266, avg loss=4.024, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 224576 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3510: loss=5.125, avg loss=4.134, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 224640 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145fe8000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3511: loss=4.169, avg loss=4.138, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 224704 images, time remaining=4.3 hours
3512: loss=3.605, avg loss=4.084, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=841.3 milliseconds, train=1.5 seconds, 224768 images, time remaining=4.3 hours
3513: loss=4.147, avg loss=4.091, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=883.1 milliseconds, train=1.5 seconds, 224832 images, time remaining=4.3 hours
3514: loss=3.545, avg loss=4.036, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 224896 images, time remaining=4.3 hours
3515: loss=4.311, avg loss=4.064, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=627.3 milliseconds, train=1.5 seconds, 224960 images, time remaining=4.3 hours
3516: loss=4.292, avg loss=4.086, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=679.3 milliseconds, train=1.5 seconds, 225024 images, time remaining=4.3 hours
3517: loss=3.194, avg loss=3.997, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=893.2 milliseconds, train=1.5 seconds, 225088 images, time remaining=4.3 hours
3518: loss=4.173, avg loss=4.015, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=549.0 milliseconds, train=1.5 seconds, 225152 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3519: loss=3.763, avg loss=3.990, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 225216 images, time remaining=4.3 hours
3520: loss=4.278, avg loss=4.019, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=653.5 milliseconds, train=1.5 seconds, 225280 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3521: loss=4.738, avg loss=4.090, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=765.2 milliseconds, train=4.4 seconds, 225344 images, time remaining=4.3 hours
3522: loss=5.378, avg loss=4.219, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=553.2 milliseconds, train=4.4 seconds, 225408 images, time remaining=4.3 hours
3523: loss=4.730, avg loss=4.270, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=4.4 seconds, 225472 images, time remaining=4.3 hours
3524: loss=4.476, avg loss=4.291, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=869.9 milliseconds, train=4.4 seconds, 225536 images, time remaining=4.3 hours
3525: loss=5.118, avg loss=4.374, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=4.4 seconds, 225600 images, time remaining=4.3 hours
3526: loss=4.394, avg loss=4.376, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=716.8 milliseconds, train=4.4 seconds, 225664 images, time remaining=4.3 hours
3527: loss=5.188, avg loss=4.457, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=4.4 seconds, 225728 images, time remaining=4.3 hours
3528: loss=4.842, avg loss=4.495, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=784.6 milliseconds, train=4.4 seconds, 225792 images, time remaining=4.3 hours
3529: loss=4.095, avg loss=4.455, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 225856 images, time remaining=4.3 hours
3530: loss=4.680, avg loss=4.478, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 225920 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145a9c000000
3531: loss=5.070, avg loss=4.537, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=568.3 milliseconds, train=1.4 seconds, 225984 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3532: loss=4.273, avg loss=4.511, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 226048 images, time remaining=4.3 hours
3533: loss=4.209, avg loss=4.480, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=755.3 milliseconds, train=1.4 seconds, 226112 images, time remaining=4.3 hours
3534: loss=3.793, avg loss=4.412, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 226176 images, time remaining=4.3 hours
3535: loss=5.287, avg loss=4.499, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=534.4 milliseconds, train=1.5 seconds, 226240 images, time remaining=4.3 hours
3536: loss=3.996, avg loss=4.449, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=747.3 milliseconds, train=1.4 seconds, 226304 images, time remaining=4.3 hours
3537: loss=4.297, avg loss=4.434, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=430.8 milliseconds, train=1.4 seconds, 226368 images, time remaining=4.3 hours
3538: loss=4.464, avg loss=4.437, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=515.6 milliseconds, train=1.4 seconds, 226432 images, time remaining=4.3 hours
3539: loss=4.108, avg loss=4.404, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 226496 images, time remaining=4.3 hours
3540: loss=4.931, avg loss=4.457, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=675.3 milliseconds, train=1.5 seconds, 226560 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3541: loss=5.400, avg loss=4.551, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=681.5 milliseconds, train=3.4 seconds, 226624 images, time remaining=4.3 hours
3542: loss=5.161, avg loss=4.612, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=910.4 milliseconds, train=3.4 seconds, 226688 images, time remaining=4.3 hours
3543: loss=4.545, avg loss=4.605, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=807.1 milliseconds, train=3.4 seconds, 226752 images, time remaining=4.3 hours
3544: loss=4.907, avg loss=4.635, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 226816 images, time remaining=4.3 hours
3545: loss=4.031, avg loss=4.575, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=966.8 milliseconds, train=3.4 seconds, 226880 images, time remaining=4.3 hours
3546: loss=4.471, avg loss=4.565, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=813.6 milliseconds, train=3.4 seconds, 226944 images, time remaining=4.3 hours
3547: loss=3.798, avg loss=4.488, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=928.1 milliseconds, train=3.4 seconds, 227008 images, time remaining=4.3 hours
3548: loss=5.104, avg loss=4.550, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=833.7 milliseconds, train=3.4 seconds, 227072 images, time remaining=4.3 hours
3549: loss=4.762, avg loss=4.571, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=646.5 milliseconds, train=3.4 seconds, 227136 images, time remaining=4.3 hours
3550: loss=4.792, avg loss=4.593, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=590.4 milliseconds, train=3.4 seconds, 227200 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145ca4000000
3551: loss=5.418, avg loss=4.675, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=783.3 milliseconds, train=1.5 seconds, 227264 images, time remaining=4.3 hours
3552: loss=4.570, avg loss=4.665, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 227328 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3553: loss=4.895, avg loss=4.688, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 227392 images, time remaining=4.3 hours
3554: loss=4.369, avg loss=4.656, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=977.8 milliseconds, train=1.5 seconds, 227456 images, time remaining=4.3 hours
3555: loss=3.412, avg loss=4.532, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=748.1 milliseconds, train=1.5 seconds, 227520 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3556: loss=3.536, avg loss=4.432, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 227584 images, time remaining=4.3 hours
3557: loss=4.096, avg loss=4.398, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=713.7 milliseconds, train=1.5 seconds, 227648 images, time remaining=4.3 hours
3558: loss=5.181, avg loss=4.477, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=633.2 milliseconds, train=1.5 seconds, 227712 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3559: loss=3.442, avg loss=4.373, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 227776 images, time remaining=4.3 hours
3560: loss=4.403, avg loss=4.376, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 227840 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463b0000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3561: loss=3.398, avg loss=4.278, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 227904 images, time remaining=4.3 hours
3562: loss=5.133, avg loss=4.364, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 227968 images, time remaining=4.3 hours
3563: loss=3.941, avg loss=4.322, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=622.3 milliseconds, train=1.2 seconds, 228032 images, time remaining=4.3 hours
3564: loss=4.088, avg loss=4.298, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=867.0 milliseconds, train=1.2 seconds, 228096 images, time remaining=4.3 hours
3565: loss=4.106, avg loss=4.279, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=788.8 milliseconds, train=1.2 seconds, 228160 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3566: loss=3.247, avg loss=4.176, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 228224 images, time remaining=4.3 hours
3567: loss=4.045, avg loss=4.163, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=811.2 milliseconds, train=1.2 seconds, 228288 images, time remaining=4.3 hours
3568: loss=4.036, avg loss=4.150, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=811.7 milliseconds, train=1.2 seconds, 228352 images, time remaining=4.3 hours
3569: loss=4.945, avg loss=4.230, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=808.7 milliseconds, train=1.2 seconds, 228416 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3570: loss=3.715, avg loss=4.178, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 228480 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145fca000000
3571: loss=3.764, avg loss=4.137, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=615.1 milliseconds, train=1.6 seconds, 228544 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3572: loss=3.316, avg loss=4.055, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.6 seconds, 228608 images, time remaining=4.3 hours
3573: loss=3.830, avg loss=4.032, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=881.8 milliseconds, train=1.6 seconds, 228672 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3574: loss=4.136, avg loss=4.043, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.6 seconds, 228736 images, time remaining=4.3 hours
3575: loss=4.148, avg loss=4.053, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=647.8 milliseconds, train=1.6 seconds, 228800 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3576: loss=4.677, avg loss=4.115, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 228864 images, time remaining=4.3 hours
3577: loss=3.205, avg loss=4.024, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 228928 images, time remaining=4.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3578: loss=3.965, avg loss=4.018, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.6 seconds, 228992 images, time remaining=4.3 hours
3579: loss=3.673, avg loss=3.984, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=494.1 milliseconds, train=1.6 seconds, 229056 images, time remaining=4.3 hours
3580: loss=3.463, avg loss=3.932, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 229120 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3581: loss=5.156, avg loss=4.054, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 229184 images, time remaining=4.3 hours
3582: loss=6.387, avg loss=4.288, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=852.3 milliseconds, train=5.0 seconds, 229248 images, time remaining=4.3 hours
3583: loss=4.481, avg loss=4.307, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=735.9 milliseconds, train=5.0 seconds, 229312 images, time remaining=4.3 hours
3584: loss=4.942, avg loss=4.370, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=604.8 milliseconds, train=5.0 seconds, 229376 images, time remaining=4.3 hours
3585: loss=4.967, avg loss=4.430, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=5.0 seconds, 229440 images, time remaining=4.3 hours
3586: loss=5.159, avg loss=4.503, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=998.5 milliseconds, train=5.0 seconds, 229504 images, time remaining=4.3 hours
3587: loss=4.781, avg loss=4.531, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=761.2 milliseconds, train=5.0 seconds, 229568 images, time remaining=4.3 hours
3588: loss=5.805, avg loss=4.658, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 229632 images, time remaining=4.3 hours
3589: loss=4.918, avg loss=4.684, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 229696 images, time remaining=4.3 hours
3590: loss=4.420, avg loss=4.658, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=899.0 milliseconds, train=5.0 seconds, 229760 images, time remaining=4.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145ca6000000
3591: loss=5.131, avg loss=4.705, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=702.4 milliseconds, train=1.2 seconds, 229824 images, time remaining=4.3 hours
3592: loss=4.550, avg loss=4.690, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=559.5 milliseconds, train=1.2 seconds, 229888 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3593: loss=5.069, avg loss=4.728, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 229952 images, time remaining=4.2 hours
3594: loss=3.939, avg loss=4.649, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 230016 images, time remaining=4.2 hours
3595: loss=3.950, avg loss=4.579, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=449.7 milliseconds, train=1.2 seconds, 230080 images, time remaining=4.2 hours
3596: loss=4.670, avg loss=4.588, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 230144 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3597: loss=4.673, avg loss=4.596, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 230208 images, time remaining=4.2 hours
3598: loss=5.142, avg loss=4.651, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=814.0 milliseconds, train=1.2 seconds, 230272 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3599: loss=4.649, avg loss=4.651, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 230336 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3600: loss=4.095, avg loss=4.595, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 230400 images, time remaining=4.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3601: loss=11.288, avg loss=5.265, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=5.0 seconds, 230464 images, time remaining=4.2 hours
3602: loss=7.998, avg loss=5.538, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 230528 images, time remaining=4.2 hours
3603: loss=6.522, avg loss=5.636, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=780.9 milliseconds, train=5.0 seconds, 230592 images, time remaining=4.2 hours
3604: loss=5.463, avg loss=5.619, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=876.6 milliseconds, train=5.0 seconds, 230656 images, time remaining=4.2 hours
3605: loss=6.343, avg loss=5.691, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=970.1 milliseconds, train=4.9 seconds, 230720 images, time remaining=4.2 hours
3606: loss=7.256, avg loss=5.848, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.9 seconds, 230784 images, time remaining=4.2 hours
3607: loss=5.800, avg loss=5.843, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=874.8 milliseconds, train=4.9 seconds, 230848 images, time remaining=4.2 hours
3608: loss=7.460, avg loss=6.005, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=867.2 milliseconds, train=4.9 seconds, 230912 images, time remaining=4.2 hours
3609: loss=5.314, avg loss=5.936, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=4.9 seconds, 230976 images, time remaining=4.2 hours
3610: loss=6.735, avg loss=6.016, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=904.6 milliseconds, train=4.9 seconds, 231040 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3611: loss=6.598, avg loss=6.074, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=870.3 milliseconds, train=4.6 seconds, 231104 images, time remaining=4.2 hours
3612: loss=6.180, avg loss=6.084, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=4.6 seconds, 231168 images, time remaining=4.2 hours
3613: loss=6.578, avg loss=6.134, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.7 seconds, train=4.6 seconds, 231232 images, time remaining=4.2 hours
3614: loss=6.544, avg loss=6.175, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=743.4 milliseconds, train=4.6 seconds, 231296 images, time remaining=4.2 hours
3615: loss=6.545, avg loss=6.212, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 231360 images, time remaining=4.2 hours
3616: loss=6.237, avg loss=6.214, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 231424 images, time remaining=4.2 hours
3617: loss=5.516, avg loss=6.144, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=721.2 milliseconds, train=4.6 seconds, 231488 images, time remaining=4.2 hours
3618: loss=5.538, avg loss=6.084, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.6 seconds, train=4.6 seconds, 231552 images, time remaining=4.2 hours
3619: loss=5.858, avg loss=6.061, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 231616 images, time remaining=4.2 hours
3620: loss=5.554, avg loss=6.010, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.7 seconds, train=4.6 seconds, 231680 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1459d4000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3621: loss=5.830, avg loss=5.992, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 231744 images, time remaining=4.2 hours
3622: loss=5.565, avg loss=5.950, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=516.6 milliseconds, train=1.5 seconds, 231808 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3623: loss=4.537, avg loss=5.808, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 231872 images, time remaining=4.2 hours
3624: loss=5.316, avg loss=5.759, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 231936 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3625: loss=4.449, avg loss=5.628, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 232000 images, time remaining=4.2 hours
3626: loss=4.395, avg loss=5.505, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=896.0 milliseconds, train=1.5 seconds, 232064 images, time remaining=4.2 hours
3627: loss=4.364, avg loss=5.391, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=633.5 milliseconds, train=1.5 seconds, 232128 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3628: loss=4.619, avg loss=5.314, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 232192 images, time remaining=4.2 hours
3629: loss=5.551, avg loss=5.337, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=488.3 milliseconds, train=1.6 seconds, 232256 images, time remaining=4.2 hours
3630: loss=4.587, avg loss=5.262, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=533.1 milliseconds, train=1.5 seconds, 232320 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
3631: loss=5.780, avg loss=5.314, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=722.1 milliseconds, train=2.0 seconds, 232384 images, time remaining=4.2 hours
3632: loss=4.976, avg loss=5.280, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 232448 images, time remaining=4.2 hours
3633: loss=4.876, avg loss=5.240, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 232512 images, time remaining=4.2 hours
3634: loss=4.446, avg loss=5.160, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=2.0 seconds, 232576 images, time remaining=4.2 hours
3635: loss=4.335, avg loss=5.078, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 232640 images, time remaining=4.2 hours
3636: loss=4.101, avg loss=4.980, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=758.3 milliseconds, train=2.0 seconds, 232704 images, time remaining=4.2 hours
3637: loss=4.407, avg loss=4.923, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 232768 images, time remaining=4.2 hours
3638: loss=4.935, avg loss=4.924, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=646.8 milliseconds, train=2.0 seconds, 232832 images, time remaining=4.2 hours
3639: loss=4.037, avg loss=4.835, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.8 seconds, train=2.0 seconds, 232896 images, time remaining=4.2 hours
3640: loss=5.417, avg loss=4.894, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=683.9 milliseconds, train=2.0 seconds, 232960 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145f64800000
3641: loss=5.420, avg loss=4.946, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=383.5 milliseconds, train=1.2 seconds, 233024 images, time remaining=4.2 hours
3642: loss=4.474, avg loss=4.899, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=855.5 milliseconds, train=1.2 seconds, 233088 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3643: loss=4.462, avg loss=4.855, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 233152 images, time remaining=4.2 hours
3644: loss=5.644, avg loss=4.934, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 233216 images, time remaining=4.2 hours
3645: loss=4.606, avg loss=4.901, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=681.8 milliseconds, train=1.2 seconds, 233280 images, time remaining=4.2 hours
3646: loss=3.731, avg loss=4.784, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=436.6 milliseconds, train=1.2 seconds, 233344 images, time remaining=4.2 hours
3647: loss=4.831, avg loss=4.789, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=540.2 milliseconds, train=1.2 seconds, 233408 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3648: loss=4.218, avg loss=4.732, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 233472 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3649: loss=4.246, avg loss=4.683, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 233536 images, time remaining=4.2 hours
3650: loss=4.650, avg loss=4.680, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=469.0 milliseconds, train=1.2 seconds, 233600 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3651: loss=4.459, avg loss=4.658, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 233664 images, time remaining=4.2 hours
3652: loss=4.559, avg loss=4.648, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 233728 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3653: loss=5.591, avg loss=4.742, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 233792 images, time remaining=4.2 hours
3654: loss=3.694, avg loss=4.638, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=567.3 milliseconds, train=1.5 seconds, 233856 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3655: loss=4.029, avg loss=4.577, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 233920 images, time remaining=4.2 hours
3656: loss=3.901, avg loss=4.509, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=458.3 milliseconds, train=1.5 seconds, 233984 images, time remaining=4.2 hours
3657: loss=4.416, avg loss=4.500, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=959.7 milliseconds, train=1.5 seconds, 234048 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3658: loss=4.307, avg loss=4.480, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 234112 images, time remaining=4.2 hours
3659: loss=4.044, avg loss=4.437, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 234176 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3660: loss=4.014, avg loss=4.395, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 234240 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3661: loss=6.777, avg loss=4.633, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.7 seconds, train=4.9 seconds, 234304 images, time remaining=4.2 hours
3662: loss=7.022, avg loss=4.872, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=858.6 milliseconds, train=4.8 seconds, 234368 images, time remaining=4.2 hours
3663: loss=6.769, avg loss=5.061, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=4.9 seconds, 234432 images, time remaining=4.2 hours
3664: loss=5.532, avg loss=5.109, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=874.3 milliseconds, train=4.8 seconds, 234496 images, time remaining=4.2 hours
3665: loss=5.738, avg loss=5.171, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=889.6 milliseconds, train=4.9 seconds, 234560 images, time remaining=4.2 hours
3666: loss=4.878, avg loss=5.142, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.8 seconds, 234624 images, time remaining=4.2 hours
3667: loss=5.953, avg loss=5.223, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=512.8 milliseconds, train=4.8 seconds, 234688 images, time remaining=4.2 hours
3668: loss=7.026, avg loss=5.403, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=683.5 milliseconds, train=4.8 seconds, 234752 images, time remaining=4.2 hours
3669: loss=5.748, avg loss=5.438, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=666.4 milliseconds, train=4.9 seconds, 234816 images, time remaining=4.2 hours
3670: loss=5.258, avg loss=5.420, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=789.7 milliseconds, train=4.8 seconds, 234880 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3671: loss=4.804, avg loss=5.358, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=667.1 milliseconds, train=3.3 seconds, 234944 images, time remaining=4.2 hours
3672: loss=5.151, avg loss=5.338, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=716.7 milliseconds, train=3.3 seconds, 235008 images, time remaining=4.2 hours
3673: loss=5.117, avg loss=5.316, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 235072 images, time remaining=4.2 hours
3674: loss=4.942, avg loss=5.278, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=935.8 milliseconds, train=3.4 seconds, 235136 images, time remaining=4.2 hours
3675: loss=4.271, avg loss=5.177, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 235200 images, time remaining=4.2 hours
3676: loss=4.515, avg loss=5.111, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=779.6 milliseconds, train=3.3 seconds, 235264 images, time remaining=4.2 hours
3677: loss=4.718, avg loss=5.072, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 235328 images, time remaining=4.2 hours
3678: loss=5.053, avg loss=5.070, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 235392 images, time remaining=4.2 hours
3679: loss=5.522, avg loss=5.115, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=721.7 milliseconds, train=3.3 seconds, 235456 images, time remaining=4.2 hours
3680: loss=4.725, avg loss=5.076, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 235520 images, time remaining=4.2 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1457c4000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3681: loss=4.113, avg loss=4.980, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 235584 images, time remaining=4.2 hours
3682: loss=3.872, avg loss=4.869, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 235648 images, time remaining=4.2 hours
3683: loss=5.384, avg loss=4.921, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=812.6 milliseconds, train=1.6 seconds, 235712 images, time remaining=4.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3684: loss=4.546, avg loss=4.883, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 235776 images, time remaining=4.2 hours
3685: loss=4.438, avg loss=4.839, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=570.0 milliseconds, train=1.6 seconds, 235840 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3686: loss=3.994, avg loss=4.754, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.7 seconds, train=1.6 seconds, 235904 images, time remaining=4.1 hours
3687: loss=4.010, avg loss=4.680, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=528.9 milliseconds, train=1.6 seconds, 235968 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3688: loss=3.994, avg loss=4.611, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 236032 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3689: loss=4.276, avg loss=4.578, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.6 seconds, 236096 images, time remaining=4.1 hours
3690: loss=3.979, avg loss=4.518, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=803.2 milliseconds, train=1.6 seconds, 236160 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3691: loss=5.900, avg loss=4.656, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.0 seconds, train=4.5 seconds, 236224 images, time remaining=4.1 hours
3692: loss=7.221, avg loss=4.913, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=4.5 seconds, 236288 images, time remaining=4.1 hours
3693: loss=4.931, avg loss=4.914, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 236352 images, time remaining=4.1 hours
3694: loss=4.760, avg loss=4.899, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=4.5 seconds, 236416 images, time remaining=4.1 hours
3695: loss=5.742, avg loss=4.983, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 236480 images, time remaining=4.1 hours
3696: loss=5.179, avg loss=5.003, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 236544 images, time remaining=4.1 hours
3697: loss=4.704, avg loss=4.973, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=828.6 milliseconds, train=4.5 seconds, 236608 images, time remaining=4.1 hours
3698: loss=5.976, avg loss=5.073, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=612.4 milliseconds, train=4.5 seconds, 236672 images, time remaining=4.1 hours
3699: loss=4.978, avg loss=5.064, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=627.4 milliseconds, train=4.5 seconds, 236736 images, time remaining=4.1 hours
3700: loss=4.588, avg loss=5.016, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=809.3 milliseconds, train=4.5 seconds, 236800 images, time remaining=4.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3701: loss=4.182, avg loss=4.933, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=559.6 milliseconds, train=4.5 seconds, 236864 images, time remaining=4.1 hours
3702: loss=4.870, avg loss=4.926, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=728.4 milliseconds, train=4.5 seconds, 236928 images, time remaining=4.1 hours
3703: loss=4.573, avg loss=4.891, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=742.6 milliseconds, train=4.5 seconds, 236992 images, time remaining=4.1 hours
3704: loss=4.642, avg loss=4.866, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=719.6 milliseconds, train=4.5 seconds, 237056 images, time remaining=4.1 hours
3705: loss=4.468, avg loss=4.826, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=822.8 milliseconds, train=4.5 seconds, 237120 images, time remaining=4.1 hours
3706: loss=3.946, avg loss=4.738, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=721.1 milliseconds, train=4.5 seconds, 237184 images, time remaining=4.1 hours
3707: loss=4.471, avg loss=4.712, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=650.6 milliseconds, train=4.5 seconds, 237248 images, time remaining=4.1 hours
3708: loss=5.360, avg loss=4.776, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=613.9 milliseconds, train=4.5 seconds, 237312 images, time remaining=4.1 hours
3709: loss=4.656, avg loss=4.764, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.1 seconds, train=4.5 seconds, 237376 images, time remaining=4.1 hours
3710: loss=4.806, avg loss=4.769, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 237440 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145ca6000000
3711: loss=5.492, avg loss=4.841, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=510.6 milliseconds, train=1.4 seconds, 237504 images, time remaining=4.1 hours
3712: loss=5.250, avg loss=4.882, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=569.5 milliseconds, train=1.4 seconds, 237568 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3713: loss=4.745, avg loss=4.868, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 237632 images, time remaining=4.1 hours
3714: loss=3.400, avg loss=4.721, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=612.6 milliseconds, train=1.4 seconds, 237696 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3715: loss=4.820, avg loss=4.731, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 237760 images, time remaining=4.1 hours
3716: loss=3.817, avg loss=4.640, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=788.4 milliseconds, train=1.4 seconds, 237824 images, time remaining=4.1 hours
3717: loss=4.131, avg loss=4.589, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=380.7 milliseconds, train=1.4 seconds, 237888 images, time remaining=4.1 hours
3718: loss=4.305, avg loss=4.560, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=830.5 milliseconds, train=1.4 seconds, 237952 images, time remaining=4.1 hours
3719: loss=4.528, avg loss=4.557, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=836.6 milliseconds, train=1.4 seconds, 238016 images, time remaining=4.1 hours
3720: loss=4.243, avg loss=4.526, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=702.3 milliseconds, train=1.4 seconds, 238080 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3721: loss=5.989, avg loss=4.672, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 238144 images, time remaining=4.1 hours
3722: loss=4.631, avg loss=4.668, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=938.2 milliseconds, train=2.1 seconds, 238208 images, time remaining=4.1 hours
3723: loss=4.600, avg loss=4.661, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=704.2 milliseconds, train=2.1 seconds, 238272 images, time remaining=4.1 hours
3724: loss=4.624, avg loss=4.657, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 238336 images, time remaining=4.1 hours
3725: loss=3.277, avg loss=4.519, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 238400 images, time remaining=4.1 hours
3726: loss=4.340, avg loss=4.501, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=674.3 milliseconds, train=2.1 seconds, 238464 images, time remaining=4.1 hours
3727: loss=4.244, avg loss=4.476, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=716.0 milliseconds, train=2.1 seconds, 238528 images, time remaining=4.1 hours
3728: loss=4.669, avg loss=4.495, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=647.5 milliseconds, train=2.1 seconds, 238592 images, time remaining=4.1 hours
3729: loss=4.457, avg loss=4.491, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=685.9 milliseconds, train=2.1 seconds, 238656 images, time remaining=4.1 hours
3730: loss=4.275, avg loss=4.470, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=478.9 milliseconds, train=2.1 seconds, 238720 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3731: loss=3.637, avg loss=4.386, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=678.9 milliseconds, train=2.0 seconds, 238784 images, time remaining=4.1 hours
3732: loss=3.695, avg loss=4.317, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=601.6 milliseconds, train=2.0 seconds, 238848 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3733: loss=3.803, avg loss=4.266, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=2.0 seconds, 238912 images, time remaining=4.1 hours
3734: loss=3.658, avg loss=4.205, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=918.5 milliseconds, train=2.0 seconds, 238976 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3735: loss=3.887, avg loss=4.173, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=2.0 seconds, 239040 images, time remaining=4.1 hours
3736: loss=3.661, avg loss=4.122, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=947.5 milliseconds, train=2.0 seconds, 239104 images, time remaining=4.1 hours
3737: loss=3.372, avg loss=4.047, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 239168 images, time remaining=4.1 hours
3738: loss=5.106, avg loss=4.153, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 239232 images, time remaining=4.1 hours
3739: loss=4.060, avg loss=4.144, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=552.8 milliseconds, train=2.0 seconds, 239296 images, time remaining=4.1 hours
3740: loss=4.269, avg loss=4.156, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 239360 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x14648c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3741: loss=3.873, avg loss=4.128, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 239424 images, time remaining=4.1 hours
3742: loss=4.076, avg loss=4.123, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 239488 images, time remaining=4.1 hours
3743: loss=3.502, avg loss=4.061, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=606.9 milliseconds, train=1.5 seconds, 239552 images, time remaining=4.1 hours
3744: loss=3.475, avg loss=4.002, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=484.1 milliseconds, train=1.5 seconds, 239616 images, time remaining=4.1 hours
3745: loss=4.226, avg loss=4.024, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=507.8 milliseconds, train=1.5 seconds, 239680 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3746: loss=4.244, avg loss=4.046, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 239744 images, time remaining=4.1 hours
3747: loss=3.391, avg loss=3.981, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 239808 images, time remaining=4.1 hours
3748: loss=3.560, avg loss=3.939, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=494.1 milliseconds, train=1.5 seconds, 239872 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3749: loss=3.340, avg loss=3.879, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 239936 images, time remaining=4.1 hours
3750: loss=3.741, avg loss=3.865, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=664.2 milliseconds, train=1.5 seconds, 240000 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146385a00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3751: loss=4.122, avg loss=3.891, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 240064 images, time remaining=4.1 hours
3752: loss=4.187, avg loss=3.920, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 240128 images, time remaining=4.1 hours
3753: loss=4.285, avg loss=3.957, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 240192 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3754: loss=3.500, avg loss=3.911, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 240256 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3755: loss=3.629, avg loss=3.883, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 240320 images, time remaining=4.1 hours
3756: loss=3.487, avg loss=3.843, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=592.1 milliseconds, train=1.2 seconds, 240384 images, time remaining=4.1 hours
3757: loss=4.829, avg loss=3.942, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=511.3 milliseconds, train=1.2 seconds, 240448 images, time remaining=4.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3758: loss=3.650, avg loss=3.913, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 240512 images, time remaining=4.1 hours
3759: loss=4.036, avg loss=3.925, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=790.4 milliseconds, train=1.2 seconds, 240576 images, time remaining=4.1 hours
3760: loss=3.666, avg loss=3.899, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=723.1 milliseconds, train=1.2 seconds, 240640 images, time remaining=4.1 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3761: loss=3.654, avg loss=3.875, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.8 seconds, train=1.8 seconds, 240704 images, time remaining=4.1 hours
3762: loss=3.971, avg loss=3.884, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=1.8 seconds, 240768 images, time remaining=4.1 hours
3763: loss=3.670, avg loss=3.863, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=648.4 milliseconds, train=1.9 seconds, 240832 images, time remaining=4.1 hours
3764: loss=4.989, avg loss=3.976, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=479.5 milliseconds, train=1.8 seconds, 240896 images, time remaining=4.1 hours
3765: loss=3.548, avg loss=3.933, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.8 seconds, 240960 images, time remaining=4.1 hours
3766: loss=4.129, avg loss=3.952, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=555.7 milliseconds, train=1.8 seconds, 241024 images, time remaining=4.1 hours
3767: loss=4.526, avg loss=4.010, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=597.7 milliseconds, train=1.8 seconds, 241088 images, time remaining=4.1 hours
3768: loss=3.554, avg loss=3.964, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=630.5 milliseconds, train=1.8 seconds, 241152 images, time remaining=4 hours
3769: loss=3.541, avg loss=3.922, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.8 seconds, 241216 images, time remaining=4 hours
3770: loss=3.614, avg loss=3.891, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=1.8 seconds, 241280 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3771: loss=6.447, avg loss=4.147, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=694.0 milliseconds, train=5.1 seconds, 241344 images, time remaining=4 hours
3772: loss=4.246, avg loss=4.157, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=727.4 milliseconds, train=5.0 seconds, 241408 images, time remaining=4 hours
3773: loss=4.627, avg loss=4.204, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=859.6 milliseconds, train=5.0 seconds, 241472 images, time remaining=4 hours
3774: loss=4.436, avg loss=4.227, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=748.2 milliseconds, train=5.0 seconds, 241536 images, time remaining=4 hours
3775: loss=4.863, avg loss=4.290, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=628.2 milliseconds, train=5.0 seconds, 241600 images, time remaining=4 hours
3776: loss=5.072, avg loss=4.369, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=673.9 milliseconds, train=5.0 seconds, 241664 images, time remaining=4 hours
3777: loss=5.586, avg loss=4.490, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=670.2 milliseconds, train=5.0 seconds, 241728 images, time remaining=4 hours
3778: loss=4.859, avg loss=4.527, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=5.0 seconds, 241792 images, time remaining=4 hours
3779: loss=5.042, avg loss=4.579, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=5.0 seconds, 241856 images, time remaining=4 hours
3780: loss=4.459, avg loss=4.567, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.3 seconds, train=5.0 seconds, 241920 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3781: loss=4.514, avg loss=4.562, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=1.9 seconds, 241984 images, time remaining=4 hours
3782: loss=3.661, avg loss=4.471, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=768.9 milliseconds, train=1.9 seconds, 242048 images, time remaining=4 hours
3783: loss=4.171, avg loss=4.441, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=1.9 seconds, 242112 images, time remaining=4 hours
3784: loss=2.926, avg loss=4.290, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=589.5 milliseconds, train=1.9 seconds, 242176 images, time remaining=4 hours
3785: loss=4.465, avg loss=4.307, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=828.6 milliseconds, train=1.9 seconds, 242240 images, time remaining=4 hours
3786: loss=3.777, avg loss=4.254, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.4 seconds, train=1.9 seconds, 242304 images, time remaining=4 hours
3787: loss=4.245, avg loss=4.253, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=872.0 milliseconds, train=1.9 seconds, 242368 images, time remaining=4 hours
3788: loss=3.408, avg loss=4.169, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=580.5 milliseconds, train=1.9 seconds, 242432 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3789: loss=4.347, avg loss=4.187, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.1 seconds, train=1.9 seconds, 242496 images, time remaining=4 hours
3790: loss=4.095, avg loss=4.178, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.0 seconds, train=1.9 seconds, 242560 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145876000000
3791: loss=4.016, avg loss=4.161, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=902.5 milliseconds, train=1.4 seconds, 242624 images, time remaining=4 hours
3792: loss=4.224, avg loss=4.168, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=763.7 milliseconds, train=1.5 seconds, 242688 images, time remaining=4 hours
3793: loss=4.148, avg loss=4.166, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 242752 images, time remaining=4 hours
3794: loss=3.957, avg loss=4.145, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 242816 images, time remaining=4 hours
3795: loss=3.724, avg loss=4.103, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=542.6 milliseconds, train=1.4 seconds, 242880 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3796: loss=3.661, avg loss=4.059, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 242944 images, time remaining=4 hours
3797: loss=3.176, avg loss=3.970, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=668.0 milliseconds, train=1.4 seconds, 243008 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3798: loss=4.834, avg loss=4.057, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.1 seconds, train=1.4 seconds, 243072 images, time remaining=4 hours
3799: loss=3.674, avg loss=4.018, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=699.2 milliseconds, train=1.4 seconds, 243136 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3800: loss=3.428, avg loss=3.959, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 243200 images, time remaining=4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3801: loss=3.739, avg loss=3.937, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=744.6 milliseconds, train=1.9 seconds, 243264 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3802: loss=3.136, avg loss=3.857, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 243328 images, time remaining=4 hours
3803: loss=3.689, avg loss=3.840, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=742.9 milliseconds, train=2.0 seconds, 243392 images, time remaining=4 hours
3804: loss=3.886, avg loss=3.845, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=709.9 milliseconds, train=2.0 seconds, 243456 images, time remaining=4 hours
3805: loss=3.385, avg loss=3.799, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 243520 images, time remaining=4 hours
3806: loss=3.706, avg loss=3.790, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=799.1 milliseconds, train=2.0 seconds, 243584 images, time remaining=4 hours
3807: loss=3.714, avg loss=3.782, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.9 seconds, 243648 images, time remaining=4 hours
3808: loss=3.326, avg loss=3.737, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=897.7 milliseconds, train=2.0 seconds, 243712 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3809: loss=4.098, avg loss=3.773, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.2 seconds, train=1.9 seconds, 243776 images, time remaining=4 hours
3810: loss=3.289, avg loss=3.724, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 243840 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1463e0000000
3811: loss=4.165, avg loss=3.768, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=572.7 milliseconds, train=1.4 seconds, 243904 images, time remaining=4 hours
3812: loss=4.005, avg loss=3.792, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=769.3 milliseconds, train=1.5 seconds, 243968 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3813: loss=5.024, avg loss=3.915, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.4 seconds, train=1.4 seconds, 244032 images, time remaining=4 hours
3814: loss=3.549, avg loss=3.879, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=728.1 milliseconds, train=1.4 seconds, 244096 images, time remaining=4 hours
3815: loss=3.423, avg loss=3.833, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=597.8 milliseconds, train=1.4 seconds, 244160 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3816: loss=3.685, avg loss=3.818, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 244224 images, time remaining=4 hours
3817: loss=4.217, avg loss=3.858, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 244288 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3818: loss=3.940, avg loss=3.866, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 244352 images, time remaining=4 hours
3819: loss=3.261, avg loss=3.806, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=617.1 milliseconds, train=1.4 seconds, 244416 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3820: loss=3.659, avg loss=3.791, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 244480 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x14501e000000
3821: loss=3.965, avg loss=3.809, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=595.3 milliseconds, train=1.5 seconds, 244544 images, time remaining=4 hours
3822: loss=3.408, avg loss=3.769, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 244608 images, time remaining=4 hours
3823: loss=3.469, avg loss=3.739, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 244672 images, time remaining=4 hours
3824: loss=3.483, avg loss=3.713, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=609.3 milliseconds, train=1.5 seconds, 244736 images, time remaining=4 hours
3825: loss=4.187, avg loss=3.760, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=954.1 milliseconds, train=1.5 seconds, 244800 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3826: loss=3.320, avg loss=3.716, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 244864 images, time remaining=4 hours
3827: loss=3.534, avg loss=3.698, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=997.1 milliseconds, train=1.5 seconds, 244928 images, time remaining=4 hours
3828: loss=4.249, avg loss=3.753, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=528.7 milliseconds, train=1.6 seconds, 244992 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3829: loss=3.801, avg loss=3.758, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 245056 images, time remaining=4 hours
3830: loss=3.866, avg loss=3.769, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=934.1 milliseconds, train=1.5 seconds, 245120 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3831: loss=5.967, avg loss=3.989, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.3 seconds, train=4.6 seconds, 245184 images, time remaining=4 hours
3832: loss=5.142, avg loss=4.104, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 245248 images, time remaining=4 hours
3833: loss=6.732, avg loss=4.367, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.9 seconds, train=4.6 seconds, 245312 images, time remaining=4 hours
3834: loss=3.889, avg loss=4.319, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 245376 images, time remaining=4 hours
3835: loss=4.472, avg loss=4.334, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=931.1 milliseconds, train=4.6 seconds, 245440 images, time remaining=4 hours
3836: loss=4.894, avg loss=4.390, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 245504 images, time remaining=4 hours
3837: loss=5.430, avg loss=4.494, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.4 seconds, train=4.7 seconds, 245568 images, time remaining=4 hours
3838: loss=5.417, avg loss=4.587, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=669.6 milliseconds, train=4.6 seconds, 245632 images, time remaining=4 hours
3839: loss=5.358, avg loss=4.664, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=944.7 milliseconds, train=4.6 seconds, 245696 images, time remaining=4 hours
3840: loss=4.516, avg loss=4.649, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=3.4 seconds, train=4.6 seconds, 245760 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14574e000000
3841: loss=4.623, avg loss=4.646, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=1.5 seconds, train=1.7 seconds, 245824 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3842: loss=4.350, avg loss=4.617, last=88.79%, best=88.79%, next=3842, rate=0.00130000, load 64=2.0 seconds, train=1.6 seconds, 245888 images, time remaining=4 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=114568, unique_truth_count=57264
rank=0 of ranks=114568rank=100 of ranks=114568rank=200 of ranks=114568rank=300 of ranks=114568rank=400 of ranks=114568rank=500 of ranks=114568rank=600 of ranks=114568rank=700 of ranks=114568rank=800 of ranks=114568rank=900 of ranks=114568rank=1000 of ranks=114568rank=1100 of ranks=114568rank=1200 of ranks=114568rank=1300 of ranks=114568rank=1400 of ranks=114568rank=1500 of ranks=114568rank=1600 of ranks=114568rank=1700 of ranks=114568rank=1800 of ranks=114568rank=1900 of ranks=114568rank=2000 of ranks=114568rank=2100 of ranks=114568rank=2200 of ranks=114568rank=2300 of ranks=114568rank=2400 of ranks=114568rank=2500 of ranks=114568rank=2600 of ranks=114568rank=2700 of ranks=114568rank=2800 of ranks=114568rank=2900 of ranks=114568rank=3000 of ranks=114568rank=3100 of ranks=114568rank=3200 of ranks=114568rank=3300 of ranks=114568rank=3400 of ranks=114568rank=3500 of ranks=114568rank=3600 of ranks=114568rank=3700 of ranks=114568rank=3800 of ranks=114568rank=3900 of ranks=114568rank=4000 of ranks=114568rank=4100 of ranks=114568rank=4200 of ranks=114568rank=4300 of ranks=114568rank=4400 of ranks=114568rank=4500 of ranks=114568rank=4600 of ranks=114568rank=4700 of ranks=114568rank=4800 of ranks=114568rank=4900 of ranks=114568rank=5000 of ranks=114568rank=5100 of ranks=114568rank=5200 of ranks=114568rank=5300 of ranks=114568rank=5400 of ranks=114568rank=5500 of ranks=114568rank=5600 of ranks=114568rank=5700 of ranks=114568rank=5800 of ranks=114568rank=5900 of ranks=114568rank=6000 of ranks=114568rank=6100 of ranks=114568rank=6200 of ranks=114568rank=6300 of ranks=114568rank=6400 of ranks=114568rank=6500 of ranks=114568rank=6600 of ranks=114568rank=6700 of ranks=114568rank=6800 of ranks=114568rank=6900 of ranks=114568rank=7000 of ranks=114568rank=7100 of ranks=114568rank=7200 of ranks=114568rank=7300 of ranks=114568rank=7400 of ranks=114568rank=7500 of ranks=114568rank=7600 of ranks=114568rank=7700 of ranks=114568rank=7800 of ranks=114568rank=7900 of ranks=114568rank=8000 of ranks=114568rank=8100 of ranks=114568rank=8200 of ranks=114568rank=8300 of ranks=114568rank=8400 of ranks=114568rank=8500 of ranks=114568rank=8600 of ranks=114568rank=8700 of ranks=114568rank=8800 of ranks=114568rank=8900 of ranks=114568rank=9000 of ranks=114568rank=9100 of ranks=114568rank=9200 of ranks=114568rank=9300 of ranks=114568rank=9400 of ranks=114568rank=9500 of ranks=114568rank=9600 of ranks=114568rank=9700 of ranks=114568rank=9800 of ranks=114568rank=9900 of ranks=114568rank=10000 of ranks=114568rank=10100 of ranks=114568rank=10200 of ranks=114568rank=10300 of ranks=114568rank=10400 of ranks=114568rank=10500 of ranks=114568rank=10600 of ranks=114568rank=10700 of ranks=114568rank=10800 of ranks=114568rank=10900 of ranks=114568rank=11000 of ranks=114568rank=11100 of ranks=114568rank=11200 of ranks=114568rank=11300 of ranks=114568rank=11400 of ranks=114568rank=11500 of ranks=114568rank=11600 of ranks=114568rank=11700 of ranks=114568rank=11800 of ranks=114568rank=11900 of ranks=114568rank=12000 of ranks=114568rank=12100 of ranks=114568rank=12200 of ranks=114568rank=12300 of ranks=114568rank=12400 of ranks=114568rank=12500 of ranks=114568rank=12600 of ranks=114568rank=12700 of ranks=114568rank=12800 of ranks=114568rank=12900 of ranks=114568rank=13000 of ranks=114568rank=13100 of ranks=114568rank=13200 of ranks=114568rank=13300 of ranks=114568rank=13400 of ranks=114568rank=13500 of ranks=114568rank=13600 of ranks=114568rank=13700 of ranks=114568rank=13800 of ranks=114568rank=13900 of ranks=114568rank=14000 of ranks=114568rank=14100 of ranks=114568rank=14200 of ranks=114568rank=14300 of ranks=114568rank=14400 of ranks=114568rank=14500 of ranks=114568rank=14600 of ranks=114568rank=14700 of ranks=114568rank=14800 of ranks=114568rank=14900 of ranks=114568rank=15000 of ranks=114568rank=15100 of ranks=114568rank=15200 of ranks=114568rank=15300 of ranks=114568rank=15400 of ranks=114568rank=15500 of ranks=114568rank=15600 of ranks=114568rank=15700 of ranks=114568rank=15800 of ranks=114568rank=15900 of ranks=114568rank=16000 of ranks=114568rank=16100 of ranks=114568rank=16200 of ranks=114568rank=16300 of ranks=114568rank=16400 of ranks=114568rank=16500 of ranks=114568rank=16600 of ranks=114568rank=16700 of ranks=114568rank=16800 of ranks=114568rank=16900 of ranks=114568rank=17000 of ranks=114568rank=17100 of ranks=114568rank=17200 of ranks=114568rank=17300 of ranks=114568rank=17400 of ranks=114568rank=17500 of ranks=114568rank=17600 of ranks=114568rank=17700 of ranks=114568rank=17800 of ranks=114568rank=17900 of ranks=114568rank=18000 of ranks=114568rank=18100 of ranks=114568rank=18200 of ranks=114568rank=18300 of ranks=114568rank=18400 of ranks=114568rank=18500 of ranks=114568rank=18600 of ranks=114568rank=18700 of ranks=114568rank=18800 of ranks=114568rank=18900 of ranks=114568rank=19000 of ranks=114568rank=19100 of ranks=114568rank=19200 of ranks=114568rank=19300 of ranks=114568rank=19400 of ranks=114568rank=19500 of ranks=114568rank=19600 of ranks=114568rank=19700 of ranks=114568rank=19800 of ranks=114568rank=19900 of ranks=114568rank=20000 of ranks=114568rank=20100 of ranks=114568rank=20200 of ranks=114568rank=20300 of ranks=114568rank=20400 of ranks=114568rank=20500 of ranks=114568rank=20600 of ranks=114568rank=20700 of ranks=114568rank=20800 of ranks=114568rank=20900 of ranks=114568rank=21000 of ranks=114568rank=21100 of ranks=114568rank=21200 of ranks=114568rank=21300 of ranks=114568rank=21400 of ranks=114568rank=21500 of ranks=114568rank=21600 of ranks=114568rank=21700 of ranks=114568rank=21800 of ranks=114568rank=21900 of ranks=114568rank=22000 of ranks=114568rank=22100 of ranks=114568rank=22200 of ranks=114568rank=22300 of ranks=114568rank=22400 of ranks=114568rank=22500 of ranks=114568rank=22600 of ranks=114568rank=22700 of ranks=114568rank=22800 of ranks=114568rank=22900 of ranks=114568rank=23000 of ranks=114568rank=23100 of ranks=114568rank=23200 of ranks=114568rank=23300 of ranks=114568rank=23400 of ranks=114568rank=23500 of ranks=114568rank=23600 of ranks=114568rank=23700 of ranks=114568rank=23800 of ranks=114568rank=23900 of ranks=114568rank=24000 of ranks=114568rank=24100 of ranks=114568rank=24200 of ranks=114568rank=24300 of ranks=114568rank=24400 of ranks=114568rank=24500 of ranks=114568rank=24600 of ranks=114568rank=24700 of ranks=114568rank=24800 of ranks=114568rank=24900 of ranks=114568rank=25000 of ranks=114568rank=25100 of ranks=114568rank=25200 of ranks=114568rank=25300 of ranks=114568rank=25400 of ranks=114568rank=25500 of ranks=114568rank=25600 of ranks=114568rank=25700 of ranks=114568rank=25800 of ranks=114568rank=25900 of ranks=114568rank=26000 of ranks=114568rank=26100 of ranks=114568rank=26200 of ranks=114568rank=26300 of ranks=114568rank=26400 of ranks=114568rank=26500 of ranks=114568rank=26600 of ranks=114568rank=26700 of ranks=114568rank=26800 of ranks=114568rank=26900 of ranks=114568rank=27000 of ranks=114568rank=27100 of ranks=114568rank=27200 of ranks=114568rank=27300 of ranks=114568rank=27400 of ranks=114568rank=27500 of ranks=114568rank=27600 of ranks=114568rank=27700 of ranks=114568rank=27800 of ranks=114568rank=27900 of ranks=114568rank=28000 of ranks=114568rank=28100 of ranks=114568rank=28200 of ranks=114568rank=28300 of ranks=114568rank=28400 of ranks=114568rank=28500 of ranks=114568rank=28600 of ranks=114568rank=28700 of ranks=114568rank=28800 of ranks=114568rank=28900 of ranks=114568rank=29000 of ranks=114568rank=29100 of ranks=114568rank=29200 of ranks=114568rank=29300 of ranks=114568rank=29400 of ranks=114568rank=29500 of ranks=114568rank=29600 of ranks=114568rank=29700 of ranks=114568rank=29800 of ranks=114568rank=29900 of ranks=114568rank=30000 of ranks=114568rank=30100 of ranks=114568rank=30200 of ranks=114568rank=30300 of ranks=114568rank=30400 of ranks=114568rank=30500 of ranks=114568rank=30600 of ranks=114568rank=30700 of ranks=114568rank=30800 of ranks=114568rank=30900 of ranks=114568rank=31000 of ranks=114568rank=31100 of ranks=114568rank=31200 of ranks=114568rank=31300 of ranks=114568rank=31400 of ranks=114568rank=31500 of ranks=114568rank=31600 of ranks=114568rank=31700 of ranks=114568rank=31800 of ranks=114568rank=31900 of ranks=114568rank=32000 of ranks=114568rank=32100 of ranks=114568rank=32200 of ranks=114568rank=32300 of ranks=114568rank=32400 of ranks=114568rank=32500 of ranks=114568rank=32600 of ranks=114568rank=32700 of ranks=114568rank=32800 of ranks=114568rank=32900 of ranks=114568rank=33000 of ranks=114568rank=33100 of ranks=114568rank=33200 of ranks=114568rank=33300 of ranks=114568rank=33400 of ranks=114568rank=33500 of ranks=114568rank=33600 of ranks=114568rank=33700 of ranks=114568rank=33800 of ranks=114568rank=33900 of ranks=114568rank=34000 of ranks=114568rank=34100 of ranks=114568rank=34200 of ranks=114568rank=34300 of ranks=114568rank=34400 of ranks=114568rank=34500 of ranks=114568rank=34600 of ranks=114568rank=34700 of ranks=114568rank=34800 of ranks=114568rank=34900 of ranks=114568rank=35000 of ranks=114568rank=35100 of ranks=114568rank=35200 of ranks=114568rank=35300 of ranks=114568rank=35400 of ranks=114568rank=35500 of ranks=114568rank=35600 of ranks=114568rank=35700 of ranks=114568rank=35800 of ranks=114568rank=35900 of ranks=114568rank=36000 of ranks=114568rank=36100 of ranks=114568rank=36200 of ranks=114568rank=36300 of ranks=114568rank=36400 of ranks=114568rank=36500 of ranks=114568rank=36600 of ranks=114568rank=36700 of ranks=114568rank=36800 of ranks=114568rank=36900 of ranks=114568rank=37000 of ranks=114568rank=37100 of ranks=114568rank=37200 of ranks=114568rank=37300 of ranks=114568rank=37400 of ranks=114568rank=37500 of ranks=114568rank=37600 of ranks=114568rank=37700 of ranks=114568rank=37800 of ranks=114568rank=37900 of ranks=114568rank=38000 of ranks=114568rank=38100 of ranks=114568rank=38200 of ranks=114568rank=38300 of ranks=114568rank=38400 of ranks=114568rank=38500 of ranks=114568rank=38600 of ranks=114568rank=38700 of ranks=114568rank=38800 of ranks=114568rank=38900 of ranks=114568rank=39000 of ranks=114568rank=39100 of ranks=114568rank=39200 of ranks=114568rank=39300 of ranks=114568rank=39400 of ranks=114568rank=39500 of ranks=114568rank=39600 of ranks=114568rank=39700 of ranks=114568rank=39800 of ranks=114568rank=39900 of ranks=114568rank=40000 of ranks=114568rank=40100 of ranks=114568rank=40200 of ranks=114568rank=40300 of ranks=114568rank=40400 of ranks=114568rank=40500 of ranks=114568rank=40600 of ranks=114568rank=40700 of ranks=114568rank=40800 of ranks=114568rank=40900 of ranks=114568rank=41000 of ranks=114568rank=41100 of ranks=114568rank=41200 of ranks=114568rank=41300 of ranks=114568rank=41400 of ranks=114568rank=41500 of ranks=114568rank=41600 of ranks=114568rank=41700 of ranks=114568rank=41800 of ranks=114568rank=41900 of ranks=114568rank=42000 of ranks=114568rank=42100 of ranks=114568rank=42200 of ranks=114568rank=42300 of ranks=114568rank=42400 of ranks=114568rank=42500 of ranks=114568rank=42600 of ranks=114568rank=42700 of ranks=114568rank=42800 of ranks=114568rank=42900 of ranks=114568rank=43000 of ranks=114568rank=43100 of ranks=114568rank=43200 of ranks=114568rank=43300 of ranks=114568rank=43400 of ranks=114568rank=43500 of ranks=114568rank=43600 of ranks=114568rank=43700 of ranks=114568rank=43800 of ranks=114568rank=43900 of ranks=114568rank=44000 of ranks=114568rank=44100 of ranks=114568rank=44200 of ranks=114568rank=44300 of ranks=114568rank=44400 of ranks=114568rank=44500 of ranks=114568rank=44600 of ranks=114568rank=44700 of ranks=114568rank=44800 of ranks=114568rank=44900 of ranks=114568rank=45000 of ranks=114568rank=45100 of ranks=114568rank=45200 of ranks=114568rank=45300 of ranks=114568rank=45400 of ranks=114568rank=45500 of ranks=114568rank=45600 of ranks=114568rank=45700 of ranks=114568rank=45800 of ranks=114568rank=45900 of ranks=114568rank=46000 of ranks=114568rank=46100 of ranks=114568rank=46200 of ranks=114568rank=46300 of ranks=114568rank=46400 of ranks=114568rank=46500 of ranks=114568rank=46600 of ranks=114568rank=46700 of ranks=114568rank=46800 of ranks=114568rank=46900 of ranks=114568rank=47000 of ranks=114568rank=47100 of ranks=114568rank=47200 of ranks=114568rank=47300 of ranks=114568rank=47400 of ranks=114568rank=47500 of ranks=114568rank=47600 of ranks=114568rank=47700 of ranks=114568rank=47800 of ranks=114568rank=47900 of ranks=114568rank=48000 of ranks=114568rank=48100 of ranks=114568rank=48200 of ranks=114568rank=48300 of ranks=114568rank=48400 of ranks=114568rank=48500 of ranks=114568rank=48600 of ranks=114568rank=48700 of ranks=114568rank=48800 of ranks=114568rank=48900 of ranks=114568rank=49000 of ranks=114568rank=49100 of ranks=114568rank=49200 of ranks=114568rank=49300 of ranks=114568rank=49400 of ranks=114568rank=49500 of ranks=114568rank=49600 of ranks=114568rank=49700 of ranks=114568rank=49800 of ranks=114568rank=49900 of ranks=114568rank=50000 of ranks=114568rank=50100 of ranks=114568rank=50200 of ranks=114568rank=50300 of ranks=114568rank=50400 of ranks=114568rank=50500 of ranks=114568rank=50600 of ranks=114568rank=50700 of ranks=114568rank=50800 of ranks=114568rank=50900 of ranks=114568rank=51000 of ranks=114568rank=51100 of ranks=114568rank=51200 of ranks=114568rank=51300 of ranks=114568rank=51400 of ranks=114568rank=51500 of ranks=114568rank=51600 of ranks=114568rank=51700 of ranks=114568rank=51800 of ranks=114568rank=51900 of ranks=114568rank=52000 of ranks=114568rank=52100 of ranks=114568rank=52200 of ranks=114568rank=52300 of ranks=114568rank=52400 of ranks=114568rank=52500 of ranks=114568rank=52600 of ranks=114568rank=52700 of ranks=114568rank=52800 of ranks=114568rank=52900 of ranks=114568rank=53000 of ranks=114568rank=53100 of ranks=114568rank=53200 of ranks=114568rank=53300 of ranks=114568rank=53400 of ranks=114568rank=53500 of ranks=114568rank=53600 of ranks=114568rank=53700 of ranks=114568rank=53800 of ranks=114568rank=53900 of ranks=114568rank=54000 of ranks=114568rank=54100 of ranks=114568rank=54200 of ranks=114568rank=54300 of ranks=114568rank=54400 of ranks=114568rank=54500 of ranks=114568rank=54600 of ranks=114568rank=54700 of ranks=114568rank=54800 of ranks=114568rank=54900 of ranks=114568rank=55000 of ranks=114568rank=55100 of ranks=114568rank=55200 of ranks=114568rank=55300 of ranks=114568rank=55400 of ranks=114568rank=55500 of ranks=114568rank=55600 of ranks=114568rank=55700 of ranks=114568rank=55800 of ranks=114568rank=55900 of ranks=114568rank=56000 of ranks=114568rank=56100 of ranks=114568rank=56200 of ranks=114568rank=56300 of ranks=114568rank=56400 of ranks=114568rank=56500 of ranks=114568rank=56600 of ranks=114568rank=56700 of ranks=114568rank=56800 of ranks=114568rank=56900 of ranks=114568rank=57000 of ranks=114568rank=57100 of ranks=114568rank=57200 of ranks=114568rank=57300 of ranks=114568rank=57400 of ranks=114568rank=57500 of ranks=114568rank=57600 of ranks=114568rank=57700 of ranks=114568rank=57800 of ranks=114568rank=57900 of ranks=114568rank=58000 of ranks=114568rank=58100 of ranks=114568rank=58200 of ranks=114568rank=58300 of ranks=114568rank=58400 of ranks=114568rank=58500 of ranks=114568rank=58600 of ranks=114568rank=58700 of ranks=114568rank=58800 of ranks=114568rank=58900 of ranks=114568rank=59000 of ranks=114568rank=59100 of ranks=114568rank=59200 of ranks=114568rank=59300 of ranks=114568rank=59400 of ranks=114568rank=59500 of ranks=114568rank=59600 of ranks=114568rank=59700 of ranks=114568rank=59800 of ranks=114568rank=59900 of ranks=114568rank=60000 of ranks=114568rank=60100 of ranks=114568rank=60200 of ranks=114568rank=60300 of ranks=114568rank=60400 of ranks=114568rank=60500 of ranks=114568rank=60600 of ranks=114568rank=60700 of ranks=114568rank=60800 of ranks=114568rank=60900 of ranks=114568rank=61000 of ranks=114568rank=61100 of ranks=114568rank=61200 of ranks=114568rank=61300 of ranks=114568rank=61400 of ranks=114568rank=61500 of ranks=114568rank=61600 of ranks=114568rank=61700 of ranks=114568rank=61800 of ranks=114568rank=61900 of ranks=114568rank=62000 of ranks=114568rank=62100 of ranks=114568rank=62200 of ranks=114568rank=62300 of ranks=114568rank=62400 of ranks=114568rank=62500 of ranks=114568rank=62600 of ranks=114568rank=62700 of ranks=114568rank=62800 of ranks=114568rank=62900 of ranks=114568rank=63000 of ranks=114568rank=63100 of ranks=114568rank=63200 of ranks=114568rank=63300 of ranks=114568rank=63400 of ranks=114568rank=63500 of ranks=114568rank=63600 of ranks=114568rank=63700 of ranks=114568rank=63800 of ranks=114568rank=63900 of ranks=114568rank=64000 of ranks=114568rank=64100 of ranks=114568rank=64200 of ranks=114568rank=64300 of ranks=114568rank=64400 of ranks=114568rank=64500 of ranks=114568rank=64600 of ranks=114568rank=64700 of ranks=114568rank=64800 of ranks=114568rank=64900 of ranks=114568rank=65000 of ranks=114568rank=65100 of ranks=114568rank=65200 of ranks=114568rank=65300 of ranks=114568rank=65400 of ranks=114568rank=65500 of ranks=114568rank=65600 of ranks=114568rank=65700 of ranks=114568rank=65800 of ranks=114568rank=65900 of ranks=114568rank=66000 of ranks=114568rank=66100 of ranks=114568rank=66200 of ranks=114568rank=66300 of ranks=114568rank=66400 of ranks=114568rank=66500 of ranks=114568rank=66600 of ranks=114568rank=66700 of ranks=114568rank=66800 of ranks=114568rank=66900 of ranks=114568rank=67000 of ranks=114568rank=67100 of ranks=114568rank=67200 of ranks=114568rank=67300 of ranks=114568rank=67400 of ranks=114568rank=67500 of ranks=114568rank=67600 of ranks=114568rank=67700 of ranks=114568rank=67800 of ranks=114568rank=67900 of ranks=114568rank=68000 of ranks=114568rank=68100 of ranks=114568rank=68200 of ranks=114568rank=68300 of ranks=114568rank=68400 of ranks=114568rank=68500 of ranks=114568rank=68600 of ranks=114568rank=68700 of ranks=114568rank=68800 of ranks=114568rank=68900 of ranks=114568rank=69000 of ranks=114568rank=69100 of ranks=114568rank=69200 of ranks=114568rank=69300 of ranks=114568rank=69400 of ranks=114568rank=69500 of ranks=114568rank=69600 of ranks=114568rank=69700 of ranks=114568rank=69800 of ranks=114568rank=69900 of ranks=114568rank=70000 of ranks=114568rank=70100 of ranks=114568rank=70200 of ranks=114568rank=70300 of ranks=114568rank=70400 of ranks=114568rank=70500 of ranks=114568rank=70600 of ranks=114568rank=70700 of ranks=114568rank=70800 of ranks=114568rank=70900 of ranks=114568rank=71000 of ranks=114568rank=71100 of ranks=114568rank=71200 of ranks=114568rank=71300 of ranks=114568rank=71400 of ranks=114568rank=71500 of ranks=114568rank=71600 of ranks=114568rank=71700 of ranks=114568rank=71800 of ranks=114568rank=71900 of ranks=114568rank=72000 of ranks=114568rank=72100 of ranks=114568rank=72200 of ranks=114568rank=72300 of ranks=114568rank=72400 of ranks=114568rank=72500 of ranks=114568rank=72600 of ranks=114568rank=72700 of ranks=114568rank=72800 of ranks=114568rank=72900 of ranks=114568rank=73000 of ranks=114568rank=73100 of ranks=114568rank=73200 of ranks=114568rank=73300 of ranks=114568rank=73400 of ranks=114568rank=73500 of ranks=114568rank=73600 of ranks=114568rank=73700 of ranks=114568rank=73800 of ranks=114568rank=73900 of ranks=114568rank=74000 of ranks=114568rank=74100 of ranks=114568rank=74200 of ranks=114568rank=74300 of ranks=114568rank=74400 of ranks=114568rank=74500 of ranks=114568rank=74600 of ranks=114568rank=74700 of ranks=114568rank=74800 of ranks=114568rank=74900 of ranks=114568rank=75000 of ranks=114568rank=75100 of ranks=114568rank=75200 of ranks=114568rank=75300 of ranks=114568rank=75400 of ranks=114568rank=75500 of ranks=114568rank=75600 of ranks=114568rank=75700 of ranks=114568rank=75800 of ranks=114568rank=75900 of ranks=114568rank=76000 of ranks=114568rank=76100 of ranks=114568rank=76200 of ranks=114568rank=76300 of ranks=114568rank=76400 of ranks=114568rank=76500 of ranks=114568rank=76600 of ranks=114568rank=76700 of ranks=114568rank=76800 of ranks=114568rank=76900 of ranks=114568rank=77000 of ranks=114568rank=77100 of ranks=114568rank=77200 of ranks=114568rank=77300 of ranks=114568rank=77400 of ranks=114568rank=77500 of ranks=114568rank=77600 of ranks=114568rank=77700 of ranks=114568rank=77800 of ranks=114568rank=77900 of ranks=114568rank=78000 of ranks=114568rank=78100 of ranks=114568rank=78200 of ranks=114568rank=78300 of ranks=114568rank=78400 of ranks=114568rank=78500 of ranks=114568rank=78600 of ranks=114568rank=78700 of ranks=114568rank=78800 of ranks=114568rank=78900 of ranks=114568rank=79000 of ranks=114568rank=79100 of ranks=114568rank=79200 of ranks=114568rank=79300 of ranks=114568rank=79400 of ranks=114568rank=79500 of ranks=114568rank=79600 of ranks=114568rank=79700 of ranks=114568rank=79800 of ranks=114568rank=79900 of ranks=114568rank=80000 of ranks=114568rank=80100 of ranks=114568rank=80200 of ranks=114568rank=80300 of ranks=114568rank=80400 of ranks=114568rank=80500 of ranks=114568rank=80600 of ranks=114568rank=80700 of ranks=114568rank=80800 of ranks=114568rank=80900 of ranks=114568rank=81000 of ranks=114568rank=81100 of ranks=114568rank=81200 of ranks=114568rank=81300 of ranks=114568rank=81400 of ranks=114568rank=81500 of ranks=114568rank=81600 of ranks=114568rank=81700 of ranks=114568rank=81800 of ranks=114568rank=81900 of ranks=114568rank=82000 of ranks=114568rank=82100 of ranks=114568rank=82200 of ranks=114568rank=82300 of ranks=114568rank=82400 of ranks=114568rank=82500 of ranks=114568rank=82600 of ranks=114568rank=82700 of ranks=114568rank=82800 of ranks=114568rank=82900 of ranks=114568rank=83000 of ranks=114568rank=83100 of ranks=114568rank=83200 of ranks=114568rank=83300 of ranks=114568rank=83400 of ranks=114568rank=83500 of ranks=114568rank=83600 of ranks=114568rank=83700 of ranks=114568rank=83800 of ranks=114568rank=83900 of ranks=114568rank=84000 of ranks=114568rank=84100 of ranks=114568rank=84200 of ranks=114568rank=84300 of ranks=114568rank=84400 of ranks=114568rank=84500 of ranks=114568rank=84600 of ranks=114568rank=84700 of ranks=114568rank=84800 of ranks=114568rank=84900 of ranks=114568rank=85000 of ranks=114568rank=85100 of ranks=114568rank=85200 of ranks=114568rank=85300 of ranks=114568rank=85400 of ranks=114568rank=85500 of ranks=114568rank=85600 of ranks=114568rank=85700 of ranks=114568rank=85800 of ranks=114568rank=85900 of ranks=114568rank=86000 of ranks=114568rank=86100 of ranks=114568rank=86200 of ranks=114568rank=86300 of ranks=114568rank=86400 of ranks=114568rank=86500 of ranks=114568rank=86600 of ranks=114568rank=86700 of ranks=114568rank=86800 of ranks=114568rank=86900 of ranks=114568rank=87000 of ranks=114568rank=87100 of ranks=114568rank=87200 of ranks=114568rank=87300 of ranks=114568rank=87400 of ranks=114568rank=87500 of ranks=114568rank=87600 of ranks=114568rank=87700 of ranks=114568rank=87800 of ranks=114568rank=87900 of ranks=114568rank=88000 of ranks=114568rank=88100 of ranks=114568rank=88200 of ranks=114568rank=88300 of ranks=114568rank=88400 of ranks=114568rank=88500 of ranks=114568rank=88600 of ranks=114568rank=88700 of ranks=114568rank=88800 of ranks=114568rank=88900 of ranks=114568rank=89000 of ranks=114568rank=89100 of ranks=114568rank=89200 of ranks=114568rank=89300 of ranks=114568rank=89400 of ranks=114568rank=89500 of ranks=114568rank=89600 of ranks=114568rank=89700 of ranks=114568rank=89800 of ranks=114568rank=89900 of ranks=114568rank=90000 of ranks=114568rank=90100 of ranks=114568rank=90200 of ranks=114568rank=90300 of ranks=114568rank=90400 of ranks=114568rank=90500 of ranks=114568rank=90600 of ranks=114568rank=90700 of ranks=114568rank=90800 of ranks=114568rank=90900 of ranks=114568rank=91000 of ranks=114568rank=91100 of ranks=114568rank=91200 of ranks=114568rank=91300 of ranks=114568rank=91400 of ranks=114568rank=91500 of ranks=114568rank=91600 of ranks=114568rank=91700 of ranks=114568rank=91800 of ranks=114568rank=91900 of ranks=114568rank=92000 of ranks=114568rank=92100 of ranks=114568rank=92200 of ranks=114568rank=92300 of ranks=114568rank=92400 of ranks=114568rank=92500 of ranks=114568rank=92600 of ranks=114568rank=92700 of ranks=114568rank=92800 of ranks=114568rank=92900 of ranks=114568rank=93000 of ranks=114568rank=93100 of ranks=114568rank=93200 of ranks=114568rank=93300 of ranks=114568rank=93400 of ranks=114568rank=93500 of ranks=114568rank=93600 of ranks=114568rank=93700 of ranks=114568rank=93800 of ranks=114568rank=93900 of ranks=114568rank=94000 of ranks=114568rank=94100 of ranks=114568rank=94200 of ranks=114568rank=94300 of ranks=114568rank=94400 of ranks=114568rank=94500 of ranks=114568rank=94600 of ranks=114568rank=94700 of ranks=114568rank=94800 of ranks=114568rank=94900 of ranks=114568rank=95000 of ranks=114568rank=95100 of ranks=114568rank=95200 of ranks=114568rank=95300 of ranks=114568rank=95400 of ranks=114568rank=95500 of ranks=114568rank=95600 of ranks=114568rank=95700 of ranks=114568rank=95800 of ranks=114568rank=95900 of ranks=114568rank=96000 of ranks=114568rank=96100 of ranks=114568rank=96200 of ranks=114568rank=96300 of ranks=114568rank=96400 of ranks=114568rank=96500 of ranks=114568rank=96600 of ranks=114568rank=96700 of ranks=114568rank=96800 of ranks=114568rank=96900 of ranks=114568rank=97000 of ranks=114568rank=97100 of ranks=114568rank=97200 of ranks=114568rank=97300 of ranks=114568rank=97400 of ranks=114568rank=97500 of ranks=114568rank=97600 of ranks=114568rank=97700 of ranks=114568rank=97800 of ranks=114568rank=97900 of ranks=114568rank=98000 of ranks=114568rank=98100 of ranks=114568rank=98200 of ranks=114568rank=98300 of ranks=114568rank=98400 of ranks=114568rank=98500 of ranks=114568rank=98600 of ranks=114568rank=98700 of ranks=114568rank=98800 of ranks=114568rank=98900 of ranks=114568rank=99000 of ranks=114568rank=99100 of ranks=114568rank=99200 of ranks=114568rank=99300 of ranks=114568rank=99400 of ranks=114568rank=99500 of ranks=114568rank=99600 of ranks=114568rank=99700 of ranks=114568rank=99800 of ranks=114568rank=99900 of ranks=114568rank=100000 of ranks=114568rank=100100 of ranks=114568rank=100200 of ranks=114568rank=100300 of ranks=114568rank=100400 of ranks=114568rank=100500 of ranks=114568rank=100600 of ranks=114568rank=100700 of ranks=114568rank=100800 of ranks=114568rank=100900 of ranks=114568rank=101000 of ranks=114568rank=101100 of ranks=114568rank=101200 of ranks=114568rank=101300 of ranks=114568rank=101400 of ranks=114568rank=101500 of ranks=114568rank=101600 of ranks=114568rank=101700 of ranks=114568rank=101800 of ranks=114568rank=101900 of ranks=114568rank=102000 of ranks=114568rank=102100 of ranks=114568rank=102200 of ranks=114568rank=102300 of ranks=114568rank=102400 of ranks=114568rank=102500 of ranks=114568rank=102600 of ranks=114568rank=102700 of ranks=114568rank=102800 of ranks=114568rank=102900 of ranks=114568rank=103000 of ranks=114568rank=103100 of ranks=114568rank=103200 of ranks=114568rank=103300 of ranks=114568rank=103400 of ranks=114568rank=103500 of ranks=114568rank=103600 of ranks=114568rank=103700 of ranks=114568rank=103800 of ranks=114568rank=103900 of ranks=114568rank=104000 of ranks=114568rank=104100 of ranks=114568rank=104200 of ranks=114568rank=104300 of ranks=114568rank=104400 of ranks=114568rank=104500 of ranks=114568rank=104600 of ranks=114568rank=104700 of ranks=114568rank=104800 of ranks=114568rank=104900 of ranks=114568rank=105000 of ranks=114568rank=105100 of ranks=114568rank=105200 of ranks=114568rank=105300 of ranks=114568rank=105400 of ranks=114568rank=105500 of ranks=114568rank=105600 of ranks=114568rank=105700 of ranks=114568rank=105800 of ranks=114568rank=105900 of ranks=114568rank=106000 of ranks=114568rank=106100 of ranks=114568rank=106200 of ranks=114568rank=106300 of ranks=114568rank=106400 of ranks=114568rank=106500 of ranks=114568rank=106600 of ranks=114568rank=106700 of ranks=114568rank=106800 of ranks=114568rank=106900 of ranks=114568rank=107000 of ranks=114568rank=107100 of ranks=114568rank=107200 of ranks=114568rank=107300 of ranks=114568rank=107400 of ranks=114568rank=107500 of ranks=114568rank=107600 of ranks=114568rank=107700 of ranks=114568rank=107800 of ranks=114568rank=107900 of ranks=114568rank=108000 of ranks=114568rank=108100 of ranks=114568rank=108200 of ranks=114568rank=108300 of ranks=114568rank=108400 of ranks=114568rank=108500 of ranks=114568rank=108600 of ranks=114568rank=108700 of ranks=114568rank=108800 of ranks=114568rank=108900 of ranks=114568rank=109000 of ranks=114568rank=109100 of ranks=114568rank=109200 of ranks=114568rank=109300 of ranks=114568rank=109400 of ranks=114568rank=109500 of ranks=114568rank=109600 of ranks=114568rank=109700 of ranks=114568rank=109800 of ranks=114568rank=109900 of ranks=114568rank=110000 of ranks=114568rank=110100 of ranks=114568rank=110200 of ranks=114568rank=110300 of ranks=114568rank=110400 of ranks=114568rank=110500 of ranks=114568rank=110600 of ranks=114568rank=110700 of ranks=114568rank=110800 of ranks=114568rank=110900 of ranks=114568rank=111000 of ranks=114568rank=111100 of ranks=114568rank=111200 of ranks=114568rank=111300 of ranks=114568rank=111400 of ranks=114568rank=111500 of ranks=114568rank=111600 of ranks=114568rank=111700 of ranks=114568rank=111800 of ranks=114568rank=111900 of ranks=114568rank=112000 of ranks=114568rank=112100 of ranks=114568rank=112200 of ranks=114568rank=112300 of ranks=114568rank=112400 of ranks=114568rank=112500 of ranks=114568rank=112600 of ranks=114568rank=112700 of ranks=114568rank=112800 of ranks=114568rank=112900 of ranks=114568rank=113000 of ranks=114568rank=113100 of ranks=114568rank=113200 of ranks=114568rank=113300 of ranks=114568rank=113400 of ranks=114568rank=113500 of ranks=114568rank=113600 of ranks=114568rank=113700 of ranks=114568rank=113800 of ranks=114568rank=113900 of ranks=114568rank=114000 of ranks=114568rank=114100 of ranks=114568rank=114200 of ranks=114568rank=114300 of ranks=114568rank=114400 of ranks=114568rank=114500 of ranks=114568

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              90.3179    469   5535     29    498           73.7831
   1 car                    97.1959  49737  33485    579  50316           77.2243
   2 truck                  90.6251   1781   6462     44   1825           65.9317
   3 bus                    78.2545    354   4792     12    366           51.1936
   4 pedestrian             87.0174   3918   8035    341   4259           65.9835

for conf_thresh=0.25, precision=0.88, recall=0.92, F1 score=0.90
for conf_thresh=0.25, TP=52799, FP=7074, FN=4465, average IoU=75.98%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=88.68%
Total detection time: 153 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
3843: loss=4.056, avg loss=4.561, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=683.7 milliseconds, train=2.1 seconds, 245952 images, time remaining=4 hours
3844: loss=3.933, avg loss=4.498, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=723.3 milliseconds, train=2.1 seconds, 246016 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3845: loss=4.428, avg loss=4.491, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 246080 images, time remaining=4 hours
3846: loss=4.841, avg loss=4.526, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=766.1 milliseconds, train=2.1 seconds, 246144 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3847: loss=3.706, avg loss=4.444, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 246208 images, time remaining=4 hours
3848: loss=4.481, avg loss=4.448, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=742.2 milliseconds, train=2.1 seconds, 246272 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3849: loss=4.443, avg loss=4.447, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 246336 images, time remaining=4 hours
3850: loss=3.644, avg loss=4.367, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=810.4 milliseconds, train=2.1 seconds, 246400 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
3851: loss=4.059, avg loss=4.336, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=620.0 milliseconds, train=2.0 seconds, 246464 images, time remaining=4 hours
3852: loss=3.870, avg loss=4.289, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=804.8 milliseconds, train=2.0 seconds, 246528 images, time remaining=4 hours
3853: loss=4.710, avg loss=4.331, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 246592 images, time remaining=4 hours
3854: loss=4.297, avg loss=4.328, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 246656 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3855: loss=3.665, avg loss=4.262, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=2.0 seconds, 246720 images, time remaining=4 hours
3856: loss=3.806, avg loss=4.216, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 246784 images, time remaining=4 hours
3857: loss=3.884, avg loss=4.183, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=698.1 milliseconds, train=1.9 seconds, 246848 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3858: loss=2.879, avg loss=4.053, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=1.9 seconds, 246912 images, time remaining=4 hours
3859: loss=3.089, avg loss=3.956, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 246976 images, time remaining=4 hours
3860: loss=4.374, avg loss=3.998, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 247040 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3861: loss=6.434, avg loss=4.242, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=558.2 milliseconds, train=4.8 seconds, 247104 images, time remaining=4 hours
3862: loss=6.768, avg loss=4.494, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=4.7 seconds, 247168 images, time remaining=4 hours
3863: loss=4.043, avg loss=4.449, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=4.7 seconds, 247232 images, time remaining=4 hours
3864: loss=5.765, avg loss=4.581, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=648.7 milliseconds, train=4.7 seconds, 247296 images, time remaining=4 hours
3865: loss=4.483, avg loss=4.571, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=937.0 milliseconds, train=4.7 seconds, 247360 images, time remaining=4 hours
3866: loss=3.865, avg loss=4.500, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=816.3 milliseconds, train=4.7 seconds, 247424 images, time remaining=4 hours
3867: loss=5.127, avg loss=4.563, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=4.7 seconds, 247488 images, time remaining=4 hours
3868: loss=5.503, avg loss=4.657, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=4.7 seconds, 247552 images, time remaining=4 hours
3869: loss=5.763, avg loss=4.768, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=4.7 seconds, 247616 images, time remaining=4 hours
3870: loss=4.590, avg loss=4.750, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=892.6 milliseconds, train=4.7 seconds, 247680 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x145016000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3871: loss=5.178, avg loss=4.793, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=1.8 seconds, 247744 images, time remaining=4 hours
3872: loss=4.398, avg loss=4.753, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.8 seconds, 247808 images, time remaining=4 hours
3873: loss=4.575, avg loss=4.735, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=1.8 seconds, 247872 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3874: loss=4.438, avg loss=4.706, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=1.9 seconds, 247936 images, time remaining=4 hours
3875: loss=3.068, avg loss=4.542, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.9 seconds, 248000 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3876: loss=4.630, avg loss=4.551, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.0 seconds, train=1.8 seconds, 248064 images, time remaining=4 hours
3877: loss=4.068, avg loss=4.502, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=729.1 milliseconds, train=1.8 seconds, 248128 images, time remaining=4 hours
3878: loss=4.509, avg loss=4.503, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 248192 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3879: loss=4.385, avg loss=4.491, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=1.8 seconds, 248256 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3880: loss=3.945, avg loss=4.437, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.9 seconds, train=1.8 seconds, 248320 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145784000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3881: loss=4.142, avg loss=4.407, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.7 seconds, train=1.2 seconds, 248384 images, time remaining=4 hours
3882: loss=3.887, avg loss=4.355, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=840.0 milliseconds, train=1.2 seconds, 248448 images, time remaining=4 hours
3883: loss=3.911, avg loss=4.311, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=882.2 milliseconds, train=1.2 seconds, 248512 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3884: loss=3.881, avg loss=4.268, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.5 seconds, train=1.2 seconds, 248576 images, time remaining=4 hours
3885: loss=3.549, avg loss=4.196, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=984.0 milliseconds, train=1.2 seconds, 248640 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3886: loss=3.879, avg loss=4.164, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 248704 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3887: loss=4.722, avg loss=4.220, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=1.2 seconds, 248768 images, time remaining=4 hours
3888: loss=4.431, avg loss=4.241, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=931.4 milliseconds, train=1.2 seconds, 248832 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3889: loss=3.766, avg loss=4.194, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 248896 images, time remaining=4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3890: loss=4.193, avg loss=4.194, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 248960 images, time remaining=4 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3891: loss=6.029, avg loss=4.377, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 249024 images, time remaining=4 hours
3892: loss=5.988, avg loss=4.538, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=933.8 milliseconds, train=4.1 seconds, 249088 images, time remaining=4 hours
3893: loss=5.094, avg loss=4.594, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=964.2 milliseconds, train=4.1 seconds, 249152 images, time remaining=4 hours
3894: loss=4.518, avg loss=4.586, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=4.1 seconds, 249216 images, time remaining=4 hours
3895: loss=4.741, avg loss=4.602, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.7 seconds, train=4.1 seconds, 249280 images, time remaining=4 hours
3896: loss=3.824, avg loss=4.524, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=722.1 milliseconds, train=4.1 seconds, 249344 images, time remaining=4 hours
3897: loss=4.671, avg loss=4.539, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=4.1 seconds, 249408 images, time remaining=4 hours
3898: loss=5.071, avg loss=4.592, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=4.1 seconds, 249472 images, time remaining=4 hours
3899: loss=5.051, avg loss=4.638, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=4.1 seconds, 249536 images, time remaining=4 hours
3900: loss=4.619, avg loss=4.636, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=628.2 milliseconds, train=4.1 seconds, 249600 images, time remaining=4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3901: loss=5.914, avg loss=4.764, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=924.5 milliseconds, train=5.1 seconds, 249664 images, time remaining=4 hours
3902: loss=4.397, avg loss=4.727, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=719.4 milliseconds, train=5.0 seconds, 249728 images, time remaining=4 hours
3903: loss=4.316, avg loss=4.686, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=992.2 milliseconds, train=5.0 seconds, 249792 images, time remaining=4 hours
3904: loss=4.436, avg loss=4.661, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=815.9 milliseconds, train=5.0 seconds, 249856 images, time remaining=4 hours
3905: loss=4.946, avg loss=4.689, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=5.0 seconds, 249920 images, time remaining=3.9 hours
3906: loss=5.583, avg loss=4.779, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=5.0 seconds, 249984 images, time remaining=3.9 hours
3907: loss=4.664, avg loss=4.767, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=5.0 seconds, 250048 images, time remaining=3.9 hours
3908: loss=5.621, avg loss=4.853, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 250112 images, time remaining=3.9 hours
3909: loss=5.064, avg loss=4.874, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=5.0 seconds, 250176 images, time remaining=3.9 hours
3910: loss=5.578, avg loss=4.944, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.2 seconds, train=5.0 seconds, 250240 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3911: loss=4.013, avg loss=4.851, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 250304 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3912: loss=4.388, avg loss=4.805, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 250368 images, time remaining=3.9 hours
3913: loss=3.974, avg loss=4.722, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 250432 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3914: loss=4.542, avg loss=4.704, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 250496 images, time remaining=3.9 hours
3915: loss=4.977, avg loss=4.731, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 250560 images, time remaining=3.9 hours
3916: loss=3.564, avg loss=4.614, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 250624 images, time remaining=3.9 hours
3917: loss=4.057, avg loss=4.559, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=940.8 milliseconds, train=2.1 seconds, 250688 images, time remaining=3.9 hours
3918: loss=3.524, avg loss=4.455, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=841.3 milliseconds, train=2.1 seconds, 250752 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3919: loss=3.575, avg loss=4.367, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 250816 images, time remaining=3.9 hours
3920: loss=3.420, avg loss=4.273, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 250880 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3921: loss=5.825, avg loss=4.428, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 250944 images, time remaining=3.9 hours
3922: loss=4.616, avg loss=4.447, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=853.2 milliseconds, train=3.9 seconds, 251008 images, time remaining=3.9 hours
3923: loss=4.682, avg loss=4.470, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 251072 images, time remaining=3.9 hours
3924: loss=4.328, avg loss=4.456, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=888.7 milliseconds, train=3.9 seconds, 251136 images, time remaining=3.9 hours
3925: loss=4.906, avg loss=4.501, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=752.5 milliseconds, train=3.9 seconds, 251200 images, time remaining=3.9 hours
3926: loss=3.730, avg loss=4.424, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 251264 images, time remaining=3.9 hours
3927: loss=4.438, avg loss=4.425, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 251328 images, time remaining=3.9 hours
3928: loss=4.248, avg loss=4.408, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 251392 images, time remaining=3.9 hours
3929: loss=4.848, avg loss=4.452, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=820.2 milliseconds, train=3.9 seconds, 251456 images, time remaining=3.9 hours
3930: loss=4.867, avg loss=4.493, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=951.1 milliseconds, train=3.9 seconds, 251520 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145c70000000
3931: loss=5.612, avg loss=4.605, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=867.1 milliseconds, train=1.6 seconds, 251584 images, time remaining=3.9 hours
3932: loss=4.784, avg loss=4.623, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=922.0 milliseconds, train=1.6 seconds, 251648 images, time remaining=3.9 hours
3933: loss=4.602, avg loss=4.621, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 251712 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3934: loss=4.002, avg loss=4.559, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 251776 images, time remaining=3.9 hours
3935: loss=3.999, avg loss=4.503, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 251840 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3936: loss=4.357, avg loss=4.488, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 251904 images, time remaining=3.9 hours
3937: loss=4.543, avg loss=4.494, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 251968 images, time remaining=3.9 hours
3938: loss=3.905, avg loss=4.435, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=856.3 milliseconds, train=1.6 seconds, 252032 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3939: loss=4.256, avg loss=4.417, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 252096 images, time remaining=3.9 hours
3940: loss=4.255, avg loss=4.401, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 252160 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145c72000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3941: loss=4.632, avg loss=4.424, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 252224 images, time remaining=3.9 hours
3942: loss=3.862, avg loss=4.368, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 252288 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3943: loss=3.795, avg loss=4.310, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 252352 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3944: loss=4.226, avg loss=4.302, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 252416 images, time remaining=3.9 hours
3945: loss=3.831, avg loss=4.255, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=792.5 milliseconds, train=1.5 seconds, 252480 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3946: loss=3.842, avg loss=4.214, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 252544 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3947: loss=4.339, avg loss=4.226, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 252608 images, time remaining=3.9 hours
3948: loss=4.414, avg loss=4.245, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=939.2 milliseconds, train=1.5 seconds, 252672 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3949: loss=3.937, avg loss=4.214, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 252736 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3950: loss=3.406, avg loss=4.133, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 252800 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3951: loss=3.663, avg loss=4.086, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=821.6 milliseconds, train=3.3 seconds, 252864 images, time remaining=3.9 hours
3952: loss=4.328, avg loss=4.111, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 252928 images, time remaining=3.9 hours
3953: loss=4.192, avg loss=4.119, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 252992 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3954: loss=3.049, avg loss=4.012, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 253056 images, time remaining=3.9 hours
3955: loss=3.531, avg loss=3.964, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 253120 images, time remaining=3.9 hours
3956: loss=3.827, avg loss=3.950, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 253184 images, time remaining=3.9 hours
3957: loss=3.765, avg loss=3.931, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 253248 images, time remaining=3.9 hours
3958: loss=3.572, avg loss=3.896, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 253312 images, time remaining=3.9 hours
3959: loss=4.243, avg loss=3.930, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 253376 images, time remaining=3.9 hours
3960: loss=4.094, avg loss=3.947, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 253440 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3961: loss=5.434, avg loss=4.095, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=5.1 seconds, 253504 images, time remaining=3.9 hours
3962: loss=5.093, avg loss=4.195, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.7 seconds, train=5.1 seconds, 253568 images, time remaining=3.9 hours
3963: loss=5.647, avg loss=4.340, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=5.0 seconds, 253632 images, time remaining=3.9 hours
3964: loss=4.666, avg loss=4.373, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 253696 images, time remaining=3.9 hours
3965: loss=4.593, avg loss=4.395, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=5.0 seconds, 253760 images, time remaining=3.9 hours
3966: loss=4.997, avg loss=4.455, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 253824 images, time remaining=3.9 hours
3967: loss=4.906, avg loss=4.500, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=5.0 seconds, 253888 images, time remaining=3.9 hours
3968: loss=3.607, avg loss=4.411, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=856.1 milliseconds, train=5.0 seconds, 253952 images, time remaining=3.9 hours
3969: loss=4.663, avg loss=4.436, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=5.0 seconds, 254016 images, time remaining=3.9 hours
3970: loss=3.917, avg loss=4.384, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 254080 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
3971: loss=4.939, avg loss=4.440, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 254144 images, time remaining=3.9 hours
3972: loss=4.947, avg loss=4.490, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=704.6 milliseconds, train=1.9 seconds, 254208 images, time remaining=3.9 hours
3973: loss=4.191, avg loss=4.460, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.9 seconds, 254272 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3974: loss=3.024, avg loss=4.317, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 254336 images, time remaining=3.9 hours
3975: loss=3.542, avg loss=4.239, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=752.2 milliseconds, train=1.9 seconds, 254400 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3976: loss=3.850, avg loss=4.200, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.2 seconds, train=1.9 seconds, 254464 images, time remaining=3.9 hours
3977: loss=4.390, avg loss=4.219, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 254528 images, time remaining=3.9 hours
3978: loss=3.779, avg loss=4.175, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.9 seconds, 254592 images, time remaining=3.9 hours
3979: loss=3.640, avg loss=4.122, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=740.9 milliseconds, train=1.9 seconds, 254656 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3980: loss=4.078, avg loss=4.117, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 254720 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
3981: loss=5.043, avg loss=4.210, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=769.3 milliseconds, train=4.6 seconds, 254784 images, time remaining=3.9 hours
3982: loss=5.018, avg loss=4.291, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=4.6 seconds, 254848 images, time remaining=3.9 hours
3983: loss=5.297, avg loss=4.392, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.7 seconds, train=4.6 seconds, 254912 images, time remaining=3.9 hours
3984: loss=5.475, avg loss=4.500, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 254976 images, time remaining=3.9 hours
3985: loss=5.036, avg loss=4.553, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 255040 images, time remaining=3.9 hours
3986: loss=4.924, avg loss=4.591, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 255104 images, time remaining=3.9 hours
3987: loss=4.871, avg loss=4.619, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.0 seconds, train=4.6 seconds, 255168 images, time remaining=3.9 hours
3988: loss=4.869, avg loss=4.644, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 255232 images, time remaining=3.9 hours
3989: loss=4.403, avg loss=4.620, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 255296 images, time remaining=3.9 hours
3990: loss=5.127, avg loss=4.670, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 255360 images, time remaining=3.9 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145ad8000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3991: loss=5.594, avg loss=4.763, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 255424 images, time remaining=3.9 hours
3992: loss=5.083, avg loss=4.795, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 255488 images, time remaining=3.9 hours
3993: loss=4.416, avg loss=4.757, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 255552 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3994: loss=4.710, avg loss=4.752, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 255616 images, time remaining=3.9 hours
3995: loss=3.964, avg loss=4.673, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=711.7 milliseconds, train=1.4 seconds, 255680 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3996: loss=4.391, avg loss=4.645, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.2 seconds, train=1.4 seconds, 255744 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3997: loss=3.937, avg loss=4.574, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 255808 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
3998: loss=4.995, avg loss=4.616, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.1 seconds, train=1.4 seconds, 255872 images, time remaining=3.9 hours
3999: loss=5.312, avg loss=4.686, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 255936 images, time remaining=3.9 hours
4000: loss=4.233, avg loss=4.641, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=845.3 milliseconds, train=1.4 seconds, 256000 images, time remaining=3.9 hours
Saving weights to /workspace/.cache/splits/combined_4000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4001: loss=5.178, avg loss=4.694, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=924.8 milliseconds, train=3.4 seconds, 256064 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4002: loss=5.725, avg loss=4.797, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.4 seconds, train=3.4 seconds, 256128 images, time remaining=3.9 hours
4003: loss=5.008, avg loss=4.818, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=3.4 seconds, 256192 images, time remaining=3.9 hours
4004: loss=4.489, avg loss=4.786, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 256256 images, time remaining=3.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4005: loss=4.524, avg loss=4.759, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.8 seconds, train=3.4 seconds, 256320 images, time remaining=3.9 hours
4006: loss=4.166, avg loss=4.700, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 256384 images, time remaining=3.9 hours
4007: loss=4.537, avg loss=4.684, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=3.4 seconds, 256448 images, time remaining=3.9 hours
4008: loss=5.252, avg loss=4.741, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 256512 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4009: loss=5.398, avg loss=4.806, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.1 seconds, train=3.4 seconds, 256576 images, time remaining=3.8 hours
4010: loss=5.060, avg loss=4.832, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 256640 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4011: loss=4.312, avg loss=4.780, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=700.7 milliseconds, train=3.2 seconds, 256704 images, time remaining=3.8 hours
4012: loss=3.683, avg loss=4.670, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 256768 images, time remaining=3.8 hours
4013: loss=4.210, avg loss=4.624, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.4 seconds, train=3.2 seconds, 256832 images, time remaining=3.8 hours
4014: loss=4.474, avg loss=4.609, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 256896 images, time remaining=3.8 hours
4015: loss=4.207, avg loss=4.569, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=3.2 seconds, 256960 images, time remaining=3.8 hours
4016: loss=3.628, avg loss=4.475, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 257024 images, time remaining=3.8 hours
4017: loss=3.950, avg loss=4.422, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=3.2 seconds, 257088 images, time remaining=3.8 hours
4018: loss=4.730, avg loss=4.453, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 257152 images, time remaining=3.8 hours
4019: loss=4.450, avg loss=4.453, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=3.2 seconds, 257216 images, time remaining=3.8 hours
4020: loss=3.321, avg loss=4.340, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 257280 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14648c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4021: loss=4.472, avg loss=4.353, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=1.2 seconds, 257344 images, time remaining=3.8 hours
4022: loss=3.311, avg loss=4.249, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=932.7 milliseconds, train=1.2 seconds, 257408 images, time remaining=3.8 hours
4023: loss=4.263, avg loss=4.250, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=815.5 milliseconds, train=1.2 seconds, 257472 images, time remaining=3.8 hours
4024: loss=4.313, avg loss=4.256, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 257536 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4025: loss=3.380, avg loss=4.169, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 257600 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4026: loss=4.187, avg loss=4.171, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 257664 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4027: loss=3.944, avg loss=4.148, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 257728 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4028: loss=3.741, avg loss=4.107, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 257792 images, time remaining=3.8 hours
4029: loss=3.735, avg loss=4.070, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 257856 images, time remaining=3.8 hours
4030: loss=4.150, avg loss=4.078, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=873.3 milliseconds, train=1.2 seconds, 257920 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4031: loss=6.722, avg loss=4.342, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 257984 images, time remaining=3.8 hours
4032: loss=6.190, avg loss=4.527, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 258048 images, time remaining=3.8 hours
4033: loss=5.850, avg loss=4.659, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=856.1 milliseconds, train=4.5 seconds, 258112 images, time remaining=3.8 hours
4034: loss=4.181, avg loss=4.612, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 258176 images, time remaining=3.8 hours
4035: loss=3.622, avg loss=4.513, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=4.5 seconds, 258240 images, time remaining=3.8 hours
4036: loss=5.086, avg loss=4.570, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.5 seconds, 258304 images, time remaining=3.8 hours
4037: loss=4.664, avg loss=4.579, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 258368 images, time remaining=3.8 hours
4038: loss=5.131, avg loss=4.635, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 258432 images, time remaining=3.8 hours
4039: loss=4.605, avg loss=4.632, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 258496 images, time remaining=3.8 hours
4040: loss=4.525, avg loss=4.621, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=495.8 milliseconds, train=4.5 seconds, 258560 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145cec000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4041: loss=7.577, avg loss=4.917, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=1.2 seconds, 258624 images, time remaining=3.8 hours
4042: loss=5.227, avg loss=4.948, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 258688 images, time remaining=3.8 hours
4043: loss=5.609, avg loss=5.014, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=692.9 milliseconds, train=1.2 seconds, 258752 images, time remaining=3.8 hours
4044: loss=4.767, avg loss=4.989, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=886.5 milliseconds, train=1.2 seconds, 258816 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4045: loss=4.708, avg loss=4.961, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 258880 images, time remaining=3.8 hours
4046: loss=4.406, avg loss=4.905, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 258944 images, time remaining=3.8 hours
4047: loss=5.423, avg loss=4.957, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 259008 images, time remaining=3.8 hours
4048: loss=5.079, avg loss=4.969, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=764.5 milliseconds, train=1.2 seconds, 259072 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4049: loss=5.559, avg loss=5.028, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 259136 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4050: loss=3.839, avg loss=4.909, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 259200 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4051: loss=6.573, avg loss=5.076, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 259264 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4052: loss=5.755, avg loss=5.144, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.7 seconds, train=2.1 seconds, 259328 images, time remaining=3.8 hours
4053: loss=4.615, avg loss=5.091, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 259392 images, time remaining=3.8 hours
4054: loss=4.524, avg loss=5.034, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 259456 images, time remaining=3.8 hours
4055: loss=4.121, avg loss=4.943, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 259520 images, time remaining=3.8 hours
4056: loss=3.959, avg loss=4.844, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 259584 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4057: loss=4.109, avg loss=4.771, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 259648 images, time remaining=3.8 hours
4058: loss=4.295, avg loss=4.723, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 259712 images, time remaining=3.8 hours
4059: loss=4.644, avg loss=4.715, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 259776 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4060: loss=5.358, avg loss=4.780, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 259840 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146452000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4061: loss=6.575, avg loss=4.959, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 259904 images, time remaining=3.8 hours
4062: loss=5.074, avg loss=4.971, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=657.0 milliseconds, train=1.2 seconds, 259968 images, time remaining=3.8 hours
4063: loss=4.265, avg loss=4.900, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=777.8 milliseconds, train=1.2 seconds, 260032 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4064: loss=4.198, avg loss=4.830, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 260096 images, time remaining=3.8 hours
4065: loss=4.857, avg loss=4.833, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 260160 images, time remaining=3.8 hours
4066: loss=3.892, avg loss=4.738, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=640.8 milliseconds, train=1.2 seconds, 260224 images, time remaining=3.8 hours
4067: loss=3.885, avg loss=4.653, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 260288 images, time remaining=3.8 hours
4068: loss=4.765, avg loss=4.664, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 260352 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4069: loss=3.767, avg loss=4.575, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 260416 images, time remaining=3.8 hours
4070: loss=5.151, avg loss=4.632, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=795.1 milliseconds, train=1.2 seconds, 260480 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1463ba000000
4071: loss=4.068, avg loss=4.576, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=947.5 milliseconds, train=1.4 seconds, 260544 images, time remaining=3.8 hours
4072: loss=5.156, avg loss=4.634, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 260608 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4073: loss=4.971, avg loss=4.668, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 260672 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4074: loss=3.553, avg loss=4.556, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 260736 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4075: loss=4.372, avg loss=4.538, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 260800 images, time remaining=3.8 hours
4076: loss=3.905, avg loss=4.474, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 260864 images, time remaining=3.8 hours
4077: loss=4.223, avg loss=4.449, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 260928 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4078: loss=3.858, avg loss=4.390, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=1.4 seconds, 260992 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4079: loss=4.444, avg loss=4.395, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 261056 images, time remaining=3.8 hours
4080: loss=3.983, avg loss=4.354, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=610.7 milliseconds, train=1.4 seconds, 261120 images, time remaining=3.8 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145ffe000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4081: loss=3.660, avg loss=4.285, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 261184 images, time remaining=3.8 hours
4082: loss=3.457, avg loss=4.202, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=668.5 milliseconds, train=1.5 seconds, 261248 images, time remaining=3.8 hours
4083: loss=3.752, avg loss=4.157, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=790.3 milliseconds, train=1.5 seconds, 261312 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4084: loss=3.118, avg loss=4.053, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 261376 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4085: loss=3.744, avg loss=4.022, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 261440 images, time remaining=3.8 hours
4086: loss=3.801, avg loss=4.000, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=762.6 milliseconds, train=1.5 seconds, 261504 images, time remaining=3.8 hours
4087: loss=3.434, avg loss=3.944, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=785.3 milliseconds, train=1.5 seconds, 261568 images, time remaining=3.8 hours
4088: loss=3.308, avg loss=3.880, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 261632 images, time remaining=3.8 hours
4089: loss=4.136, avg loss=3.906, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=810.0 milliseconds, train=1.5 seconds, 261696 images, time remaining=3.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4090: loss=3.439, avg loss=3.859, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 261760 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4091: loss=5.098, avg loss=3.983, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.3 seconds, train=3.4 seconds, 261824 images, time remaining=3.7 hours
4092: loss=4.061, avg loss=3.991, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=990.1 milliseconds, train=3.4 seconds, 261888 images, time remaining=3.7 hours
4093: loss=3.610, avg loss=3.953, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=758.8 milliseconds, train=3.4 seconds, 261952 images, time remaining=3.7 hours
4094: loss=4.280, avg loss=3.985, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.9 seconds, train=3.4 seconds, 262016 images, time remaining=3.7 hours
4095: loss=3.216, avg loss=3.908, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=859.8 milliseconds, train=3.4 seconds, 262080 images, time remaining=3.7 hours
4096: loss=4.191, avg loss=3.937, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=3.4 seconds, 262144 images, time remaining=3.7 hours
4097: loss=3.354, avg loss=3.878, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 262208 images, time remaining=3.7 hours
4098: loss=3.295, avg loss=3.820, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 262272 images, time remaining=3.7 hours
4099: loss=3.714, avg loss=3.809, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=813.8 milliseconds, train=3.4 seconds, 262336 images, time remaining=3.7 hours
4100: loss=4.903, avg loss=3.919, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 262400 images, time remaining=3.7 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4101: loss=4.586, avg loss=3.985, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=523.8 milliseconds, train=4.1 seconds, 262464 images, time remaining=3.7 hours
4102: loss=3.642, avg loss=3.951, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.6 seconds, train=4.1 seconds, 262528 images, time remaining=3.7 hours
4103: loss=3.635, avg loss=3.919, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 262592 images, time remaining=3.7 hours
4104: loss=3.785, avg loss=3.906, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.9 seconds, train=4.1 seconds, 262656 images, time remaining=3.7 hours
4105: loss=4.230, avg loss=3.938, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=4.1 seconds, 262720 images, time remaining=3.7 hours
4106: loss=4.614, avg loss=4.006, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=4.1 seconds, 262784 images, time remaining=3.7 hours
4107: loss=3.638, avg loss=3.969, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=904.8 milliseconds, train=4.1 seconds, 262848 images, time remaining=3.7 hours
4108: loss=3.697, avg loss=3.942, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 262912 images, time remaining=3.7 hours
4109: loss=4.074, avg loss=3.955, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=916.8 milliseconds, train=4.1 seconds, 262976 images, time remaining=3.7 hours
4110: loss=3.478, avg loss=3.907, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 263040 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4111: loss=4.217, avg loss=3.938, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=575.6 milliseconds, train=3.3 seconds, 263104 images, time remaining=3.7 hours
4112: loss=3.997, avg loss=3.944, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 263168 images, time remaining=3.7 hours
4113: loss=3.857, avg loss=3.936, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 263232 images, time remaining=3.7 hours
4114: loss=4.281, avg loss=3.970, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 263296 images, time remaining=3.7 hours
4115: loss=3.677, avg loss=3.941, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=972.6 milliseconds, train=3.2 seconds, 263360 images, time remaining=3.7 hours
4116: loss=4.141, avg loss=3.961, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 263424 images, time remaining=3.7 hours
4117: loss=4.224, avg loss=3.987, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=692.1 milliseconds, train=3.3 seconds, 263488 images, time remaining=3.7 hours
4118: loss=3.646, avg loss=3.953, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 263552 images, time remaining=3.7 hours
4119: loss=3.153, avg loss=3.873, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 263616 images, time remaining=3.7 hours
4120: loss=3.556, avg loss=3.841, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=940.4 milliseconds, train=3.3 seconds, 263680 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4121: loss=4.224, avg loss=3.880, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=781.6 milliseconds, train=4.1 seconds, 263744 images, time remaining=3.7 hours
4122: loss=5.026, avg loss=3.994, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=671.3 milliseconds, train=4.1 seconds, 263808 images, time remaining=3.7 hours
4123: loss=3.395, avg loss=3.934, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 263872 images, time remaining=3.7 hours
4124: loss=3.695, avg loss=3.910, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=4.1 seconds, 263936 images, time remaining=3.7 hours
4125: loss=4.843, avg loss=4.004, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 264000 images, time remaining=3.7 hours
4126: loss=4.157, avg loss=4.019, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=4.1 seconds, 264064 images, time remaining=3.7 hours
4127: loss=3.288, avg loss=3.946, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=4.1 seconds, 264128 images, time remaining=3.7 hours
4128: loss=4.071, avg loss=3.958, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=929.4 milliseconds, train=4.1 seconds, 264192 images, time remaining=3.7 hours
4129: loss=4.296, avg loss=3.992, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 264256 images, time remaining=3.7 hours
4130: loss=4.434, avg loss=4.036, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 264320 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145f65000000
4131: loss=3.572, avg loss=3.990, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=690.9 milliseconds, train=1.2 seconds, 264384 images, time remaining=3.7 hours
4132: loss=4.641, avg loss=4.055, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=518.2 milliseconds, train=1.2 seconds, 264448 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4133: loss=5.443, avg loss=4.194, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 264512 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4134: loss=4.364, avg loss=4.211, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 264576 images, time remaining=3.7 hours
4135: loss=3.593, avg loss=4.149, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=867.1 milliseconds, train=1.2 seconds, 264640 images, time remaining=3.7 hours
4136: loss=3.539, avg loss=4.088, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=832.6 milliseconds, train=1.2 seconds, 264704 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4137: loss=4.006, avg loss=4.080, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 264768 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4138: loss=4.170, avg loss=4.089, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 264832 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4139: loss=4.017, avg loss=4.082, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 264896 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4140: loss=3.840, avg loss=4.057, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 264960 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146492000000
4141: loss=4.542, avg loss=4.106, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 265024 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4142: loss=4.369, avg loss=4.132, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 265088 images, time remaining=3.7 hours
4143: loss=3.693, avg loss=4.088, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=954.9 milliseconds, train=1.5 seconds, 265152 images, time remaining=3.7 hours
4144: loss=4.257, avg loss=4.105, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 265216 images, time remaining=3.7 hours
4145: loss=3.893, avg loss=4.084, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=897.6 milliseconds, train=1.5 seconds, 265280 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4146: loss=5.145, avg loss=4.190, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 265344 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4147: loss=3.534, avg loss=4.124, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 265408 images, time remaining=3.7 hours
4148: loss=3.593, avg loss=4.071, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 265472 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4149: loss=4.594, avg loss=4.124, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 265536 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4150: loss=3.856, avg loss=4.097, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 265600 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4151: loss=4.800, avg loss=4.167, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=4.7 seconds, 265664 images, time remaining=3.7 hours
4152: loss=4.107, avg loss=4.161, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=4.7 seconds, 265728 images, time remaining=3.7 hours
4153: loss=4.698, avg loss=4.215, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.9 seconds, train=4.6 seconds, 265792 images, time remaining=3.7 hours
4154: loss=4.189, avg loss=4.212, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 265856 images, time remaining=3.7 hours
4155: loss=4.100, avg loss=4.201, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=4.6 seconds, 265920 images, time remaining=3.7 hours
4156: loss=4.109, avg loss=4.192, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=4.6 seconds, 265984 images, time remaining=3.7 hours
4157: loss=4.397, avg loss=4.212, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=757.1 milliseconds, train=4.6 seconds, 266048 images, time remaining=3.7 hours
4158: loss=4.413, avg loss=4.232, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 266112 images, time remaining=3.7 hours
4159: loss=4.296, avg loss=4.239, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 266176 images, time remaining=3.7 hours
4160: loss=4.008, avg loss=4.216, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 266240 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4161: loss=4.916, avg loss=4.286, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=3.6 seconds, 266304 images, time remaining=3.7 hours
4162: loss=3.765, avg loss=4.234, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 266368 images, time remaining=3.7 hours
4163: loss=4.412, avg loss=4.251, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=658.1 milliseconds, train=3.6 seconds, 266432 images, time remaining=3.7 hours
4164: loss=3.834, avg loss=4.210, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 266496 images, time remaining=3.7 hours
4165: loss=3.774, avg loss=4.166, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=776.6 milliseconds, train=3.6 seconds, 266560 images, time remaining=3.7 hours
4166: loss=4.770, avg loss=4.226, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=947.0 milliseconds, train=3.6 seconds, 266624 images, time remaining=3.7 hours
4167: loss=4.113, avg loss=4.215, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=695.6 milliseconds, train=3.6 seconds, 266688 images, time remaining=3.7 hours
4168: loss=4.043, avg loss=4.198, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=947.4 milliseconds, train=3.6 seconds, 266752 images, time remaining=3.7 hours
4169: loss=4.300, avg loss=4.208, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=759.1 milliseconds, train=3.6 seconds, 266816 images, time remaining=3.7 hours
4170: loss=4.099, avg loss=4.197, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 266880 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14589e000000
4171: loss=4.050, avg loss=4.182, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=490.2 milliseconds, train=1.5 seconds, 266944 images, time remaining=3.7 hours
4172: loss=3.379, avg loss=4.102, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=736.6 milliseconds, train=1.5 seconds, 267008 images, time remaining=3.7 hours
4173: loss=3.177, avg loss=4.010, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=724.8 milliseconds, train=1.5 seconds, 267072 images, time remaining=3.7 hours
4174: loss=3.123, avg loss=3.921, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=769.6 milliseconds, train=1.5 seconds, 267136 images, time remaining=3.7 hours
4175: loss=3.357, avg loss=3.865, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=982.6 milliseconds, train=1.5 seconds, 267200 images, time remaining=3.7 hours
4176: loss=3.993, avg loss=3.877, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=693.0 milliseconds, train=1.5 seconds, 267264 images, time remaining=3.7 hours
4177: loss=3.192, avg loss=3.809, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=867.3 milliseconds, train=1.5 seconds, 267328 images, time remaining=3.7 hours
4178: loss=3.507, avg loss=3.779, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 267392 images, time remaining=3.7 hours
4179: loss=4.366, avg loss=3.837, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=826.3 milliseconds, train=1.5 seconds, 267456 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4180: loss=3.326, avg loss=3.786, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 267520 images, time remaining=3.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14585f000000
4181: loss=3.343, avg loss=3.742, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 267584 images, time remaining=3.7 hours
4182: loss=3.626, avg loss=3.730, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 267648 images, time remaining=3.7 hours
4183: loss=3.050, avg loss=3.662, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 267712 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4184: loss=3.543, avg loss=3.650, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 267776 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4185: loss=3.336, avg loss=3.619, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=4.2 seconds, train=1.2 seconds, 267840 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4186: loss=3.408, avg loss=3.598, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.9 seconds, train=1.2 seconds, 267904 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4187: loss=3.333, avg loss=3.571, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 267968 images, time remaining=3.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4188: loss=3.697, avg loss=3.584, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 268032 images, time remaining=3.7 hours
4189: loss=3.666, avg loss=3.592, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=820.4 milliseconds, train=1.2 seconds, 268096 images, time remaining=3.6 hours
4190: loss=4.072, avg loss=3.640, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=992.5 milliseconds, train=1.2 seconds, 268160 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4191: loss=4.504, avg loss=3.726, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=737.8 milliseconds, train=2.1 seconds, 268224 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4192: loss=4.052, avg loss=3.759, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.5 seconds, train=2.1 seconds, 268288 images, time remaining=3.6 hours
4193: loss=4.441, avg loss=3.827, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 268352 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4194: loss=4.209, avg loss=3.865, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.1 seconds, train=2.1 seconds, 268416 images, time remaining=3.6 hours
4195: loss=2.840, avg loss=3.763, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.9 seconds, train=2.1 seconds, 268480 images, time remaining=3.6 hours
4196: loss=3.198, avg loss=3.706, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=848.1 milliseconds, train=2.1 seconds, 268544 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4197: loss=4.267, avg loss=3.762, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 268608 images, time remaining=3.6 hours
4198: loss=3.644, avg loss=3.751, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 268672 images, time remaining=3.6 hours
4199: loss=3.567, avg loss=3.732, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 268736 images, time remaining=3.6 hours
4200: loss=3.768, avg loss=3.736, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=729.6 milliseconds, train=2.1 seconds, 268800 images, time remaining=3.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4201: loss=4.129, avg loss=3.775, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.8 seconds, train=4.5 seconds, 268864 images, time remaining=3.6 hours
4202: loss=4.107, avg loss=3.808, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=4.5 seconds, 268928 images, time remaining=3.6 hours
4203: loss=3.435, avg loss=3.771, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.5 seconds, train=4.5 seconds, 268992 images, time remaining=3.6 hours
4204: loss=4.114, avg loss=3.805, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 269056 images, time remaining=3.6 hours
4205: loss=5.477, avg loss=3.972, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=4.5 seconds, 269120 images, time remaining=3.6 hours
4206: loss=3.959, avg loss=3.971, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.5 seconds, 269184 images, time remaining=3.6 hours
4207: loss=3.632, avg loss=3.937, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 269248 images, time remaining=3.6 hours
4208: loss=3.865, avg loss=3.930, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=3.8 seconds, train=4.5 seconds, 269312 images, time remaining=3.6 hours
4209: loss=4.032, avg loss=3.940, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 269376 images, time remaining=3.6 hours
4210: loss=5.294, avg loss=4.076, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 269440 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
4211: loss=4.569, avg loss=4.125, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=512.1 milliseconds, train=1.5 seconds, 269504 images, time remaining=3.6 hours
4212: loss=4.990, avg loss=4.211, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=917.2 milliseconds, train=1.5 seconds, 269568 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4213: loss=4.370, avg loss=4.227, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 269632 images, time remaining=3.6 hours
4214: loss=3.386, avg loss=4.143, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=494.2 milliseconds, train=1.5 seconds, 269696 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4215: loss=3.526, avg loss=4.081, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 269760 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4216: loss=3.427, avg loss=4.016, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 269824 images, time remaining=3.6 hours
4217: loss=3.621, avg loss=3.977, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=978.0 milliseconds, train=1.5 seconds, 269888 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4218: loss=4.073, avg loss=3.986, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 269952 images, time remaining=3.6 hours
4219: loss=4.053, avg loss=3.993, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 270016 images, time remaining=3.6 hours
4220: loss=3.762, avg loss=3.970, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 270080 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4221: loss=7.732, avg loss=4.346, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 270144 images, time remaining=3.6 hours
4222: loss=6.765, avg loss=4.588, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 270208 images, time remaining=3.6 hours
4223: loss=4.933, avg loss=4.622, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=680.3 milliseconds, train=3.9 seconds, 270272 images, time remaining=3.6 hours
4224: loss=4.068, avg loss=4.567, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=879.6 milliseconds, train=3.9 seconds, 270336 images, time remaining=3.6 hours
4225: loss=4.123, avg loss=4.523, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=858.2 milliseconds, train=3.9 seconds, 270400 images, time remaining=3.6 hours
4226: loss=4.464, avg loss=4.517, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=601.4 milliseconds, train=3.9 seconds, 270464 images, time remaining=3.6 hours
4227: loss=4.312, avg loss=4.496, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 270528 images, time remaining=3.6 hours
4228: loss=4.073, avg loss=4.454, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 270592 images, time remaining=3.6 hours
4229: loss=5.094, avg loss=4.518, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 270656 images, time remaining=3.6 hours
4230: loss=4.955, avg loss=4.562, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 270720 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4231: loss=4.322, avg loss=4.538, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 270784 images, time remaining=3.6 hours
4232: loss=5.330, avg loss=4.617, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=4.2 seconds, 270848 images, time remaining=3.6 hours
4233: loss=4.046, avg loss=4.560, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=791.5 milliseconds, train=4.1 seconds, 270912 images, time remaining=3.6 hours
4234: loss=4.721, avg loss=4.576, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 270976 images, time remaining=3.6 hours
4235: loss=4.563, avg loss=4.575, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=876.7 milliseconds, train=4.1 seconds, 271040 images, time remaining=3.6 hours
4236: loss=4.081, avg loss=4.525, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=4.1 seconds, 271104 images, time remaining=3.6 hours
4237: loss=3.519, avg loss=4.425, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 271168 images, time remaining=3.6 hours
4238: loss=3.986, avg loss=4.381, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=730.2 milliseconds, train=4.1 seconds, 271232 images, time remaining=3.6 hours
4239: loss=5.069, avg loss=4.450, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=703.7 milliseconds, train=4.1 seconds, 271296 images, time remaining=3.6 hours
4240: loss=3.890, avg loss=4.394, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 271360 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4241: loss=4.810, avg loss=4.435, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 271424 images, time remaining=3.6 hours
4242: loss=5.123, avg loss=4.504, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=758.0 milliseconds, train=5.0 seconds, 271488 images, time remaining=3.6 hours
4243: loss=4.816, avg loss=4.535, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=802.4 milliseconds, train=5.0 seconds, 271552 images, time remaining=3.6 hours
4244: loss=4.982, avg loss=4.580, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.5 seconds, train=5.0 seconds, 271616 images, time remaining=3.6 hours
4245: loss=4.682, avg loss=4.590, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=788.7 milliseconds, train=5.0 seconds, 271680 images, time remaining=3.6 hours
4246: loss=5.024, avg loss=4.633, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.0 seconds, train=5.0 seconds, 271744 images, time remaining=3.6 hours
4247: loss=5.093, avg loss=4.679, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=1.4 seconds, train=5.0 seconds, 271808 images, time remaining=3.6 hours
4248: loss=5.105, avg loss=4.722, last=88.68%, best=88.79%, next=4248, rate=0.00130000, load 64=807.2 milliseconds, train=5.0 seconds, 271872 images, time remaining=3.6 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=155362, unique_truth_count=57264
rank=0 of ranks=155362rank=100 of ranks=155362rank=200 of ranks=155362rank=300 of ranks=155362rank=400 of ranks=155362rank=500 of ranks=155362rank=600 of ranks=155362rank=700 of ranks=155362rank=800 of ranks=155362rank=900 of ranks=155362rank=1000 of ranks=155362rank=1100 of ranks=155362rank=1200 of ranks=155362rank=1300 of ranks=155362rank=1400 of ranks=155362rank=1500 of ranks=155362rank=1600 of ranks=155362rank=1700 of ranks=155362rank=1800 of ranks=155362rank=1900 of ranks=155362rank=2000 of ranks=155362rank=2100 of ranks=155362rank=2200 of ranks=155362rank=2300 of ranks=155362rank=2400 of ranks=155362rank=2500 of ranks=155362rank=2600 of ranks=155362rank=2700 of ranks=155362rank=2800 of ranks=155362rank=2900 of ranks=155362rank=3000 of ranks=155362rank=3100 of ranks=155362rank=3200 of ranks=155362rank=3300 of ranks=155362rank=3400 of ranks=155362rank=3500 of ranks=155362rank=3600 of ranks=155362rank=3700 of ranks=155362rank=3800 of ranks=155362rank=3900 of ranks=155362rank=4000 of ranks=155362rank=4100 of ranks=155362rank=4200 of ranks=155362rank=4300 of ranks=155362rank=4400 of ranks=155362rank=4500 of ranks=155362rank=4600 of ranks=155362rank=4700 of ranks=155362rank=4800 of ranks=155362rank=4900 of ranks=155362rank=5000 of ranks=155362rank=5100 of ranks=155362rank=5200 of ranks=155362rank=5300 of ranks=155362rank=5400 of ranks=155362rank=5500 of ranks=155362rank=5600 of ranks=155362rank=5700 of ranks=155362rank=5800 of ranks=155362rank=5900 of ranks=155362rank=6000 of ranks=155362rank=6100 of ranks=155362rank=6200 of ranks=155362rank=6300 of ranks=155362rank=6400 of ranks=155362rank=6500 of ranks=155362rank=6600 of ranks=155362rank=6700 of ranks=155362rank=6800 of ranks=155362rank=6900 of ranks=155362rank=7000 of ranks=155362rank=7100 of ranks=155362rank=7200 of ranks=155362rank=7300 of ranks=155362rank=7400 of ranks=155362rank=7500 of ranks=155362rank=7600 of ranks=155362rank=7700 of ranks=155362rank=7800 of ranks=155362rank=7900 of ranks=155362rank=8000 of ranks=155362rank=8100 of ranks=155362rank=8200 of ranks=155362rank=8300 of ranks=155362rank=8400 of ranks=155362rank=8500 of ranks=155362rank=8600 of ranks=155362rank=8700 of ranks=155362rank=8800 of ranks=155362rank=8900 of ranks=155362rank=9000 of ranks=155362rank=9100 of ranks=155362rank=9200 of ranks=155362rank=9300 of ranks=155362rank=9400 of ranks=155362rank=9500 of ranks=155362rank=9600 of ranks=155362rank=9700 of ranks=155362rank=9800 of ranks=155362rank=9900 of ranks=155362rank=10000 of ranks=155362rank=10100 of ranks=155362rank=10200 of ranks=155362rank=10300 of ranks=155362rank=10400 of ranks=155362rank=10500 of ranks=155362rank=10600 of ranks=155362rank=10700 of ranks=155362rank=10800 of ranks=155362rank=10900 of ranks=155362rank=11000 of ranks=155362rank=11100 of ranks=155362rank=11200 of ranks=155362rank=11300 of ranks=155362rank=11400 of ranks=155362rank=11500 of ranks=155362rank=11600 of ranks=155362rank=11700 of ranks=155362rank=11800 of ranks=155362rank=11900 of ranks=155362rank=12000 of ranks=155362rank=12100 of ranks=155362rank=12200 of ranks=155362rank=12300 of ranks=155362rank=12400 of ranks=155362rank=12500 of ranks=155362rank=12600 of ranks=155362rank=12700 of ranks=155362rank=12800 of ranks=155362rank=12900 of ranks=155362rank=13000 of ranks=155362rank=13100 of ranks=155362rank=13200 of ranks=155362rank=13300 of ranks=155362rank=13400 of ranks=155362rank=13500 of ranks=155362rank=13600 of ranks=155362rank=13700 of ranks=155362rank=13800 of ranks=155362rank=13900 of ranks=155362rank=14000 of ranks=155362rank=14100 of ranks=155362rank=14200 of ranks=155362rank=14300 of ranks=155362rank=14400 of ranks=155362rank=14500 of ranks=155362rank=14600 of ranks=155362rank=14700 of ranks=155362rank=14800 of ranks=155362rank=14900 of ranks=155362rank=15000 of ranks=155362rank=15100 of ranks=155362rank=15200 of ranks=155362rank=15300 of ranks=155362rank=15400 of ranks=155362rank=15500 of ranks=155362rank=15600 of ranks=155362rank=15700 of ranks=155362rank=15800 of ranks=155362rank=15900 of ranks=155362rank=16000 of ranks=155362rank=16100 of ranks=155362rank=16200 of ranks=155362rank=16300 of ranks=155362rank=16400 of ranks=155362rank=16500 of ranks=155362rank=16600 of ranks=155362rank=16700 of ranks=155362rank=16800 of ranks=155362rank=16900 of ranks=155362rank=17000 of ranks=155362rank=17100 of ranks=155362rank=17200 of ranks=155362rank=17300 of ranks=155362rank=17400 of ranks=155362rank=17500 of ranks=155362rank=17600 of ranks=155362rank=17700 of ranks=155362rank=17800 of ranks=155362rank=17900 of ranks=155362rank=18000 of ranks=155362rank=18100 of ranks=155362rank=18200 of ranks=155362rank=18300 of ranks=155362rank=18400 of ranks=155362rank=18500 of ranks=155362rank=18600 of ranks=155362rank=18700 of ranks=155362rank=18800 of ranks=155362rank=18900 of ranks=155362rank=19000 of ranks=155362rank=19100 of ranks=155362rank=19200 of ranks=155362rank=19300 of ranks=155362rank=19400 of ranks=155362rank=19500 of ranks=155362rank=19600 of ranks=155362rank=19700 of ranks=155362rank=19800 of ranks=155362rank=19900 of ranks=155362rank=20000 of ranks=155362rank=20100 of ranks=155362rank=20200 of ranks=155362rank=20300 of ranks=155362rank=20400 of ranks=155362rank=20500 of ranks=155362rank=20600 of ranks=155362rank=20700 of ranks=155362rank=20800 of ranks=155362rank=20900 of ranks=155362rank=21000 of ranks=155362rank=21100 of ranks=155362rank=21200 of ranks=155362rank=21300 of ranks=155362rank=21400 of ranks=155362rank=21500 of ranks=155362rank=21600 of ranks=155362rank=21700 of ranks=155362rank=21800 of ranks=155362rank=21900 of ranks=155362rank=22000 of ranks=155362rank=22100 of ranks=155362rank=22200 of ranks=155362rank=22300 of ranks=155362rank=22400 of ranks=155362rank=22500 of ranks=155362rank=22600 of ranks=155362rank=22700 of ranks=155362rank=22800 of ranks=155362rank=22900 of ranks=155362rank=23000 of ranks=155362rank=23100 of ranks=155362rank=23200 of ranks=155362rank=23300 of ranks=155362rank=23400 of ranks=155362rank=23500 of ranks=155362rank=23600 of ranks=155362rank=23700 of ranks=155362rank=23800 of ranks=155362rank=23900 of ranks=155362rank=24000 of ranks=155362rank=24100 of ranks=155362rank=24200 of ranks=155362rank=24300 of ranks=155362rank=24400 of ranks=155362rank=24500 of ranks=155362rank=24600 of ranks=155362rank=24700 of ranks=155362rank=24800 of ranks=155362rank=24900 of ranks=155362rank=25000 of ranks=155362rank=25100 of ranks=155362rank=25200 of ranks=155362rank=25300 of ranks=155362rank=25400 of ranks=155362rank=25500 of ranks=155362rank=25600 of ranks=155362rank=25700 of ranks=155362rank=25800 of ranks=155362rank=25900 of ranks=155362rank=26000 of ranks=155362rank=26100 of ranks=155362rank=26200 of ranks=155362rank=26300 of ranks=155362rank=26400 of ranks=155362rank=26500 of ranks=155362rank=26600 of ranks=155362rank=26700 of ranks=155362rank=26800 of ranks=155362rank=26900 of ranks=155362rank=27000 of ranks=155362rank=27100 of ranks=155362rank=27200 of ranks=155362rank=27300 of ranks=155362rank=27400 of ranks=155362rank=27500 of ranks=155362rank=27600 of ranks=155362rank=27700 of ranks=155362rank=27800 of ranks=155362rank=27900 of ranks=155362rank=28000 of ranks=155362rank=28100 of ranks=155362rank=28200 of ranks=155362rank=28300 of ranks=155362rank=28400 of ranks=155362rank=28500 of ranks=155362rank=28600 of ranks=155362rank=28700 of ranks=155362rank=28800 of ranks=155362rank=28900 of ranks=155362rank=29000 of ranks=155362rank=29100 of ranks=155362rank=29200 of ranks=155362rank=29300 of ranks=155362rank=29400 of ranks=155362rank=29500 of ranks=155362rank=29600 of ranks=155362rank=29700 of ranks=155362rank=29800 of ranks=155362rank=29900 of ranks=155362rank=30000 of ranks=155362rank=30100 of ranks=155362rank=30200 of ranks=155362rank=30300 of ranks=155362rank=30400 of ranks=155362rank=30500 of ranks=155362rank=30600 of ranks=155362rank=30700 of ranks=155362rank=30800 of ranks=155362rank=30900 of ranks=155362rank=31000 of ranks=155362rank=31100 of ranks=155362rank=31200 of ranks=155362rank=31300 of ranks=155362rank=31400 of ranks=155362rank=31500 of ranks=155362rank=31600 of ranks=155362rank=31700 of ranks=155362rank=31800 of ranks=155362rank=31900 of ranks=155362rank=32000 of ranks=155362rank=32100 of ranks=155362rank=32200 of ranks=155362rank=32300 of ranks=155362rank=32400 of ranks=155362rank=32500 of ranks=155362rank=32600 of ranks=155362rank=32700 of ranks=155362rank=32800 of ranks=155362rank=32900 of ranks=155362rank=33000 of ranks=155362rank=33100 of ranks=155362rank=33200 of ranks=155362rank=33300 of ranks=155362rank=33400 of ranks=155362rank=33500 of ranks=155362rank=33600 of ranks=155362rank=33700 of ranks=155362rank=33800 of ranks=155362rank=33900 of ranks=155362rank=34000 of ranks=155362rank=34100 of ranks=155362rank=34200 of ranks=155362rank=34300 of ranks=155362rank=34400 of ranks=155362rank=34500 of ranks=155362rank=34600 of ranks=155362rank=34700 of ranks=155362rank=34800 of ranks=155362rank=34900 of ranks=155362rank=35000 of ranks=155362rank=35100 of ranks=155362rank=35200 of ranks=155362rank=35300 of ranks=155362rank=35400 of ranks=155362rank=35500 of ranks=155362rank=35600 of ranks=155362rank=35700 of ranks=155362rank=35800 of ranks=155362rank=35900 of ranks=155362rank=36000 of ranks=155362rank=36100 of ranks=155362rank=36200 of ranks=155362rank=36300 of ranks=155362rank=36400 of ranks=155362rank=36500 of ranks=155362rank=36600 of ranks=155362rank=36700 of ranks=155362rank=36800 of ranks=155362rank=36900 of ranks=155362rank=37000 of ranks=155362rank=37100 of ranks=155362rank=37200 of ranks=155362rank=37300 of ranks=155362rank=37400 of ranks=155362rank=37500 of ranks=155362rank=37600 of ranks=155362rank=37700 of ranks=155362rank=37800 of ranks=155362rank=37900 of ranks=155362rank=38000 of ranks=155362rank=38100 of ranks=155362rank=38200 of ranks=155362rank=38300 of ranks=155362rank=38400 of ranks=155362rank=38500 of ranks=155362rank=38600 of ranks=155362rank=38700 of ranks=155362rank=38800 of ranks=155362rank=38900 of ranks=155362rank=39000 of ranks=155362rank=39100 of ranks=155362rank=39200 of ranks=155362rank=39300 of ranks=155362rank=39400 of ranks=155362rank=39500 of ranks=155362rank=39600 of ranks=155362rank=39700 of ranks=155362rank=39800 of ranks=155362rank=39900 of ranks=155362rank=40000 of ranks=155362rank=40100 of ranks=155362rank=40200 of ranks=155362rank=40300 of ranks=155362rank=40400 of ranks=155362rank=40500 of ranks=155362rank=40600 of ranks=155362rank=40700 of ranks=155362rank=40800 of ranks=155362rank=40900 of ranks=155362rank=41000 of ranks=155362rank=41100 of ranks=155362rank=41200 of ranks=155362rank=41300 of ranks=155362rank=41400 of ranks=155362rank=41500 of ranks=155362rank=41600 of ranks=155362rank=41700 of ranks=155362rank=41800 of ranks=155362rank=41900 of ranks=155362rank=42000 of ranks=155362rank=42100 of ranks=155362rank=42200 of ranks=155362rank=42300 of ranks=155362rank=42400 of ranks=155362rank=42500 of ranks=155362rank=42600 of ranks=155362rank=42700 of ranks=155362rank=42800 of ranks=155362rank=42900 of ranks=155362rank=43000 of ranks=155362rank=43100 of ranks=155362rank=43200 of ranks=155362rank=43300 of ranks=155362rank=43400 of ranks=155362rank=43500 of ranks=155362rank=43600 of ranks=155362rank=43700 of ranks=155362rank=43800 of ranks=155362rank=43900 of ranks=155362rank=44000 of ranks=155362rank=44100 of ranks=155362rank=44200 of ranks=155362rank=44300 of ranks=155362rank=44400 of ranks=155362rank=44500 of ranks=155362rank=44600 of ranks=155362rank=44700 of ranks=155362rank=44800 of ranks=155362rank=44900 of ranks=155362rank=45000 of ranks=155362rank=45100 of ranks=155362rank=45200 of ranks=155362rank=45300 of ranks=155362rank=45400 of ranks=155362rank=45500 of ranks=155362rank=45600 of ranks=155362rank=45700 of ranks=155362rank=45800 of ranks=155362rank=45900 of ranks=155362rank=46000 of ranks=155362rank=46100 of ranks=155362rank=46200 of ranks=155362rank=46300 of ranks=155362rank=46400 of ranks=155362rank=46500 of ranks=155362rank=46600 of ranks=155362rank=46700 of ranks=155362rank=46800 of ranks=155362rank=46900 of ranks=155362rank=47000 of ranks=155362rank=47100 of ranks=155362rank=47200 of ranks=155362rank=47300 of ranks=155362rank=47400 of ranks=155362rank=47500 of ranks=155362rank=47600 of ranks=155362rank=47700 of ranks=155362rank=47800 of ranks=155362rank=47900 of ranks=155362rank=48000 of ranks=155362rank=48100 of ranks=155362rank=48200 of ranks=155362rank=48300 of ranks=155362rank=48400 of ranks=155362rank=48500 of ranks=155362rank=48600 of ranks=155362rank=48700 of ranks=155362rank=48800 of ranks=155362rank=48900 of ranks=155362rank=49000 of ranks=155362rank=49100 of ranks=155362rank=49200 of ranks=155362rank=49300 of ranks=155362rank=49400 of ranks=155362rank=49500 of ranks=155362rank=49600 of ranks=155362rank=49700 of ranks=155362rank=49800 of ranks=155362rank=49900 of ranks=155362rank=50000 of ranks=155362rank=50100 of ranks=155362rank=50200 of ranks=155362rank=50300 of ranks=155362rank=50400 of ranks=155362rank=50500 of ranks=155362rank=50600 of ranks=155362rank=50700 of ranks=155362rank=50800 of ranks=155362rank=50900 of ranks=155362rank=51000 of ranks=155362rank=51100 of ranks=155362rank=51200 of ranks=155362rank=51300 of ranks=155362rank=51400 of ranks=155362rank=51500 of ranks=155362rank=51600 of ranks=155362rank=51700 of ranks=155362rank=51800 of ranks=155362rank=51900 of ranks=155362rank=52000 of ranks=155362rank=52100 of ranks=155362rank=52200 of ranks=155362rank=52300 of ranks=155362rank=52400 of ranks=155362rank=52500 of ranks=155362rank=52600 of ranks=155362rank=52700 of ranks=155362rank=52800 of ranks=155362rank=52900 of ranks=155362rank=53000 of ranks=155362rank=53100 of ranks=155362rank=53200 of ranks=155362rank=53300 of ranks=155362rank=53400 of ranks=155362rank=53500 of ranks=155362rank=53600 of ranks=155362rank=53700 of ranks=155362rank=53800 of ranks=155362rank=53900 of ranks=155362rank=54000 of ranks=155362rank=54100 of ranks=155362rank=54200 of ranks=155362rank=54300 of ranks=155362rank=54400 of ranks=155362rank=54500 of ranks=155362rank=54600 of ranks=155362rank=54700 of ranks=155362rank=54800 of ranks=155362rank=54900 of ranks=155362rank=55000 of ranks=155362rank=55100 of ranks=155362rank=55200 of ranks=155362rank=55300 of ranks=155362rank=55400 of ranks=155362rank=55500 of ranks=155362rank=55600 of ranks=155362rank=55700 of ranks=155362rank=55800 of ranks=155362rank=55900 of ranks=155362rank=56000 of ranks=155362rank=56100 of ranks=155362rank=56200 of ranks=155362rank=56300 of ranks=155362rank=56400 of ranks=155362rank=56500 of ranks=155362rank=56600 of ranks=155362rank=56700 of ranks=155362rank=56800 of ranks=155362rank=56900 of ranks=155362rank=57000 of ranks=155362rank=57100 of ranks=155362rank=57200 of ranks=155362rank=57300 of ranks=155362rank=57400 of ranks=155362rank=57500 of ranks=155362rank=57600 of ranks=155362rank=57700 of ranks=155362rank=57800 of ranks=155362rank=57900 of ranks=155362rank=58000 of ranks=155362rank=58100 of ranks=155362rank=58200 of ranks=155362rank=58300 of ranks=155362rank=58400 of ranks=155362rank=58500 of ranks=155362rank=58600 of ranks=155362rank=58700 of ranks=155362rank=58800 of ranks=155362rank=58900 of ranks=155362rank=59000 of ranks=155362rank=59100 of ranks=155362rank=59200 of ranks=155362rank=59300 of ranks=155362rank=59400 of ranks=155362rank=59500 of ranks=155362rank=59600 of ranks=155362rank=59700 of ranks=155362rank=59800 of ranks=155362rank=59900 of ranks=155362rank=60000 of ranks=155362rank=60100 of ranks=155362rank=60200 of ranks=155362rank=60300 of ranks=155362rank=60400 of ranks=155362rank=60500 of ranks=155362rank=60600 of ranks=155362rank=60700 of ranks=155362rank=60800 of ranks=155362rank=60900 of ranks=155362rank=61000 of ranks=155362rank=61100 of ranks=155362rank=61200 of ranks=155362rank=61300 of ranks=155362rank=61400 of ranks=155362rank=61500 of ranks=155362rank=61600 of ranks=155362rank=61700 of ranks=155362rank=61800 of ranks=155362rank=61900 of ranks=155362rank=62000 of ranks=155362rank=62100 of ranks=155362rank=62200 of ranks=155362rank=62300 of ranks=155362rank=62400 of ranks=155362rank=62500 of ranks=155362rank=62600 of ranks=155362rank=62700 of ranks=155362rank=62800 of ranks=155362rank=62900 of ranks=155362rank=63000 of ranks=155362rank=63100 of ranks=155362rank=63200 of ranks=155362rank=63300 of ranks=155362rank=63400 of ranks=155362rank=63500 of ranks=155362rank=63600 of ranks=155362rank=63700 of ranks=155362rank=63800 of ranks=155362rank=63900 of ranks=155362rank=64000 of ranks=155362rank=64100 of ranks=155362rank=64200 of ranks=155362rank=64300 of ranks=155362rank=64400 of ranks=155362rank=64500 of ranks=155362rank=64600 of ranks=155362rank=64700 of ranks=155362rank=64800 of ranks=155362rank=64900 of ranks=155362rank=65000 of ranks=155362rank=65100 of ranks=155362rank=65200 of ranks=155362rank=65300 of ranks=155362rank=65400 of ranks=155362rank=65500 of ranks=155362rank=65600 of ranks=155362rank=65700 of ranks=155362rank=65800 of ranks=155362rank=65900 of ranks=155362rank=66000 of ranks=155362rank=66100 of ranks=155362rank=66200 of ranks=155362rank=66300 of ranks=155362rank=66400 of ranks=155362rank=66500 of ranks=155362rank=66600 of ranks=155362rank=66700 of ranks=155362rank=66800 of ranks=155362rank=66900 of ranks=155362rank=67000 of ranks=155362rank=67100 of ranks=155362rank=67200 of ranks=155362rank=67300 of ranks=155362rank=67400 of ranks=155362rank=67500 of ranks=155362rank=67600 of ranks=155362rank=67700 of ranks=155362rank=67800 of ranks=155362rank=67900 of ranks=155362rank=68000 of ranks=155362rank=68100 of ranks=155362rank=68200 of ranks=155362rank=68300 of ranks=155362rank=68400 of ranks=155362rank=68500 of ranks=155362rank=68600 of ranks=155362rank=68700 of ranks=155362rank=68800 of ranks=155362rank=68900 of ranks=155362rank=69000 of ranks=155362rank=69100 of ranks=155362rank=69200 of ranks=155362rank=69300 of ranks=155362rank=69400 of ranks=155362rank=69500 of ranks=155362rank=69600 of ranks=155362rank=69700 of ranks=155362rank=69800 of ranks=155362rank=69900 of ranks=155362rank=70000 of ranks=155362rank=70100 of ranks=155362rank=70200 of ranks=155362rank=70300 of ranks=155362rank=70400 of ranks=155362rank=70500 of ranks=155362rank=70600 of ranks=155362rank=70700 of ranks=155362rank=70800 of ranks=155362rank=70900 of ranks=155362rank=71000 of ranks=155362rank=71100 of ranks=155362rank=71200 of ranks=155362rank=71300 of ranks=155362rank=71400 of ranks=155362rank=71500 of ranks=155362rank=71600 of ranks=155362rank=71700 of ranks=155362rank=71800 of ranks=155362rank=71900 of ranks=155362rank=72000 of ranks=155362rank=72100 of ranks=155362rank=72200 of ranks=155362rank=72300 of ranks=155362rank=72400 of ranks=155362rank=72500 of ranks=155362rank=72600 of ranks=155362rank=72700 of ranks=155362rank=72800 of ranks=155362rank=72900 of ranks=155362rank=73000 of ranks=155362rank=73100 of ranks=155362rank=73200 of ranks=155362rank=73300 of ranks=155362rank=73400 of ranks=155362rank=73500 of ranks=155362rank=73600 of ranks=155362rank=73700 of ranks=155362rank=73800 of ranks=155362rank=73900 of ranks=155362rank=74000 of ranks=155362rank=74100 of ranks=155362rank=74200 of ranks=155362rank=74300 of ranks=155362rank=74400 of ranks=155362rank=74500 of ranks=155362rank=74600 of ranks=155362rank=74700 of ranks=155362rank=74800 of ranks=155362rank=74900 of ranks=155362rank=75000 of ranks=155362rank=75100 of ranks=155362rank=75200 of ranks=155362rank=75300 of ranks=155362rank=75400 of ranks=155362rank=75500 of ranks=155362rank=75600 of ranks=155362rank=75700 of ranks=155362rank=75800 of ranks=155362rank=75900 of ranks=155362rank=76000 of ranks=155362rank=76100 of ranks=155362rank=76200 of ranks=155362rank=76300 of ranks=155362rank=76400 of ranks=155362rank=76500 of ranks=155362rank=76600 of ranks=155362rank=76700 of ranks=155362rank=76800 of ranks=155362rank=76900 of ranks=155362rank=77000 of ranks=155362rank=77100 of ranks=155362rank=77200 of ranks=155362rank=77300 of ranks=155362rank=77400 of ranks=155362rank=77500 of ranks=155362rank=77600 of ranks=155362rank=77700 of ranks=155362rank=77800 of ranks=155362rank=77900 of ranks=155362rank=78000 of ranks=155362rank=78100 of ranks=155362rank=78200 of ranks=155362rank=78300 of ranks=155362rank=78400 of ranks=155362rank=78500 of ranks=155362rank=78600 of ranks=155362rank=78700 of ranks=155362rank=78800 of ranks=155362rank=78900 of ranks=155362rank=79000 of ranks=155362rank=79100 of ranks=155362rank=79200 of ranks=155362rank=79300 of ranks=155362rank=79400 of ranks=155362rank=79500 of ranks=155362rank=79600 of ranks=155362rank=79700 of ranks=155362rank=79800 of ranks=155362rank=79900 of ranks=155362rank=80000 of ranks=155362rank=80100 of ranks=155362rank=80200 of ranks=155362rank=80300 of ranks=155362rank=80400 of ranks=155362rank=80500 of ranks=155362rank=80600 of ranks=155362rank=80700 of ranks=155362rank=80800 of ranks=155362rank=80900 of ranks=155362rank=81000 of ranks=155362rank=81100 of ranks=155362rank=81200 of ranks=155362rank=81300 of ranks=155362rank=81400 of ranks=155362rank=81500 of ranks=155362rank=81600 of ranks=155362rank=81700 of ranks=155362rank=81800 of ranks=155362rank=81900 of ranks=155362rank=82000 of ranks=155362rank=82100 of ranks=155362rank=82200 of ranks=155362rank=82300 of ranks=155362rank=82400 of ranks=155362rank=82500 of ranks=155362rank=82600 of ranks=155362rank=82700 of ranks=155362rank=82800 of ranks=155362rank=82900 of ranks=155362rank=83000 of ranks=155362rank=83100 of ranks=155362rank=83200 of ranks=155362rank=83300 of ranks=155362rank=83400 of ranks=155362rank=83500 of ranks=155362rank=83600 of ranks=155362rank=83700 of ranks=155362rank=83800 of ranks=155362rank=83900 of ranks=155362rank=84000 of ranks=155362rank=84100 of ranks=155362rank=84200 of ranks=155362rank=84300 of ranks=155362rank=84400 of ranks=155362rank=84500 of ranks=155362rank=84600 of ranks=155362rank=84700 of ranks=155362rank=84800 of ranks=155362rank=84900 of ranks=155362rank=85000 of ranks=155362rank=85100 of ranks=155362rank=85200 of ranks=155362rank=85300 of ranks=155362rank=85400 of ranks=155362rank=85500 of ranks=155362rank=85600 of ranks=155362rank=85700 of ranks=155362rank=85800 of ranks=155362rank=85900 of ranks=155362rank=86000 of ranks=155362rank=86100 of ranks=155362rank=86200 of ranks=155362rank=86300 of ranks=155362rank=86400 of ranks=155362rank=86500 of ranks=155362rank=86600 of ranks=155362rank=86700 of ranks=155362rank=86800 of ranks=155362rank=86900 of ranks=155362rank=87000 of ranks=155362rank=87100 of ranks=155362rank=87200 of ranks=155362rank=87300 of ranks=155362rank=87400 of ranks=155362rank=87500 of ranks=155362rank=87600 of ranks=155362rank=87700 of ranks=155362rank=87800 of ranks=155362rank=87900 of ranks=155362rank=88000 of ranks=155362rank=88100 of ranks=155362rank=88200 of ranks=155362rank=88300 of ranks=155362rank=88400 of ranks=155362rank=88500 of ranks=155362rank=88600 of ranks=155362rank=88700 of ranks=155362rank=88800 of ranks=155362rank=88900 of ranks=155362rank=89000 of ranks=155362rank=89100 of ranks=155362rank=89200 of ranks=155362rank=89300 of ranks=155362rank=89400 of ranks=155362rank=89500 of ranks=155362rank=89600 of ranks=155362rank=89700 of ranks=155362rank=89800 of ranks=155362rank=89900 of ranks=155362rank=90000 of ranks=155362rank=90100 of ranks=155362rank=90200 of ranks=155362rank=90300 of ranks=155362rank=90400 of ranks=155362rank=90500 of ranks=155362rank=90600 of ranks=155362rank=90700 of ranks=155362rank=90800 of ranks=155362rank=90900 of ranks=155362rank=91000 of ranks=155362rank=91100 of ranks=155362rank=91200 of ranks=155362rank=91300 of ranks=155362rank=91400 of ranks=155362rank=91500 of ranks=155362rank=91600 of ranks=155362rank=91700 of ranks=155362rank=91800 of ranks=155362rank=91900 of ranks=155362rank=92000 of ranks=155362rank=92100 of ranks=155362rank=92200 of ranks=155362rank=92300 of ranks=155362rank=92400 of ranks=155362rank=92500 of ranks=155362rank=92600 of ranks=155362rank=92700 of ranks=155362rank=92800 of ranks=155362rank=92900 of ranks=155362rank=93000 of ranks=155362rank=93100 of ranks=155362rank=93200 of ranks=155362rank=93300 of ranks=155362rank=93400 of ranks=155362rank=93500 of ranks=155362rank=93600 of ranks=155362rank=93700 of ranks=155362rank=93800 of ranks=155362rank=93900 of ranks=155362rank=94000 of ranks=155362rank=94100 of ranks=155362rank=94200 of ranks=155362rank=94300 of ranks=155362rank=94400 of ranks=155362rank=94500 of ranks=155362rank=94600 of ranks=155362rank=94700 of ranks=155362rank=94800 of ranks=155362rank=94900 of ranks=155362rank=95000 of ranks=155362rank=95100 of ranks=155362rank=95200 of ranks=155362rank=95300 of ranks=155362rank=95400 of ranks=155362rank=95500 of ranks=155362rank=95600 of ranks=155362rank=95700 of ranks=155362rank=95800 of ranks=155362rank=95900 of ranks=155362rank=96000 of ranks=155362rank=96100 of ranks=155362rank=96200 of ranks=155362rank=96300 of ranks=155362rank=96400 of ranks=155362rank=96500 of ranks=155362rank=96600 of ranks=155362rank=96700 of ranks=155362rank=96800 of ranks=155362rank=96900 of ranks=155362rank=97000 of ranks=155362rank=97100 of ranks=155362rank=97200 of ranks=155362rank=97300 of ranks=155362rank=97400 of ranks=155362rank=97500 of ranks=155362rank=97600 of ranks=155362rank=97700 of ranks=155362rank=97800 of ranks=155362rank=97900 of ranks=155362rank=98000 of ranks=155362rank=98100 of ranks=155362rank=98200 of ranks=155362rank=98300 of ranks=155362rank=98400 of ranks=155362rank=98500 of ranks=155362rank=98600 of ranks=155362rank=98700 of ranks=155362rank=98800 of ranks=155362rank=98900 of ranks=155362rank=99000 of ranks=155362rank=99100 of ranks=155362rank=99200 of ranks=155362rank=99300 of ranks=155362rank=99400 of ranks=155362rank=99500 of ranks=155362rank=99600 of ranks=155362rank=99700 of ranks=155362rank=99800 of ranks=155362rank=99900 of ranks=155362rank=100000 of ranks=155362rank=100100 of ranks=155362rank=100200 of ranks=155362rank=100300 of ranks=155362rank=100400 of ranks=155362rank=100500 of ranks=155362rank=100600 of ranks=155362rank=100700 of ranks=155362rank=100800 of ranks=155362rank=100900 of ranks=155362rank=101000 of ranks=155362rank=101100 of ranks=155362rank=101200 of ranks=155362rank=101300 of ranks=155362rank=101400 of ranks=155362rank=101500 of ranks=155362rank=101600 of ranks=155362rank=101700 of ranks=155362rank=101800 of ranks=155362rank=101900 of ranks=155362rank=102000 of ranks=155362rank=102100 of ranks=155362rank=102200 of ranks=155362rank=102300 of ranks=155362rank=102400 of ranks=155362rank=102500 of ranks=155362rank=102600 of ranks=155362rank=102700 of ranks=155362rank=102800 of ranks=155362rank=102900 of ranks=155362rank=103000 of ranks=155362rank=103100 of ranks=155362rank=103200 of ranks=155362rank=103300 of ranks=155362rank=103400 of ranks=155362rank=103500 of ranks=155362rank=103600 of ranks=155362rank=103700 of ranks=155362rank=103800 of ranks=155362rank=103900 of ranks=155362rank=104000 of ranks=155362rank=104100 of ranks=155362rank=104200 of ranks=155362rank=104300 of ranks=155362rank=104400 of ranks=155362rank=104500 of ranks=155362rank=104600 of ranks=155362rank=104700 of ranks=155362rank=104800 of ranks=155362rank=104900 of ranks=155362rank=105000 of ranks=155362rank=105100 of ranks=155362rank=105200 of ranks=155362rank=105300 of ranks=155362rank=105400 of ranks=155362rank=105500 of ranks=155362rank=105600 of ranks=155362rank=105700 of ranks=155362rank=105800 of ranks=155362rank=105900 of ranks=155362rank=106000 of ranks=155362rank=106100 of ranks=155362rank=106200 of ranks=155362rank=106300 of ranks=155362rank=106400 of ranks=155362rank=106500 of ranks=155362rank=106600 of ranks=155362rank=106700 of ranks=155362rank=106800 of ranks=155362rank=106900 of ranks=155362rank=107000 of ranks=155362rank=107100 of ranks=155362rank=107200 of ranks=155362rank=107300 of ranks=155362rank=107400 of ranks=155362rank=107500 of ranks=155362rank=107600 of ranks=155362rank=107700 of ranks=155362rank=107800 of ranks=155362rank=107900 of ranks=155362rank=108000 of ranks=155362rank=108100 of ranks=155362rank=108200 of ranks=155362rank=108300 of ranks=155362rank=108400 of ranks=155362rank=108500 of ranks=155362rank=108600 of ranks=155362rank=108700 of ranks=155362rank=108800 of ranks=155362rank=108900 of ranks=155362rank=109000 of ranks=155362rank=109100 of ranks=155362rank=109200 of ranks=155362rank=109300 of ranks=155362rank=109400 of ranks=155362rank=109500 of ranks=155362rank=109600 of ranks=155362rank=109700 of ranks=155362rank=109800 of ranks=155362rank=109900 of ranks=155362rank=110000 of ranks=155362rank=110100 of ranks=155362rank=110200 of ranks=155362rank=110300 of ranks=155362rank=110400 of ranks=155362rank=110500 of ranks=155362rank=110600 of ranks=155362rank=110700 of ranks=155362rank=110800 of ranks=155362rank=110900 of ranks=155362rank=111000 of ranks=155362rank=111100 of ranks=155362rank=111200 of ranks=155362rank=111300 of ranks=155362rank=111400 of ranks=155362rank=111500 of ranks=155362rank=111600 of ranks=155362rank=111700 of ranks=155362rank=111800 of ranks=155362rank=111900 of ranks=155362rank=112000 of ranks=155362rank=112100 of ranks=155362rank=112200 of ranks=155362rank=112300 of ranks=155362rank=112400 of ranks=155362rank=112500 of ranks=155362rank=112600 of ranks=155362rank=112700 of ranks=155362rank=112800 of ranks=155362rank=112900 of ranks=155362rank=113000 of ranks=155362rank=113100 of ranks=155362rank=113200 of ranks=155362rank=113300 of ranks=155362rank=113400 of ranks=155362rank=113500 of ranks=155362rank=113600 of ranks=155362rank=113700 of ranks=155362rank=113800 of ranks=155362rank=113900 of ranks=155362rank=114000 of ranks=155362rank=114100 of ranks=155362rank=114200 of ranks=155362rank=114300 of ranks=155362rank=114400 of ranks=155362rank=114500 of ranks=155362rank=114600 of ranks=155362rank=114700 of ranks=155362rank=114800 of ranks=155362rank=114900 of ranks=155362rank=115000 of ranks=155362rank=115100 of ranks=155362rank=115200 of ranks=155362rank=115300 of ranks=155362rank=115400 of ranks=155362rank=115500 of ranks=155362rank=115600 of ranks=155362rank=115700 of ranks=155362rank=115800 of ranks=155362rank=115900 of ranks=155362rank=116000 of ranks=155362rank=116100 of ranks=155362rank=116200 of ranks=155362rank=116300 of ranks=155362rank=116400 of ranks=155362rank=116500 of ranks=155362rank=116600 of ranks=155362rank=116700 of ranks=155362rank=116800 of ranks=155362rank=116900 of ranks=155362rank=117000 of ranks=155362rank=117100 of ranks=155362rank=117200 of ranks=155362rank=117300 of ranks=155362rank=117400 of ranks=155362rank=117500 of ranks=155362rank=117600 of ranks=155362rank=117700 of ranks=155362rank=117800 of ranks=155362rank=117900 of ranks=155362rank=118000 of ranks=155362rank=118100 of ranks=155362rank=118200 of ranks=155362rank=118300 of ranks=155362rank=118400 of ranks=155362rank=118500 of ranks=155362rank=118600 of ranks=155362rank=118700 of ranks=155362rank=118800 of ranks=155362rank=118900 of ranks=155362rank=119000 of ranks=155362rank=119100 of ranks=155362rank=119200 of ranks=155362rank=119300 of ranks=155362rank=119400 of ranks=155362rank=119500 of ranks=155362rank=119600 of ranks=155362rank=119700 of ranks=155362rank=119800 of ranks=155362rank=119900 of ranks=155362rank=120000 of ranks=155362rank=120100 of ranks=155362rank=120200 of ranks=155362rank=120300 of ranks=155362rank=120400 of ranks=155362rank=120500 of ranks=155362rank=120600 of ranks=155362rank=120700 of ranks=155362rank=120800 of ranks=155362rank=120900 of ranks=155362rank=121000 of ranks=155362rank=121100 of ranks=155362rank=121200 of ranks=155362rank=121300 of ranks=155362rank=121400 of ranks=155362rank=121500 of ranks=155362rank=121600 of ranks=155362rank=121700 of ranks=155362rank=121800 of ranks=155362rank=121900 of ranks=155362rank=122000 of ranks=155362rank=122100 of ranks=155362rank=122200 of ranks=155362rank=122300 of ranks=155362rank=122400 of ranks=155362rank=122500 of ranks=155362rank=122600 of ranks=155362rank=122700 of ranks=155362rank=122800 of ranks=155362rank=122900 of ranks=155362rank=123000 of ranks=155362rank=123100 of ranks=155362rank=123200 of ranks=155362rank=123300 of ranks=155362rank=123400 of ranks=155362rank=123500 of ranks=155362rank=123600 of ranks=155362rank=123700 of ranks=155362rank=123800 of ranks=155362rank=123900 of ranks=155362rank=124000 of ranks=155362rank=124100 of ranks=155362rank=124200 of ranks=155362rank=124300 of ranks=155362rank=124400 of ranks=155362rank=124500 of ranks=155362rank=124600 of ranks=155362rank=124700 of ranks=155362rank=124800 of ranks=155362rank=124900 of ranks=155362rank=125000 of ranks=155362rank=125100 of ranks=155362rank=125200 of ranks=155362rank=125300 of ranks=155362rank=125400 of ranks=155362rank=125500 of ranks=155362rank=125600 of ranks=155362rank=125700 of ranks=155362rank=125800 of ranks=155362rank=125900 of ranks=155362rank=126000 of ranks=155362rank=126100 of ranks=155362rank=126200 of ranks=155362rank=126300 of ranks=155362rank=126400 of ranks=155362rank=126500 of ranks=155362rank=126600 of ranks=155362rank=126700 of ranks=155362rank=126800 of ranks=155362rank=126900 of ranks=155362rank=127000 of ranks=155362rank=127100 of ranks=155362rank=127200 of ranks=155362rank=127300 of ranks=155362rank=127400 of ranks=155362rank=127500 of ranks=155362rank=127600 of ranks=155362rank=127700 of ranks=155362rank=127800 of ranks=155362rank=127900 of ranks=155362rank=128000 of ranks=155362rank=128100 of ranks=155362rank=128200 of ranks=155362rank=128300 of ranks=155362rank=128400 of ranks=155362rank=128500 of ranks=155362rank=128600 of ranks=155362rank=128700 of ranks=155362rank=128800 of ranks=155362rank=128900 of ranks=155362rank=129000 of ranks=155362rank=129100 of ranks=155362rank=129200 of ranks=155362rank=129300 of ranks=155362rank=129400 of ranks=155362rank=129500 of ranks=155362rank=129600 of ranks=155362rank=129700 of ranks=155362rank=129800 of ranks=155362rank=129900 of ranks=155362rank=130000 of ranks=155362rank=130100 of ranks=155362rank=130200 of ranks=155362rank=130300 of ranks=155362rank=130400 of ranks=155362rank=130500 of ranks=155362rank=130600 of ranks=155362rank=130700 of ranks=155362rank=130800 of ranks=155362rank=130900 of ranks=155362rank=131000 of ranks=155362rank=131100 of ranks=155362rank=131200 of ranks=155362rank=131300 of ranks=155362rank=131400 of ranks=155362rank=131500 of ranks=155362rank=131600 of ranks=155362rank=131700 of ranks=155362rank=131800 of ranks=155362rank=131900 of ranks=155362rank=132000 of ranks=155362rank=132100 of ranks=155362rank=132200 of ranks=155362rank=132300 of ranks=155362rank=132400 of ranks=155362rank=132500 of ranks=155362rank=132600 of ranks=155362rank=132700 of ranks=155362rank=132800 of ranks=155362rank=132900 of ranks=155362rank=133000 of ranks=155362rank=133100 of ranks=155362rank=133200 of ranks=155362rank=133300 of ranks=155362rank=133400 of ranks=155362rank=133500 of ranks=155362rank=133600 of ranks=155362rank=133700 of ranks=155362rank=133800 of ranks=155362rank=133900 of ranks=155362rank=134000 of ranks=155362rank=134100 of ranks=155362rank=134200 of ranks=155362rank=134300 of ranks=155362rank=134400 of ranks=155362rank=134500 of ranks=155362rank=134600 of ranks=155362rank=134700 of ranks=155362rank=134800 of ranks=155362rank=134900 of ranks=155362rank=135000 of ranks=155362rank=135100 of ranks=155362rank=135200 of ranks=155362rank=135300 of ranks=155362rank=135400 of ranks=155362rank=135500 of ranks=155362rank=135600 of ranks=155362rank=135700 of ranks=155362rank=135800 of ranks=155362rank=135900 of ranks=155362rank=136000 of ranks=155362rank=136100 of ranks=155362rank=136200 of ranks=155362rank=136300 of ranks=155362rank=136400 of ranks=155362rank=136500 of ranks=155362rank=136600 of ranks=155362rank=136700 of ranks=155362rank=136800 of ranks=155362rank=136900 of ranks=155362rank=137000 of ranks=155362rank=137100 of ranks=155362rank=137200 of ranks=155362rank=137300 of ranks=155362rank=137400 of ranks=155362rank=137500 of ranks=155362rank=137600 of ranks=155362rank=137700 of ranks=155362rank=137800 of ranks=155362rank=137900 of ranks=155362rank=138000 of ranks=155362rank=138100 of ranks=155362rank=138200 of ranks=155362rank=138300 of ranks=155362rank=138400 of ranks=155362rank=138500 of ranks=155362rank=138600 of ranks=155362rank=138700 of ranks=155362rank=138800 of ranks=155362rank=138900 of ranks=155362rank=139000 of ranks=155362rank=139100 of ranks=155362rank=139200 of ranks=155362rank=139300 of ranks=155362rank=139400 of ranks=155362rank=139500 of ranks=155362rank=139600 of ranks=155362rank=139700 of ranks=155362rank=139800 of ranks=155362rank=139900 of ranks=155362rank=140000 of ranks=155362rank=140100 of ranks=155362rank=140200 of ranks=155362rank=140300 of ranks=155362rank=140400 of ranks=155362rank=140500 of ranks=155362rank=140600 of ranks=155362rank=140700 of ranks=155362rank=140800 of ranks=155362rank=140900 of ranks=155362rank=141000 of ranks=155362rank=141100 of ranks=155362rank=141200 of ranks=155362rank=141300 of ranks=155362rank=141400 of ranks=155362rank=141500 of ranks=155362rank=141600 of ranks=155362rank=141700 of ranks=155362rank=141800 of ranks=155362rank=141900 of ranks=155362rank=142000 of ranks=155362rank=142100 of ranks=155362rank=142200 of ranks=155362rank=142300 of ranks=155362rank=142400 of ranks=155362rank=142500 of ranks=155362rank=142600 of ranks=155362rank=142700 of ranks=155362rank=142800 of ranks=155362rank=142900 of ranks=155362rank=143000 of ranks=155362rank=143100 of ranks=155362rank=143200 of ranks=155362rank=143300 of ranks=155362rank=143400 of ranks=155362rank=143500 of ranks=155362rank=143600 of ranks=155362rank=143700 of ranks=155362rank=143800 of ranks=155362rank=143900 of ranks=155362rank=144000 of ranks=155362rank=144100 of ranks=155362rank=144200 of ranks=155362rank=144300 of ranks=155362rank=144400 of ranks=155362rank=144500 of ranks=155362rank=144600 of ranks=155362rank=144700 of ranks=155362rank=144800 of ranks=155362rank=144900 of ranks=155362rank=145000 of ranks=155362rank=145100 of ranks=155362rank=145200 of ranks=155362rank=145300 of ranks=155362rank=145400 of ranks=155362rank=145500 of ranks=155362rank=145600 of ranks=155362rank=145700 of ranks=155362rank=145800 of ranks=155362rank=145900 of ranks=155362rank=146000 of ranks=155362rank=146100 of ranks=155362rank=146200 of ranks=155362rank=146300 of ranks=155362rank=146400 of ranks=155362rank=146500 of ranks=155362rank=146600 of ranks=155362rank=146700 of ranks=155362rank=146800 of ranks=155362rank=146900 of ranks=155362rank=147000 of ranks=155362rank=147100 of ranks=155362rank=147200 of ranks=155362rank=147300 of ranks=155362rank=147400 of ranks=155362rank=147500 of ranks=155362rank=147600 of ranks=155362rank=147700 of ranks=155362rank=147800 of ranks=155362rank=147900 of ranks=155362rank=148000 of ranks=155362rank=148100 of ranks=155362rank=148200 of ranks=155362rank=148300 of ranks=155362rank=148400 of ranks=155362rank=148500 of ranks=155362rank=148600 of ranks=155362rank=148700 of ranks=155362rank=148800 of ranks=155362rank=148900 of ranks=155362rank=149000 of ranks=155362rank=149100 of ranks=155362rank=149200 of ranks=155362rank=149300 of ranks=155362rank=149400 of ranks=155362rank=149500 of ranks=155362rank=149600 of ranks=155362rank=149700 of ranks=155362rank=149800 of ranks=155362rank=149900 of ranks=155362rank=150000 of ranks=155362rank=150100 of ranks=155362rank=150200 of ranks=155362rank=150300 of ranks=155362rank=150400 of ranks=155362rank=150500 of ranks=155362rank=150600 of ranks=155362rank=150700 of ranks=155362rank=150800 of ranks=155362rank=150900 of ranks=155362rank=151000 of ranks=155362rank=151100 of ranks=155362rank=151200 of ranks=155362rank=151300 of ranks=155362rank=151400 of ranks=155362rank=151500 of ranks=155362rank=151600 of ranks=155362rank=151700 of ranks=155362rank=151800 of ranks=155362rank=151900 of ranks=155362rank=152000 of ranks=155362rank=152100 of ranks=155362rank=152200 of ranks=155362rank=152300 of ranks=155362rank=152400 of ranks=155362rank=152500 of ranks=155362rank=152600 of ranks=155362rank=152700 of ranks=155362rank=152800 of ranks=155362rank=152900 of ranks=155362rank=153000 of ranks=155362rank=153100 of ranks=155362rank=153200 of ranks=155362rank=153300 of ranks=155362rank=153400 of ranks=155362rank=153500 of ranks=155362rank=153600 of ranks=155362rank=153700 of ranks=155362rank=153800 of ranks=155362rank=153900 of ranks=155362rank=154000 of ranks=155362rank=154100 of ranks=155362rank=154200 of ranks=155362rank=154300 of ranks=155362rank=154400 of ranks=155362rank=154500 of ranks=155362rank=154600 of ranks=155362rank=154700 of ranks=155362rank=154800 of ranks=155362rank=154900 of ranks=155362rank=155000 of ranks=155362rank=155100 of ranks=155362rank=155200 of ranks=155362rank=155300 of ranks=155362

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              87.2819    467   8503     31    498           62.1874
   1 car                    97.5023  49860  43444    456  50316           78.4111
   2 truck                  91.5555   1786  20976     39   1825           61.5997
   3 bus                    84.1873    354   7915     12    366           39.2553
   4 pedestrian             87.3153   3883  18174    376   4259           71.9301

for conf_thresh=0.25, precision=0.89, recall=0.92, F1 score=0.90
for conf_thresh=0.25, TP=52604, FP=6750, FN=4660, average IoU=76.92%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=89.57%
Total detection time: 210 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
4249: loss=5.437, avg loss=4.794, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 271936 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4250: loss=4.111, avg loss=4.725, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 272000 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4251: loss=3.839, avg loss=4.637, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 272064 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4252: loss=3.595, avg loss=4.532, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.8 seconds, train=3.2 seconds, 272128 images, time remaining=3.6 hours
4253: loss=3.853, avg loss=4.465, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 272192 images, time remaining=3.6 hours
4254: loss=4.319, avg loss=4.450, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=3.3 seconds, 272256 images, time remaining=3.6 hours
4255: loss=4.157, avg loss=4.421, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 272320 images, time remaining=3.6 hours
4256: loss=4.235, avg loss=4.402, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 272384 images, time remaining=3.6 hours
4257: loss=3.348, avg loss=4.297, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 272448 images, time remaining=3.6 hours
4258: loss=3.553, avg loss=4.222, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=773.4 milliseconds, train=3.3 seconds, 272512 images, time remaining=3.6 hours
4259: loss=4.536, avg loss=4.254, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 272576 images, time remaining=3.6 hours
4260: loss=3.930, avg loss=4.221, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=3.3 seconds, 272640 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4261: loss=3.787, avg loss=4.178, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=983.5 milliseconds, train=3.6 seconds, 272704 images, time remaining=3.6 hours
4262: loss=4.113, avg loss=4.172, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=3.6 seconds, 272768 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4263: loss=4.482, avg loss=4.203, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.5 seconds, train=3.6 seconds, 272832 images, time remaining=3.6 hours
4264: loss=5.586, avg loss=4.341, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=3.6 seconds, 272896 images, time remaining=3.6 hours
4265: loss=3.994, avg loss=4.306, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=3.6 seconds, 272960 images, time remaining=3.6 hours
4266: loss=3.899, avg loss=4.265, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=919.3 milliseconds, train=3.6 seconds, 273024 images, time remaining=3.6 hours
4267: loss=3.008, avg loss=4.140, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 273088 images, time remaining=3.6 hours
4268: loss=4.071, avg loss=4.133, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 273152 images, time remaining=3.6 hours
4269: loss=3.755, avg loss=4.095, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=3.6 seconds, 273216 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4270: loss=4.041, avg loss=4.090, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=3.6 seconds, 273280 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145d65000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4271: loss=5.037, avg loss=4.184, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 273344 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4272: loss=4.300, avg loss=4.196, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=1.4 seconds, 273408 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4273: loss=3.600, avg loss=4.136, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 273472 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4274: loss=4.024, avg loss=4.125, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.6 seconds, train=1.4 seconds, 273536 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4275: loss=4.344, avg loss=4.147, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 273600 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4276: loss=3.375, avg loss=4.070, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=1.4 seconds, 273664 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4277: loss=3.309, avg loss=3.994, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 273728 images, time remaining=3.6 hours
4278: loss=3.572, avg loss=3.952, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 273792 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4279: loss=3.384, avg loss=3.895, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 273856 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4280: loss=3.646, avg loss=3.870, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 273920 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4281: loss=3.991, avg loss=3.882, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=813.1 milliseconds, train=1.9 seconds, 273984 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4282: loss=3.911, avg loss=3.885, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.9 seconds, train=2.0 seconds, 274048 images, time remaining=3.6 hours
4283: loss=3.831, avg loss=3.879, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 274112 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4284: loss=3.237, avg loss=3.815, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=1.9 seconds, 274176 images, time remaining=3.6 hours
4285: loss=3.401, avg loss=3.774, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 274240 images, time remaining=3.6 hours
4286: loss=3.038, avg loss=3.700, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 274304 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4287: loss=2.861, avg loss=3.616, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 274368 images, time remaining=3.6 hours
4288: loss=3.596, avg loss=3.614, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=887.8 milliseconds, train=2.0 seconds, 274432 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4289: loss=3.315, avg loss=3.584, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 274496 images, time remaining=3.6 hours
4290: loss=3.748, avg loss=3.601, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 274560 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145d56000000
4291: loss=4.409, avg loss=3.682, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 274624 images, time remaining=3.6 hours
4292: loss=3.554, avg loss=3.669, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 274688 images, time remaining=3.6 hours
4293: loss=4.052, avg loss=3.707, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=666.8 milliseconds, train=1.5 seconds, 274752 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4294: loss=4.209, avg loss=3.757, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.8 seconds, train=1.4 seconds, 274816 images, time remaining=3.6 hours
4295: loss=3.972, avg loss=3.779, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 274880 images, time remaining=3.6 hours
4296: loss=3.757, avg loss=3.777, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=872.6 milliseconds, train=1.5 seconds, 274944 images, time remaining=3.6 hours
4297: loss=4.048, avg loss=3.804, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.4 seconds, 275008 images, time remaining=3.6 hours
4298: loss=2.707, avg loss=3.694, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=986.0 milliseconds, train=1.4 seconds, 275072 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4299: loss=3.970, avg loss=3.722, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 275136 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4300: loss=3.725, avg loss=3.722, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 275200 images, time remaining=3.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4301: loss=6.771, avg loss=4.027, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 275264 images, time remaining=3.6 hours
4302: loss=6.014, avg loss=4.226, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=893.1 milliseconds, train=5.1 seconds, 275328 images, time remaining=3.6 hours
4303: loss=5.566, avg loss=4.360, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=717.6 milliseconds, train=5.0 seconds, 275392 images, time remaining=3.6 hours
4304: loss=4.484, avg loss=4.372, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=5.0 seconds, 275456 images, time remaining=3.6 hours
4305: loss=4.457, avg loss=4.381, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.0 seconds, train=5.0 seconds, 275520 images, time remaining=3.6 hours
4306: loss=4.718, avg loss=4.414, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=5.0 seconds, 275584 images, time remaining=3.6 hours
4307: loss=5.785, avg loss=4.551, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=847.5 milliseconds, train=5.0 seconds, 275648 images, time remaining=3.6 hours
4308: loss=5.855, avg loss=4.682, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=5.0 seconds, 275712 images, time remaining=3.6 hours
4309: loss=5.047, avg loss=4.718, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.6 seconds, train=5.0 seconds, 275776 images, time remaining=3.6 hours
4310: loss=4.807, avg loss=4.727, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=5.0 seconds, 275840 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145910000000
4311: loss=4.524, avg loss=4.707, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 275904 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4312: loss=4.702, avg loss=4.706, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=1.6 seconds, 275968 images, time remaining=3.6 hours
4313: loss=4.107, avg loss=4.646, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 276032 images, time remaining=3.6 hours
4314: loss=4.430, avg loss=4.625, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=1.7 seconds, 276096 images, time remaining=3.6 hours
4315: loss=3.714, avg loss=4.534, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 276160 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4316: loss=3.593, avg loss=4.440, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 276224 images, time remaining=3.6 hours
4317: loss=4.213, avg loss=4.417, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 276288 images, time remaining=3.6 hours
4318: loss=3.644, avg loss=4.340, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 276352 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4319: loss=4.037, avg loss=4.310, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.1 seconds, train=1.6 seconds, 276416 images, time remaining=3.6 hours
4320: loss=3.990, avg loss=4.278, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=908.1 milliseconds, train=1.7 seconds, 276480 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4321: loss=6.336, avg loss=4.483, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=3.6 seconds, 276544 images, time remaining=3.6 hours
4322: loss=5.399, avg loss=4.575, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=3.6 seconds, 276608 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4323: loss=4.523, avg loss=4.570, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.9 seconds, train=3.6 seconds, 276672 images, time remaining=3.6 hours
4324: loss=4.470, avg loss=4.560, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=3.6 seconds, 276736 images, time remaining=3.6 hours
4325: loss=4.638, avg loss=4.568, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 276800 images, time remaining=3.6 hours
4326: loss=4.381, avg loss=4.549, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=991.3 milliseconds, train=3.6 seconds, 276864 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4327: loss=3.767, avg loss=4.471, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.2 seconds, train=3.6 seconds, 276928 images, time remaining=3.6 hours
4328: loss=3.625, avg loss=4.386, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=3.6 seconds, 276992 images, time remaining=3.6 hours
4329: loss=3.991, avg loss=4.347, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 277056 images, time remaining=3.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4330: loss=5.031, avg loss=4.415, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=5.2 seconds, train=3.6 seconds, 277120 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4331: loss=4.406, avg loss=4.414, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 277184 images, time remaining=3.6 hours
4332: loss=4.723, avg loss=4.445, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 277248 images, time remaining=3.6 hours
4333: loss=4.806, avg loss=4.481, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.8 seconds, train=4.5 seconds, 277312 images, time remaining=3.6 hours
4334: loss=4.489, avg loss=4.482, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 277376 images, time remaining=3.6 hours
4335: loss=4.378, avg loss=4.472, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 277440 images, time remaining=3.6 hours
4336: loss=5.043, avg loss=4.529, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=4.5 seconds, 277504 images, time remaining=3.6 hours
4337: loss=4.642, avg loss=4.540, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=4.5 seconds, 277568 images, time remaining=3.6 hours
4338: loss=4.154, avg loss=4.502, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 277632 images, time remaining=3.6 hours
4339: loss=4.668, avg loss=4.518, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.2 seconds, train=4.5 seconds, 277696 images, time remaining=3.6 hours
4340: loss=4.651, avg loss=4.532, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.9 seconds, train=4.5 seconds, 277760 images, time remaining=3.6 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4341: loss=4.187, avg loss=4.497, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 277824 images, time remaining=3.6 hours
4342: loss=4.976, avg loss=4.545, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=981.2 milliseconds, train=2.1 seconds, 277888 images, time remaining=3.6 hours
4343: loss=3.559, avg loss=4.446, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=931.0 milliseconds, train=2.1 seconds, 277952 images, time remaining=3.6 hours
4344: loss=3.206, avg loss=4.322, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 278016 images, time remaining=3.6 hours
4345: loss=4.073, avg loss=4.297, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 278080 images, time remaining=3.6 hours
4346: loss=4.396, avg loss=4.307, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 278144 images, time remaining=3.6 hours
4347: loss=3.808, avg loss=4.257, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=2.1 seconds, 278208 images, time remaining=3.6 hours
4348: loss=3.662, avg loss=4.198, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=851.1 milliseconds, train=2.1 seconds, 278272 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4349: loss=3.556, avg loss=4.134, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 278336 images, time remaining=3.5 hours
4350: loss=3.242, avg loss=4.044, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=917.7 milliseconds, train=2.1 seconds, 278400 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4351: loss=3.900, avg loss=4.030, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=950.8 milliseconds, train=3.4 seconds, 278464 images, time remaining=3.5 hours
4352: loss=3.906, avg loss=4.018, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 278528 images, time remaining=3.5 hours
4353: loss=4.745, avg loss=4.090, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=875.9 milliseconds, train=3.4 seconds, 278592 images, time remaining=3.5 hours
4354: loss=3.867, avg loss=4.068, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=3.4 seconds, 278656 images, time remaining=3.5 hours
4355: loss=4.132, avg loss=4.074, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=3.4 seconds, 278720 images, time remaining=3.5 hours
4356: loss=4.156, avg loss=4.083, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=871.5 milliseconds, train=3.4 seconds, 278784 images, time remaining=3.5 hours
4357: loss=3.395, avg loss=4.014, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=3.4 seconds, 278848 images, time remaining=3.5 hours
4358: loss=4.085, avg loss=4.021, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 278912 images, time remaining=3.5 hours
4359: loss=3.985, avg loss=4.017, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 278976 images, time remaining=3.5 hours
4360: loss=4.170, avg loss=4.033, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 279040 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4361: loss=3.483, avg loss=3.978, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=4.1 seconds, 279104 images, time remaining=3.5 hours
4362: loss=2.603, avg loss=3.840, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 279168 images, time remaining=3.5 hours
4363: loss=3.310, avg loss=3.787, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 279232 images, time remaining=3.5 hours
4364: loss=2.937, avg loss=3.702, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=4.1 seconds, 279296 images, time remaining=3.5 hours
4365: loss=3.863, avg loss=3.718, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.6 seconds, train=4.1 seconds, 279360 images, time remaining=3.5 hours
4366: loss=3.029, avg loss=3.649, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 279424 images, time remaining=3.5 hours
4367: loss=3.966, avg loss=3.681, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 279488 images, time remaining=3.5 hours
4368: loss=4.607, avg loss=3.774, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 279552 images, time remaining=3.5 hours
4369: loss=3.842, avg loss=3.781, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.6 seconds, train=4.1 seconds, 279616 images, time remaining=3.5 hours
4370: loss=4.080, avg loss=3.810, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=953.2 milliseconds, train=4.1 seconds, 279680 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14558fc00000
4371: loss=3.698, avg loss=3.799, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=787.4 milliseconds, train=1.6 seconds, 279744 images, time remaining=3.5 hours
4372: loss=3.069, avg loss=3.726, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=718.7 milliseconds, train=1.6 seconds, 279808 images, time remaining=3.5 hours
4373: loss=3.996, avg loss=3.753, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=777.3 milliseconds, train=1.6 seconds, 279872 images, time remaining=3.5 hours
4374: loss=3.563, avg loss=3.734, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 279936 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4375: loss=4.410, avg loss=3.802, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=1.6 seconds, 280000 images, time remaining=3.5 hours
4376: loss=3.686, avg loss=3.790, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 280064 images, time remaining=3.5 hours
4377: loss=4.017, avg loss=3.813, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=832.2 milliseconds, train=1.6 seconds, 280128 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4378: loss=3.540, avg loss=3.786, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 280192 images, time remaining=3.5 hours
4379: loss=3.353, avg loss=3.742, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=922.9 milliseconds, train=1.6 seconds, 280256 images, time remaining=3.5 hours
4380: loss=3.265, avg loss=3.695, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 280320 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4381: loss=6.492, avg loss=3.974, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=4.4 seconds, 280384 images, time remaining=3.5 hours
4382: loss=4.377, avg loss=4.015, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=4.4 seconds, 280448 images, time remaining=3.5 hours
4383: loss=5.630, avg loss=4.176, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 280512 images, time remaining=3.5 hours
4384: loss=4.222, avg loss=4.181, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=4.4 seconds, 280576 images, time remaining=3.5 hours
4385: loss=3.171, avg loss=4.080, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=561.9 milliseconds, train=4.4 seconds, 280640 images, time remaining=3.5 hours
4386: loss=4.410, avg loss=4.113, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=4.4 seconds, 280704 images, time remaining=3.5 hours
4387: loss=3.768, avg loss=4.078, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 280768 images, time remaining=3.5 hours
4388: loss=3.653, avg loss=4.036, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=4.4 seconds, 280832 images, time remaining=3.5 hours
4389: loss=4.408, avg loss=4.073, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.1 seconds, train=4.4 seconds, 280896 images, time remaining=3.5 hours
4390: loss=4.705, avg loss=4.136, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=4.4 seconds, 280960 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145bda000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4391: loss=4.995, avg loss=4.222, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=1.6 seconds, 281024 images, time remaining=3.5 hours
4392: loss=5.240, avg loss=4.324, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 281088 images, time remaining=3.5 hours
4393: loss=4.430, avg loss=4.334, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=957.4 milliseconds, train=1.6 seconds, 281152 images, time remaining=3.5 hours
4394: loss=4.780, avg loss=4.379, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=912.1 milliseconds, train=1.6 seconds, 281216 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4395: loss=3.742, avg loss=4.315, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 281280 images, time remaining=3.5 hours
4396: loss=3.884, avg loss=4.272, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 281344 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4397: loss=3.142, avg loss=4.159, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.5 seconds, train=1.6 seconds, 281408 images, time remaining=3.5 hours
4398: loss=3.428, avg loss=4.086, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 281472 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4399: loss=3.553, avg loss=4.033, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 281536 images, time remaining=3.5 hours
4400: loss=3.676, avg loss=3.997, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 281600 images, time remaining=3.5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x146016000000
4401: loss=3.383, avg loss=3.936, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=773.6 milliseconds, train=1.4 seconds, 281664 images, time remaining=3.5 hours
4402: loss=3.516, avg loss=3.894, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=898.5 milliseconds, train=1.4 seconds, 281728 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4403: loss=3.670, avg loss=3.871, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.0 seconds, train=1.4 seconds, 281792 images, time remaining=3.5 hours
4404: loss=3.468, avg loss=3.831, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=873.9 milliseconds, train=1.4 seconds, 281856 images, time remaining=3.5 hours
4405: loss=3.054, avg loss=3.753, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=560.6 milliseconds, train=1.4 seconds, 281920 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4406: loss=3.737, avg loss=3.752, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 281984 images, time remaining=3.5 hours
4407: loss=4.319, avg loss=3.808, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=866.7 milliseconds, train=1.4 seconds, 282048 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4408: loss=3.350, avg loss=3.763, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 282112 images, time remaining=3.5 hours
4409: loss=3.782, avg loss=3.764, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=489.0 milliseconds, train=1.4 seconds, 282176 images, time remaining=3.5 hours
4410: loss=3.750, avg loss=3.763, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=814.5 milliseconds, train=1.4 seconds, 282240 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146016000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4411: loss=3.993, avg loss=3.786, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 282304 images, time remaining=3.5 hours
4412: loss=3.077, avg loss=3.715, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=696.3 milliseconds, train=1.2 seconds, 282368 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4413: loss=3.731, avg loss=3.717, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 282432 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4414: loss=3.567, avg loss=3.702, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 282496 images, time remaining=3.5 hours
4415: loss=2.846, avg loss=3.616, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=617.1 milliseconds, train=1.2 seconds, 282560 images, time remaining=3.5 hours
4416: loss=3.574, avg loss=3.612, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=996.5 milliseconds, train=1.2 seconds, 282624 images, time remaining=3.5 hours
4417: loss=3.528, avg loss=3.604, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=688.4 milliseconds, train=1.2 seconds, 282688 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4418: loss=3.150, avg loss=3.558, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 282752 images, time remaining=3.5 hours
4419: loss=3.443, avg loss=3.547, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=784.1 milliseconds, train=1.2 seconds, 282816 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4420: loss=2.914, avg loss=3.483, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 282880 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4421: loss=5.619, avg loss=3.697, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=755.9 milliseconds, train=4.4 seconds, 282944 images, time remaining=3.5 hours
4422: loss=5.532, avg loss=3.881, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=4.4 seconds, 283008 images, time remaining=3.5 hours
4423: loss=5.211, avg loss=4.014, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=4.4 seconds, 283072 images, time remaining=3.5 hours
4424: loss=4.084, avg loss=4.021, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=4.5 seconds, 283136 images, time remaining=3.5 hours
4425: loss=3.970, avg loss=4.016, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=4.4 seconds, 283200 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4426: loss=3.564, avg loss=3.970, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.7 seconds, train=4.4 seconds, 283264 images, time remaining=3.5 hours
4427: loss=4.607, avg loss=4.034, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=689.3 milliseconds, train=4.4 seconds, 283328 images, time remaining=3.5 hours
4428: loss=4.544, avg loss=4.085, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=4.4 seconds, 283392 images, time remaining=3.5 hours
4429: loss=4.583, avg loss=4.135, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.3 seconds, train=4.4 seconds, 283456 images, time remaining=3.5 hours
4430: loss=3.656, avg loss=4.087, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=4.4 seconds, 283520 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4431: loss=4.044, avg loss=4.083, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 283584 images, time remaining=3.5 hours
4432: loss=5.059, avg loss=4.180, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.1 seconds, train=5.1 seconds, 283648 images, time remaining=3.5 hours
4433: loss=4.184, avg loss=4.181, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 283712 images, time remaining=3.5 hours
4434: loss=4.806, avg loss=4.243, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=691.9 milliseconds, train=5.1 seconds, 283776 images, time remaining=3.5 hours
4435: loss=5.092, avg loss=4.328, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 283840 images, time remaining=3.5 hours
4436: loss=4.255, avg loss=4.321, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 283904 images, time remaining=3.5 hours
4437: loss=5.497, avg loss=4.439, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=5.1 seconds, 283968 images, time remaining=3.5 hours
4438: loss=3.884, avg loss=4.383, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 284032 images, time remaining=3.5 hours
4439: loss=4.716, avg loss=4.416, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=5.1 seconds, 284096 images, time remaining=3.5 hours
4440: loss=4.783, avg loss=4.453, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=5.1 seconds, 284160 images, time remaining=3.5 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4441: loss=4.117, avg loss=4.419, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 284224 images, time remaining=3.5 hours
4442: loss=3.726, avg loss=4.350, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=838.0 milliseconds, train=2.1 seconds, 284288 images, time remaining=3.5 hours
4443: loss=4.443, avg loss=4.359, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=787.1 milliseconds, train=2.1 seconds, 284352 images, time remaining=3.5 hours
4444: loss=3.945, avg loss=4.318, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 284416 images, time remaining=3.5 hours
4445: loss=3.249, avg loss=4.211, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=786.1 milliseconds, train=2.1 seconds, 284480 images, time remaining=3.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4446: loss=2.992, avg loss=4.089, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.4 seconds, train=2.1 seconds, 284544 images, time remaining=3.5 hours
4447: loss=3.353, avg loss=4.016, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=690.1 milliseconds, train=2.1 seconds, 284608 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4448: loss=3.654, avg loss=3.979, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.2 seconds, train=2.1 seconds, 284672 images, time remaining=3.4 hours
4449: loss=4.272, avg loss=4.009, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=853.5 milliseconds, train=2.1 seconds, 284736 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4450: loss=3.923, avg loss=4.000, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 284800 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4451: loss=3.124, avg loss=3.912, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=793.8 milliseconds, train=1.9 seconds, 284864 images, time remaining=3.4 hours
4452: loss=3.265, avg loss=3.848, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=677.4 milliseconds, train=1.9 seconds, 284928 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4453: loss=2.838, avg loss=3.747, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=1.9 seconds, 284992 images, time remaining=3.4 hours
4454: loss=4.013, avg loss=3.773, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 285056 images, time remaining=3.4 hours
4455: loss=3.863, avg loss=3.782, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 285120 images, time remaining=3.4 hours
4456: loss=2.765, avg loss=3.681, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.9 seconds, 285184 images, time remaining=3.4 hours
4457: loss=3.674, avg loss=3.680, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=663.8 milliseconds, train=1.9 seconds, 285248 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4458: loss=2.938, avg loss=3.606, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=1.9 seconds, 285312 images, time remaining=3.4 hours
4459: loss=3.408, avg loss=3.586, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=1.9 seconds, 285376 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4460: loss=4.095, avg loss=3.637, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=1.9 seconds, 285440 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x144fa2a00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4461: loss=3.880, avg loss=3.661, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 285504 images, time remaining=3.4 hours
4462: loss=4.374, avg loss=3.733, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=692.1 milliseconds, train=1.2 seconds, 285568 images, time remaining=3.4 hours
4463: loss=3.286, avg loss=3.688, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=551.0 milliseconds, train=1.2 seconds, 285632 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4464: loss=3.429, avg loss=3.662, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.5 seconds, train=1.2 seconds, 285696 images, time remaining=3.4 hours
4465: loss=3.538, avg loss=3.650, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 285760 images, time remaining=3.4 hours
4466: loss=3.875, avg loss=3.672, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=846.1 milliseconds, train=1.2 seconds, 285824 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4467: loss=3.158, avg loss=3.621, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 285888 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4468: loss=3.638, avg loss=3.622, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 285952 images, time remaining=3.4 hours
4469: loss=3.544, avg loss=3.615, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=856.2 milliseconds, train=1.2 seconds, 286016 images, time remaining=3.4 hours
4470: loss=3.591, avg loss=3.612, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=883.0 milliseconds, train=1.2 seconds, 286080 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4471: loss=3.507, avg loss=3.602, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 286144 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4472: loss=3.555, avg loss=3.597, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.3 seconds, train=2.1 seconds, 286208 images, time remaining=3.4 hours
4473: loss=3.000, avg loss=3.537, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 286272 images, time remaining=3.4 hours
4474: loss=3.744, avg loss=3.558, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=950.4 milliseconds, train=2.1 seconds, 286336 images, time remaining=3.4 hours
4475: loss=2.888, avg loss=3.491, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=809.2 milliseconds, train=2.1 seconds, 286400 images, time remaining=3.4 hours
4476: loss=3.163, avg loss=3.458, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 286464 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4477: loss=3.657, avg loss=3.478, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=2.1 seconds, 286528 images, time remaining=3.4 hours
4478: loss=3.417, avg loss=3.472, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=561.0 milliseconds, train=2.1 seconds, 286592 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4479: loss=3.935, avg loss=3.518, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.2 seconds, train=2.1 seconds, 286656 images, time remaining=3.4 hours
4480: loss=4.524, avg loss=3.619, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 286720 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4481: loss=4.312, avg loss=3.688, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=932.3 milliseconds, train=1.9 seconds, 286784 images, time remaining=3.4 hours
4482: loss=3.691, avg loss=3.688, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=737.4 milliseconds, train=1.9 seconds, 286848 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4483: loss=3.614, avg loss=3.681, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=1.9 seconds, 286912 images, time remaining=3.4 hours
4484: loss=3.397, avg loss=3.653, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.9 seconds, 286976 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4485: loss=3.161, avg loss=3.603, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 287040 images, time remaining=3.4 hours
4486: loss=3.629, avg loss=3.606, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=1.9 seconds, 287104 images, time remaining=3.4 hours
4487: loss=3.720, avg loss=3.617, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=850.0 milliseconds, train=1.9 seconds, 287168 images, time remaining=3.4 hours
4488: loss=3.097, avg loss=3.565, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=622.2 milliseconds, train=1.9 seconds, 287232 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4489: loss=3.053, avg loss=3.514, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 287296 images, time remaining=3.4 hours
4490: loss=3.159, avg loss=3.479, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=1.9 seconds, 287360 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146452000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4491: loss=3.787, avg loss=3.509, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.3 seconds, train=1.4 seconds, 287424 images, time remaining=3.4 hours
4492: loss=3.610, avg loss=3.519, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=852.0 milliseconds, train=1.4 seconds, 287488 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4493: loss=2.716, avg loss=3.439, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.1 seconds, train=1.4 seconds, 287552 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4494: loss=3.469, avg loss=3.442, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 287616 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4495: loss=3.309, avg loss=3.429, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.8 seconds, train=1.4 seconds, 287680 images, time remaining=3.4 hours
4496: loss=2.945, avg loss=3.380, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=782.8 milliseconds, train=1.4 seconds, 287744 images, time remaining=3.4 hours
4497: loss=3.516, avg loss=3.394, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=927.9 milliseconds, train=1.5 seconds, 287808 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4498: loss=3.553, avg loss=3.410, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 287872 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4499: loss=3.472, avg loss=3.416, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 287936 images, time remaining=3.4 hours
4500: loss=3.645, avg loss=3.439, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=730.2 milliseconds, train=1.4 seconds, 288000 images, time remaining=3.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4501: loss=3.936, avg loss=3.489, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.5 seconds, train=3.2 seconds, 288064 images, time remaining=3.4 hours
4502: loss=2.897, avg loss=3.430, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 288128 images, time remaining=3.4 hours
4503: loss=3.530, avg loss=3.440, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 288192 images, time remaining=3.4 hours
4504: loss=3.014, avg loss=3.397, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=3.2 seconds, 288256 images, time remaining=3.4 hours
4505: loss=3.448, avg loss=3.402, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 288320 images, time remaining=3.4 hours
4506: loss=3.840, avg loss=3.446, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 288384 images, time remaining=3.4 hours
4507: loss=4.016, avg loss=3.503, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=3.3 seconds, 288448 images, time remaining=3.4 hours
4508: loss=3.648, avg loss=3.518, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=667.3 milliseconds, train=3.3 seconds, 288512 images, time remaining=3.4 hours
4509: loss=3.272, avg loss=3.493, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 288576 images, time remaining=3.4 hours
4510: loss=3.527, avg loss=3.496, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=733.9 milliseconds, train=3.2 seconds, 288640 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145f98000000
4511: loss=2.718, avg loss=3.419, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=644.6 milliseconds, train=1.5 seconds, 288704 images, time remaining=3.4 hours
4512: loss=3.021, avg loss=3.379, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=734.0 milliseconds, train=1.5 seconds, 288768 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4513: loss=2.544, avg loss=3.295, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 288832 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4514: loss=3.543, avg loss=3.320, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.5 seconds, train=1.5 seconds, 288896 images, time remaining=3.4 hours
4515: loss=3.353, avg loss=3.323, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 288960 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4516: loss=2.806, avg loss=3.272, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 289024 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4517: loss=2.935, avg loss=3.238, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 289088 images, time remaining=3.4 hours
4518: loss=4.076, avg loss=3.322, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 289152 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4519: loss=3.550, avg loss=3.345, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=1.5 seconds, 289216 images, time remaining=3.4 hours
4520: loss=3.638, avg loss=3.374, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 289280 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4521: loss=5.723, avg loss=3.609, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=4.8 seconds, 289344 images, time remaining=3.4 hours
4522: loss=5.300, avg loss=3.778, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=982.2 milliseconds, train=4.8 seconds, 289408 images, time remaining=3.4 hours
4523: loss=4.471, avg loss=3.847, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=4.8 seconds, 289472 images, time remaining=3.4 hours
4524: loss=5.279, avg loss=3.990, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=4.9 seconds, 289536 images, time remaining=3.4 hours
4525: loss=3.891, avg loss=3.980, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=4.8 seconds, 289600 images, time remaining=3.4 hours
4526: loss=4.775, avg loss=4.060, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=4.8 seconds, 289664 images, time remaining=3.4 hours
4527: loss=4.884, avg loss=4.142, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=902.9 milliseconds, train=4.8 seconds, 289728 images, time remaining=3.4 hours
4528: loss=4.413, avg loss=4.169, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=831.4 milliseconds, train=4.8 seconds, 289792 images, time remaining=3.4 hours
4529: loss=4.252, avg loss=4.178, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.4 seconds, train=4.8 seconds, 289856 images, time remaining=3.4 hours
4530: loss=4.680, avg loss=4.228, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.5 seconds, train=4.8 seconds, 289920 images, time remaining=3.4 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4531: loss=4.021, avg loss=4.207, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=484.5 milliseconds, train=3.2 seconds, 289984 images, time remaining=3.4 hours
4532: loss=4.492, avg loss=4.236, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=769.2 milliseconds, train=3.3 seconds, 290048 images, time remaining=3.4 hours
4533: loss=2.883, avg loss=4.100, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=3.2 seconds, 290112 images, time remaining=3.4 hours
4534: loss=3.923, avg loss=4.083, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.8 seconds, train=3.3 seconds, 290176 images, time remaining=3.4 hours
4535: loss=3.402, avg loss=4.015, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 290240 images, time remaining=3.4 hours
4536: loss=3.528, avg loss=3.966, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 290304 images, time remaining=3.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4537: loss=3.484, avg loss=3.918, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.5 seconds, train=3.2 seconds, 290368 images, time remaining=3.4 hours
4538: loss=3.718, avg loss=3.898, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 290432 images, time remaining=3.4 hours
4539: loss=3.768, avg loss=3.885, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=3.2 seconds, 290496 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4540: loss=3.991, avg loss=3.895, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 290560 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4541: loss=4.172, avg loss=3.923, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.9 seconds, train=2.1 seconds, 290624 images, time remaining=3.3 hours
4542: loss=3.746, avg loss=3.905, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 290688 images, time remaining=3.3 hours
4543: loss=3.717, avg loss=3.887, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=969.0 milliseconds, train=2.1 seconds, 290752 images, time remaining=3.3 hours
4544: loss=3.488, avg loss=3.847, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 290816 images, time remaining=3.3 hours
4545: loss=4.081, avg loss=3.870, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 290880 images, time remaining=3.3 hours
4546: loss=3.527, avg loss=3.836, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=983.6 milliseconds, train=2.1 seconds, 290944 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4547: loss=3.379, avg loss=3.790, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 291008 images, time remaining=3.3 hours
4548: loss=3.804, avg loss=3.792, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=922.7 milliseconds, train=2.1 seconds, 291072 images, time remaining=3.3 hours
4549: loss=3.179, avg loss=3.730, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 291136 images, time remaining=3.3 hours
4550: loss=3.368, avg loss=3.694, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 291200 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145078000000
4551: loss=4.365, avg loss=3.761, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=881.3 milliseconds, train=1.2 seconds, 291264 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4552: loss=3.532, avg loss=3.738, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 291328 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4553: loss=3.354, avg loss=3.700, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=1.2 seconds, 291392 images, time remaining=3.3 hours
4554: loss=3.707, avg loss=3.701, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 291456 images, time remaining=3.3 hours
4555: loss=2.978, avg loss=3.628, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 291520 images, time remaining=3.3 hours
4556: loss=3.569, avg loss=3.622, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=900.3 milliseconds, train=1.2 seconds, 291584 images, time remaining=3.3 hours
4557: loss=2.907, avg loss=3.551, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=952.0 milliseconds, train=1.2 seconds, 291648 images, time remaining=3.3 hours
4558: loss=2.805, avg loss=3.476, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=713.8 milliseconds, train=1.2 seconds, 291712 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4559: loss=3.614, avg loss=3.490, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=1.2 seconds, 291776 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4560: loss=3.984, avg loss=3.539, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 291840 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145f62400000
4561: loss=3.987, avg loss=3.584, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=793.4 milliseconds, train=1.4 seconds, 291904 images, time remaining=3.3 hours
4562: loss=4.042, avg loss=3.630, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=529.0 milliseconds, train=1.4 seconds, 291968 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4563: loss=3.534, avg loss=3.620, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 292032 images, time remaining=3.3 hours
4564: loss=3.327, avg loss=3.591, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=783.6 milliseconds, train=1.4 seconds, 292096 images, time remaining=3.3 hours
4565: loss=3.650, avg loss=3.597, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=707.7 milliseconds, train=1.4 seconds, 292160 images, time remaining=3.3 hours
4566: loss=3.768, avg loss=3.614, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=820.0 milliseconds, train=1.4 seconds, 292224 images, time remaining=3.3 hours
4567: loss=3.331, avg loss=3.586, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=591.7 milliseconds, train=1.4 seconds, 292288 images, time remaining=3.3 hours
4568: loss=3.025, avg loss=3.530, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 292352 images, time remaining=3.3 hours
4569: loss=3.528, avg loss=3.530, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 292416 images, time remaining=3.3 hours
4570: loss=3.315, avg loss=3.508, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=961.6 milliseconds, train=1.4 seconds, 292480 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4571: loss=3.755, avg loss=3.533, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=917.0 milliseconds, train=2.0 seconds, 292544 images, time remaining=3.3 hours
4572: loss=3.811, avg loss=3.561, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=808.5 milliseconds, train=2.0 seconds, 292608 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4573: loss=2.523, avg loss=3.457, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 292672 images, time remaining=3.3 hours
4574: loss=3.127, avg loss=3.424, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=681.4 milliseconds, train=2.0 seconds, 292736 images, time remaining=3.3 hours
4575: loss=2.831, avg loss=3.365, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=2.0 seconds, 292800 images, time remaining=3.3 hours
4576: loss=3.459, avg loss=3.374, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=2.0 seconds, 292864 images, time remaining=3.3 hours
4577: loss=3.703, avg loss=3.407, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 292928 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4578: loss=3.173, avg loss=3.384, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.6 seconds, train=1.9 seconds, 292992 images, time remaining=3.3 hours
4579: loss=2.614, avg loss=3.307, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=519.1 milliseconds, train=2.0 seconds, 293056 images, time remaining=3.3 hours
4580: loss=3.520, avg loss=3.328, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 293120 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4581: loss=5.051, avg loss=3.500, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=581.1 milliseconds, train=4.9 seconds, 293184 images, time remaining=3.3 hours
4582: loss=5.013, avg loss=3.651, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=711.0 milliseconds, train=4.9 seconds, 293248 images, time remaining=3.3 hours
4583: loss=4.016, avg loss=3.688, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=4.9 seconds, 293312 images, time remaining=3.3 hours
4584: loss=3.904, avg loss=3.710, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=4.9 seconds, 293376 images, time remaining=3.3 hours
4585: loss=3.705, avg loss=3.709, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.5 seconds, train=4.8 seconds, 293440 images, time remaining=3.3 hours
4586: loss=3.867, avg loss=3.725, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=4.8 seconds, 293504 images, time remaining=3.3 hours
4587: loss=4.290, avg loss=3.781, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=4.8 seconds, 293568 images, time remaining=3.3 hours
4588: loss=4.124, avg loss=3.816, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.4 seconds, train=4.8 seconds, 293632 images, time remaining=3.3 hours
4589: loss=4.138, avg loss=3.848, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=656.6 milliseconds, train=4.9 seconds, 293696 images, time remaining=3.3 hours
4590: loss=3.892, avg loss=3.852, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=4.8 seconds, 293760 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x14637a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4591: loss=4.022, avg loss=3.869, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.6 seconds, train=1.4 seconds, 293824 images, time remaining=3.3 hours
4592: loss=3.479, avg loss=3.830, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=719.6 milliseconds, train=1.4 seconds, 293888 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4593: loss=3.205, avg loss=3.768, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.6 seconds, train=1.4 seconds, 293952 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4594: loss=3.764, avg loss=3.767, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.3 seconds, train=1.4 seconds, 294016 images, time remaining=3.3 hours
4595: loss=3.332, avg loss=3.724, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=780.5 milliseconds, train=1.4 seconds, 294080 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4596: loss=3.724, avg loss=3.724, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.5 seconds, train=1.4 seconds, 294144 images, time remaining=3.3 hours
4597: loss=3.848, avg loss=3.736, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 294208 images, time remaining=3.3 hours
4598: loss=3.890, avg loss=3.752, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 294272 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4599: loss=4.050, avg loss=3.782, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 294336 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4600: loss=3.464, avg loss=3.750, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 294400 images, time remaining=3.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4601: loss=4.138, avg loss=3.789, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=654.4 milliseconds, train=2.1 seconds, 294464 images, time remaining=3.3 hours
4602: loss=3.509, avg loss=3.761, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 294528 images, time remaining=3.3 hours
4603: loss=3.865, avg loss=3.771, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 294592 images, time remaining=3.3 hours
4604: loss=3.762, avg loss=3.770, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=829.0 milliseconds, train=2.1 seconds, 294656 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4605: loss=3.327, avg loss=3.726, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 294720 images, time remaining=3.3 hours
4606: loss=4.409, avg loss=3.794, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=513.2 milliseconds, train=2.1 seconds, 294784 images, time remaining=3.3 hours
4607: loss=3.582, avg loss=3.773, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=694.6 milliseconds, train=2.1 seconds, 294848 images, time remaining=3.3 hours
4608: loss=3.730, avg loss=3.769, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 294912 images, time remaining=3.3 hours
4609: loss=3.375, avg loss=3.729, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 294976 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4610: loss=3.826, avg loss=3.739, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.4 seconds, train=2.1 seconds, 295040 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146028000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4611: loss=3.126, avg loss=3.678, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 295104 images, time remaining=3.3 hours
4612: loss=4.166, avg loss=3.727, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 295168 images, time remaining=3.3 hours
4613: loss=3.087, avg loss=3.663, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=988.1 milliseconds, train=1.5 seconds, 295232 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4614: loss=3.569, avg loss=3.653, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 295296 images, time remaining=3.3 hours
4615: loss=2.906, avg loss=3.578, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 295360 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4616: loss=3.287, avg loss=3.549, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.7 seconds, train=1.5 seconds, 295424 images, time remaining=3.3 hours
4617: loss=2.964, avg loss=3.491, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=971.9 milliseconds, train=1.5 seconds, 295488 images, time remaining=3.3 hours
4618: loss=3.476, avg loss=3.489, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 295552 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4619: loss=3.402, avg loss=3.481, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 295616 images, time remaining=3.3 hours
4620: loss=3.376, avg loss=3.470, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 295680 images, time remaining=3.3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146344000000
4621: loss=2.523, avg loss=3.375, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=844.3 milliseconds, train=1.2 seconds, 295744 images, time remaining=3.3 hours
4622: loss=3.187, avg loss=3.356, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=947.3 milliseconds, train=1.2 seconds, 295808 images, time remaining=3.3 hours
4623: loss=3.053, avg loss=3.326, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=899.3 milliseconds, train=1.2 seconds, 295872 images, time remaining=3.3 hours
4624: loss=4.107, avg loss=3.404, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=612.7 milliseconds, train=1.2 seconds, 295936 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4625: loss=3.715, avg loss=3.435, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.1 seconds, train=1.2 seconds, 296000 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4626: loss=3.276, avg loss=3.419, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 296064 images, time remaining=3.2 hours
4627: loss=3.372, avg loss=3.415, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=743.8 milliseconds, train=1.2 seconds, 296128 images, time remaining=3.2 hours
4628: loss=3.388, avg loss=3.412, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=553.2 milliseconds, train=1.2 seconds, 296192 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4629: loss=3.061, avg loss=3.377, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=4.4 seconds, train=1.2 seconds, 296256 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4630: loss=3.214, avg loss=3.361, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 296320 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4631: loss=3.769, avg loss=3.401, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=595.3 milliseconds, train=2.0 seconds, 296384 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4632: loss=3.118, avg loss=3.373, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 296448 images, time remaining=3.2 hours
4633: loss=3.140, avg loss=3.350, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=902.8 milliseconds, train=2.0 seconds, 296512 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4634: loss=3.123, avg loss=3.327, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=2.0 seconds, 296576 images, time remaining=3.2 hours
4635: loss=3.474, avg loss=3.342, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 296640 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4636: loss=3.236, avg loss=3.331, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.5 seconds, train=2.0 seconds, 296704 images, time remaining=3.2 hours
4637: loss=3.268, avg loss=3.325, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=630.9 milliseconds, train=2.0 seconds, 296768 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4638: loss=3.041, avg loss=3.296, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 296832 images, time remaining=3.2 hours
4639: loss=3.620, avg loss=3.329, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=593.3 milliseconds, train=2.0 seconds, 296896 images, time remaining=3.2 hours
4640: loss=3.072, avg loss=3.303, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 296960 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4641: loss=3.594, avg loss=3.332, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=600.1 milliseconds, train=3.4 seconds, 297024 images, time remaining=3.2 hours
4642: loss=3.201, avg loss=3.319, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 297088 images, time remaining=3.2 hours
4643: loss=4.477, avg loss=3.435, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=881.9 milliseconds, train=3.4 seconds, 297152 images, time remaining=3.2 hours
4644: loss=5.071, avg loss=3.599, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.0 seconds, train=3.4 seconds, 297216 images, time remaining=3.2 hours
4645: loss=3.643, avg loss=3.603, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=513.5 milliseconds, train=3.4 seconds, 297280 images, time remaining=3.2 hours
4646: loss=3.815, avg loss=3.624, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.5 seconds, train=3.4 seconds, 297344 images, time remaining=3.2 hours
4647: loss=3.624, avg loss=3.624, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=341.9 milliseconds, train=3.4 seconds, 297408 images, time remaining=3.2 hours
4648: loss=3.799, avg loss=3.642, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=631.1 milliseconds, train=3.4 seconds, 297472 images, time remaining=3.2 hours
4649: loss=3.509, avg loss=3.628, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 297536 images, time remaining=3.2 hours
4650: loss=3.163, avg loss=3.582, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.6 seconds, train=3.4 seconds, 297600 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4651: loss=3.028, avg loss=3.526, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=956.6 milliseconds, train=1.9 seconds, 297664 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4652: loss=3.423, avg loss=3.516, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.9 seconds, train=1.9 seconds, 297728 images, time remaining=3.2 hours
4653: loss=4.107, avg loss=3.575, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 297792 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4654: loss=3.402, avg loss=3.558, last=89.57%, best=89.57%, next=4654, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 297856 images, time remaining=3.2 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=117305, unique_truth_count=57264
rank=0 of ranks=117305rank=100 of ranks=117305rank=200 of ranks=117305rank=300 of ranks=117305rank=400 of ranks=117305rank=500 of ranks=117305rank=600 of ranks=117305rank=700 of ranks=117305rank=800 of ranks=117305rank=900 of ranks=117305rank=1000 of ranks=117305rank=1100 of ranks=117305rank=1200 of ranks=117305rank=1300 of ranks=117305rank=1400 of ranks=117305rank=1500 of ranks=117305rank=1600 of ranks=117305rank=1700 of ranks=117305rank=1800 of ranks=117305rank=1900 of ranks=117305rank=2000 of ranks=117305rank=2100 of ranks=117305rank=2200 of ranks=117305rank=2300 of ranks=117305rank=2400 of ranks=117305rank=2500 of ranks=117305rank=2600 of ranks=117305rank=2700 of ranks=117305rank=2800 of ranks=117305rank=2900 of ranks=117305rank=3000 of ranks=117305rank=3100 of ranks=117305rank=3200 of ranks=117305rank=3300 of ranks=117305rank=3400 of ranks=117305rank=3500 of ranks=117305rank=3600 of ranks=117305rank=3700 of ranks=117305rank=3800 of ranks=117305rank=3900 of ranks=117305rank=4000 of ranks=117305rank=4100 of ranks=117305rank=4200 of ranks=117305rank=4300 of ranks=117305rank=4400 of ranks=117305rank=4500 of ranks=117305rank=4600 of ranks=117305rank=4700 of ranks=117305rank=4800 of ranks=117305rank=4900 of ranks=117305rank=5000 of ranks=117305rank=5100 of ranks=117305rank=5200 of ranks=117305rank=5300 of ranks=117305rank=5400 of ranks=117305rank=5500 of ranks=117305rank=5600 of ranks=117305rank=5700 of ranks=117305rank=5800 of ranks=117305rank=5900 of ranks=117305rank=6000 of ranks=117305rank=6100 of ranks=117305rank=6200 of ranks=117305rank=6300 of ranks=117305rank=6400 of ranks=117305rank=6500 of ranks=117305rank=6600 of ranks=117305rank=6700 of ranks=117305rank=6800 of ranks=117305rank=6900 of ranks=117305rank=7000 of ranks=117305rank=7100 of ranks=117305rank=7200 of ranks=117305rank=7300 of ranks=117305rank=7400 of ranks=117305rank=7500 of ranks=117305rank=7600 of ranks=117305rank=7700 of ranks=117305rank=7800 of ranks=117305rank=7900 of ranks=117305rank=8000 of ranks=117305rank=8100 of ranks=117305rank=8200 of ranks=117305rank=8300 of ranks=117305rank=8400 of ranks=117305rank=8500 of ranks=117305rank=8600 of ranks=117305rank=8700 of ranks=117305rank=8800 of ranks=117305rank=8900 of ranks=117305rank=9000 of ranks=117305rank=9100 of ranks=117305rank=9200 of ranks=117305rank=9300 of ranks=117305rank=9400 of ranks=117305rank=9500 of ranks=117305rank=9600 of ranks=117305rank=9700 of ranks=117305rank=9800 of ranks=117305rank=9900 of ranks=117305rank=10000 of ranks=117305rank=10100 of ranks=117305rank=10200 of ranks=117305rank=10300 of ranks=117305rank=10400 of ranks=117305rank=10500 of ranks=117305rank=10600 of ranks=117305rank=10700 of ranks=117305rank=10800 of ranks=117305rank=10900 of ranks=117305rank=11000 of ranks=117305rank=11100 of ranks=117305rank=11200 of ranks=117305rank=11300 of ranks=117305rank=11400 of ranks=117305rank=11500 of ranks=117305rank=11600 of ranks=117305rank=11700 of ranks=117305rank=11800 of ranks=117305rank=11900 of ranks=117305rank=12000 of ranks=117305rank=12100 of ranks=117305rank=12200 of ranks=117305rank=12300 of ranks=117305rank=12400 of ranks=117305rank=12500 of ranks=117305rank=12600 of ranks=117305rank=12700 of ranks=117305rank=12800 of ranks=117305rank=12900 of ranks=117305rank=13000 of ranks=117305rank=13100 of ranks=117305rank=13200 of ranks=117305rank=13300 of ranks=117305rank=13400 of ranks=117305rank=13500 of ranks=117305rank=13600 of ranks=117305rank=13700 of ranks=117305rank=13800 of ranks=117305rank=13900 of ranks=117305rank=14000 of ranks=117305rank=14100 of ranks=117305rank=14200 of ranks=117305rank=14300 of ranks=117305rank=14400 of ranks=117305rank=14500 of ranks=117305rank=14600 of ranks=117305rank=14700 of ranks=117305rank=14800 of ranks=117305rank=14900 of ranks=117305rank=15000 of ranks=117305rank=15100 of ranks=117305rank=15200 of ranks=117305rank=15300 of ranks=117305rank=15400 of ranks=117305rank=15500 of ranks=117305rank=15600 of ranks=117305rank=15700 of ranks=117305rank=15800 of ranks=117305rank=15900 of ranks=117305rank=16000 of ranks=117305rank=16100 of ranks=117305rank=16200 of ranks=117305rank=16300 of ranks=117305rank=16400 of ranks=117305rank=16500 of ranks=117305rank=16600 of ranks=117305rank=16700 of ranks=117305rank=16800 of ranks=117305rank=16900 of ranks=117305rank=17000 of ranks=117305rank=17100 of ranks=117305rank=17200 of ranks=117305rank=17300 of ranks=117305rank=17400 of ranks=117305rank=17500 of ranks=117305rank=17600 of ranks=117305rank=17700 of ranks=117305rank=17800 of ranks=117305rank=17900 of ranks=117305rank=18000 of ranks=117305rank=18100 of ranks=117305rank=18200 of ranks=117305rank=18300 of ranks=117305rank=18400 of ranks=117305rank=18500 of ranks=117305rank=18600 of ranks=117305rank=18700 of ranks=117305rank=18800 of ranks=117305rank=18900 of ranks=117305rank=19000 of ranks=117305rank=19100 of ranks=117305rank=19200 of ranks=117305rank=19300 of ranks=117305rank=19400 of ranks=117305rank=19500 of ranks=117305rank=19600 of ranks=117305rank=19700 of ranks=117305rank=19800 of ranks=117305rank=19900 of ranks=117305rank=20000 of ranks=117305rank=20100 of ranks=117305rank=20200 of ranks=117305rank=20300 of ranks=117305rank=20400 of ranks=117305rank=20500 of ranks=117305rank=20600 of ranks=117305rank=20700 of ranks=117305rank=20800 of ranks=117305rank=20900 of ranks=117305rank=21000 of ranks=117305rank=21100 of ranks=117305rank=21200 of ranks=117305rank=21300 of ranks=117305rank=21400 of ranks=117305rank=21500 of ranks=117305rank=21600 of ranks=117305rank=21700 of ranks=117305rank=21800 of ranks=117305rank=21900 of ranks=117305rank=22000 of ranks=117305rank=22100 of ranks=117305rank=22200 of ranks=117305rank=22300 of ranks=117305rank=22400 of ranks=117305rank=22500 of ranks=117305rank=22600 of ranks=117305rank=22700 of ranks=117305rank=22800 of ranks=117305rank=22900 of ranks=117305rank=23000 of ranks=117305rank=23100 of ranks=117305rank=23200 of ranks=117305rank=23300 of ranks=117305rank=23400 of ranks=117305rank=23500 of ranks=117305rank=23600 of ranks=117305rank=23700 of ranks=117305rank=23800 of ranks=117305rank=23900 of ranks=117305rank=24000 of ranks=117305rank=24100 of ranks=117305rank=24200 of ranks=117305rank=24300 of ranks=117305rank=24400 of ranks=117305rank=24500 of ranks=117305rank=24600 of ranks=117305rank=24700 of ranks=117305rank=24800 of ranks=117305rank=24900 of ranks=117305rank=25000 of ranks=117305rank=25100 of ranks=117305rank=25200 of ranks=117305rank=25300 of ranks=117305rank=25400 of ranks=117305rank=25500 of ranks=117305rank=25600 of ranks=117305rank=25700 of ranks=117305rank=25800 of ranks=117305rank=25900 of ranks=117305rank=26000 of ranks=117305rank=26100 of ranks=117305rank=26200 of ranks=117305rank=26300 of ranks=117305rank=26400 of ranks=117305rank=26500 of ranks=117305rank=26600 of ranks=117305rank=26700 of ranks=117305rank=26800 of ranks=117305rank=26900 of ranks=117305rank=27000 of ranks=117305rank=27100 of ranks=117305rank=27200 of ranks=117305rank=27300 of ranks=117305rank=27400 of ranks=117305rank=27500 of ranks=117305rank=27600 of ranks=117305rank=27700 of ranks=117305rank=27800 of ranks=117305rank=27900 of ranks=117305rank=28000 of ranks=117305rank=28100 of ranks=117305rank=28200 of ranks=117305rank=28300 of ranks=117305rank=28400 of ranks=117305rank=28500 of ranks=117305rank=28600 of ranks=117305rank=28700 of ranks=117305rank=28800 of ranks=117305rank=28900 of ranks=117305rank=29000 of ranks=117305rank=29100 of ranks=117305rank=29200 of ranks=117305rank=29300 of ranks=117305rank=29400 of ranks=117305rank=29500 of ranks=117305rank=29600 of ranks=117305rank=29700 of ranks=117305rank=29800 of ranks=117305rank=29900 of ranks=117305rank=30000 of ranks=117305rank=30100 of ranks=117305rank=30200 of ranks=117305rank=30300 of ranks=117305rank=30400 of ranks=117305rank=30500 of ranks=117305rank=30600 of ranks=117305rank=30700 of ranks=117305rank=30800 of ranks=117305rank=30900 of ranks=117305rank=31000 of ranks=117305rank=31100 of ranks=117305rank=31200 of ranks=117305rank=31300 of ranks=117305rank=31400 of ranks=117305rank=31500 of ranks=117305rank=31600 of ranks=117305rank=31700 of ranks=117305rank=31800 of ranks=117305rank=31900 of ranks=117305rank=32000 of ranks=117305rank=32100 of ranks=117305rank=32200 of ranks=117305rank=32300 of ranks=117305rank=32400 of ranks=117305rank=32500 of ranks=117305rank=32600 of ranks=117305rank=32700 of ranks=117305rank=32800 of ranks=117305rank=32900 of ranks=117305rank=33000 of ranks=117305rank=33100 of ranks=117305rank=33200 of ranks=117305rank=33300 of ranks=117305rank=33400 of ranks=117305rank=33500 of ranks=117305rank=33600 of ranks=117305rank=33700 of ranks=117305rank=33800 of ranks=117305rank=33900 of ranks=117305rank=34000 of ranks=117305rank=34100 of ranks=117305rank=34200 of ranks=117305rank=34300 of ranks=117305rank=34400 of ranks=117305rank=34500 of ranks=117305rank=34600 of ranks=117305rank=34700 of ranks=117305rank=34800 of ranks=117305rank=34900 of ranks=117305rank=35000 of ranks=117305rank=35100 of ranks=117305rank=35200 of ranks=117305rank=35300 of ranks=117305rank=35400 of ranks=117305rank=35500 of ranks=117305rank=35600 of ranks=117305rank=35700 of ranks=117305rank=35800 of ranks=117305rank=35900 of ranks=117305rank=36000 of ranks=117305rank=36100 of ranks=117305rank=36200 of ranks=117305rank=36300 of ranks=117305rank=36400 of ranks=117305rank=36500 of ranks=117305rank=36600 of ranks=117305rank=36700 of ranks=117305rank=36800 of ranks=117305rank=36900 of ranks=117305rank=37000 of ranks=117305rank=37100 of ranks=117305rank=37200 of ranks=117305rank=37300 of ranks=117305rank=37400 of ranks=117305rank=37500 of ranks=117305rank=37600 of ranks=117305rank=37700 of ranks=117305rank=37800 of ranks=117305rank=37900 of ranks=117305rank=38000 of ranks=117305rank=38100 of ranks=117305rank=38200 of ranks=117305rank=38300 of ranks=117305rank=38400 of ranks=117305rank=38500 of ranks=117305rank=38600 of ranks=117305rank=38700 of ranks=117305rank=38800 of ranks=117305rank=38900 of ranks=117305rank=39000 of ranks=117305rank=39100 of ranks=117305rank=39200 of ranks=117305rank=39300 of ranks=117305rank=39400 of ranks=117305rank=39500 of ranks=117305rank=39600 of ranks=117305rank=39700 of ranks=117305rank=39800 of ranks=117305rank=39900 of ranks=117305rank=40000 of ranks=117305rank=40100 of ranks=117305rank=40200 of ranks=117305rank=40300 of ranks=117305rank=40400 of ranks=117305rank=40500 of ranks=117305rank=40600 of ranks=117305rank=40700 of ranks=117305rank=40800 of ranks=117305rank=40900 of ranks=117305rank=41000 of ranks=117305rank=41100 of ranks=117305rank=41200 of ranks=117305rank=41300 of ranks=117305rank=41400 of ranks=117305rank=41500 of ranks=117305rank=41600 of ranks=117305rank=41700 of ranks=117305rank=41800 of ranks=117305rank=41900 of ranks=117305rank=42000 of ranks=117305rank=42100 of ranks=117305rank=42200 of ranks=117305rank=42300 of ranks=117305rank=42400 of ranks=117305rank=42500 of ranks=117305rank=42600 of ranks=117305rank=42700 of ranks=117305rank=42800 of ranks=117305rank=42900 of ranks=117305rank=43000 of ranks=117305rank=43100 of ranks=117305rank=43200 of ranks=117305rank=43300 of ranks=117305rank=43400 of ranks=117305rank=43500 of ranks=117305rank=43600 of ranks=117305rank=43700 of ranks=117305rank=43800 of ranks=117305rank=43900 of ranks=117305rank=44000 of ranks=117305rank=44100 of ranks=117305rank=44200 of ranks=117305rank=44300 of ranks=117305rank=44400 of ranks=117305rank=44500 of ranks=117305rank=44600 of ranks=117305rank=44700 of ranks=117305rank=44800 of ranks=117305rank=44900 of ranks=117305rank=45000 of ranks=117305rank=45100 of ranks=117305rank=45200 of ranks=117305rank=45300 of ranks=117305rank=45400 of ranks=117305rank=45500 of ranks=117305rank=45600 of ranks=117305rank=45700 of ranks=117305rank=45800 of ranks=117305rank=45900 of ranks=117305rank=46000 of ranks=117305rank=46100 of ranks=117305rank=46200 of ranks=117305rank=46300 of ranks=117305rank=46400 of ranks=117305rank=46500 of ranks=117305rank=46600 of ranks=117305rank=46700 of ranks=117305rank=46800 of ranks=117305rank=46900 of ranks=117305rank=47000 of ranks=117305rank=47100 of ranks=117305rank=47200 of ranks=117305rank=47300 of ranks=117305rank=47400 of ranks=117305rank=47500 of ranks=117305rank=47600 of ranks=117305rank=47700 of ranks=117305rank=47800 of ranks=117305rank=47900 of ranks=117305rank=48000 of ranks=117305rank=48100 of ranks=117305rank=48200 of ranks=117305rank=48300 of ranks=117305rank=48400 of ranks=117305rank=48500 of ranks=117305rank=48600 of ranks=117305rank=48700 of ranks=117305rank=48800 of ranks=117305rank=48900 of ranks=117305rank=49000 of ranks=117305rank=49100 of ranks=117305rank=49200 of ranks=117305rank=49300 of ranks=117305rank=49400 of ranks=117305rank=49500 of ranks=117305rank=49600 of ranks=117305rank=49700 of ranks=117305rank=49800 of ranks=117305rank=49900 of ranks=117305rank=50000 of ranks=117305rank=50100 of ranks=117305rank=50200 of ranks=117305rank=50300 of ranks=117305rank=50400 of ranks=117305rank=50500 of ranks=117305rank=50600 of ranks=117305rank=50700 of ranks=117305rank=50800 of ranks=117305rank=50900 of ranks=117305rank=51000 of ranks=117305rank=51100 of ranks=117305rank=51200 of ranks=117305rank=51300 of ranks=117305rank=51400 of ranks=117305rank=51500 of ranks=117305rank=51600 of ranks=117305rank=51700 of ranks=117305rank=51800 of ranks=117305rank=51900 of ranks=117305rank=52000 of ranks=117305rank=52100 of ranks=117305rank=52200 of ranks=117305rank=52300 of ranks=117305rank=52400 of ranks=117305rank=52500 of ranks=117305rank=52600 of ranks=117305rank=52700 of ranks=117305rank=52800 of ranks=117305rank=52900 of ranks=117305rank=53000 of ranks=117305rank=53100 of ranks=117305rank=53200 of ranks=117305rank=53300 of ranks=117305rank=53400 of ranks=117305rank=53500 of ranks=117305rank=53600 of ranks=117305rank=53700 of ranks=117305rank=53800 of ranks=117305rank=53900 of ranks=117305rank=54000 of ranks=117305rank=54100 of ranks=117305rank=54200 of ranks=117305rank=54300 of ranks=117305rank=54400 of ranks=117305rank=54500 of ranks=117305rank=54600 of ranks=117305rank=54700 of ranks=117305rank=54800 of ranks=117305rank=54900 of ranks=117305rank=55000 of ranks=117305rank=55100 of ranks=117305rank=55200 of ranks=117305rank=55300 of ranks=117305rank=55400 of ranks=117305rank=55500 of ranks=117305rank=55600 of ranks=117305rank=55700 of ranks=117305rank=55800 of ranks=117305rank=55900 of ranks=117305rank=56000 of ranks=117305rank=56100 of ranks=117305rank=56200 of ranks=117305rank=56300 of ranks=117305rank=56400 of ranks=117305rank=56500 of ranks=117305rank=56600 of ranks=117305rank=56700 of ranks=117305rank=56800 of ranks=117305rank=56900 of ranks=117305rank=57000 of ranks=117305rank=57100 of ranks=117305rank=57200 of ranks=117305rank=57300 of ranks=117305rank=57400 of ranks=117305rank=57500 of ranks=117305rank=57600 of ranks=117305rank=57700 of ranks=117305rank=57800 of ranks=117305rank=57900 of ranks=117305rank=58000 of ranks=117305rank=58100 of ranks=117305rank=58200 of ranks=117305rank=58300 of ranks=117305rank=58400 of ranks=117305rank=58500 of ranks=117305rank=58600 of ranks=117305rank=58700 of ranks=117305rank=58800 of ranks=117305rank=58900 of ranks=117305rank=59000 of ranks=117305rank=59100 of ranks=117305rank=59200 of ranks=117305rank=59300 of ranks=117305rank=59400 of ranks=117305rank=59500 of ranks=117305rank=59600 of ranks=117305rank=59700 of ranks=117305rank=59800 of ranks=117305rank=59900 of ranks=117305rank=60000 of ranks=117305rank=60100 of ranks=117305rank=60200 of ranks=117305rank=60300 of ranks=117305rank=60400 of ranks=117305rank=60500 of ranks=117305rank=60600 of ranks=117305rank=60700 of ranks=117305rank=60800 of ranks=117305rank=60900 of ranks=117305rank=61000 of ranks=117305rank=61100 of ranks=117305rank=61200 of ranks=117305rank=61300 of ranks=117305rank=61400 of ranks=117305rank=61500 of ranks=117305rank=61600 of ranks=117305rank=61700 of ranks=117305rank=61800 of ranks=117305rank=61900 of ranks=117305rank=62000 of ranks=117305rank=62100 of ranks=117305rank=62200 of ranks=117305rank=62300 of ranks=117305rank=62400 of ranks=117305rank=62500 of ranks=117305rank=62600 of ranks=117305rank=62700 of ranks=117305rank=62800 of ranks=117305rank=62900 of ranks=117305rank=63000 of ranks=117305rank=63100 of ranks=117305rank=63200 of ranks=117305rank=63300 of ranks=117305rank=63400 of ranks=117305rank=63500 of ranks=117305rank=63600 of ranks=117305rank=63700 of ranks=117305rank=63800 of ranks=117305rank=63900 of ranks=117305rank=64000 of ranks=117305rank=64100 of ranks=117305rank=64200 of ranks=117305rank=64300 of ranks=117305rank=64400 of ranks=117305rank=64500 of ranks=117305rank=64600 of ranks=117305rank=64700 of ranks=117305rank=64800 of ranks=117305rank=64900 of ranks=117305rank=65000 of ranks=117305rank=65100 of ranks=117305rank=65200 of ranks=117305rank=65300 of ranks=117305rank=65400 of ranks=117305rank=65500 of ranks=117305rank=65600 of ranks=117305rank=65700 of ranks=117305rank=65800 of ranks=117305rank=65900 of ranks=117305rank=66000 of ranks=117305rank=66100 of ranks=117305rank=66200 of ranks=117305rank=66300 of ranks=117305rank=66400 of ranks=117305rank=66500 of ranks=117305rank=66600 of ranks=117305rank=66700 of ranks=117305rank=66800 of ranks=117305rank=66900 of ranks=117305rank=67000 of ranks=117305rank=67100 of ranks=117305rank=67200 of ranks=117305rank=67300 of ranks=117305rank=67400 of ranks=117305rank=67500 of ranks=117305rank=67600 of ranks=117305rank=67700 of ranks=117305rank=67800 of ranks=117305rank=67900 of ranks=117305rank=68000 of ranks=117305rank=68100 of ranks=117305rank=68200 of ranks=117305rank=68300 of ranks=117305rank=68400 of ranks=117305rank=68500 of ranks=117305rank=68600 of ranks=117305rank=68700 of ranks=117305rank=68800 of ranks=117305rank=68900 of ranks=117305rank=69000 of ranks=117305rank=69100 of ranks=117305rank=69200 of ranks=117305rank=69300 of ranks=117305rank=69400 of ranks=117305rank=69500 of ranks=117305rank=69600 of ranks=117305rank=69700 of ranks=117305rank=69800 of ranks=117305rank=69900 of ranks=117305rank=70000 of ranks=117305rank=70100 of ranks=117305rank=70200 of ranks=117305rank=70300 of ranks=117305rank=70400 of ranks=117305rank=70500 of ranks=117305rank=70600 of ranks=117305rank=70700 of ranks=117305rank=70800 of ranks=117305rank=70900 of ranks=117305rank=71000 of ranks=117305rank=71100 of ranks=117305rank=71200 of ranks=117305rank=71300 of ranks=117305rank=71400 of ranks=117305rank=71500 of ranks=117305rank=71600 of ranks=117305rank=71700 of ranks=117305rank=71800 of ranks=117305rank=71900 of ranks=117305rank=72000 of ranks=117305rank=72100 of ranks=117305rank=72200 of ranks=117305rank=72300 of ranks=117305rank=72400 of ranks=117305rank=72500 of ranks=117305rank=72600 of ranks=117305rank=72700 of ranks=117305rank=72800 of ranks=117305rank=72900 of ranks=117305rank=73000 of ranks=117305rank=73100 of ranks=117305rank=73200 of ranks=117305rank=73300 of ranks=117305rank=73400 of ranks=117305rank=73500 of ranks=117305rank=73600 of ranks=117305rank=73700 of ranks=117305rank=73800 of ranks=117305rank=73900 of ranks=117305rank=74000 of ranks=117305rank=74100 of ranks=117305rank=74200 of ranks=117305rank=74300 of ranks=117305rank=74400 of ranks=117305rank=74500 of ranks=117305rank=74600 of ranks=117305rank=74700 of ranks=117305rank=74800 of ranks=117305rank=74900 of ranks=117305rank=75000 of ranks=117305rank=75100 of ranks=117305rank=75200 of ranks=117305rank=75300 of ranks=117305rank=75400 of ranks=117305rank=75500 of ranks=117305rank=75600 of ranks=117305rank=75700 of ranks=117305rank=75800 of ranks=117305rank=75900 of ranks=117305rank=76000 of ranks=117305rank=76100 of ranks=117305rank=76200 of ranks=117305rank=76300 of ranks=117305rank=76400 of ranks=117305rank=76500 of ranks=117305rank=76600 of ranks=117305rank=76700 of ranks=117305rank=76800 of ranks=117305rank=76900 of ranks=117305rank=77000 of ranks=117305rank=77100 of ranks=117305rank=77200 of ranks=117305rank=77300 of ranks=117305rank=77400 of ranks=117305rank=77500 of ranks=117305rank=77600 of ranks=117305rank=77700 of ranks=117305rank=77800 of ranks=117305rank=77900 of ranks=117305rank=78000 of ranks=117305rank=78100 of ranks=117305rank=78200 of ranks=117305rank=78300 of ranks=117305rank=78400 of ranks=117305rank=78500 of ranks=117305rank=78600 of ranks=117305rank=78700 of ranks=117305rank=78800 of ranks=117305rank=78900 of ranks=117305rank=79000 of ranks=117305rank=79100 of ranks=117305rank=79200 of ranks=117305rank=79300 of ranks=117305rank=79400 of ranks=117305rank=79500 of ranks=117305rank=79600 of ranks=117305rank=79700 of ranks=117305rank=79800 of ranks=117305rank=79900 of ranks=117305rank=80000 of ranks=117305rank=80100 of ranks=117305rank=80200 of ranks=117305rank=80300 of ranks=117305rank=80400 of ranks=117305rank=80500 of ranks=117305rank=80600 of ranks=117305rank=80700 of ranks=117305rank=80800 of ranks=117305rank=80900 of ranks=117305rank=81000 of ranks=117305rank=81100 of ranks=117305rank=81200 of ranks=117305rank=81300 of ranks=117305rank=81400 of ranks=117305rank=81500 of ranks=117305rank=81600 of ranks=117305rank=81700 of ranks=117305rank=81800 of ranks=117305rank=81900 of ranks=117305rank=82000 of ranks=117305rank=82100 of ranks=117305rank=82200 of ranks=117305rank=82300 of ranks=117305rank=82400 of ranks=117305rank=82500 of ranks=117305rank=82600 of ranks=117305rank=82700 of ranks=117305rank=82800 of ranks=117305rank=82900 of ranks=117305rank=83000 of ranks=117305rank=83100 of ranks=117305rank=83200 of ranks=117305rank=83300 of ranks=117305rank=83400 of ranks=117305rank=83500 of ranks=117305rank=83600 of ranks=117305rank=83700 of ranks=117305rank=83800 of ranks=117305rank=83900 of ranks=117305rank=84000 of ranks=117305rank=84100 of ranks=117305rank=84200 of ranks=117305rank=84300 of ranks=117305rank=84400 of ranks=117305rank=84500 of ranks=117305rank=84600 of ranks=117305rank=84700 of ranks=117305rank=84800 of ranks=117305rank=84900 of ranks=117305rank=85000 of ranks=117305rank=85100 of ranks=117305rank=85200 of ranks=117305rank=85300 of ranks=117305rank=85400 of ranks=117305rank=85500 of ranks=117305rank=85600 of ranks=117305rank=85700 of ranks=117305rank=85800 of ranks=117305rank=85900 of ranks=117305rank=86000 of ranks=117305rank=86100 of ranks=117305rank=86200 of ranks=117305rank=86300 of ranks=117305rank=86400 of ranks=117305rank=86500 of ranks=117305rank=86600 of ranks=117305rank=86700 of ranks=117305rank=86800 of ranks=117305rank=86900 of ranks=117305rank=87000 of ranks=117305rank=87100 of ranks=117305rank=87200 of ranks=117305rank=87300 of ranks=117305rank=87400 of ranks=117305rank=87500 of ranks=117305rank=87600 of ranks=117305rank=87700 of ranks=117305rank=87800 of ranks=117305rank=87900 of ranks=117305rank=88000 of ranks=117305rank=88100 of ranks=117305rank=88200 of ranks=117305rank=88300 of ranks=117305rank=88400 of ranks=117305rank=88500 of ranks=117305rank=88600 of ranks=117305rank=88700 of ranks=117305rank=88800 of ranks=117305rank=88900 of ranks=117305rank=89000 of ranks=117305rank=89100 of ranks=117305rank=89200 of ranks=117305rank=89300 of ranks=117305rank=89400 of ranks=117305rank=89500 of ranks=117305rank=89600 of ranks=117305rank=89700 of ranks=117305rank=89800 of ranks=117305rank=89900 of ranks=117305rank=90000 of ranks=117305rank=90100 of ranks=117305rank=90200 of ranks=117305rank=90300 of ranks=117305rank=90400 of ranks=117305rank=90500 of ranks=117305rank=90600 of ranks=117305rank=90700 of ranks=117305rank=90800 of ranks=117305rank=90900 of ranks=117305rank=91000 of ranks=117305rank=91100 of ranks=117305rank=91200 of ranks=117305rank=91300 of ranks=117305rank=91400 of ranks=117305rank=91500 of ranks=117305rank=91600 of ranks=117305rank=91700 of ranks=117305rank=91800 of ranks=117305rank=91900 of ranks=117305rank=92000 of ranks=117305rank=92100 of ranks=117305rank=92200 of ranks=117305rank=92300 of ranks=117305rank=92400 of ranks=117305rank=92500 of ranks=117305rank=92600 of ranks=117305rank=92700 of ranks=117305rank=92800 of ranks=117305rank=92900 of ranks=117305rank=93000 of ranks=117305rank=93100 of ranks=117305rank=93200 of ranks=117305rank=93300 of ranks=117305rank=93400 of ranks=117305rank=93500 of ranks=117305rank=93600 of ranks=117305rank=93700 of ranks=117305rank=93800 of ranks=117305rank=93900 of ranks=117305rank=94000 of ranks=117305rank=94100 of ranks=117305rank=94200 of ranks=117305rank=94300 of ranks=117305rank=94400 of ranks=117305rank=94500 of ranks=117305rank=94600 of ranks=117305rank=94700 of ranks=117305rank=94800 of ranks=117305rank=94900 of ranks=117305rank=95000 of ranks=117305rank=95100 of ranks=117305rank=95200 of ranks=117305rank=95300 of ranks=117305rank=95400 of ranks=117305rank=95500 of ranks=117305rank=95600 of ranks=117305rank=95700 of ranks=117305rank=95800 of ranks=117305rank=95900 of ranks=117305rank=96000 of ranks=117305rank=96100 of ranks=117305rank=96200 of ranks=117305rank=96300 of ranks=117305rank=96400 of ranks=117305rank=96500 of ranks=117305rank=96600 of ranks=117305rank=96700 of ranks=117305rank=96800 of ranks=117305rank=96900 of ranks=117305rank=97000 of ranks=117305rank=97100 of ranks=117305rank=97200 of ranks=117305rank=97300 of ranks=117305rank=97400 of ranks=117305rank=97500 of ranks=117305rank=97600 of ranks=117305rank=97700 of ranks=117305rank=97800 of ranks=117305rank=97900 of ranks=117305rank=98000 of ranks=117305rank=98100 of ranks=117305rank=98200 of ranks=117305rank=98300 of ranks=117305rank=98400 of ranks=117305rank=98500 of ranks=117305rank=98600 of ranks=117305rank=98700 of ranks=117305rank=98800 of ranks=117305rank=98900 of ranks=117305rank=99000 of ranks=117305rank=99100 of ranks=117305rank=99200 of ranks=117305rank=99300 of ranks=117305rank=99400 of ranks=117305rank=99500 of ranks=117305rank=99600 of ranks=117305rank=99700 of ranks=117305rank=99800 of ranks=117305rank=99900 of ranks=117305rank=100000 of ranks=117305rank=100100 of ranks=117305rank=100200 of ranks=117305rank=100300 of ranks=117305rank=100400 of ranks=117305rank=100500 of ranks=117305rank=100600 of ranks=117305rank=100700 of ranks=117305rank=100800 of ranks=117305rank=100900 of ranks=117305rank=101000 of ranks=117305rank=101100 of ranks=117305rank=101200 of ranks=117305rank=101300 of ranks=117305rank=101400 of ranks=117305rank=101500 of ranks=117305rank=101600 of ranks=117305rank=101700 of ranks=117305rank=101800 of ranks=117305rank=101900 of ranks=117305rank=102000 of ranks=117305rank=102100 of ranks=117305rank=102200 of ranks=117305rank=102300 of ranks=117305rank=102400 of ranks=117305rank=102500 of ranks=117305rank=102600 of ranks=117305rank=102700 of ranks=117305rank=102800 of ranks=117305rank=102900 of ranks=117305rank=103000 of ranks=117305rank=103100 of ranks=117305rank=103200 of ranks=117305rank=103300 of ranks=117305rank=103400 of ranks=117305rank=103500 of ranks=117305rank=103600 of ranks=117305rank=103700 of ranks=117305rank=103800 of ranks=117305rank=103900 of ranks=117305rank=104000 of ranks=117305rank=104100 of ranks=117305rank=104200 of ranks=117305rank=104300 of ranks=117305rank=104400 of ranks=117305rank=104500 of ranks=117305rank=104600 of ranks=117305rank=104700 of ranks=117305rank=104800 of ranks=117305rank=104900 of ranks=117305rank=105000 of ranks=117305rank=105100 of ranks=117305rank=105200 of ranks=117305rank=105300 of ranks=117305rank=105400 of ranks=117305rank=105500 of ranks=117305rank=105600 of ranks=117305rank=105700 of ranks=117305rank=105800 of ranks=117305rank=105900 of ranks=117305rank=106000 of ranks=117305rank=106100 of ranks=117305rank=106200 of ranks=117305rank=106300 of ranks=117305rank=106400 of ranks=117305rank=106500 of ranks=117305rank=106600 of ranks=117305rank=106700 of ranks=117305rank=106800 of ranks=117305rank=106900 of ranks=117305rank=107000 of ranks=117305rank=107100 of ranks=117305rank=107200 of ranks=117305rank=107300 of ranks=117305rank=107400 of ranks=117305rank=107500 of ranks=117305rank=107600 of ranks=117305rank=107700 of ranks=117305rank=107800 of ranks=117305rank=107900 of ranks=117305rank=108000 of ranks=117305rank=108100 of ranks=117305rank=108200 of ranks=117305rank=108300 of ranks=117305rank=108400 of ranks=117305rank=108500 of ranks=117305rank=108600 of ranks=117305rank=108700 of ranks=117305rank=108800 of ranks=117305rank=108900 of ranks=117305rank=109000 of ranks=117305rank=109100 of ranks=117305rank=109200 of ranks=117305rank=109300 of ranks=117305rank=109400 of ranks=117305rank=109500 of ranks=117305rank=109600 of ranks=117305rank=109700 of ranks=117305rank=109800 of ranks=117305rank=109900 of ranks=117305rank=110000 of ranks=117305rank=110100 of ranks=117305rank=110200 of ranks=117305rank=110300 of ranks=117305rank=110400 of ranks=117305rank=110500 of ranks=117305rank=110600 of ranks=117305rank=110700 of ranks=117305rank=110800 of ranks=117305rank=110900 of ranks=117305rank=111000 of ranks=117305rank=111100 of ranks=117305rank=111200 of ranks=117305rank=111300 of ranks=117305rank=111400 of ranks=117305rank=111500 of ranks=117305rank=111600 of ranks=117305rank=111700 of ranks=117305rank=111800 of ranks=117305rank=111900 of ranks=117305rank=112000 of ranks=117305rank=112100 of ranks=117305rank=112200 of ranks=117305rank=112300 of ranks=117305rank=112400 of ranks=117305rank=112500 of ranks=117305rank=112600 of ranks=117305rank=112700 of ranks=117305rank=112800 of ranks=117305rank=112900 of ranks=117305rank=113000 of ranks=117305rank=113100 of ranks=117305rank=113200 of ranks=117305rank=113300 of ranks=117305rank=113400 of ranks=117305rank=113500 of ranks=117305rank=113600 of ranks=117305rank=113700 of ranks=117305rank=113800 of ranks=117305rank=113900 of ranks=117305rank=114000 of ranks=117305rank=114100 of ranks=117305rank=114200 of ranks=117305rank=114300 of ranks=117305rank=114400 of ranks=117305rank=114500 of ranks=117305rank=114600 of ranks=117305rank=114700 of ranks=117305rank=114800 of ranks=117305rank=114900 of ranks=117305rank=115000 of ranks=117305rank=115100 of ranks=117305rank=115200 of ranks=117305rank=115300 of ranks=117305rank=115400 of ranks=117305rank=115500 of ranks=117305rank=115600 of ranks=117305rank=115700 of ranks=117305rank=115800 of ranks=117305rank=115900 of ranks=117305rank=116000 of ranks=117305rank=116100 of ranks=117305rank=116200 of ranks=117305rank=116300 of ranks=117305rank=116400 of ranks=117305rank=116500 of ranks=117305rank=116600 of ranks=117305rank=116700 of ranks=117305rank=116800 of ranks=117305rank=116900 of ranks=117305rank=117000 of ranks=117305rank=117100 of ranks=117305rank=117200 of ranks=117305rank=117300 of ranks=117305

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              93.8985    478   2338     20    498           70.2150
   1 car                    98.2505  49980  37422    336  50316           78.7279
   2 truck                  94.1061   1802   9957     23   1825           58.4464
   3 bus                    88.5525    361   3487      5    366           61.4398
   4 pedestrian             94.1471   4103   7377    156   4259           69.1920

for conf_thresh=0.25, precision=0.87, recall=0.95, F1 score=0.91
for conf_thresh=0.25, TP=54351, FP=7793, FN=2913, average IoU=77.14%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=93.79%
Total detection time: 177 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
4655: loss=3.561, avg loss=3.558, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=990.5 milliseconds, train=2.1 seconds, 297920 images, time remaining=3.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4656: loss=3.487, avg loss=3.551, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 297984 images, time remaining=3.3 hours
4657: loss=3.531, avg loss=3.549, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 298048 images, time remaining=3.3 hours
4658: loss=3.162, avg loss=3.510, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=489.4 milliseconds, train=2.1 seconds, 298112 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4659: loss=3.231, avg loss=3.482, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 298176 images, time remaining=3.2 hours
4660: loss=2.813, avg loss=3.415, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 298240 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4661: loss=4.191, avg loss=3.493, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.7 seconds, train=3.9 seconds, 298304 images, time remaining=3.2 hours
4662: loss=3.957, avg loss=3.539, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=784.1 milliseconds, train=3.9 seconds, 298368 images, time remaining=3.2 hours
4663: loss=3.625, avg loss=3.548, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=3.9 seconds, 298432 images, time remaining=3.2 hours
4664: loss=3.517, avg loss=3.545, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 298496 images, time remaining=3.2 hours
4665: loss=3.153, avg loss=3.506, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 298560 images, time remaining=3.2 hours
4666: loss=4.039, avg loss=3.559, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 298624 images, time remaining=3.2 hours
4667: loss=4.133, avg loss=3.616, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.2 seconds, train=3.9 seconds, 298688 images, time remaining=3.2 hours
4668: loss=3.822, avg loss=3.637, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 298752 images, time remaining=3.2 hours
4669: loss=3.733, avg loss=3.647, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 298816 images, time remaining=3.2 hours
4670: loss=3.803, avg loss=3.662, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 298880 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4671: loss=3.284, avg loss=3.624, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=4.7 seconds, 298944 images, time remaining=3.2 hours
4672: loss=4.779, avg loss=3.740, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=4.7 seconds, 299008 images, time remaining=3.2 hours
4673: loss=3.788, avg loss=3.745, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.1 seconds, train=4.7 seconds, 299072 images, time remaining=3.2 hours
4674: loss=4.666, avg loss=3.837, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=724.5 milliseconds, train=4.7 seconds, 299136 images, time remaining=3.2 hours
4675: loss=3.684, avg loss=3.822, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=4.7 seconds, 299200 images, time remaining=3.2 hours
4676: loss=3.827, avg loss=3.822, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.7 seconds, train=4.7 seconds, 299264 images, time remaining=3.2 hours
4677: loss=3.948, avg loss=3.835, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=4.7 seconds, 299328 images, time remaining=3.2 hours
4678: loss=3.948, avg loss=3.846, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.2 seconds, train=4.7 seconds, 299392 images, time remaining=3.2 hours
4679: loss=4.235, avg loss=3.885, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=4.7 seconds, 299456 images, time remaining=3.2 hours
4680: loss=4.356, avg loss=3.932, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=727.0 milliseconds, train=4.7 seconds, 299520 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4681: loss=3.596, avg loss=3.898, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 299584 images, time remaining=3.2 hours
4682: loss=3.533, avg loss=3.862, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=447.9 milliseconds, train=2.1 seconds, 299648 images, time remaining=3.2 hours
4683: loss=3.355, avg loss=3.811, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 299712 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4684: loss=2.996, avg loss=3.730, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.9 seconds, train=2.1 seconds, 299776 images, time remaining=3.2 hours
4685: loss=2.973, avg loss=3.654, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 299840 images, time remaining=3.2 hours
4686: loss=3.297, avg loss=3.618, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 299904 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4687: loss=3.601, avg loss=3.616, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 299968 images, time remaining=3.2 hours
4688: loss=3.679, avg loss=3.623, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 300032 images, time remaining=3.2 hours
4689: loss=3.214, avg loss=3.582, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 300096 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4690: loss=4.258, avg loss=3.649, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 300160 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4691: loss=3.714, avg loss=3.656, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.6 seconds, 300224 images, time remaining=3.2 hours
4692: loss=4.049, avg loss=3.695, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.1 seconds, train=3.6 seconds, 300288 images, time remaining=3.2 hours
4693: loss=3.713, avg loss=3.697, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=3.6 seconds, 300352 images, time remaining=3.2 hours
4694: loss=3.559, avg loss=3.683, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=3.6 seconds, 300416 images, time remaining=3.2 hours
4695: loss=3.684, avg loss=3.683, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=971.4 milliseconds, train=3.6 seconds, 300480 images, time remaining=3.2 hours
4696: loss=3.997, avg loss=3.715, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=3.6 seconds, 300544 images, time remaining=3.2 hours
4697: loss=3.133, avg loss=3.656, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=3.6 seconds, 300608 images, time remaining=3.2 hours
4698: loss=3.195, avg loss=3.610, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=3.6 seconds, 300672 images, time remaining=3.2 hours
4699: loss=3.496, avg loss=3.599, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=782.4 milliseconds, train=3.6 seconds, 300736 images, time remaining=3.2 hours
4700: loss=3.552, avg loss=3.594, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.6 seconds, 300800 images, time remaining=3.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4701: loss=3.610, avg loss=3.596, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 300864 images, time remaining=3.2 hours
4702: loss=3.735, avg loss=3.610, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=687.2 milliseconds, train=3.3 seconds, 300928 images, time remaining=3.2 hours
4703: loss=4.052, avg loss=3.654, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 300992 images, time remaining=3.2 hours
4704: loss=3.400, avg loss=3.628, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 301056 images, time remaining=3.2 hours
4705: loss=2.588, avg loss=3.524, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 301120 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4706: loss=4.009, avg loss=3.573, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.2 seconds, train=3.3 seconds, 301184 images, time remaining=3.2 hours
4707: loss=2.761, avg loss=3.492, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=628.6 milliseconds, train=3.3 seconds, 301248 images, time remaining=3.2 hours
4708: loss=3.398, avg loss=3.482, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 301312 images, time remaining=3.2 hours
4709: loss=3.487, avg loss=3.483, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 301376 images, time remaining=3.2 hours
4710: loss=3.295, avg loss=3.464, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 301440 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4711: loss=4.772, avg loss=3.595, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 301504 images, time remaining=3.2 hours
4712: loss=4.067, avg loss=3.642, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 301568 images, time remaining=3.2 hours
4713: loss=4.219, avg loss=3.700, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=999.8 milliseconds, train=4.6 seconds, 301632 images, time remaining=3.2 hours
4714: loss=4.474, avg loss=3.777, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 301696 images, time remaining=3.2 hours
4715: loss=3.811, avg loss=3.781, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=586.8 milliseconds, train=4.6 seconds, 301760 images, time remaining=3.2 hours
4716: loss=3.636, avg loss=3.766, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=4.6 seconds, 301824 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4717: loss=4.840, avg loss=3.874, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=5.1 seconds, train=4.6 seconds, 301888 images, time remaining=3.2 hours
4718: loss=3.660, avg loss=3.852, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.6 seconds, train=4.6 seconds, 301952 images, time remaining=3.2 hours
4719: loss=5.080, avg loss=3.975, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=628.0 milliseconds, train=4.6 seconds, 302016 images, time remaining=3.2 hours
4720: loss=3.654, avg loss=3.943, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=4.6 seconds, 302080 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4721: loss=3.608, avg loss=3.909, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 302144 images, time remaining=3.2 hours
4722: loss=4.114, avg loss=3.930, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 302208 images, time remaining=3.2 hours
4723: loss=3.355, avg loss=3.872, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 302272 images, time remaining=3.2 hours
4724: loss=3.738, avg loss=3.859, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.1 seconds, train=3.9 seconds, 302336 images, time remaining=3.2 hours
4725: loss=3.831, avg loss=3.856, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 302400 images, time remaining=3.2 hours
4726: loss=3.377, avg loss=3.808, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=3.9 seconds, 302464 images, time remaining=3.2 hours
4727: loss=4.221, avg loss=3.850, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 302528 images, time remaining=3.2 hours
4728: loss=4.128, avg loss=3.877, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=698.9 milliseconds, train=3.9 seconds, 302592 images, time remaining=3.2 hours
4729: loss=4.221, avg loss=3.912, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 302656 images, time remaining=3.2 hours
4730: loss=3.541, avg loss=3.875, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=3.9 seconds, 302720 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4731: loss=3.392, avg loss=3.826, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.2 seconds, train=4.1 seconds, 302784 images, time remaining=3.2 hours
4732: loss=3.835, avg loss=3.827, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 302848 images, time remaining=3.2 hours
4733: loss=3.610, avg loss=3.806, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 302912 images, time remaining=3.2 hours
4734: loss=3.475, avg loss=3.772, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=4.1 seconds, 302976 images, time remaining=3.2 hours
4735: loss=3.423, avg loss=3.738, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=793.6 milliseconds, train=4.1 seconds, 303040 images, time remaining=3.2 hours
4736: loss=3.920, avg loss=3.756, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 303104 images, time remaining=3.2 hours
4737: loss=3.251, avg loss=3.705, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.0 seconds, train=4.1 seconds, 303168 images, time remaining=3.2 hours
4738: loss=3.335, avg loss=3.668, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=879.8 milliseconds, train=4.1 seconds, 303232 images, time remaining=3.2 hours
4739: loss=3.264, avg loss=3.628, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=4.1 seconds, 303296 images, time remaining=3.2 hours
4740: loss=3.330, avg loss=3.598, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=618.4 milliseconds, train=4.1 seconds, 303360 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146352000000
4741: loss=3.755, avg loss=3.614, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=911.6 milliseconds, train=1.5 seconds, 303424 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4742: loss=3.582, avg loss=3.611, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 303488 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4743: loss=3.530, avg loss=3.603, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 303552 images, time remaining=3.2 hours
4744: loss=3.124, avg loss=3.555, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 303616 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4745: loss=3.518, avg loss=3.551, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.0 seconds, train=1.5 seconds, 303680 images, time remaining=3.2 hours
4746: loss=3.553, avg loss=3.551, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=958.8 milliseconds, train=1.5 seconds, 303744 images, time remaining=3.2 hours
4747: loss=2.935, avg loss=3.490, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=453.1 milliseconds, train=1.5 seconds, 303808 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4748: loss=4.093, avg loss=3.550, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=1.5 seconds, 303872 images, time remaining=3.2 hours
4749: loss=3.335, avg loss=3.528, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=557.3 milliseconds, train=1.5 seconds, 303936 images, time remaining=3.2 hours
4750: loss=2.845, avg loss=3.460, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=708.7 milliseconds, train=1.5 seconds, 304000 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4751: loss=3.740, avg loss=3.488, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.1 seconds, train=1.9 seconds, 304064 images, time remaining=3.2 hours
4752: loss=2.805, avg loss=3.420, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=692.5 milliseconds, train=1.8 seconds, 304128 images, time remaining=3.2 hours
4753: loss=3.045, avg loss=3.382, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=814.9 milliseconds, train=1.8 seconds, 304192 images, time remaining=3.2 hours
4754: loss=3.142, avg loss=3.358, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=1.8 seconds, 304256 images, time remaining=3.2 hours
4755: loss=3.159, avg loss=3.338, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=1.8 seconds, 304320 images, time remaining=3.2 hours
4756: loss=3.332, avg loss=3.338, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=709.7 milliseconds, train=1.8 seconds, 304384 images, time remaining=3.2 hours
4757: loss=4.135, avg loss=3.417, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=723.5 milliseconds, train=1.8 seconds, 304448 images, time remaining=3.2 hours
4758: loss=2.164, avg loss=3.292, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=740.7 milliseconds, train=1.8 seconds, 304512 images, time remaining=3.2 hours
4759: loss=2.265, avg loss=3.189, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.8 seconds, 304576 images, time remaining=3.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4760: loss=4.082, avg loss=3.279, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=1.8 seconds, 304640 images, time remaining=3.2 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4761: loss=3.735, avg loss=3.324, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=954.1 milliseconds, train=3.9 seconds, 304704 images, time remaining=3.2 hours
4762: loss=3.987, avg loss=3.391, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 304768 images, time remaining=3.2 hours
4763: loss=3.694, avg loss=3.421, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 304832 images, time remaining=3.2 hours
4764: loss=3.398, avg loss=3.419, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=567.5 milliseconds, train=3.9 seconds, 304896 images, time remaining=3.2 hours
4765: loss=3.361, avg loss=3.413, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 304960 images, time remaining=3.2 hours
4766: loss=3.279, avg loss=3.400, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=823.7 milliseconds, train=3.9 seconds, 305024 images, time remaining=3.1 hours
4767: loss=4.297, avg loss=3.489, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 305088 images, time remaining=3.1 hours
4768: loss=3.640, avg loss=3.504, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 305152 images, time remaining=3.1 hours
4769: loss=3.636, avg loss=3.517, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=729.4 milliseconds, train=3.9 seconds, 305216 images, time remaining=3.1 hours
4770: loss=4.108, avg loss=3.576, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=863.8 milliseconds, train=3.9 seconds, 305280 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4771: loss=4.348, avg loss=3.654, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=901.1 milliseconds, train=3.3 seconds, 305344 images, time remaining=3.1 hours
4772: loss=3.797, avg loss=3.668, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=745.7 milliseconds, train=3.3 seconds, 305408 images, time remaining=3.1 hours
4773: loss=3.297, avg loss=3.631, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 305472 images, time remaining=3.1 hours
4774: loss=4.048, avg loss=3.673, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=576.4 milliseconds, train=3.3 seconds, 305536 images, time remaining=3.1 hours
4775: loss=3.246, avg loss=3.630, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=526.1 milliseconds, train=3.3 seconds, 305600 images, time remaining=3.1 hours
4776: loss=3.290, avg loss=3.596, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 305664 images, time remaining=3.1 hours
4777: loss=2.998, avg loss=3.536, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 305728 images, time remaining=3.1 hours
4778: loss=3.187, avg loss=3.501, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=3.3 seconds, 305792 images, time remaining=3.1 hours
4779: loss=2.844, avg loss=3.435, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 305856 images, time remaining=3.1 hours
4780: loss=3.182, avg loss=3.410, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 305920 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145b46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4781: loss=3.558, avg loss=3.425, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 305984 images, time remaining=3.1 hours
4782: loss=3.972, avg loss=3.480, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=804.4 milliseconds, train=1.5 seconds, 306048 images, time remaining=3.1 hours
4783: loss=3.132, avg loss=3.445, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=929.7 milliseconds, train=1.5 seconds, 306112 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4784: loss=3.300, avg loss=3.430, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 306176 images, time remaining=3.1 hours
4785: loss=3.550, avg loss=3.442, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=650.0 milliseconds, train=1.5 seconds, 306240 images, time remaining=3.1 hours
4786: loss=3.229, avg loss=3.421, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=883.8 milliseconds, train=1.5 seconds, 306304 images, time remaining=3.1 hours
4787: loss=3.263, avg loss=3.405, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=927.8 milliseconds, train=1.5 seconds, 306368 images, time remaining=3.1 hours
4788: loss=2.571, avg loss=3.322, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 306432 images, time remaining=3.1 hours
4789: loss=3.548, avg loss=3.344, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=486.9 milliseconds, train=1.5 seconds, 306496 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4790: loss=2.945, avg loss=3.304, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 306560 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4791: loss=4.946, avg loss=3.469, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 306624 images, time remaining=3.1 hours
4792: loss=4.589, avg loss=3.581, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=774.1 milliseconds, train=4.5 seconds, 306688 images, time remaining=3.1 hours
4793: loss=3.201, avg loss=3.543, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=550.0 milliseconds, train=4.5 seconds, 306752 images, time remaining=3.1 hours
4794: loss=3.742, avg loss=3.563, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=4.5 seconds, 306816 images, time remaining=3.1 hours
4795: loss=3.333, avg loss=3.540, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.2 seconds, train=4.5 seconds, 306880 images, time remaining=3.1 hours
4796: loss=4.450, avg loss=3.631, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=783.4 milliseconds, train=4.5 seconds, 306944 images, time remaining=3.1 hours
4797: loss=3.753, avg loss=3.643, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 307008 images, time remaining=3.1 hours
4798: loss=4.093, avg loss=3.688, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=788.2 milliseconds, train=4.5 seconds, 307072 images, time remaining=3.1 hours
4799: loss=4.345, avg loss=3.754, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=981.7 milliseconds, train=4.5 seconds, 307136 images, time remaining=3.1 hours
4800: loss=3.684, avg loss=3.747, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=618.8 milliseconds, train=4.5 seconds, 307200 images, time remaining=3.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4801: loss=3.040, avg loss=3.676, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=780.6 milliseconds, train=3.4 seconds, 307264 images, time remaining=3.1 hours
4802: loss=3.372, avg loss=3.646, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=610.1 milliseconds, train=3.4 seconds, 307328 images, time remaining=3.1 hours
4803: loss=3.258, avg loss=3.607, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=560.0 milliseconds, train=3.4 seconds, 307392 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4804: loss=3.624, avg loss=3.608, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=3.4 seconds, 307456 images, time remaining=3.1 hours
4805: loss=3.576, avg loss=3.605, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 307520 images, time remaining=3.1 hours
4806: loss=3.353, avg loss=3.580, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=3.4 seconds, 307584 images, time remaining=3.1 hours
4807: loss=3.189, avg loss=3.541, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=609.4 milliseconds, train=3.4 seconds, 307648 images, time remaining=3.1 hours
4808: loss=3.467, avg loss=3.534, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.4 seconds, 307712 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4809: loss=3.624, avg loss=3.543, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.8 seconds, train=3.4 seconds, 307776 images, time remaining=3.1 hours
4810: loss=3.557, avg loss=3.544, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=783.7 milliseconds, train=3.4 seconds, 307840 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4811: loss=3.867, avg loss=3.576, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=832.3 milliseconds, train=3.9 seconds, 307904 images, time remaining=3.1 hours
4812: loss=3.306, avg loss=3.549, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 307968 images, time remaining=3.1 hours
4813: loss=4.189, avg loss=3.613, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=941.0 milliseconds, train=3.9 seconds, 308032 images, time remaining=3.1 hours
4814: loss=4.019, avg loss=3.654, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=874.4 milliseconds, train=3.9 seconds, 308096 images, time remaining=3.1 hours
4815: loss=3.291, avg loss=3.618, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.8 seconds, train=3.9 seconds, 308160 images, time remaining=3.1 hours
4816: loss=4.074, avg loss=3.663, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=573.7 milliseconds, train=3.9 seconds, 308224 images, time remaining=3.1 hours
4817: loss=3.489, avg loss=3.646, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 308288 images, time remaining=3.1 hours
4818: loss=3.078, avg loss=3.589, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 308352 images, time remaining=3.1 hours
4819: loss=3.305, avg loss=3.561, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 308416 images, time remaining=3.1 hours
4820: loss=2.925, avg loss=3.497, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 308480 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4821: loss=3.267, avg loss=3.474, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=4.4 seconds, 308544 images, time remaining=3.1 hours
4822: loss=4.167, avg loss=3.543, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 308608 images, time remaining=3.1 hours
4823: loss=3.498, avg loss=3.539, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=622.1 milliseconds, train=4.4 seconds, 308672 images, time remaining=3.1 hours
4824: loss=3.850, avg loss=3.570, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=4.4 seconds, 308736 images, time remaining=3.1 hours
4825: loss=3.521, avg loss=3.565, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=4.4 seconds, 308800 images, time remaining=3.1 hours
4826: loss=3.669, avg loss=3.575, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=4.4 seconds, 308864 images, time remaining=3.1 hours
4827: loss=4.151, avg loss=3.633, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=4.4 seconds, 308928 images, time remaining=3.1 hours
4828: loss=3.215, avg loss=3.591, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=694.1 milliseconds, train=4.4 seconds, 308992 images, time remaining=3.1 hours
4829: loss=3.613, avg loss=3.593, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=4.4 seconds, 309056 images, time remaining=3.1 hours
4830: loss=3.573, avg loss=3.591, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=972.3 milliseconds, train=4.4 seconds, 309120 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4831: loss=3.513, avg loss=3.584, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 309184 images, time remaining=3.1 hours
4832: loss=3.190, avg loss=3.544, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 309248 images, time remaining=3.1 hours
4833: loss=3.468, avg loss=3.537, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 309312 images, time remaining=3.1 hours
4834: loss=3.740, avg loss=3.557, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=786.5 milliseconds, train=2.1 seconds, 309376 images, time remaining=3.1 hours
4835: loss=3.560, avg loss=3.557, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 309440 images, time remaining=3.1 hours
4836: loss=3.116, avg loss=3.513, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 309504 images, time remaining=3.1 hours
4837: loss=3.534, avg loss=3.515, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 309568 images, time remaining=3.1 hours
4838: loss=3.165, avg loss=3.480, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 309632 images, time remaining=3.1 hours
4839: loss=3.577, avg loss=3.490, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=418.3 milliseconds, train=2.1 seconds, 309696 images, time remaining=3.1 hours
4840: loss=3.236, avg loss=3.465, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 309760 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4841: loss=5.481, avg loss=3.666, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=713.2 milliseconds, train=5.1 seconds, 309824 images, time remaining=3.1 hours
4842: loss=4.176, avg loss=3.717, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=762.9 milliseconds, train=5.0 seconds, 309888 images, time remaining=3.1 hours
4843: loss=3.577, avg loss=3.703, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=462.5 milliseconds, train=5.0 seconds, 309952 images, time remaining=3.1 hours
4844: loss=3.455, avg loss=3.678, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=734.5 milliseconds, train=5.0 seconds, 310016 images, time remaining=3.1 hours
4845: loss=4.018, avg loss=3.712, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=5.0 seconds, 310080 images, time remaining=3.1 hours
4846: loss=4.217, avg loss=3.763, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=660.6 milliseconds, train=5.0 seconds, 310144 images, time remaining=3.1 hours
4847: loss=5.451, avg loss=3.931, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=5.0 seconds, 310208 images, time remaining=3.1 hours
4848: loss=3.822, avg loss=3.921, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=808.2 milliseconds, train=5.0 seconds, 310272 images, time remaining=3.1 hours
4849: loss=3.578, avg loss=3.886, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=546.3 milliseconds, train=5.1 seconds, 310336 images, time remaining=3.1 hours
4850: loss=4.789, avg loss=3.977, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=896.5 milliseconds, train=5.0 seconds, 310400 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145f5a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4851: loss=3.531, avg loss=3.932, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=1.7 seconds, 310464 images, time remaining=3.1 hours
4852: loss=3.557, avg loss=3.894, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=407.3 milliseconds, train=1.6 seconds, 310528 images, time remaining=3.1 hours
4853: loss=3.476, avg loss=3.853, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 310592 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4854: loss=3.367, avg loss=3.804, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 310656 images, time remaining=3.1 hours
4855: loss=3.658, avg loss=3.789, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=648.0 milliseconds, train=1.7 seconds, 310720 images, time remaining=3.1 hours
4856: loss=3.495, avg loss=3.760, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=592.5 milliseconds, train=1.6 seconds, 310784 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4857: loss=3.275, avg loss=3.711, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 310848 images, time remaining=3.1 hours
4858: loss=3.962, avg loss=3.736, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=549.2 milliseconds, train=1.6 seconds, 310912 images, time remaining=3.1 hours
4859: loss=3.934, avg loss=3.756, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 310976 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4860: loss=3.405, avg loss=3.721, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 311040 images, time remaining=3.1 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4861: loss=3.680, avg loss=3.717, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=607.6 milliseconds, train=3.2 seconds, 311104 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4862: loss=4.249, avg loss=3.770, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.2 seconds, train=3.2 seconds, 311168 images, time remaining=3.1 hours
4863: loss=3.427, avg loss=3.736, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=792.3 milliseconds, train=3.2 seconds, 311232 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4864: loss=3.019, avg loss=3.664, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.7 seconds, train=3.1 seconds, 311296 images, time remaining=3.1 hours
4865: loss=3.621, avg loss=3.660, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=987.8 milliseconds, train=3.2 seconds, 311360 images, time remaining=3.1 hours
4866: loss=4.334, avg loss=3.727, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.2 seconds, 311424 images, time remaining=3.1 hours
4867: loss=3.277, avg loss=3.682, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=554.2 milliseconds, train=3.2 seconds, 311488 images, time remaining=3.1 hours
4868: loss=3.254, avg loss=3.640, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=3.2 seconds, 311552 images, time remaining=3.1 hours
4869: loss=3.693, avg loss=3.645, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=808.5 milliseconds, train=3.2 seconds, 311616 images, time remaining=3.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4870: loss=3.649, avg loss=3.645, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.2 seconds, train=3.2 seconds, 311680 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4871: loss=2.807, avg loss=3.561, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 311744 images, time remaining=3 hours
4872: loss=3.521, avg loss=3.557, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 311808 images, time remaining=3 hours
4873: loss=3.563, avg loss=3.558, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=580.2 milliseconds, train=2.1 seconds, 311872 images, time remaining=3 hours
4874: loss=4.137, avg loss=3.616, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=981.9 milliseconds, train=2.1 seconds, 311936 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4875: loss=3.401, avg loss=3.594, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.2 seconds, train=2.1 seconds, 312000 images, time remaining=3 hours
4876: loss=4.114, avg loss=3.646, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=642.6 milliseconds, train=2.1 seconds, 312064 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4877: loss=2.846, avg loss=3.566, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 312128 images, time remaining=3 hours
4878: loss=3.102, avg loss=3.520, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 312192 images, time remaining=3 hours
4879: loss=3.384, avg loss=3.506, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 312256 images, time remaining=3 hours
4880: loss=3.385, avg loss=3.494, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=726.3 milliseconds, train=2.1 seconds, 312320 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4881: loss=3.790, avg loss=3.524, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=968.8 milliseconds, train=3.4 seconds, 312384 images, time remaining=3 hours
4882: loss=4.204, avg loss=3.592, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=3.4 seconds, 312448 images, time remaining=3 hours
4883: loss=3.490, avg loss=3.582, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=666.8 milliseconds, train=3.4 seconds, 312512 images, time remaining=3 hours
4884: loss=3.434, avg loss=3.567, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=812.2 milliseconds, train=3.4 seconds, 312576 images, time remaining=3 hours
4885: loss=3.556, avg loss=3.566, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=622.7 milliseconds, train=3.4 seconds, 312640 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4886: loss=2.996, avg loss=3.509, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=3.4 seconds, 312704 images, time remaining=3 hours
4887: loss=3.930, avg loss=3.551, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=770.9 milliseconds, train=3.4 seconds, 312768 images, time remaining=3 hours
4888: loss=3.874, avg loss=3.583, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=3.4 seconds, 312832 images, time remaining=3 hours
4889: loss=3.915, avg loss=3.616, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=3.4 seconds, 312896 images, time remaining=3 hours
4890: loss=3.398, avg loss=3.595, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=3.4 seconds, 312960 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146372000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4891: loss=3.959, avg loss=3.631, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=1.2 seconds, 313024 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4892: loss=3.732, avg loss=3.641, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 313088 images, time remaining=3 hours
4893: loss=3.734, avg loss=3.650, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=539.4 milliseconds, train=1.2 seconds, 313152 images, time remaining=3 hours
4894: loss=2.618, avg loss=3.547, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=736.4 milliseconds, train=1.2 seconds, 313216 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4895: loss=3.614, avg loss=3.554, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.2 seconds, train=1.2 seconds, 313280 images, time remaining=3 hours
4896: loss=4.284, avg loss=3.627, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=827.6 milliseconds, train=1.2 seconds, 313344 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4897: loss=3.656, avg loss=3.630, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 313408 images, time remaining=3 hours
4898: loss=4.039, avg loss=3.671, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=872.3 milliseconds, train=1.2 seconds, 313472 images, time remaining=3 hours
4899: loss=5.133, avg loss=3.817, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=673.8 milliseconds, train=1.2 seconds, 313536 images, time remaining=3 hours
4900: loss=3.538, avg loss=3.789, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 313600 images, time remaining=3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4901: loss=5.082, avg loss=3.918, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=590.7 milliseconds, train=3.3 seconds, 313664 images, time remaining=3 hours
4902: loss=4.784, avg loss=4.005, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=3.3 seconds, 313728 images, time remaining=3 hours
4903: loss=3.330, avg loss=3.937, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=785.2 milliseconds, train=3.3 seconds, 313792 images, time remaining=3 hours
4904: loss=3.110, avg loss=3.855, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 313856 images, time remaining=3 hours
4905: loss=4.134, avg loss=3.882, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 313920 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4906: loss=3.505, avg loss=3.845, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 313984 images, time remaining=3 hours
4907: loss=3.961, avg loss=3.856, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 314048 images, time remaining=3 hours
4908: loss=4.928, avg loss=3.964, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 314112 images, time remaining=3 hours
4909: loss=4.019, avg loss=3.969, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 314176 images, time remaining=3 hours
4910: loss=4.524, avg loss=4.025, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=751.0 milliseconds, train=3.4 seconds, 314240 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4911: loss=4.002, avg loss=4.022, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=918.6 milliseconds, train=5.1 seconds, 314304 images, time remaining=3 hours
4912: loss=3.961, avg loss=4.016, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=801.6 milliseconds, train=5.1 seconds, 314368 images, time remaining=3 hours
4913: loss=4.559, avg loss=4.071, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 314432 images, time remaining=3 hours
4914: loss=4.988, avg loss=4.162, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=5.1 seconds, 314496 images, time remaining=3 hours
4915: loss=4.023, avg loss=4.148, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.9 seconds, train=5.1 seconds, 314560 images, time remaining=3 hours
4916: loss=5.335, avg loss=4.267, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=5.1 seconds, 314624 images, time remaining=3 hours
4917: loss=4.051, avg loss=4.245, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=5.1 seconds, 314688 images, time remaining=3 hours
4918: loss=4.319, avg loss=4.253, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=552.3 milliseconds, train=5.1 seconds, 314752 images, time remaining=3 hours
4919: loss=3.853, avg loss=4.213, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 314816 images, time remaining=3 hours
4920: loss=4.503, avg loss=4.242, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 314880 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145b22000000
4921: loss=4.292, avg loss=4.247, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=878.3 milliseconds, train=1.4 seconds, 314944 images, time remaining=3 hours
4922: loss=5.381, avg loss=4.360, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=717.2 milliseconds, train=1.5 seconds, 315008 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4923: loss=4.196, avg loss=4.344, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=1.4 seconds, 315072 images, time remaining=3 hours
4924: loss=4.278, avg loss=4.337, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=668.4 milliseconds, train=1.4 seconds, 315136 images, time remaining=3 hours
4925: loss=3.294, avg loss=4.233, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=782.6 milliseconds, train=1.4 seconds, 315200 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4926: loss=3.692, avg loss=4.179, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=1.4 seconds, 315264 images, time remaining=3 hours
4927: loss=3.156, avg loss=4.077, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=991.0 milliseconds, train=1.4 seconds, 315328 images, time remaining=3 hours
4928: loss=4.240, avg loss=4.093, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 315392 images, time remaining=3 hours
4929: loss=4.053, avg loss=4.089, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=483.2 milliseconds, train=1.4 seconds, 315456 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4930: loss=4.032, avg loss=4.083, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=1.4 seconds, 315520 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4931: loss=4.180, avg loss=4.093, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 315584 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4932: loss=3.629, avg loss=4.046, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 315648 images, time remaining=3 hours
4933: loss=3.420, avg loss=3.984, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=870.8 milliseconds, train=2.1 seconds, 315712 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4934: loss=3.233, avg loss=3.909, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 315776 images, time remaining=3 hours
4935: loss=3.314, avg loss=3.849, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=578.0 milliseconds, train=2.1 seconds, 315840 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4936: loss=2.910, avg loss=3.755, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.1 seconds, train=2.1 seconds, 315904 images, time remaining=3 hours
4937: loss=3.377, avg loss=3.717, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=714.9 milliseconds, train=2.1 seconds, 315968 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4938: loss=3.247, avg loss=3.670, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 316032 images, time remaining=3 hours
4939: loss=3.682, avg loss=3.672, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=667.5 milliseconds, train=2.1 seconds, 316096 images, time remaining=3 hours
4940: loss=4.020, avg loss=3.706, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 316160 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4941: loss=3.322, avg loss=3.668, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=591.3 milliseconds, train=4.5 seconds, 316224 images, time remaining=3 hours
4942: loss=4.064, avg loss=3.708, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=4.5 seconds, 316288 images, time remaining=3 hours
4943: loss=3.615, avg loss=3.698, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=538.0 milliseconds, train=4.5 seconds, 316352 images, time remaining=3 hours
4944: loss=4.679, avg loss=3.796, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=728.5 milliseconds, train=4.5 seconds, 316416 images, time remaining=3 hours
4945: loss=4.073, avg loss=3.824, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=4.5 seconds, 316480 images, time remaining=3 hours
4946: loss=3.435, avg loss=3.785, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=4.5 seconds, 316544 images, time remaining=3 hours
4947: loss=4.287, avg loss=3.835, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=4.5 seconds, 316608 images, time remaining=3 hours
4948: loss=3.453, avg loss=3.797, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=834.7 milliseconds, train=4.5 seconds, 316672 images, time remaining=3 hours
4949: loss=4.731, avg loss=3.890, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=4.5 seconds, 316736 images, time remaining=3 hours
4950: loss=3.301, avg loss=3.832, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=558.6 milliseconds, train=4.5 seconds, 316800 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4951: loss=3.622, avg loss=3.811, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=969.4 milliseconds, train=2.0 seconds, 316864 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4952: loss=3.057, avg loss=3.735, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=2.0 seconds, 316928 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4953: loss=3.541, avg loss=3.716, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.1 seconds, train=2.0 seconds, 316992 images, time remaining=3 hours
4954: loss=3.159, avg loss=3.660, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=703.5 milliseconds, train=2.0 seconds, 317056 images, time remaining=3 hours
4955: loss=3.060, avg loss=3.600, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=2.0 seconds, 317120 images, time remaining=3 hours
4956: loss=3.163, avg loss=3.556, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=2.0 seconds, 317184 images, time remaining=3 hours
4957: loss=3.090, avg loss=3.510, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=840.9 milliseconds, train=2.0 seconds, 317248 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4958: loss=3.620, avg loss=3.521, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=2.0 seconds, 317312 images, time remaining=3 hours
4959: loss=3.845, avg loss=3.553, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 317376 images, time remaining=3 hours
4960: loss=3.173, avg loss=3.515, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 317440 images, time remaining=3 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4961: loss=3.481, avg loss=3.512, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=694.2 milliseconds, train=3.6 seconds, 317504 images, time remaining=3 hours
4962: loss=4.145, avg loss=3.575, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=3.6 seconds, 317568 images, time remaining=3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4963: loss=3.756, avg loss=3.593, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.1 seconds, train=3.6 seconds, 317632 images, time remaining=3 hours
4964: loss=3.611, avg loss=3.595, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=771.1 milliseconds, train=3.6 seconds, 317696 images, time remaining=3 hours
4965: loss=3.740, avg loss=3.609, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 317760 images, time remaining=3 hours
4966: loss=3.650, avg loss=3.614, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 317824 images, time remaining=3 hours
4967: loss=3.502, avg loss=3.602, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 317888 images, time remaining=3 hours
4968: loss=3.832, avg loss=3.625, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=684.3 milliseconds, train=3.6 seconds, 317952 images, time remaining=3 hours
4969: loss=2.762, avg loss=3.539, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=3.6 seconds, 318016 images, time remaining=3 hours
4970: loss=3.721, avg loss=3.557, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=742.5 milliseconds, train=3.6 seconds, 318080 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145c3ae00000
4971: loss=3.981, avg loss=3.600, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=547.0 milliseconds, train=1.5 seconds, 318144 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4972: loss=3.721, avg loss=3.612, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.9 seconds, train=1.5 seconds, 318208 images, time remaining=2.9 hours
4973: loss=3.580, avg loss=3.608, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 318272 images, time remaining=2.9 hours
4974: loss=3.445, avg loss=3.592, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=897.7 milliseconds, train=1.5 seconds, 318336 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4975: loss=3.485, avg loss=3.581, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 318400 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4976: loss=3.035, avg loss=3.527, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 318464 images, time remaining=2.9 hours
4977: loss=3.391, avg loss=3.513, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=775.2 milliseconds, train=1.5 seconds, 318528 images, time remaining=2.9 hours
4978: loss=3.024, avg loss=3.464, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=529.8 milliseconds, train=1.5 seconds, 318592 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4979: loss=3.543, avg loss=3.472, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 318656 images, time remaining=2.9 hours
4980: loss=3.335, avg loss=3.458, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=551.0 milliseconds, train=1.5 seconds, 318720 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 1312x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
4981: loss=5.418, avg loss=3.654, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=4.8 seconds, 318784 images, time remaining=2.9 hours
4982: loss=4.882, avg loss=3.777, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=4.7 seconds, 318848 images, time remaining=2.9 hours
4983: loss=4.603, avg loss=3.860, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=894.5 milliseconds, train=4.7 seconds, 318912 images, time remaining=2.9 hours
4984: loss=4.452, avg loss=3.919, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=757.2 milliseconds, train=4.7 seconds, 318976 images, time remaining=2.9 hours
4985: loss=3.519, avg loss=3.879, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.5 seconds, train=4.7 seconds, 319040 images, time remaining=2.9 hours
4986: loss=4.123, avg loss=3.903, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.2 seconds, train=4.7 seconds, 319104 images, time remaining=2.9 hours
4987: loss=4.744, avg loss=3.988, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=4.7 seconds, 319168 images, time remaining=2.9 hours
4988: loss=4.622, avg loss=4.051, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.2 seconds, train=4.7 seconds, 319232 images, time remaining=2.9 hours
4989: loss=3.923, avg loss=4.038, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.1 seconds, train=4.7 seconds, 319296 images, time remaining=2.9 hours
4990: loss=4.329, avg loss=4.067, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=4.7 seconds, 319360 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
4991: loss=5.148, avg loss=4.175, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=316.2 milliseconds, train=2.1 seconds, 319424 images, time remaining=2.9 hours
4992: loss=4.084, avg loss=4.166, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 319488 images, time remaining=2.9 hours
4993: loss=3.499, avg loss=4.099, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 319552 images, time remaining=2.9 hours
4994: loss=2.832, avg loss=3.973, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=453.6 milliseconds, train=2.1 seconds, 319616 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4995: loss=3.177, avg loss=3.893, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 319680 images, time remaining=2.9 hours
4996: loss=2.904, avg loss=3.794, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=674.8 milliseconds, train=2.1 seconds, 319744 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4997: loss=3.543, avg loss=3.769, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.6 seconds, train=2.1 seconds, 319808 images, time remaining=2.9 hours
4998: loss=3.171, avg loss=3.709, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=812.1 milliseconds, train=2.1 seconds, 319872 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
4999: loss=3.880, avg loss=3.726, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 319936 images, time remaining=2.9 hours
5000: loss=4.141, avg loss=3.768, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 320000 images, time remaining=2.9 hours
Saving weights to /workspace/.cache/splits/combined_5000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5001: loss=3.347, avg loss=3.726, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=1.9 seconds, 320064 images, time remaining=2.9 hours
5002: loss=3.497, avg loss=3.703, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=821.7 milliseconds, train=1.9 seconds, 320128 images, time remaining=2.9 hours
5003: loss=3.150, avg loss=3.648, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=565.4 milliseconds, train=1.9 seconds, 320192 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5004: loss=3.196, avg loss=3.602, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 320256 images, time remaining=2.9 hours
5005: loss=3.473, avg loss=3.590, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=630.1 milliseconds, train=1.9 seconds, 320320 images, time remaining=2.9 hours
5006: loss=4.203, avg loss=3.651, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=638.8 milliseconds, train=1.9 seconds, 320384 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5007: loss=2.686, avg loss=3.554, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 320448 images, time remaining=2.9 hours
5008: loss=3.344, avg loss=3.533, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=860.5 milliseconds, train=1.9 seconds, 320512 images, time remaining=2.9 hours
5009: loss=3.347, avg loss=3.515, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=1.9 seconds, 320576 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5010: loss=3.400, avg loss=3.503, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.3 seconds, train=1.9 seconds, 320640 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
5011: loss=3.916, avg loss=3.544, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=954.8 milliseconds, train=1.6 seconds, 320704 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5012: loss=2.570, avg loss=3.447, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 320768 images, time remaining=2.9 hours
5013: loss=3.377, avg loss=3.440, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=540.1 milliseconds, train=1.5 seconds, 320832 images, time remaining=2.9 hours
5014: loss=3.384, avg loss=3.434, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 320896 images, time remaining=2.9 hours
5015: loss=3.626, avg loss=3.454, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 320960 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5016: loss=3.712, avg loss=3.479, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 321024 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5017: loss=3.555, avg loss=3.487, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 321088 images, time remaining=2.9 hours
5018: loss=4.153, avg loss=3.554, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=546.4 milliseconds, train=1.5 seconds, 321152 images, time remaining=2.9 hours
5019: loss=3.599, avg loss=3.558, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 321216 images, time remaining=2.9 hours
5020: loss=3.117, avg loss=3.514, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 321280 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b9a000000
5021: loss=3.383, avg loss=3.501, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=932.6 milliseconds, train=1.2 seconds, 321344 images, time remaining=2.9 hours
5022: loss=3.531, avg loss=3.504, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 321408 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5023: loss=3.193, avg loss=3.473, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 321472 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5024: loss=4.263, avg loss=3.552, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=1.2 seconds, 321536 images, time remaining=2.9 hours
5025: loss=3.066, avg loss=3.503, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=508.9 milliseconds, train=1.2 seconds, 321600 images, time remaining=2.9 hours
5026: loss=2.730, avg loss=3.426, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 321664 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5027: loss=3.339, avg loss=3.417, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=4.1 seconds, train=1.2 seconds, 321728 images, time remaining=2.9 hours
5028: loss=3.691, avg loss=3.445, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 321792 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5029: loss=2.962, avg loss=3.396, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.7 seconds, train=1.2 seconds, 321856 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5030: loss=3.400, avg loss=3.397, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 321920 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b9a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5031: loss=3.906, avg loss=3.448, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 321984 images, time remaining=2.9 hours
5032: loss=3.339, avg loss=3.437, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=627.6 milliseconds, train=1.2 seconds, 322048 images, time remaining=2.9 hours
5033: loss=3.754, avg loss=3.469, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=682.9 milliseconds, train=1.2 seconds, 322112 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5034: loss=3.814, avg loss=3.503, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.3 seconds, train=1.2 seconds, 322176 images, time remaining=2.9 hours
5035: loss=3.142, avg loss=3.467, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 322240 images, time remaining=2.9 hours
5036: loss=3.712, avg loss=3.492, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 322304 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5037: loss=3.282, avg loss=3.471, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 322368 images, time remaining=2.9 hours
5038: loss=3.045, avg loss=3.428, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 322432 images, time remaining=2.9 hours
5039: loss=3.256, avg loss=3.411, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=597.9 milliseconds, train=1.3 seconds, 322496 images, time remaining=2.9 hours
5040: loss=2.656, avg loss=3.335, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=601.8 milliseconds, train=1.2 seconds, 322560 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5041: loss=3.726, avg loss=3.374, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 322624 images, time remaining=2.9 hours
5042: loss=4.193, avg loss=3.456, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 322688 images, time remaining=2.9 hours
5043: loss=3.353, avg loss=3.446, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 322752 images, time remaining=2.9 hours
5044: loss=3.286, avg loss=3.430, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 322816 images, time remaining=2.9 hours
5045: loss=3.070, avg loss=3.394, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=649.3 milliseconds, train=3.3 seconds, 322880 images, time remaining=2.9 hours
5046: loss=4.329, avg loss=3.487, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 322944 images, time remaining=2.9 hours
5047: loss=3.658, avg loss=3.505, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 323008 images, time remaining=2.9 hours
5048: loss=3.615, avg loss=3.516, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 323072 images, time remaining=2.9 hours
5049: loss=2.627, avg loss=3.427, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 323136 images, time remaining=2.9 hours
5050: loss=3.366, avg loss=3.421, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=3.3 seconds, 323200 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5051: loss=3.034, avg loss=3.382, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=623.2 milliseconds, train=3.2 seconds, 323264 images, time remaining=2.9 hours
5052: loss=3.214, avg loss=3.365, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=946.3 milliseconds, train=3.3 seconds, 323328 images, time remaining=2.9 hours
5053: loss=2.675, avg loss=3.296, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=701.7 milliseconds, train=3.3 seconds, 323392 images, time remaining=2.9 hours
5054: loss=3.448, avg loss=3.311, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 323456 images, time remaining=2.9 hours
5055: loss=2.882, avg loss=3.268, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=3.0 seconds, train=3.3 seconds, 323520 images, time remaining=2.9 hours
5056: loss=3.110, avg loss=3.253, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 323584 images, time remaining=2.9 hours
5057: loss=3.481, avg loss=3.275, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.9 seconds, train=3.2 seconds, 323648 images, time remaining=2.9 hours
5058: loss=3.252, avg loss=3.273, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=932.7 milliseconds, train=3.3 seconds, 323712 images, time remaining=2.9 hours
5059: loss=3.624, avg loss=3.308, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 323776 images, time remaining=2.9 hours
5060: loss=3.487, avg loss=3.326, last=93.79%, best=93.79%, next=5060, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 323840 images, time remaining=2.9 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=125643, unique_truth_count=57264
rank=0 of ranks=125643rank=100 of ranks=125643rank=200 of ranks=125643rank=300 of ranks=125643rank=400 of ranks=125643rank=500 of ranks=125643rank=600 of ranks=125643rank=700 of ranks=125643rank=800 of ranks=125643rank=900 of ranks=125643rank=1000 of ranks=125643rank=1100 of ranks=125643rank=1200 of ranks=125643rank=1300 of ranks=125643rank=1400 of ranks=125643rank=1500 of ranks=125643rank=1600 of ranks=125643rank=1700 of ranks=125643rank=1800 of ranks=125643rank=1900 of ranks=125643rank=2000 of ranks=125643rank=2100 of ranks=125643rank=2200 of ranks=125643rank=2300 of ranks=125643rank=2400 of ranks=125643rank=2500 of ranks=125643rank=2600 of ranks=125643rank=2700 of ranks=125643rank=2800 of ranks=125643rank=2900 of ranks=125643rank=3000 of ranks=125643rank=3100 of ranks=125643rank=3200 of ranks=125643rank=3300 of ranks=125643rank=3400 of ranks=125643rank=3500 of ranks=125643rank=3600 of ranks=125643rank=3700 of ranks=125643rank=3800 of ranks=125643rank=3900 of ranks=125643rank=4000 of ranks=125643rank=4100 of ranks=125643rank=4200 of ranks=125643rank=4300 of ranks=125643rank=4400 of ranks=125643rank=4500 of ranks=125643rank=4600 of ranks=125643rank=4700 of ranks=125643rank=4800 of ranks=125643rank=4900 of ranks=125643rank=5000 of ranks=125643rank=5100 of ranks=125643rank=5200 of ranks=125643rank=5300 of ranks=125643rank=5400 of ranks=125643rank=5500 of ranks=125643rank=5600 of ranks=125643rank=5700 of ranks=125643rank=5800 of ranks=125643rank=5900 of ranks=125643rank=6000 of ranks=125643rank=6100 of ranks=125643rank=6200 of ranks=125643rank=6300 of ranks=125643rank=6400 of ranks=125643rank=6500 of ranks=125643rank=6600 of ranks=125643rank=6700 of ranks=125643rank=6800 of ranks=125643rank=6900 of ranks=125643rank=7000 of ranks=125643rank=7100 of ranks=125643rank=7200 of ranks=125643rank=7300 of ranks=125643rank=7400 of ranks=125643rank=7500 of ranks=125643rank=7600 of ranks=125643rank=7700 of ranks=125643rank=7800 of ranks=125643rank=7900 of ranks=125643rank=8000 of ranks=125643rank=8100 of ranks=125643rank=8200 of ranks=125643rank=8300 of ranks=125643rank=8400 of ranks=125643rank=8500 of ranks=125643rank=8600 of ranks=125643rank=8700 of ranks=125643rank=8800 of ranks=125643rank=8900 of ranks=125643rank=9000 of ranks=125643rank=9100 of ranks=125643rank=9200 of ranks=125643rank=9300 of ranks=125643rank=9400 of ranks=125643rank=9500 of ranks=125643rank=9600 of ranks=125643rank=9700 of ranks=125643rank=9800 of ranks=125643rank=9900 of ranks=125643rank=10000 of ranks=125643rank=10100 of ranks=125643rank=10200 of ranks=125643rank=10300 of ranks=125643rank=10400 of ranks=125643rank=10500 of ranks=125643rank=10600 of ranks=125643rank=10700 of ranks=125643rank=10800 of ranks=125643rank=10900 of ranks=125643rank=11000 of ranks=125643rank=11100 of ranks=125643rank=11200 of ranks=125643rank=11300 of ranks=125643rank=11400 of ranks=125643rank=11500 of ranks=125643rank=11600 of ranks=125643rank=11700 of ranks=125643rank=11800 of ranks=125643rank=11900 of ranks=125643rank=12000 of ranks=125643rank=12100 of ranks=125643rank=12200 of ranks=125643rank=12300 of ranks=125643rank=12400 of ranks=125643rank=12500 of ranks=125643rank=12600 of ranks=125643rank=12700 of ranks=125643rank=12800 of ranks=125643rank=12900 of ranks=125643rank=13000 of ranks=125643rank=13100 of ranks=125643rank=13200 of ranks=125643rank=13300 of ranks=125643rank=13400 of ranks=125643rank=13500 of ranks=125643rank=13600 of ranks=125643rank=13700 of ranks=125643rank=13800 of ranks=125643rank=13900 of ranks=125643rank=14000 of ranks=125643rank=14100 of ranks=125643rank=14200 of ranks=125643rank=14300 of ranks=125643rank=14400 of ranks=125643rank=14500 of ranks=125643rank=14600 of ranks=125643rank=14700 of ranks=125643rank=14800 of ranks=125643rank=14900 of ranks=125643rank=15000 of ranks=125643rank=15100 of ranks=125643rank=15200 of ranks=125643rank=15300 of ranks=125643rank=15400 of ranks=125643rank=15500 of ranks=125643rank=15600 of ranks=125643rank=15700 of ranks=125643rank=15800 of ranks=125643rank=15900 of ranks=125643rank=16000 of ranks=125643rank=16100 of ranks=125643rank=16200 of ranks=125643rank=16300 of ranks=125643rank=16400 of ranks=125643rank=16500 of ranks=125643rank=16600 of ranks=125643rank=16700 of ranks=125643rank=16800 of ranks=125643rank=16900 of ranks=125643rank=17000 of ranks=125643rank=17100 of ranks=125643rank=17200 of ranks=125643rank=17300 of ranks=125643rank=17400 of ranks=125643rank=17500 of ranks=125643rank=17600 of ranks=125643rank=17700 of ranks=125643rank=17800 of ranks=125643rank=17900 of ranks=125643rank=18000 of ranks=125643rank=18100 of ranks=125643rank=18200 of ranks=125643rank=18300 of ranks=125643rank=18400 of ranks=125643rank=18500 of ranks=125643rank=18600 of ranks=125643rank=18700 of ranks=125643rank=18800 of ranks=125643rank=18900 of ranks=125643rank=19000 of ranks=125643rank=19100 of ranks=125643rank=19200 of ranks=125643rank=19300 of ranks=125643rank=19400 of ranks=125643rank=19500 of ranks=125643rank=19600 of ranks=125643rank=19700 of ranks=125643rank=19800 of ranks=125643rank=19900 of ranks=125643rank=20000 of ranks=125643rank=20100 of ranks=125643rank=20200 of ranks=125643rank=20300 of ranks=125643rank=20400 of ranks=125643rank=20500 of ranks=125643rank=20600 of ranks=125643rank=20700 of ranks=125643rank=20800 of ranks=125643rank=20900 of ranks=125643rank=21000 of ranks=125643rank=21100 of ranks=125643rank=21200 of ranks=125643rank=21300 of ranks=125643rank=21400 of ranks=125643rank=21500 of ranks=125643rank=21600 of ranks=125643rank=21700 of ranks=125643rank=21800 of ranks=125643rank=21900 of ranks=125643rank=22000 of ranks=125643rank=22100 of ranks=125643rank=22200 of ranks=125643rank=22300 of ranks=125643rank=22400 of ranks=125643rank=22500 of ranks=125643rank=22600 of ranks=125643rank=22700 of ranks=125643rank=22800 of ranks=125643rank=22900 of ranks=125643rank=23000 of ranks=125643rank=23100 of ranks=125643rank=23200 of ranks=125643rank=23300 of ranks=125643rank=23400 of ranks=125643rank=23500 of ranks=125643rank=23600 of ranks=125643rank=23700 of ranks=125643rank=23800 of ranks=125643rank=23900 of ranks=125643rank=24000 of ranks=125643rank=24100 of ranks=125643rank=24200 of ranks=125643rank=24300 of ranks=125643rank=24400 of ranks=125643rank=24500 of ranks=125643rank=24600 of ranks=125643rank=24700 of ranks=125643rank=24800 of ranks=125643rank=24900 of ranks=125643rank=25000 of ranks=125643rank=25100 of ranks=125643rank=25200 of ranks=125643rank=25300 of ranks=125643rank=25400 of ranks=125643rank=25500 of ranks=125643rank=25600 of ranks=125643rank=25700 of ranks=125643rank=25800 of ranks=125643rank=25900 of ranks=125643rank=26000 of ranks=125643rank=26100 of ranks=125643rank=26200 of ranks=125643rank=26300 of ranks=125643rank=26400 of ranks=125643rank=26500 of ranks=125643rank=26600 of ranks=125643rank=26700 of ranks=125643rank=26800 of ranks=125643rank=26900 of ranks=125643rank=27000 of ranks=125643rank=27100 of ranks=125643rank=27200 of ranks=125643rank=27300 of ranks=125643rank=27400 of ranks=125643rank=27500 of ranks=125643rank=27600 of ranks=125643rank=27700 of ranks=125643rank=27800 of ranks=125643rank=27900 of ranks=125643rank=28000 of ranks=125643rank=28100 of ranks=125643rank=28200 of ranks=125643rank=28300 of ranks=125643rank=28400 of ranks=125643rank=28500 of ranks=125643rank=28600 of ranks=125643rank=28700 of ranks=125643rank=28800 of ranks=125643rank=28900 of ranks=125643rank=29000 of ranks=125643rank=29100 of ranks=125643rank=29200 of ranks=125643rank=29300 of ranks=125643rank=29400 of ranks=125643rank=29500 of ranks=125643rank=29600 of ranks=125643rank=29700 of ranks=125643rank=29800 of ranks=125643rank=29900 of ranks=125643rank=30000 of ranks=125643rank=30100 of ranks=125643rank=30200 of ranks=125643rank=30300 of ranks=125643rank=30400 of ranks=125643rank=30500 of ranks=125643rank=30600 of ranks=125643rank=30700 of ranks=125643rank=30800 of ranks=125643rank=30900 of ranks=125643rank=31000 of ranks=125643rank=31100 of ranks=125643rank=31200 of ranks=125643rank=31300 of ranks=125643rank=31400 of ranks=125643rank=31500 of ranks=125643rank=31600 of ranks=125643rank=31700 of ranks=125643rank=31800 of ranks=125643rank=31900 of ranks=125643rank=32000 of ranks=125643rank=32100 of ranks=125643rank=32200 of ranks=125643rank=32300 of ranks=125643rank=32400 of ranks=125643rank=32500 of ranks=125643rank=32600 of ranks=125643rank=32700 of ranks=125643rank=32800 of ranks=125643rank=32900 of ranks=125643rank=33000 of ranks=125643rank=33100 of ranks=125643rank=33200 of ranks=125643rank=33300 of ranks=125643rank=33400 of ranks=125643rank=33500 of ranks=125643rank=33600 of ranks=125643rank=33700 of ranks=125643rank=33800 of ranks=125643rank=33900 of ranks=125643rank=34000 of ranks=125643rank=34100 of ranks=125643rank=34200 of ranks=125643rank=34300 of ranks=125643rank=34400 of ranks=125643rank=34500 of ranks=125643rank=34600 of ranks=125643rank=34700 of ranks=125643rank=34800 of ranks=125643rank=34900 of ranks=125643rank=35000 of ranks=125643rank=35100 of ranks=125643rank=35200 of ranks=125643rank=35300 of ranks=125643rank=35400 of ranks=125643rank=35500 of ranks=125643rank=35600 of ranks=125643rank=35700 of ranks=125643rank=35800 of ranks=125643rank=35900 of ranks=125643rank=36000 of ranks=125643rank=36100 of ranks=125643rank=36200 of ranks=125643rank=36300 of ranks=125643rank=36400 of ranks=125643rank=36500 of ranks=125643rank=36600 of ranks=125643rank=36700 of ranks=125643rank=36800 of ranks=125643rank=36900 of ranks=125643rank=37000 of ranks=125643rank=37100 of ranks=125643rank=37200 of ranks=125643rank=37300 of ranks=125643rank=37400 of ranks=125643rank=37500 of ranks=125643rank=37600 of ranks=125643rank=37700 of ranks=125643rank=37800 of ranks=125643rank=37900 of ranks=125643rank=38000 of ranks=125643rank=38100 of ranks=125643rank=38200 of ranks=125643rank=38300 of ranks=125643rank=38400 of ranks=125643rank=38500 of ranks=125643rank=38600 of ranks=125643rank=38700 of ranks=125643rank=38800 of ranks=125643rank=38900 of ranks=125643rank=39000 of ranks=125643rank=39100 of ranks=125643rank=39200 of ranks=125643rank=39300 of ranks=125643rank=39400 of ranks=125643rank=39500 of ranks=125643rank=39600 of ranks=125643rank=39700 of ranks=125643rank=39800 of ranks=125643rank=39900 of ranks=125643rank=40000 of ranks=125643rank=40100 of ranks=125643rank=40200 of ranks=125643rank=40300 of ranks=125643rank=40400 of ranks=125643rank=40500 of ranks=125643rank=40600 of ranks=125643rank=40700 of ranks=125643rank=40800 of ranks=125643rank=40900 of ranks=125643rank=41000 of ranks=125643rank=41100 of ranks=125643rank=41200 of ranks=125643rank=41300 of ranks=125643rank=41400 of ranks=125643rank=41500 of ranks=125643rank=41600 of ranks=125643rank=41700 of ranks=125643rank=41800 of ranks=125643rank=41900 of ranks=125643rank=42000 of ranks=125643rank=42100 of ranks=125643rank=42200 of ranks=125643rank=42300 of ranks=125643rank=42400 of ranks=125643rank=42500 of ranks=125643rank=42600 of ranks=125643rank=42700 of ranks=125643rank=42800 of ranks=125643rank=42900 of ranks=125643rank=43000 of ranks=125643rank=43100 of ranks=125643rank=43200 of ranks=125643rank=43300 of ranks=125643rank=43400 of ranks=125643rank=43500 of ranks=125643rank=43600 of ranks=125643rank=43700 of ranks=125643rank=43800 of ranks=125643rank=43900 of ranks=125643rank=44000 of ranks=125643rank=44100 of ranks=125643rank=44200 of ranks=125643rank=44300 of ranks=125643rank=44400 of ranks=125643rank=44500 of ranks=125643rank=44600 of ranks=125643rank=44700 of ranks=125643rank=44800 of ranks=125643rank=44900 of ranks=125643rank=45000 of ranks=125643rank=45100 of ranks=125643rank=45200 of ranks=125643rank=45300 of ranks=125643rank=45400 of ranks=125643rank=45500 of ranks=125643rank=45600 of ranks=125643rank=45700 of ranks=125643rank=45800 of ranks=125643rank=45900 of ranks=125643rank=46000 of ranks=125643rank=46100 of ranks=125643rank=46200 of ranks=125643rank=46300 of ranks=125643rank=46400 of ranks=125643rank=46500 of ranks=125643rank=46600 of ranks=125643rank=46700 of ranks=125643rank=46800 of ranks=125643rank=46900 of ranks=125643rank=47000 of ranks=125643rank=47100 of ranks=125643rank=47200 of ranks=125643rank=47300 of ranks=125643rank=47400 of ranks=125643rank=47500 of ranks=125643rank=47600 of ranks=125643rank=47700 of ranks=125643rank=47800 of ranks=125643rank=47900 of ranks=125643rank=48000 of ranks=125643rank=48100 of ranks=125643rank=48200 of ranks=125643rank=48300 of ranks=125643rank=48400 of ranks=125643rank=48500 of ranks=125643rank=48600 of ranks=125643rank=48700 of ranks=125643rank=48800 of ranks=125643rank=48900 of ranks=125643rank=49000 of ranks=125643rank=49100 of ranks=125643rank=49200 of ranks=125643rank=49300 of ranks=125643rank=49400 of ranks=125643rank=49500 of ranks=125643rank=49600 of ranks=125643rank=49700 of ranks=125643rank=49800 of ranks=125643rank=49900 of ranks=125643rank=50000 of ranks=125643rank=50100 of ranks=125643rank=50200 of ranks=125643rank=50300 of ranks=125643rank=50400 of ranks=125643rank=50500 of ranks=125643rank=50600 of ranks=125643rank=50700 of ranks=125643rank=50800 of ranks=125643rank=50900 of ranks=125643rank=51000 of ranks=125643rank=51100 of ranks=125643rank=51200 of ranks=125643rank=51300 of ranks=125643rank=51400 of ranks=125643rank=51500 of ranks=125643rank=51600 of ranks=125643rank=51700 of ranks=125643rank=51800 of ranks=125643rank=51900 of ranks=125643rank=52000 of ranks=125643rank=52100 of ranks=125643rank=52200 of ranks=125643rank=52300 of ranks=125643rank=52400 of ranks=125643rank=52500 of ranks=125643rank=52600 of ranks=125643rank=52700 of ranks=125643rank=52800 of ranks=125643rank=52900 of ranks=125643rank=53000 of ranks=125643rank=53100 of ranks=125643rank=53200 of ranks=125643rank=53300 of ranks=125643rank=53400 of ranks=125643rank=53500 of ranks=125643rank=53600 of ranks=125643rank=53700 of ranks=125643rank=53800 of ranks=125643rank=53900 of ranks=125643rank=54000 of ranks=125643rank=54100 of ranks=125643rank=54200 of ranks=125643rank=54300 of ranks=125643rank=54400 of ranks=125643rank=54500 of ranks=125643rank=54600 of ranks=125643rank=54700 of ranks=125643rank=54800 of ranks=125643rank=54900 of ranks=125643rank=55000 of ranks=125643rank=55100 of ranks=125643rank=55200 of ranks=125643rank=55300 of ranks=125643rank=55400 of ranks=125643rank=55500 of ranks=125643rank=55600 of ranks=125643rank=55700 of ranks=125643rank=55800 of ranks=125643rank=55900 of ranks=125643rank=56000 of ranks=125643rank=56100 of ranks=125643rank=56200 of ranks=125643rank=56300 of ranks=125643rank=56400 of ranks=125643rank=56500 of ranks=125643rank=56600 of ranks=125643rank=56700 of ranks=125643rank=56800 of ranks=125643rank=56900 of ranks=125643rank=57000 of ranks=125643rank=57100 of ranks=125643rank=57200 of ranks=125643rank=57300 of ranks=125643rank=57400 of ranks=125643rank=57500 of ranks=125643rank=57600 of ranks=125643rank=57700 of ranks=125643rank=57800 of ranks=125643rank=57900 of ranks=125643rank=58000 of ranks=125643rank=58100 of ranks=125643rank=58200 of ranks=125643rank=58300 of ranks=125643rank=58400 of ranks=125643rank=58500 of ranks=125643rank=58600 of ranks=125643rank=58700 of ranks=125643rank=58800 of ranks=125643rank=58900 of ranks=125643rank=59000 of ranks=125643rank=59100 of ranks=125643rank=59200 of ranks=125643rank=59300 of ranks=125643rank=59400 of ranks=125643rank=59500 of ranks=125643rank=59600 of ranks=125643rank=59700 of ranks=125643rank=59800 of ranks=125643rank=59900 of ranks=125643rank=60000 of ranks=125643rank=60100 of ranks=125643rank=60200 of ranks=125643rank=60300 of ranks=125643rank=60400 of ranks=125643rank=60500 of ranks=125643rank=60600 of ranks=125643rank=60700 of ranks=125643rank=60800 of ranks=125643rank=60900 of ranks=125643rank=61000 of ranks=125643rank=61100 of ranks=125643rank=61200 of ranks=125643rank=61300 of ranks=125643rank=61400 of ranks=125643rank=61500 of ranks=125643rank=61600 of ranks=125643rank=61700 of ranks=125643rank=61800 of ranks=125643rank=61900 of ranks=125643rank=62000 of ranks=125643rank=62100 of ranks=125643rank=62200 of ranks=125643rank=62300 of ranks=125643rank=62400 of ranks=125643rank=62500 of ranks=125643rank=62600 of ranks=125643rank=62700 of ranks=125643rank=62800 of ranks=125643rank=62900 of ranks=125643rank=63000 of ranks=125643rank=63100 of ranks=125643rank=63200 of ranks=125643rank=63300 of ranks=125643rank=63400 of ranks=125643rank=63500 of ranks=125643rank=63600 of ranks=125643rank=63700 of ranks=125643rank=63800 of ranks=125643rank=63900 of ranks=125643rank=64000 of ranks=125643rank=64100 of ranks=125643rank=64200 of ranks=125643rank=64300 of ranks=125643rank=64400 of ranks=125643rank=64500 of ranks=125643rank=64600 of ranks=125643rank=64700 of ranks=125643rank=64800 of ranks=125643rank=64900 of ranks=125643rank=65000 of ranks=125643rank=65100 of ranks=125643rank=65200 of ranks=125643rank=65300 of ranks=125643rank=65400 of ranks=125643rank=65500 of ranks=125643rank=65600 of ranks=125643rank=65700 of ranks=125643rank=65800 of ranks=125643rank=65900 of ranks=125643rank=66000 of ranks=125643rank=66100 of ranks=125643rank=66200 of ranks=125643rank=66300 of ranks=125643rank=66400 of ranks=125643rank=66500 of ranks=125643rank=66600 of ranks=125643rank=66700 of ranks=125643rank=66800 of ranks=125643rank=66900 of ranks=125643rank=67000 of ranks=125643rank=67100 of ranks=125643rank=67200 of ranks=125643rank=67300 of ranks=125643rank=67400 of ranks=125643rank=67500 of ranks=125643rank=67600 of ranks=125643rank=67700 of ranks=125643rank=67800 of ranks=125643rank=67900 of ranks=125643rank=68000 of ranks=125643rank=68100 of ranks=125643rank=68200 of ranks=125643rank=68300 of ranks=125643rank=68400 of ranks=125643rank=68500 of ranks=125643rank=68600 of ranks=125643rank=68700 of ranks=125643rank=68800 of ranks=125643rank=68900 of ranks=125643rank=69000 of ranks=125643rank=69100 of ranks=125643rank=69200 of ranks=125643rank=69300 of ranks=125643rank=69400 of ranks=125643rank=69500 of ranks=125643rank=69600 of ranks=125643rank=69700 of ranks=125643rank=69800 of ranks=125643rank=69900 of ranks=125643rank=70000 of ranks=125643rank=70100 of ranks=125643rank=70200 of ranks=125643rank=70300 of ranks=125643rank=70400 of ranks=125643rank=70500 of ranks=125643rank=70600 of ranks=125643rank=70700 of ranks=125643rank=70800 of ranks=125643rank=70900 of ranks=125643rank=71000 of ranks=125643rank=71100 of ranks=125643rank=71200 of ranks=125643rank=71300 of ranks=125643rank=71400 of ranks=125643rank=71500 of ranks=125643rank=71600 of ranks=125643rank=71700 of ranks=125643rank=71800 of ranks=125643rank=71900 of ranks=125643rank=72000 of ranks=125643rank=72100 of ranks=125643rank=72200 of ranks=125643rank=72300 of ranks=125643rank=72400 of ranks=125643rank=72500 of ranks=125643rank=72600 of ranks=125643rank=72700 of ranks=125643rank=72800 of ranks=125643rank=72900 of ranks=125643rank=73000 of ranks=125643rank=73100 of ranks=125643rank=73200 of ranks=125643rank=73300 of ranks=125643rank=73400 of ranks=125643rank=73500 of ranks=125643rank=73600 of ranks=125643rank=73700 of ranks=125643rank=73800 of ranks=125643rank=73900 of ranks=125643rank=74000 of ranks=125643rank=74100 of ranks=125643rank=74200 of ranks=125643rank=74300 of ranks=125643rank=74400 of ranks=125643rank=74500 of ranks=125643rank=74600 of ranks=125643rank=74700 of ranks=125643rank=74800 of ranks=125643rank=74900 of ranks=125643rank=75000 of ranks=125643rank=75100 of ranks=125643rank=75200 of ranks=125643rank=75300 of ranks=125643rank=75400 of ranks=125643rank=75500 of ranks=125643rank=75600 of ranks=125643rank=75700 of ranks=125643rank=75800 of ranks=125643rank=75900 of ranks=125643rank=76000 of ranks=125643rank=76100 of ranks=125643rank=76200 of ranks=125643rank=76300 of ranks=125643rank=76400 of ranks=125643rank=76500 of ranks=125643rank=76600 of ranks=125643rank=76700 of ranks=125643rank=76800 of ranks=125643rank=76900 of ranks=125643rank=77000 of ranks=125643rank=77100 of ranks=125643rank=77200 of ranks=125643rank=77300 of ranks=125643rank=77400 of ranks=125643rank=77500 of ranks=125643rank=77600 of ranks=125643rank=77700 of ranks=125643rank=77800 of ranks=125643rank=77900 of ranks=125643rank=78000 of ranks=125643rank=78100 of ranks=125643rank=78200 of ranks=125643rank=78300 of ranks=125643rank=78400 of ranks=125643rank=78500 of ranks=125643rank=78600 of ranks=125643rank=78700 of ranks=125643rank=78800 of ranks=125643rank=78900 of ranks=125643rank=79000 of ranks=125643rank=79100 of ranks=125643rank=79200 of ranks=125643rank=79300 of ranks=125643rank=79400 of ranks=125643rank=79500 of ranks=125643rank=79600 of ranks=125643rank=79700 of ranks=125643rank=79800 of ranks=125643rank=79900 of ranks=125643rank=80000 of ranks=125643rank=80100 of ranks=125643rank=80200 of ranks=125643rank=80300 of ranks=125643rank=80400 of ranks=125643rank=80500 of ranks=125643rank=80600 of ranks=125643rank=80700 of ranks=125643rank=80800 of ranks=125643rank=80900 of ranks=125643rank=81000 of ranks=125643rank=81100 of ranks=125643rank=81200 of ranks=125643rank=81300 of ranks=125643rank=81400 of ranks=125643rank=81500 of ranks=125643rank=81600 of ranks=125643rank=81700 of ranks=125643rank=81800 of ranks=125643rank=81900 of ranks=125643rank=82000 of ranks=125643rank=82100 of ranks=125643rank=82200 of ranks=125643rank=82300 of ranks=125643rank=82400 of ranks=125643rank=82500 of ranks=125643rank=82600 of ranks=125643rank=82700 of ranks=125643rank=82800 of ranks=125643rank=82900 of ranks=125643rank=83000 of ranks=125643rank=83100 of ranks=125643rank=83200 of ranks=125643rank=83300 of ranks=125643rank=83400 of ranks=125643rank=83500 of ranks=125643rank=83600 of ranks=125643rank=83700 of ranks=125643rank=83800 of ranks=125643rank=83900 of ranks=125643rank=84000 of ranks=125643rank=84100 of ranks=125643rank=84200 of ranks=125643rank=84300 of ranks=125643rank=84400 of ranks=125643rank=84500 of ranks=125643rank=84600 of ranks=125643rank=84700 of ranks=125643rank=84800 of ranks=125643rank=84900 of ranks=125643rank=85000 of ranks=125643rank=85100 of ranks=125643rank=85200 of ranks=125643rank=85300 of ranks=125643rank=85400 of ranks=125643rank=85500 of ranks=125643rank=85600 of ranks=125643rank=85700 of ranks=125643rank=85800 of ranks=125643rank=85900 of ranks=125643rank=86000 of ranks=125643rank=86100 of ranks=125643rank=86200 of ranks=125643rank=86300 of ranks=125643rank=86400 of ranks=125643rank=86500 of ranks=125643rank=86600 of ranks=125643rank=86700 of ranks=125643rank=86800 of ranks=125643rank=86900 of ranks=125643rank=87000 of ranks=125643rank=87100 of ranks=125643rank=87200 of ranks=125643rank=87300 of ranks=125643rank=87400 of ranks=125643rank=87500 of ranks=125643rank=87600 of ranks=125643rank=87700 of ranks=125643rank=87800 of ranks=125643rank=87900 of ranks=125643rank=88000 of ranks=125643rank=88100 of ranks=125643rank=88200 of ranks=125643rank=88300 of ranks=125643rank=88400 of ranks=125643rank=88500 of ranks=125643rank=88600 of ranks=125643rank=88700 of ranks=125643rank=88800 of ranks=125643rank=88900 of ranks=125643rank=89000 of ranks=125643rank=89100 of ranks=125643rank=89200 of ranks=125643rank=89300 of ranks=125643rank=89400 of ranks=125643rank=89500 of ranks=125643rank=89600 of ranks=125643rank=89700 of ranks=125643rank=89800 of ranks=125643rank=89900 of ranks=125643rank=90000 of ranks=125643rank=90100 of ranks=125643rank=90200 of ranks=125643rank=90300 of ranks=125643rank=90400 of ranks=125643rank=90500 of ranks=125643rank=90600 of ranks=125643rank=90700 of ranks=125643rank=90800 of ranks=125643rank=90900 of ranks=125643rank=91000 of ranks=125643rank=91100 of ranks=125643rank=91200 of ranks=125643rank=91300 of ranks=125643rank=91400 of ranks=125643rank=91500 of ranks=125643rank=91600 of ranks=125643rank=91700 of ranks=125643rank=91800 of ranks=125643rank=91900 of ranks=125643rank=92000 of ranks=125643rank=92100 of ranks=125643rank=92200 of ranks=125643rank=92300 of ranks=125643rank=92400 of ranks=125643rank=92500 of ranks=125643rank=92600 of ranks=125643rank=92700 of ranks=125643rank=92800 of ranks=125643rank=92900 of ranks=125643rank=93000 of ranks=125643rank=93100 of ranks=125643rank=93200 of ranks=125643rank=93300 of ranks=125643rank=93400 of ranks=125643rank=93500 of ranks=125643rank=93600 of ranks=125643rank=93700 of ranks=125643rank=93800 of ranks=125643rank=93900 of ranks=125643rank=94000 of ranks=125643rank=94100 of ranks=125643rank=94200 of ranks=125643rank=94300 of ranks=125643rank=94400 of ranks=125643rank=94500 of ranks=125643rank=94600 of ranks=125643rank=94700 of ranks=125643rank=94800 of ranks=125643rank=94900 of ranks=125643rank=95000 of ranks=125643rank=95100 of ranks=125643rank=95200 of ranks=125643rank=95300 of ranks=125643rank=95400 of ranks=125643rank=95500 of ranks=125643rank=95600 of ranks=125643rank=95700 of ranks=125643rank=95800 of ranks=125643rank=95900 of ranks=125643rank=96000 of ranks=125643rank=96100 of ranks=125643rank=96200 of ranks=125643rank=96300 of ranks=125643rank=96400 of ranks=125643rank=96500 of ranks=125643rank=96600 of ranks=125643rank=96700 of ranks=125643rank=96800 of ranks=125643rank=96900 of ranks=125643rank=97000 of ranks=125643rank=97100 of ranks=125643rank=97200 of ranks=125643rank=97300 of ranks=125643rank=97400 of ranks=125643rank=97500 of ranks=125643rank=97600 of ranks=125643rank=97700 of ranks=125643rank=97800 of ranks=125643rank=97900 of ranks=125643rank=98000 of ranks=125643rank=98100 of ranks=125643rank=98200 of ranks=125643rank=98300 of ranks=125643rank=98400 of ranks=125643rank=98500 of ranks=125643rank=98600 of ranks=125643rank=98700 of ranks=125643rank=98800 of ranks=125643rank=98900 of ranks=125643rank=99000 of ranks=125643rank=99100 of ranks=125643rank=99200 of ranks=125643rank=99300 of ranks=125643rank=99400 of ranks=125643rank=99500 of ranks=125643rank=99600 of ranks=125643rank=99700 of ranks=125643rank=99800 of ranks=125643rank=99900 of ranks=125643rank=100000 of ranks=125643rank=100100 of ranks=125643rank=100200 of ranks=125643rank=100300 of ranks=125643rank=100400 of ranks=125643rank=100500 of ranks=125643rank=100600 of ranks=125643rank=100700 of ranks=125643rank=100800 of ranks=125643rank=100900 of ranks=125643rank=101000 of ranks=125643rank=101100 of ranks=125643rank=101200 of ranks=125643rank=101300 of ranks=125643rank=101400 of ranks=125643rank=101500 of ranks=125643rank=101600 of ranks=125643rank=101700 of ranks=125643rank=101800 of ranks=125643rank=101900 of ranks=125643rank=102000 of ranks=125643rank=102100 of ranks=125643rank=102200 of ranks=125643rank=102300 of ranks=125643rank=102400 of ranks=125643rank=102500 of ranks=125643rank=102600 of ranks=125643rank=102700 of ranks=125643rank=102800 of ranks=125643rank=102900 of ranks=125643rank=103000 of ranks=125643rank=103100 of ranks=125643rank=103200 of ranks=125643rank=103300 of ranks=125643rank=103400 of ranks=125643rank=103500 of ranks=125643rank=103600 of ranks=125643rank=103700 of ranks=125643rank=103800 of ranks=125643rank=103900 of ranks=125643rank=104000 of ranks=125643rank=104100 of ranks=125643rank=104200 of ranks=125643rank=104300 of ranks=125643rank=104400 of ranks=125643rank=104500 of ranks=125643rank=104600 of ranks=125643rank=104700 of ranks=125643rank=104800 of ranks=125643rank=104900 of ranks=125643rank=105000 of ranks=125643rank=105100 of ranks=125643rank=105200 of ranks=125643rank=105300 of ranks=125643rank=105400 of ranks=125643rank=105500 of ranks=125643rank=105600 of ranks=125643rank=105700 of ranks=125643rank=105800 of ranks=125643rank=105900 of ranks=125643rank=106000 of ranks=125643rank=106100 of ranks=125643rank=106200 of ranks=125643rank=106300 of ranks=125643rank=106400 of ranks=125643rank=106500 of ranks=125643rank=106600 of ranks=125643rank=106700 of ranks=125643rank=106800 of ranks=125643rank=106900 of ranks=125643rank=107000 of ranks=125643rank=107100 of ranks=125643rank=107200 of ranks=125643rank=107300 of ranks=125643rank=107400 of ranks=125643rank=107500 of ranks=125643rank=107600 of ranks=125643rank=107700 of ranks=125643rank=107800 of ranks=125643rank=107900 of ranks=125643rank=108000 of ranks=125643rank=108100 of ranks=125643rank=108200 of ranks=125643rank=108300 of ranks=125643rank=108400 of ranks=125643rank=108500 of ranks=125643rank=108600 of ranks=125643rank=108700 of ranks=125643rank=108800 of ranks=125643rank=108900 of ranks=125643rank=109000 of ranks=125643rank=109100 of ranks=125643rank=109200 of ranks=125643rank=109300 of ranks=125643rank=109400 of ranks=125643rank=109500 of ranks=125643rank=109600 of ranks=125643rank=109700 of ranks=125643rank=109800 of ranks=125643rank=109900 of ranks=125643rank=110000 of ranks=125643rank=110100 of ranks=125643rank=110200 of ranks=125643rank=110300 of ranks=125643rank=110400 of ranks=125643rank=110500 of ranks=125643rank=110600 of ranks=125643rank=110700 of ranks=125643rank=110800 of ranks=125643rank=110900 of ranks=125643rank=111000 of ranks=125643rank=111100 of ranks=125643rank=111200 of ranks=125643rank=111300 of ranks=125643rank=111400 of ranks=125643rank=111500 of ranks=125643rank=111600 of ranks=125643rank=111700 of ranks=125643rank=111800 of ranks=125643rank=111900 of ranks=125643rank=112000 of ranks=125643rank=112100 of ranks=125643rank=112200 of ranks=125643rank=112300 of ranks=125643rank=112400 of ranks=125643rank=112500 of ranks=125643rank=112600 of ranks=125643rank=112700 of ranks=125643rank=112800 of ranks=125643rank=112900 of ranks=125643rank=113000 of ranks=125643rank=113100 of ranks=125643rank=113200 of ranks=125643rank=113300 of ranks=125643rank=113400 of ranks=125643rank=113500 of ranks=125643rank=113600 of ranks=125643rank=113700 of ranks=125643rank=113800 of ranks=125643rank=113900 of ranks=125643rank=114000 of ranks=125643rank=114100 of ranks=125643rank=114200 of ranks=125643rank=114300 of ranks=125643rank=114400 of ranks=125643rank=114500 of ranks=125643rank=114600 of ranks=125643rank=114700 of ranks=125643rank=114800 of ranks=125643rank=114900 of ranks=125643rank=115000 of ranks=125643rank=115100 of ranks=125643rank=115200 of ranks=125643rank=115300 of ranks=125643rank=115400 of ranks=125643rank=115500 of ranks=125643rank=115600 of ranks=125643rank=115700 of ranks=125643rank=115800 of ranks=125643rank=115900 of ranks=125643rank=116000 of ranks=125643rank=116100 of ranks=125643rank=116200 of ranks=125643rank=116300 of ranks=125643rank=116400 of ranks=125643rank=116500 of ranks=125643rank=116600 of ranks=125643rank=116700 of ranks=125643rank=116800 of ranks=125643rank=116900 of ranks=125643rank=117000 of ranks=125643rank=117100 of ranks=125643rank=117200 of ranks=125643rank=117300 of ranks=125643rank=117400 of ranks=125643rank=117500 of ranks=125643rank=117600 of ranks=125643rank=117700 of ranks=125643rank=117800 of ranks=125643rank=117900 of ranks=125643rank=118000 of ranks=125643rank=118100 of ranks=125643rank=118200 of ranks=125643rank=118300 of ranks=125643rank=118400 of ranks=125643rank=118500 of ranks=125643rank=118600 of ranks=125643rank=118700 of ranks=125643rank=118800 of ranks=125643rank=118900 of ranks=125643rank=119000 of ranks=125643rank=119100 of ranks=125643rank=119200 of ranks=125643rank=119300 of ranks=125643rank=119400 of ranks=125643rank=119500 of ranks=125643rank=119600 of ranks=125643rank=119700 of ranks=125643rank=119800 of ranks=125643rank=119900 of ranks=125643rank=120000 of ranks=125643rank=120100 of ranks=125643rank=120200 of ranks=125643rank=120300 of ranks=125643rank=120400 of ranks=125643rank=120500 of ranks=125643rank=120600 of ranks=125643rank=120700 of ranks=125643rank=120800 of ranks=125643rank=120900 of ranks=125643rank=121000 of ranks=125643rank=121100 of ranks=125643rank=121200 of ranks=125643rank=121300 of ranks=125643rank=121400 of ranks=125643rank=121500 of ranks=125643rank=121600 of ranks=125643rank=121700 of ranks=125643rank=121800 of ranks=125643rank=121900 of ranks=125643rank=122000 of ranks=125643rank=122100 of ranks=125643rank=122200 of ranks=125643rank=122300 of ranks=125643rank=122400 of ranks=125643rank=122500 of ranks=125643rank=122600 of ranks=125643rank=122700 of ranks=125643rank=122800 of ranks=125643rank=122900 of ranks=125643rank=123000 of ranks=125643rank=123100 of ranks=125643rank=123200 of ranks=125643rank=123300 of ranks=125643rank=123400 of ranks=125643rank=123500 of ranks=125643rank=123600 of ranks=125643rank=123700 of ranks=125643rank=123800 of ranks=125643rank=123900 of ranks=125643rank=124000 of ranks=125643rank=124100 of ranks=125643rank=124200 of ranks=125643rank=124300 of ranks=125643rank=124400 of ranks=125643rank=124500 of ranks=125643rank=124600 of ranks=125643rank=124700 of ranks=125643rank=124800 of ranks=125643rank=124900 of ranks=125643rank=125000 of ranks=125643rank=125100 of ranks=125643rank=125200 of ranks=125643rank=125300 of ranks=125643rank=125400 of ranks=125643rank=125500 of ranks=125643rank=125600 of ranks=125643

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              95.9742    487   3456     11    498           71.8487
   1 car                    98.2850  50000  44988    316  50316           77.2129
   2 truck                  95.0832   1804   6822     21   1825           66.3936
   3 bus                    92.2650    361   3709      5    366           64.9779
   4 pedestrian             94.5128   4116   9900    143   4259           70.3785

for conf_thresh=0.25, precision=0.87, recall=0.96, F1 score=0.91
for conf_thresh=0.25, TP=54744, FP=8524, FN=2520, average IoU=76.25%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=95.22%
Total detection time: 172 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5061: loss=4.299, avg loss=3.423, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=3.8 seconds, 323904 images, time remaining=2.9 hours
5062: loss=3.315, avg loss=3.413, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=3.8 seconds, 323968 images, time remaining=2.9 hours
5063: loss=3.285, avg loss=3.400, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=3.8 seconds, 324032 images, time remaining=2.9 hours
5064: loss=3.657, avg loss=3.425, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=980.7 milliseconds, train=3.8 seconds, 324096 images, time remaining=2.9 hours
5065: loss=3.553, avg loss=3.438, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.9 seconds, train=3.8 seconds, 324160 images, time remaining=2.9 hours
5066: loss=3.872, avg loss=3.482, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=3.8 seconds, 324224 images, time remaining=2.9 hours
5067: loss=3.607, avg loss=3.494, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=755.9 milliseconds, train=3.8 seconds, 324288 images, time remaining=2.9 hours
5068: loss=3.072, avg loss=3.452, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=3.8 seconds, 324352 images, time remaining=2.9 hours
5069: loss=2.826, avg loss=3.389, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=3.8 seconds, 324416 images, time remaining=2.9 hours
5070: loss=3.420, avg loss=3.392, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=3.8 seconds, 324480 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1463b6000000
5071: loss=3.549, avg loss=3.408, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=859.2 milliseconds, train=1.6 seconds, 324544 images, time remaining=2.9 hours
5072: loss=3.769, avg loss=3.444, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=858.8 milliseconds, train=1.5 seconds, 324608 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5073: loss=3.583, avg loss=3.458, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 324672 images, time remaining=2.9 hours
5074: loss=3.044, avg loss=3.417, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 324736 images, time remaining=2.9 hours
5075: loss=3.306, avg loss=3.406, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 324800 images, time remaining=2.9 hours
5076: loss=3.786, avg loss=3.444, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=663.3 milliseconds, train=1.6 seconds, 324864 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5077: loss=3.513, avg loss=3.450, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.9 seconds, train=1.5 seconds, 324928 images, time remaining=2.9 hours
5078: loss=3.074, avg loss=3.413, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=988.8 milliseconds, train=1.5 seconds, 324992 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5079: loss=3.180, avg loss=3.390, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 325056 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5080: loss=3.252, avg loss=3.376, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 325120 images, time remaining=2.9 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146402000000
5081: loss=2.581, avg loss=3.296, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=599.4 milliseconds, train=1.6 seconds, 325184 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5082: loss=3.327, avg loss=3.299, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 325248 images, time remaining=2.9 hours
5083: loss=3.241, avg loss=3.294, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 325312 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5084: loss=2.666, avg loss=3.231, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 325376 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5085: loss=2.480, avg loss=3.156, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 325440 images, time remaining=2.9 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5086: loss=2.610, avg loss=3.101, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.9 seconds, train=1.6 seconds, 325504 images, time remaining=2.9 hours
5087: loss=2.891, avg loss=3.080, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 325568 images, time remaining=2.8 hours
5088: loss=2.472, avg loss=3.019, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.7 seconds, 325632 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5089: loss=3.412, avg loss=3.059, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.1 seconds, train=1.6 seconds, 325696 images, time remaining=2.8 hours
5090: loss=3.117, avg loss=3.064, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 325760 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146492000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5091: loss=3.219, avg loss=3.080, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 325824 images, time remaining=2.8 hours
5092: loss=3.212, avg loss=3.093, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 325888 images, time remaining=2.8 hours
5093: loss=3.153, avg loss=3.099, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=592.2 milliseconds, train=1.5 seconds, 325952 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5094: loss=2.289, avg loss=3.018, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 326016 images, time remaining=2.8 hours
5095: loss=3.094, avg loss=3.026, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 326080 images, time remaining=2.8 hours
5096: loss=3.453, avg loss=3.068, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=736.2 milliseconds, train=1.5 seconds, 326144 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5097: loss=3.387, avg loss=3.100, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 326208 images, time remaining=2.8 hours
5098: loss=3.032, avg loss=3.093, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 326272 images, time remaining=2.8 hours
5099: loss=3.168, avg loss=3.101, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 326336 images, time remaining=2.8 hours
5100: loss=3.111, avg loss=3.102, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=857.1 milliseconds, train=1.5 seconds, 326400 images, time remaining=2.8 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5101: loss=3.722, avg loss=3.164, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=519.0 milliseconds, train=2.1 seconds, 326464 images, time remaining=2.8 hours
5102: loss=3.003, avg loss=3.148, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=900.5 milliseconds, train=2.1 seconds, 326528 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5103: loss=3.388, avg loss=3.172, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=2.1 seconds, 326592 images, time remaining=2.8 hours
5104: loss=2.890, avg loss=3.144, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=2.1 seconds, 326656 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5105: loss=3.183, avg loss=3.148, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=2.1 seconds, 326720 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5106: loss=2.796, avg loss=3.112, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 326784 images, time remaining=2.8 hours
5107: loss=3.929, avg loss=3.194, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=695.0 milliseconds, train=2.1 seconds, 326848 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5108: loss=3.372, avg loss=3.212, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=2.1 seconds, 326912 images, time remaining=2.8 hours
5109: loss=2.377, avg loss=3.128, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 326976 images, time remaining=2.8 hours
5110: loss=2.765, avg loss=3.092, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=837.3 milliseconds, train=2.1 seconds, 327040 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146028000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5111: loss=3.955, avg loss=3.178, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 327104 images, time remaining=2.8 hours
5112: loss=2.449, avg loss=3.105, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 327168 images, time remaining=2.8 hours
5113: loss=3.170, avg loss=3.112, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=669.9 milliseconds, train=1.5 seconds, 327232 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5114: loss=3.323, avg loss=3.133, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 327296 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5115: loss=3.167, avg loss=3.136, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 327360 images, time remaining=2.8 hours
5116: loss=2.073, avg loss=3.030, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=868.2 milliseconds, train=1.5 seconds, 327424 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5117: loss=2.932, avg loss=3.020, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 327488 images, time remaining=2.8 hours
5118: loss=2.880, avg loss=3.006, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 327552 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5119: loss=2.709, avg loss=2.977, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 327616 images, time remaining=2.8 hours
5120: loss=2.870, avg loss=2.966, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 327680 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5121: loss=5.222, avg loss=3.192, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 327744 images, time remaining=2.8 hours
5122: loss=4.351, avg loss=3.308, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=887.1 milliseconds, train=3.9 seconds, 327808 images, time remaining=2.8 hours
5123: loss=4.947, avg loss=3.472, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 327872 images, time remaining=2.8 hours
5124: loss=4.555, avg loss=3.580, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 327936 images, time remaining=2.8 hours
5125: loss=2.577, avg loss=3.480, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.8 seconds, 328000 images, time remaining=2.8 hours
5126: loss=3.983, avg loss=3.530, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=3.9 seconds, 328064 images, time remaining=2.8 hours
5127: loss=3.820, avg loss=3.559, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=618.1 milliseconds, train=3.9 seconds, 328128 images, time remaining=2.8 hours
5128: loss=3.683, avg loss=3.571, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 328192 images, time remaining=2.8 hours
5129: loss=4.665, avg loss=3.681, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=3.9 seconds, 328256 images, time remaining=2.8 hours
5130: loss=3.453, avg loss=3.658, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=3.8 seconds, 328320 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5131: loss=4.045, avg loss=3.697, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=3.6 seconds, 328384 images, time remaining=2.8 hours
5132: loss=3.350, avg loss=3.662, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=584.5 milliseconds, train=3.6 seconds, 328448 images, time remaining=2.8 hours
5133: loss=3.497, avg loss=3.645, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 328512 images, time remaining=2.8 hours
5134: loss=2.956, avg loss=3.577, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=3.6 seconds, 328576 images, time remaining=2.8 hours
5135: loss=2.955, avg loss=3.514, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 328640 images, time remaining=2.8 hours
5136: loss=3.779, avg loss=3.541, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=3.6 seconds, 328704 images, time remaining=2.8 hours
5137: loss=4.123, avg loss=3.599, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=795.5 milliseconds, train=3.6 seconds, 328768 images, time remaining=2.8 hours
5138: loss=3.218, avg loss=3.561, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=763.0 milliseconds, train=3.6 seconds, 328832 images, time remaining=2.8 hours
5139: loss=3.835, avg loss=3.588, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 328896 images, time remaining=2.8 hours
5140: loss=4.112, avg loss=3.641, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=398.2 milliseconds, train=3.6 seconds, 328960 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5141: loss=3.234, avg loss=3.600, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 329024 images, time remaining=2.8 hours
5142: loss=3.323, avg loss=3.572, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=844.7 milliseconds, train=3.3 seconds, 329088 images, time remaining=2.8 hours
5143: loss=2.801, avg loss=3.495, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=973.4 milliseconds, train=3.2 seconds, 329152 images, time remaining=2.8 hours
5144: loss=3.154, avg loss=3.461, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=758.1 milliseconds, train=3.3 seconds, 329216 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5145: loss=2.960, avg loss=3.411, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=3.3 seconds, 329280 images, time remaining=2.8 hours
5146: loss=3.239, avg loss=3.394, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 329344 images, time remaining=2.8 hours
5147: loss=2.719, avg loss=3.326, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.9 seconds, train=3.3 seconds, 329408 images, time remaining=2.8 hours
5148: loss=3.149, avg loss=3.309, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=859.2 milliseconds, train=3.3 seconds, 329472 images, time remaining=2.8 hours
5149: loss=3.133, avg loss=3.291, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=3.3 seconds, 329536 images, time remaining=2.8 hours
5150: loss=2.654, avg loss=3.227, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=552.0 milliseconds, train=3.3 seconds, 329600 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145b20000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5151: loss=2.902, avg loss=3.195, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=1.6 seconds, 329664 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5152: loss=3.109, avg loss=3.186, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 329728 images, time remaining=2.8 hours
5153: loss=2.499, avg loss=3.117, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=698.3 milliseconds, train=1.5 seconds, 329792 images, time remaining=2.8 hours
5154: loss=2.951, avg loss=3.101, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=945.7 milliseconds, train=1.5 seconds, 329856 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5155: loss=2.977, avg loss=3.088, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 329920 images, time remaining=2.8 hours
5156: loss=3.300, avg loss=3.110, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=611.6 milliseconds, train=1.5 seconds, 329984 images, time remaining=2.8 hours
5157: loss=3.825, avg loss=3.181, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 330048 images, time remaining=2.8 hours
5158: loss=2.777, avg loss=3.141, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=536.6 milliseconds, train=1.5 seconds, 330112 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5159: loss=3.376, avg loss=3.164, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 330176 images, time remaining=2.8 hours
5160: loss=2.969, avg loss=3.145, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=928.1 milliseconds, train=1.5 seconds, 330240 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5161: loss=4.790, avg loss=3.309, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=650.1 milliseconds, train=3.9 seconds, 330304 images, time remaining=2.8 hours
5162: loss=4.386, avg loss=3.417, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=906.5 milliseconds, train=3.9 seconds, 330368 images, time remaining=2.8 hours
5163: loss=3.952, avg loss=3.470, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=962.1 milliseconds, train=3.9 seconds, 330432 images, time remaining=2.8 hours
5164: loss=3.725, avg loss=3.496, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=676.0 milliseconds, train=3.9 seconds, 330496 images, time remaining=2.8 hours
5165: loss=3.471, avg loss=3.493, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 330560 images, time remaining=2.8 hours
5166: loss=2.949, avg loss=3.439, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 330624 images, time remaining=2.8 hours
5167: loss=4.220, avg loss=3.517, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=777.1 milliseconds, train=3.9 seconds, 330688 images, time remaining=2.8 hours
5168: loss=4.062, avg loss=3.572, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 330752 images, time remaining=2.8 hours
5169: loss=4.039, avg loss=3.618, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=3.9 seconds, 330816 images, time remaining=2.8 hours
5170: loss=4.495, avg loss=3.706, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 330880 images, time remaining=2.8 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145ca4000000
5171: loss=3.636, avg loss=3.699, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=550.3 milliseconds, train=1.4 seconds, 330944 images, time remaining=2.8 hours
5172: loss=3.121, avg loss=3.641, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=513.7 milliseconds, train=1.4 seconds, 331008 images, time remaining=2.8 hours
5173: loss=3.254, avg loss=3.602, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=653.0 milliseconds, train=1.4 seconds, 331072 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5174: loss=3.034, avg loss=3.546, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 331136 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5175: loss=4.209, avg loss=3.612, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 331200 images, time remaining=2.8 hours
5176: loss=2.797, avg loss=3.530, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=470.9 milliseconds, train=1.4 seconds, 331264 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5177: loss=2.853, avg loss=3.463, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 331328 images, time remaining=2.8 hours
5178: loss=3.545, avg loss=3.471, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=781.1 milliseconds, train=1.5 seconds, 331392 images, time remaining=2.8 hours
5179: loss=3.113, avg loss=3.435, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=799.2 milliseconds, train=1.4 seconds, 331456 images, time remaining=2.8 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5180: loss=3.123, avg loss=3.404, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=1.4 seconds, 331520 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5181: loss=6.637, avg loss=3.727, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=932.8 milliseconds, train=4.6 seconds, 331584 images, time remaining=2.7 hours
5182: loss=5.151, avg loss=3.870, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 331648 images, time remaining=2.7 hours
5183: loss=4.805, avg loss=3.963, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=4.6 seconds, 331712 images, time remaining=2.7 hours
5184: loss=4.042, avg loss=3.971, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 331776 images, time remaining=2.7 hours
5185: loss=3.366, avg loss=3.911, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=894.2 milliseconds, train=4.6 seconds, 331840 images, time remaining=2.7 hours
5186: loss=4.130, avg loss=3.932, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=4.6 seconds, 331904 images, time remaining=2.7 hours
5187: loss=4.847, avg loss=4.024, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=971.7 milliseconds, train=4.6 seconds, 331968 images, time remaining=2.7 hours
5188: loss=5.604, avg loss=4.182, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=4.6 seconds, 332032 images, time remaining=2.7 hours
5189: loss=4.490, avg loss=4.213, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 332096 images, time remaining=2.7 hours
5190: loss=4.559, avg loss=4.247, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=909.5 milliseconds, train=4.6 seconds, 332160 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5191: loss=3.784, avg loss=4.201, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.9 seconds, train=3.3 seconds, 332224 images, time remaining=2.7 hours
5192: loss=3.009, avg loss=4.082, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.2 seconds, 332288 images, time remaining=2.7 hours
5193: loss=3.061, avg loss=3.980, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 332352 images, time remaining=2.7 hours
5194: loss=2.669, avg loss=3.849, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=608.3 milliseconds, train=3.2 seconds, 332416 images, time remaining=2.7 hours
5195: loss=4.381, avg loss=3.902, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 332480 images, time remaining=2.7 hours
5196: loss=3.711, avg loss=3.883, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=779.4 milliseconds, train=3.3 seconds, 332544 images, time remaining=2.7 hours
5197: loss=3.706, avg loss=3.865, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 332608 images, time remaining=2.7 hours
5198: loss=2.897, avg loss=3.768, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=529.2 milliseconds, train=3.3 seconds, 332672 images, time remaining=2.7 hours
5199: loss=3.732, avg loss=3.765, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=3.2 seconds, 332736 images, time remaining=2.7 hours
5200: loss=3.607, avg loss=3.749, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=731.2 milliseconds, train=3.3 seconds, 332800 images, time remaining=2.7 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5201: loss=3.484, avg loss=3.722, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=932.8 milliseconds, train=2.1 seconds, 332864 images, time remaining=2.7 hours
5202: loss=3.564, avg loss=3.707, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=788.2 milliseconds, train=2.1 seconds, 332928 images, time remaining=2.7 hours
5203: loss=3.411, avg loss=3.677, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=729.1 milliseconds, train=2.1 seconds, 332992 images, time remaining=2.7 hours
5204: loss=2.696, avg loss=3.579, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=639.5 milliseconds, train=2.1 seconds, 333056 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5205: loss=2.947, avg loss=3.516, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.2 seconds, train=2.1 seconds, 333120 images, time remaining=2.7 hours
5206: loss=3.630, avg loss=3.527, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 333184 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5207: loss=3.155, avg loss=3.490, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=2.1 seconds, 333248 images, time remaining=2.7 hours
5208: loss=3.116, avg loss=3.453, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=907.3 milliseconds, train=2.1 seconds, 333312 images, time remaining=2.7 hours
5209: loss=3.590, avg loss=3.466, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 333376 images, time remaining=2.7 hours
5210: loss=3.471, avg loss=3.467, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=2.1 seconds, 333440 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5211: loss=2.842, avg loss=3.404, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=901.6 milliseconds, train=3.1 seconds, 333504 images, time remaining=2.7 hours
5212: loss=3.118, avg loss=3.376, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=3.2 seconds, 333568 images, time remaining=2.7 hours
5213: loss=3.900, avg loss=3.428, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 333632 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5214: loss=2.765, avg loss=3.362, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=3.1 seconds, 333696 images, time remaining=2.7 hours
5215: loss=3.142, avg loss=3.340, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 333760 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5216: loss=3.293, avg loss=3.335, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=3.2 seconds, 333824 images, time remaining=2.7 hours
5217: loss=2.938, avg loss=3.295, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 333888 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5218: loss=3.346, avg loss=3.300, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=3.2 seconds, 333952 images, time remaining=2.7 hours
5219: loss=3.096, avg loss=3.280, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=3.2 seconds, 334016 images, time remaining=2.7 hours
5220: loss=3.226, avg loss=3.275, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.9 seconds, train=3.2 seconds, 334080 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146498000000
5221: loss=3.465, avg loss=3.294, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=800.9 milliseconds, train=1.2 seconds, 334144 images, time remaining=2.7 hours
5222: loss=3.249, avg loss=3.289, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 334208 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5223: loss=3.328, avg loss=3.293, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.8 seconds, train=1.2 seconds, 334272 images, time remaining=2.7 hours
5224: loss=3.654, avg loss=3.329, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=791.6 milliseconds, train=1.2 seconds, 334336 images, time remaining=2.7 hours
5225: loss=3.356, avg loss=3.332, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=865.3 milliseconds, train=1.2 seconds, 334400 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5226: loss=3.486, avg loss=3.347, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 334464 images, time remaining=2.7 hours
5227: loss=3.505, avg loss=3.363, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=625.4 milliseconds, train=1.2 seconds, 334528 images, time remaining=2.7 hours
5228: loss=3.067, avg loss=3.333, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=847.9 milliseconds, train=1.2 seconds, 334592 images, time remaining=2.7 hours
5229: loss=2.676, avg loss=3.268, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=697.1 milliseconds, train=1.2 seconds, 334656 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5230: loss=3.039, avg loss=3.245, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 334720 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5231: loss=5.894, avg loss=3.510, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=439.3 milliseconds, train=3.9 seconds, 334784 images, time remaining=2.7 hours
5232: loss=4.643, avg loss=3.623, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=596.6 milliseconds, train=3.9 seconds, 334848 images, time remaining=2.7 hours
5233: loss=3.961, avg loss=3.657, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 334912 images, time remaining=2.7 hours
5234: loss=3.301, avg loss=3.621, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 334976 images, time remaining=2.7 hours
5235: loss=3.056, avg loss=3.565, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=705.2 milliseconds, train=3.9 seconds, 335040 images, time remaining=2.7 hours
5236: loss=3.055, avg loss=3.514, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 335104 images, time remaining=2.7 hours
5237: loss=3.547, avg loss=3.517, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=3.9 seconds, 335168 images, time remaining=2.7 hours
5238: loss=3.368, avg loss=3.502, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=3.9 seconds, 335232 images, time remaining=2.7 hours
5239: loss=4.219, avg loss=3.574, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=857.3 milliseconds, train=3.9 seconds, 335296 images, time remaining=2.7 hours
5240: loss=3.970, avg loss=3.613, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 335360 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5241: loss=4.031, avg loss=3.655, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 335424 images, time remaining=2.7 hours
5242: loss=4.677, avg loss=3.757, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=413.2 milliseconds, train=2.1 seconds, 335488 images, time remaining=2.7 hours
5243: loss=2.719, avg loss=3.654, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=698.3 milliseconds, train=2.1 seconds, 335552 images, time remaining=2.7 hours
5244: loss=3.549, avg loss=3.643, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=2.1 seconds, 335616 images, time remaining=2.7 hours
5245: loss=3.200, avg loss=3.599, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=638.7 milliseconds, train=2.1 seconds, 335680 images, time remaining=2.7 hours
5246: loss=4.424, avg loss=3.681, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=987.9 milliseconds, train=2.1 seconds, 335744 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5247: loss=3.049, avg loss=3.618, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.1 seconds, train=2.1 seconds, 335808 images, time remaining=2.7 hours
5248: loss=3.581, avg loss=3.614, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=726.6 milliseconds, train=2.1 seconds, 335872 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5249: loss=3.581, avg loss=3.611, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=2.1 seconds, 335936 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5250: loss=3.054, avg loss=3.555, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 336000 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5251: loss=6.009, avg loss=3.801, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 336064 images, time remaining=2.7 hours
5252: loss=5.158, avg loss=3.936, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=751.8 milliseconds, train=5.0 seconds, 336128 images, time remaining=2.7 hours
5253: loss=4.801, avg loss=4.023, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=950.8 milliseconds, train=5.1 seconds, 336192 images, time remaining=2.7 hours
5254: loss=4.658, avg loss=4.086, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 336256 images, time remaining=2.7 hours
5255: loss=4.929, avg loss=4.171, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 336320 images, time remaining=2.7 hours
5256: loss=4.784, avg loss=4.232, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=5.1 seconds, 336384 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5257: loss=5.108, avg loss=4.320, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=5.2 seconds, train=5.0 seconds, 336448 images, time remaining=2.7 hours
5258: loss=3.243, avg loss=4.212, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 336512 images, time remaining=2.7 hours
5259: loss=3.920, avg loss=4.183, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=5.0 seconds, 336576 images, time remaining=2.7 hours
5260: loss=4.698, avg loss=4.234, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=5.0 seconds, 336640 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5261: loss=4.528, avg loss=4.264, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 336704 images, time remaining=2.7 hours
5262: loss=4.267, avg loss=4.264, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 336768 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5263: loss=4.704, avg loss=4.308, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.5 seconds, train=3.3 seconds, 336832 images, time remaining=2.7 hours
5264: loss=4.158, avg loss=4.293, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=902.8 milliseconds, train=3.3 seconds, 336896 images, time remaining=2.7 hours
5265: loss=4.349, avg loss=4.299, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 336960 images, time remaining=2.7 hours
5266: loss=3.648, avg loss=4.234, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=629.0 milliseconds, train=3.3 seconds, 337024 images, time remaining=2.7 hours
5267: loss=3.848, avg loss=4.195, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=3.3 seconds, 337088 images, time remaining=2.7 hours
5268: loss=3.581, avg loss=4.134, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=940.8 milliseconds, train=3.3 seconds, 337152 images, time remaining=2.7 hours
5269: loss=2.982, avg loss=4.018, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 337216 images, time remaining=2.7 hours
5270: loss=2.913, avg loss=3.908, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=619.9 milliseconds, train=3.3 seconds, 337280 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5271: loss=3.836, avg loss=3.901, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.9 seconds, 337344 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5272: loss=3.530, avg loss=3.864, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.0 seconds, train=1.9 seconds, 337408 images, time remaining=2.7 hours
5273: loss=3.305, avg loss=3.808, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.9 seconds, 337472 images, time remaining=2.7 hours
5274: loss=3.568, avg loss=3.784, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=781.4 milliseconds, train=1.9 seconds, 337536 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5275: loss=3.054, avg loss=3.711, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=1.9 seconds, 337600 images, time remaining=2.7 hours
5276: loss=2.724, avg loss=3.612, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=892.8 milliseconds, train=1.9 seconds, 337664 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5277: loss=2.726, avg loss=3.524, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 337728 images, time remaining=2.7 hours
5278: loss=3.067, avg loss=3.478, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.9 seconds, 337792 images, time remaining=2.7 hours
5279: loss=3.129, avg loss=3.443, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=965.7 milliseconds, train=1.9 seconds, 337856 images, time remaining=2.7 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5280: loss=3.300, avg loss=3.429, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 337920 images, time remaining=2.7 hours
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5281: loss=4.158, avg loss=3.502, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=834.4 milliseconds, train=3.8 seconds, 337984 images, time remaining=2.7 hours
5282: loss=4.363, avg loss=3.588, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=3.8 seconds, 338048 images, time remaining=2.6 hours
5283: loss=3.013, avg loss=3.530, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=982.9 milliseconds, train=3.7 seconds, 338112 images, time remaining=2.6 hours
5284: loss=3.566, avg loss=3.534, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=3.8 seconds, 338176 images, time remaining=2.6 hours
5285: loss=4.150, avg loss=3.595, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=685.6 milliseconds, train=3.8 seconds, 338240 images, time remaining=2.6 hours
5286: loss=3.825, avg loss=3.618, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=651.1 milliseconds, train=3.8 seconds, 338304 images, time remaining=2.6 hours
5287: loss=3.253, avg loss=3.582, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=678.6 milliseconds, train=3.8 seconds, 338368 images, time remaining=2.6 hours
5288: loss=4.105, avg loss=3.634, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=579.6 milliseconds, train=3.8 seconds, 338432 images, time remaining=2.6 hours
5289: loss=2.945, avg loss=3.565, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=643.7 milliseconds, train=3.8 seconds, 338496 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5290: loss=4.034, avg loss=3.612, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.0 seconds, train=3.7 seconds, 338560 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14602a400000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5291: loss=4.353, avg loss=3.686, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 338624 images, time remaining=2.6 hours
5292: loss=4.259, avg loss=3.744, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 338688 images, time remaining=2.6 hours
5293: loss=4.075, avg loss=3.777, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 338752 images, time remaining=2.6 hours
5294: loss=4.617, avg loss=3.861, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=816.5 milliseconds, train=1.2 seconds, 338816 images, time remaining=2.6 hours
5295: loss=2.992, avg loss=3.774, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=577.4 milliseconds, train=1.2 seconds, 338880 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5296: loss=3.056, avg loss=3.702, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 338944 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5297: loss=3.486, avg loss=3.680, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=1.2 seconds, 339008 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5298: loss=3.255, avg loss=3.638, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 339072 images, time remaining=2.6 hours
5299: loss=3.386, avg loss=3.613, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=431.7 milliseconds, train=1.2 seconds, 339136 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5300: loss=3.217, avg loss=3.573, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 339200 images, time remaining=2.6 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1463e0000000
5301: loss=3.872, avg loss=3.603, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=470.9 milliseconds, train=1.6 seconds, 339264 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5302: loss=3.923, avg loss=3.635, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=1.6 seconds, 339328 images, time remaining=2.6 hours
5303: loss=3.561, avg loss=3.628, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=868.2 milliseconds, train=1.6 seconds, 339392 images, time remaining=2.6 hours
5304: loss=3.614, avg loss=3.626, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=867.0 milliseconds, train=1.6 seconds, 339456 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5305: loss=3.681, avg loss=3.632, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 339520 images, time remaining=2.6 hours
5306: loss=2.687, avg loss=3.537, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 339584 images, time remaining=2.6 hours
5307: loss=3.069, avg loss=3.490, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=626.3 milliseconds, train=1.7 seconds, 339648 images, time remaining=2.6 hours
5308: loss=3.206, avg loss=3.462, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=766.1 milliseconds, train=1.6 seconds, 339712 images, time remaining=2.6 hours
5309: loss=2.563, avg loss=3.372, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=862.4 milliseconds, train=1.6 seconds, 339776 images, time remaining=2.6 hours
5310: loss=3.243, avg loss=3.359, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 339840 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5311: loss=3.358, avg loss=3.359, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=539.1 milliseconds, train=3.4 seconds, 339904 images, time remaining=2.6 hours
5312: loss=2.918, avg loss=3.315, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 339968 images, time remaining=2.6 hours
5313: loss=4.225, avg loss=3.406, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=709.2 milliseconds, train=3.4 seconds, 340032 images, time remaining=2.6 hours
5314: loss=3.664, avg loss=3.432, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=964.7 milliseconds, train=3.4 seconds, 340096 images, time remaining=2.6 hours
5315: loss=2.695, avg loss=3.358, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=629.8 milliseconds, train=3.4 seconds, 340160 images, time remaining=2.6 hours
5316: loss=3.768, avg loss=3.399, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 340224 images, time remaining=2.6 hours
5317: loss=3.200, avg loss=3.379, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 340288 images, time remaining=2.6 hours
5318: loss=3.215, avg loss=3.363, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.2 seconds, train=3.4 seconds, 340352 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5319: loss=3.909, avg loss=3.417, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=3.4 seconds, 340416 images, time remaining=2.6 hours
5320: loss=3.332, avg loss=3.409, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=3.4 seconds, 340480 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5321: loss=3.544, avg loss=3.422, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 340544 images, time remaining=2.6 hours
5322: loss=3.691, avg loss=3.449, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=4.1 seconds, 340608 images, time remaining=2.6 hours
5323: loss=3.610, avg loss=3.465, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=796.2 milliseconds, train=4.1 seconds, 340672 images, time remaining=2.6 hours
5324: loss=3.832, avg loss=3.502, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 340736 images, time remaining=2.6 hours
5325: loss=3.658, avg loss=3.518, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=4.1 seconds, 340800 images, time remaining=2.6 hours
5326: loss=3.532, avg loss=3.519, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=4.1 seconds, 340864 images, time remaining=2.6 hours
5327: loss=3.660, avg loss=3.533, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=4.1 seconds, 340928 images, time remaining=2.6 hours
5328: loss=3.344, avg loss=3.514, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=4.1 seconds, 340992 images, time remaining=2.6 hours
5329: loss=3.410, avg loss=3.504, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=874.5 milliseconds, train=4.1 seconds, 341056 images, time remaining=2.6 hours
5330: loss=2.790, avg loss=3.433, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 341120 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5331: loss=3.251, avg loss=3.414, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=985.1 milliseconds, train=3.5 seconds, 341184 images, time remaining=2.6 hours
5332: loss=3.013, avg loss=3.374, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=3.5 seconds, 341248 images, time remaining=2.6 hours
5333: loss=4.530, avg loss=3.490, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=808.9 milliseconds, train=3.5 seconds, 341312 images, time remaining=2.6 hours
5334: loss=3.665, avg loss=3.507, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=3.5 seconds, 341376 images, time remaining=2.6 hours
5335: loss=2.854, avg loss=3.442, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.2 seconds, train=3.5 seconds, 341440 images, time remaining=2.6 hours
5336: loss=3.978, avg loss=3.496, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=3.5 seconds, 341504 images, time remaining=2.6 hours
5337: loss=3.442, avg loss=3.490, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=3.5 seconds, 341568 images, time remaining=2.6 hours
5338: loss=3.113, avg loss=3.452, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.5 seconds, 341632 images, time remaining=2.6 hours
5339: loss=2.700, avg loss=3.377, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=3.5 seconds, 341696 images, time remaining=2.6 hours
5340: loss=3.311, avg loss=3.371, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=733.2 milliseconds, train=3.5 seconds, 341760 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
5341: loss=3.655, avg loss=3.399, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=741.2 milliseconds, train=1.6 seconds, 341824 images, time remaining=2.6 hours
5342: loss=3.089, avg loss=3.368, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=800.6 milliseconds, train=1.5 seconds, 341888 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5343: loss=3.747, avg loss=3.406, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=1.5 seconds, 341952 images, time remaining=2.6 hours
5344: loss=3.270, avg loss=3.392, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 342016 images, time remaining=2.6 hours
5345: loss=3.659, avg loss=3.419, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=749.4 milliseconds, train=1.5 seconds, 342080 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5346: loss=3.241, avg loss=3.401, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.4 seconds, train=1.5 seconds, 342144 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5347: loss=2.988, avg loss=3.360, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 342208 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5348: loss=2.785, avg loss=3.302, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.6 seconds, 342272 images, time remaining=2.6 hours
5349: loss=3.188, avg loss=3.291, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 342336 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5350: loss=3.222, avg loss=3.284, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=1.5 seconds, 342400 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146458000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5351: loss=2.939, avg loss=3.250, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=1.4 seconds, 342464 images, time remaining=2.6 hours
5352: loss=3.263, avg loss=3.251, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=718.5 milliseconds, train=1.4 seconds, 342528 images, time remaining=2.6 hours
5353: loss=2.428, avg loss=3.169, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 342592 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5354: loss=3.002, avg loss=3.152, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 342656 images, time remaining=2.6 hours
5355: loss=2.900, avg loss=3.127, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=1.4 seconds, 342720 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5356: loss=3.341, avg loss=3.148, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=6.8 seconds, train=1.4 seconds, 342784 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5357: loss=2.904, avg loss=3.124, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.0 seconds, train=1.4 seconds, 342848 images, time remaining=2.6 hours
5358: loss=3.052, avg loss=3.117, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=994.7 milliseconds, train=1.5 seconds, 342912 images, time remaining=2.6 hours
5359: loss=2.587, avg loss=3.064, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=983.3 milliseconds, train=1.4 seconds, 342976 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5360: loss=2.756, avg loss=3.033, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 343040 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5361: loss=2.879, avg loss=3.018, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.8 seconds, 343104 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5362: loss=3.299, avg loss=3.046, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=1.8 seconds, 343168 images, time remaining=2.6 hours
5363: loss=3.206, avg loss=3.062, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=976.5 milliseconds, train=1.8 seconds, 343232 images, time remaining=2.6 hours
5364: loss=2.910, avg loss=3.047, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.8 seconds, 343296 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5365: loss=2.835, avg loss=3.025, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=1.8 seconds, 343360 images, time remaining=2.6 hours
5366: loss=2.998, avg loss=3.023, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=1.8 seconds, 343424 images, time remaining=2.6 hours
5367: loss=2.938, avg loss=3.014, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=649.1 milliseconds, train=1.8 seconds, 343488 images, time remaining=2.6 hours
5368: loss=2.698, avg loss=2.983, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=818.4 milliseconds, train=1.8 seconds, 343552 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5369: loss=2.828, avg loss=2.967, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=1.8 seconds, 343616 images, time remaining=2.6 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5370: loss=2.667, avg loss=2.937, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=1.8 seconds, 343680 images, time remaining=2.6 hours
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5371: loss=4.356, avg loss=3.079, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=4.9 seconds, 343744 images, time remaining=2.6 hours
5372: loss=3.914, avg loss=3.162, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.0 seconds, train=4.8 seconds, 343808 images, time remaining=2.6 hours
5373: loss=4.065, avg loss=3.253, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.6 seconds, train=4.8 seconds, 343872 images, time remaining=2.6 hours
5374: loss=4.710, avg loss=3.398, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.6 seconds, train=4.8 seconds, 343936 images, time remaining=2.6 hours
5375: loss=4.031, avg loss=3.462, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=4.8 seconds, 344000 images, time remaining=2.6 hours
5376: loss=4.030, avg loss=3.519, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=4.8 seconds, 344064 images, time remaining=2.6 hours
5377: loss=3.919, avg loss=3.559, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.5 seconds, train=4.8 seconds, 344128 images, time remaining=2.6 hours
5378: loss=3.549, avg loss=3.558, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.1 seconds, train=4.8 seconds, 344192 images, time remaining=2.6 hours
5379: loss=2.819, avg loss=3.484, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=4.8 seconds, 344256 images, time remaining=2.6 hours
5380: loss=3.688, avg loss=3.504, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=4.8 seconds, 344320 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5381: loss=3.877, avg loss=3.542, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=963.3 milliseconds, train=5.1 seconds, 344384 images, time remaining=2.5 hours
5382: loss=4.219, avg loss=3.609, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=5.1 seconds, 344448 images, time remaining=2.5 hours
5383: loss=3.626, avg loss=3.611, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 344512 images, time remaining=2.5 hours
5384: loss=4.057, avg loss=3.656, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 344576 images, time remaining=2.5 hours
5385: loss=4.638, avg loss=3.754, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.0 seconds, train=5.1 seconds, 344640 images, time remaining=2.5 hours
5386: loss=4.535, avg loss=3.832, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.4 seconds, train=5.1 seconds, 344704 images, time remaining=2.5 hours
5387: loss=3.877, avg loss=3.836, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=939.3 milliseconds, train=5.1 seconds, 344768 images, time remaining=2.5 hours
5388: loss=3.983, avg loss=3.851, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 344832 images, time remaining=2.5 hours
5389: loss=4.270, avg loss=3.893, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=5.1 seconds, 344896 images, time remaining=2.5 hours
5390: loss=4.907, avg loss=3.994, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=5.1 seconds, 344960 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5391: loss=3.938, avg loss=3.989, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.0 seconds, train=4.6 seconds, 345024 images, time remaining=2.5 hours
5392: loss=4.050, avg loss=3.995, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 345088 images, time remaining=2.5 hours
5393: loss=4.022, avg loss=3.998, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 345152 images, time remaining=2.5 hours
5394: loss=3.641, avg loss=3.962, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.6 seconds, train=4.6 seconds, 345216 images, time remaining=2.5 hours
5395: loss=3.796, avg loss=3.945, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=990.6 milliseconds, train=4.6 seconds, 345280 images, time remaining=2.5 hours
5396: loss=3.596, avg loss=3.910, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.8 seconds, train=4.6 seconds, 345344 images, time remaining=2.5 hours
5397: loss=3.628, avg loss=3.882, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 345408 images, time remaining=2.5 hours
5398: loss=3.330, avg loss=3.827, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 345472 images, time remaining=2.5 hours
5399: loss=3.222, avg loss=3.766, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 345536 images, time remaining=2.5 hours
5400: loss=4.056, avg loss=3.795, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.9 seconds, train=4.7 seconds, 345600 images, time remaining=2.5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5401: loss=3.577, avg loss=3.774, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=5.0 seconds, 345664 images, time remaining=2.5 hours
5402: loss=3.489, avg loss=3.745, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.1 seconds, train=5.0 seconds, 345728 images, time remaining=2.5 hours
5403: loss=3.909, avg loss=3.762, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.2 seconds, train=4.9 seconds, 345792 images, time remaining=2.5 hours
5404: loss=3.668, avg loss=3.752, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.3 seconds, train=4.9 seconds, 345856 images, time remaining=2.5 hours
5405: loss=4.901, avg loss=3.867, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.7 seconds, train=5.0 seconds, 345920 images, time remaining=2.5 hours
5406: loss=4.712, avg loss=3.952, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 345984 images, time remaining=2.5 hours
5407: loss=3.135, avg loss=3.870, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=4.9 seconds, 346048 images, time remaining=2.5 hours
5408: loss=3.596, avg loss=3.842, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.0 seconds, train=4.9 seconds, 346112 images, time remaining=2.5 hours
5409: loss=3.527, avg loss=3.811, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.8 seconds, train=4.9 seconds, 346176 images, time remaining=2.5 hours
5410: loss=3.349, avg loss=3.765, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.3 seconds, train=4.9 seconds, 346240 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463b6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5411: loss=3.994, avg loss=3.788, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 346304 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5412: loss=4.189, avg loss=3.828, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 346368 images, time remaining=2.5 hours
5413: loss=3.833, avg loss=3.828, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 346432 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5414: loss=3.548, avg loss=3.800, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 346496 images, time remaining=2.5 hours
5415: loss=3.435, avg loss=3.764, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 346560 images, time remaining=2.5 hours
5416: loss=3.180, avg loss=3.705, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=663.8 milliseconds, train=1.5 seconds, 346624 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5417: loss=3.085, avg loss=3.643, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.4 seconds, train=1.5 seconds, 346688 images, time remaining=2.5 hours
5418: loss=3.444, avg loss=3.623, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=835.0 milliseconds, train=1.5 seconds, 346752 images, time remaining=2.5 hours
5419: loss=2.665, avg loss=3.528, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=814.8 milliseconds, train=1.5 seconds, 346816 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5420: loss=2.922, avg loss=3.467, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 346880 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5421: loss=4.439, avg loss=3.564, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 346944 images, time remaining=2.5 hours
5422: loss=5.283, avg loss=3.736, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=862.0 milliseconds, train=3.9 seconds, 347008 images, time remaining=2.5 hours
5423: loss=4.010, avg loss=3.764, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=3.8 seconds, 347072 images, time remaining=2.5 hours
5424: loss=3.164, avg loss=3.704, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.1 seconds, train=3.8 seconds, 347136 images, time remaining=2.5 hours
5425: loss=3.708, avg loss=3.704, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=813.1 milliseconds, train=3.9 seconds, 347200 images, time remaining=2.5 hours
5426: loss=3.844, avg loss=3.718, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=938.6 milliseconds, train=3.9 seconds, 347264 images, time remaining=2.5 hours
5427: loss=4.112, avg loss=3.757, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 347328 images, time remaining=2.5 hours
5428: loss=4.015, avg loss=3.783, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=678.2 milliseconds, train=3.9 seconds, 347392 images, time remaining=2.5 hours
5429: loss=3.967, avg loss=3.802, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=803.3 milliseconds, train=3.9 seconds, 347456 images, time remaining=2.5 hours
5430: loss=3.373, avg loss=3.759, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=674.7 milliseconds, train=3.8 seconds, 347520 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a2e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5431: loss=4.495, avg loss=3.832, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.1 seconds, train=2.1 seconds, 347584 images, time remaining=2.5 hours
5432: loss=4.457, avg loss=3.895, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=821.9 milliseconds, train=2.1 seconds, 347648 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5433: loss=3.240, avg loss=3.829, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.9 seconds, train=2.1 seconds, 347712 images, time remaining=2.5 hours
5434: loss=2.442, avg loss=3.691, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 347776 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5435: loss=2.995, avg loss=3.621, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.6 seconds, train=2.1 seconds, 347840 images, time remaining=2.5 hours
5436: loss=3.256, avg loss=3.585, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 347904 images, time remaining=2.5 hours
5437: loss=3.161, avg loss=3.542, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 347968 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5438: loss=3.397, avg loss=3.528, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 348032 images, time remaining=2.5 hours
5439: loss=3.520, avg loss=3.527, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 348096 images, time remaining=2.5 hours
5440: loss=3.474, avg loss=3.522, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 348160 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b64e00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5441: loss=3.737, avg loss=3.543, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 348224 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5442: loss=4.084, avg loss=3.597, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.4 seconds, train=1.2 seconds, 348288 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5443: loss=3.307, avg loss=3.568, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 348352 images, time remaining=2.5 hours
5444: loss=3.664, avg loss=3.578, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=447.0 milliseconds, train=1.2 seconds, 348416 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5445: loss=3.717, avg loss=3.592, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.0 seconds, train=1.2 seconds, 348480 images, time remaining=2.5 hours
5446: loss=3.267, avg loss=3.559, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=492.0 milliseconds, train=1.2 seconds, 348544 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5447: loss=3.209, avg loss=3.524, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 348608 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5448: loss=3.324, avg loss=3.504, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 348672 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5449: loss=3.583, avg loss=3.512, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 348736 images, time remaining=2.5 hours
5450: loss=3.018, avg loss=3.463, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 348800 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a36000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5451: loss=3.073, avg loss=3.424, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 348864 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5452: loss=2.389, avg loss=3.320, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=5.4 seconds, train=1.5 seconds, 348928 images, time remaining=2.5 hours
5453: loss=3.115, avg loss=3.300, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 348992 images, time remaining=2.5 hours
5454: loss=2.850, avg loss=3.255, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=569.5 milliseconds, train=1.5 seconds, 349056 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5455: loss=2.747, avg loss=3.204, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 349120 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5456: loss=2.974, avg loss=3.181, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=3.5 seconds, train=1.5 seconds, 349184 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5457: loss=2.856, avg loss=3.149, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.6 seconds, train=1.5 seconds, 349248 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5458: loss=2.946, avg loss=3.128, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.3 seconds, train=1.5 seconds, 349312 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5459: loss=2.699, avg loss=3.085, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 349376 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5460: loss=2.883, avg loss=3.065, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=5.0 seconds, train=1.5 seconds, 349440 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 1024x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5461: loss=3.593, avg loss=3.118, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.3 seconds, train=3.2 seconds, 349504 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5462: loss=3.245, avg loss=3.131, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.1 seconds, train=3.2 seconds, 349568 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5463: loss=3.313, avg loss=3.149, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.8 seconds, train=3.2 seconds, 349632 images, time remaining=2.5 hours
5464: loss=3.434, avg loss=3.177, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=2.7 seconds, train=3.2 seconds, 349696 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5465: loss=3.265, avg loss=3.186, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=4.4 seconds, train=3.2 seconds, 349760 images, time remaining=2.5 hours
5466: loss=3.369, avg loss=3.204, last=95.22%, best=95.22%, next=5466, rate=0.00130000, load 64=1.7 seconds, train=3.2 seconds, 349824 images, time remaining=2.5 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a2e000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=110927, unique_truth_count=57264
rank=0 of ranks=110927rank=100 of ranks=110927rank=200 of ranks=110927rank=300 of ranks=110927rank=400 of ranks=110927rank=500 of ranks=110927rank=600 of ranks=110927rank=700 of ranks=110927rank=800 of ranks=110927rank=900 of ranks=110927rank=1000 of ranks=110927rank=1100 of ranks=110927rank=1200 of ranks=110927rank=1300 of ranks=110927rank=1400 of ranks=110927rank=1500 of ranks=110927rank=1600 of ranks=110927rank=1700 of ranks=110927rank=1800 of ranks=110927rank=1900 of ranks=110927rank=2000 of ranks=110927rank=2100 of ranks=110927rank=2200 of ranks=110927rank=2300 of ranks=110927rank=2400 of ranks=110927rank=2500 of ranks=110927rank=2600 of ranks=110927rank=2700 of ranks=110927rank=2800 of ranks=110927rank=2900 of ranks=110927rank=3000 of ranks=110927rank=3100 of ranks=110927rank=3200 of ranks=110927rank=3300 of ranks=110927rank=3400 of ranks=110927rank=3500 of ranks=110927rank=3600 of ranks=110927rank=3700 of ranks=110927rank=3800 of ranks=110927rank=3900 of ranks=110927rank=4000 of ranks=110927rank=4100 of ranks=110927rank=4200 of ranks=110927rank=4300 of ranks=110927rank=4400 of ranks=110927rank=4500 of ranks=110927rank=4600 of ranks=110927rank=4700 of ranks=110927rank=4800 of ranks=110927rank=4900 of ranks=110927rank=5000 of ranks=110927rank=5100 of ranks=110927rank=5200 of ranks=110927rank=5300 of ranks=110927rank=5400 of ranks=110927rank=5500 of ranks=110927rank=5600 of ranks=110927rank=5700 of ranks=110927rank=5800 of ranks=110927rank=5900 of ranks=110927rank=6000 of ranks=110927rank=6100 of ranks=110927rank=6200 of ranks=110927rank=6300 of ranks=110927rank=6400 of ranks=110927rank=6500 of ranks=110927rank=6600 of ranks=110927rank=6700 of ranks=110927rank=6800 of ranks=110927rank=6900 of ranks=110927rank=7000 of ranks=110927rank=7100 of ranks=110927rank=7200 of ranks=110927rank=7300 of ranks=110927rank=7400 of ranks=110927rank=7500 of ranks=110927rank=7600 of ranks=110927rank=7700 of ranks=110927rank=7800 of ranks=110927rank=7900 of ranks=110927rank=8000 of ranks=110927rank=8100 of ranks=110927rank=8200 of ranks=110927rank=8300 of ranks=110927rank=8400 of ranks=110927rank=8500 of ranks=110927rank=8600 of ranks=110927rank=8700 of ranks=110927rank=8800 of ranks=110927rank=8900 of ranks=110927rank=9000 of ranks=110927rank=9100 of ranks=110927rank=9200 of ranks=110927rank=9300 of ranks=110927rank=9400 of ranks=110927rank=9500 of ranks=110927rank=9600 of ranks=110927rank=9700 of ranks=110927rank=9800 of ranks=110927rank=9900 of ranks=110927rank=10000 of ranks=110927rank=10100 of ranks=110927rank=10200 of ranks=110927rank=10300 of ranks=110927rank=10400 of ranks=110927rank=10500 of ranks=110927rank=10600 of ranks=110927rank=10700 of ranks=110927rank=10800 of ranks=110927rank=10900 of ranks=110927rank=11000 of ranks=110927rank=11100 of ranks=110927rank=11200 of ranks=110927rank=11300 of ranks=110927rank=11400 of ranks=110927rank=11500 of ranks=110927rank=11600 of ranks=110927rank=11700 of ranks=110927rank=11800 of ranks=110927rank=11900 of ranks=110927rank=12000 of ranks=110927rank=12100 of ranks=110927rank=12200 of ranks=110927rank=12300 of ranks=110927rank=12400 of ranks=110927rank=12500 of ranks=110927rank=12600 of ranks=110927rank=12700 of ranks=110927rank=12800 of ranks=110927rank=12900 of ranks=110927rank=13000 of ranks=110927rank=13100 of ranks=110927rank=13200 of ranks=110927rank=13300 of ranks=110927rank=13400 of ranks=110927rank=13500 of ranks=110927rank=13600 of ranks=110927rank=13700 of ranks=110927rank=13800 of ranks=110927rank=13900 of ranks=110927rank=14000 of ranks=110927rank=14100 of ranks=110927rank=14200 of ranks=110927rank=14300 of ranks=110927rank=14400 of ranks=110927rank=14500 of ranks=110927rank=14600 of ranks=110927rank=14700 of ranks=110927rank=14800 of ranks=110927rank=14900 of ranks=110927rank=15000 of ranks=110927rank=15100 of ranks=110927rank=15200 of ranks=110927rank=15300 of ranks=110927rank=15400 of ranks=110927rank=15500 of ranks=110927rank=15600 of ranks=110927rank=15700 of ranks=110927rank=15800 of ranks=110927rank=15900 of ranks=110927rank=16000 of ranks=110927rank=16100 of ranks=110927rank=16200 of ranks=110927rank=16300 of ranks=110927rank=16400 of ranks=110927rank=16500 of ranks=110927rank=16600 of ranks=110927rank=16700 of ranks=110927rank=16800 of ranks=110927rank=16900 of ranks=110927rank=17000 of ranks=110927rank=17100 of ranks=110927rank=17200 of ranks=110927rank=17300 of ranks=110927rank=17400 of ranks=110927rank=17500 of ranks=110927rank=17600 of ranks=110927rank=17700 of ranks=110927rank=17800 of ranks=110927rank=17900 of ranks=110927rank=18000 of ranks=110927rank=18100 of ranks=110927rank=18200 of ranks=110927rank=18300 of ranks=110927rank=18400 of ranks=110927rank=18500 of ranks=110927rank=18600 of ranks=110927rank=18700 of ranks=110927rank=18800 of ranks=110927rank=18900 of ranks=110927rank=19000 of ranks=110927rank=19100 of ranks=110927rank=19200 of ranks=110927rank=19300 of ranks=110927rank=19400 of ranks=110927rank=19500 of ranks=110927rank=19600 of ranks=110927rank=19700 of ranks=110927rank=19800 of ranks=110927rank=19900 of ranks=110927rank=20000 of ranks=110927rank=20100 of ranks=110927rank=20200 of ranks=110927rank=20300 of ranks=110927rank=20400 of ranks=110927rank=20500 of ranks=110927rank=20600 of ranks=110927rank=20700 of ranks=110927rank=20800 of ranks=110927rank=20900 of ranks=110927rank=21000 of ranks=110927rank=21100 of ranks=110927rank=21200 of ranks=110927rank=21300 of ranks=110927rank=21400 of ranks=110927rank=21500 of ranks=110927rank=21600 of ranks=110927rank=21700 of ranks=110927rank=21800 of ranks=110927rank=21900 of ranks=110927rank=22000 of ranks=110927rank=22100 of ranks=110927rank=22200 of ranks=110927rank=22300 of ranks=110927rank=22400 of ranks=110927rank=22500 of ranks=110927rank=22600 of ranks=110927rank=22700 of ranks=110927rank=22800 of ranks=110927rank=22900 of ranks=110927rank=23000 of ranks=110927rank=23100 of ranks=110927rank=23200 of ranks=110927rank=23300 of ranks=110927rank=23400 of ranks=110927rank=23500 of ranks=110927rank=23600 of ranks=110927rank=23700 of ranks=110927rank=23800 of ranks=110927rank=23900 of ranks=110927rank=24000 of ranks=110927rank=24100 of ranks=110927rank=24200 of ranks=110927rank=24300 of ranks=110927rank=24400 of ranks=110927rank=24500 of ranks=110927rank=24600 of ranks=110927rank=24700 of ranks=110927rank=24800 of ranks=110927rank=24900 of ranks=110927rank=25000 of ranks=110927rank=25100 of ranks=110927rank=25200 of ranks=110927rank=25300 of ranks=110927rank=25400 of ranks=110927rank=25500 of ranks=110927rank=25600 of ranks=110927rank=25700 of ranks=110927rank=25800 of ranks=110927rank=25900 of ranks=110927rank=26000 of ranks=110927rank=26100 of ranks=110927rank=26200 of ranks=110927rank=26300 of ranks=110927rank=26400 of ranks=110927rank=26500 of ranks=110927rank=26600 of ranks=110927rank=26700 of ranks=110927rank=26800 of ranks=110927rank=26900 of ranks=110927rank=27000 of ranks=110927rank=27100 of ranks=110927rank=27200 of ranks=110927rank=27300 of ranks=110927rank=27400 of ranks=110927rank=27500 of ranks=110927rank=27600 of ranks=110927rank=27700 of ranks=110927rank=27800 of ranks=110927rank=27900 of ranks=110927rank=28000 of ranks=110927rank=28100 of ranks=110927rank=28200 of ranks=110927rank=28300 of ranks=110927rank=28400 of ranks=110927rank=28500 of ranks=110927rank=28600 of ranks=110927rank=28700 of ranks=110927rank=28800 of ranks=110927rank=28900 of ranks=110927rank=29000 of ranks=110927rank=29100 of ranks=110927rank=29200 of ranks=110927rank=29300 of ranks=110927rank=29400 of ranks=110927rank=29500 of ranks=110927rank=29600 of ranks=110927rank=29700 of ranks=110927rank=29800 of ranks=110927rank=29900 of ranks=110927rank=30000 of ranks=110927rank=30100 of ranks=110927rank=30200 of ranks=110927rank=30300 of ranks=110927rank=30400 of ranks=110927rank=30500 of ranks=110927rank=30600 of ranks=110927rank=30700 of ranks=110927rank=30800 of ranks=110927rank=30900 of ranks=110927rank=31000 of ranks=110927rank=31100 of ranks=110927rank=31200 of ranks=110927rank=31300 of ranks=110927rank=31400 of ranks=110927rank=31500 of ranks=110927rank=31600 of ranks=110927rank=31700 of ranks=110927rank=31800 of ranks=110927rank=31900 of ranks=110927rank=32000 of ranks=110927rank=32100 of ranks=110927rank=32200 of ranks=110927rank=32300 of ranks=110927rank=32400 of ranks=110927rank=32500 of ranks=110927rank=32600 of ranks=110927rank=32700 of ranks=110927rank=32800 of ranks=110927rank=32900 of ranks=110927rank=33000 of ranks=110927rank=33100 of ranks=110927rank=33200 of ranks=110927rank=33300 of ranks=110927rank=33400 of ranks=110927rank=33500 of ranks=110927rank=33600 of ranks=110927rank=33700 of ranks=110927rank=33800 of ranks=110927rank=33900 of ranks=110927rank=34000 of ranks=110927rank=34100 of ranks=110927rank=34200 of ranks=110927rank=34300 of ranks=110927rank=34400 of ranks=110927rank=34500 of ranks=110927rank=34600 of ranks=110927rank=34700 of ranks=110927rank=34800 of ranks=110927rank=34900 of ranks=110927rank=35000 of ranks=110927rank=35100 of ranks=110927rank=35200 of ranks=110927rank=35300 of ranks=110927rank=35400 of ranks=110927rank=35500 of ranks=110927rank=35600 of ranks=110927rank=35700 of ranks=110927rank=35800 of ranks=110927rank=35900 of ranks=110927rank=36000 of ranks=110927rank=36100 of ranks=110927rank=36200 of ranks=110927rank=36300 of ranks=110927rank=36400 of ranks=110927rank=36500 of ranks=110927rank=36600 of ranks=110927rank=36700 of ranks=110927rank=36800 of ranks=110927rank=36900 of ranks=110927rank=37000 of ranks=110927rank=37100 of ranks=110927rank=37200 of ranks=110927rank=37300 of ranks=110927rank=37400 of ranks=110927rank=37500 of ranks=110927rank=37600 of ranks=110927rank=37700 of ranks=110927rank=37800 of ranks=110927rank=37900 of ranks=110927rank=38000 of ranks=110927rank=38100 of ranks=110927rank=38200 of ranks=110927rank=38300 of ranks=110927rank=38400 of ranks=110927rank=38500 of ranks=110927rank=38600 of ranks=110927rank=38700 of ranks=110927rank=38800 of ranks=110927rank=38900 of ranks=110927rank=39000 of ranks=110927rank=39100 of ranks=110927rank=39200 of ranks=110927rank=39300 of ranks=110927rank=39400 of ranks=110927rank=39500 of ranks=110927rank=39600 of ranks=110927rank=39700 of ranks=110927rank=39800 of ranks=110927rank=39900 of ranks=110927rank=40000 of ranks=110927rank=40100 of ranks=110927rank=40200 of ranks=110927rank=40300 of ranks=110927rank=40400 of ranks=110927rank=40500 of ranks=110927rank=40600 of ranks=110927rank=40700 of ranks=110927rank=40800 of ranks=110927rank=40900 of ranks=110927rank=41000 of ranks=110927rank=41100 of ranks=110927rank=41200 of ranks=110927rank=41300 of ranks=110927rank=41400 of ranks=110927rank=41500 of ranks=110927rank=41600 of ranks=110927rank=41700 of ranks=110927rank=41800 of ranks=110927rank=41900 of ranks=110927rank=42000 of ranks=110927rank=42100 of ranks=110927rank=42200 of ranks=110927rank=42300 of ranks=110927rank=42400 of ranks=110927rank=42500 of ranks=110927rank=42600 of ranks=110927rank=42700 of ranks=110927rank=42800 of ranks=110927rank=42900 of ranks=110927rank=43000 of ranks=110927rank=43100 of ranks=110927rank=43200 of ranks=110927rank=43300 of ranks=110927rank=43400 of ranks=110927rank=43500 of ranks=110927rank=43600 of ranks=110927rank=43700 of ranks=110927rank=43800 of ranks=110927rank=43900 of ranks=110927rank=44000 of ranks=110927rank=44100 of ranks=110927rank=44200 of ranks=110927rank=44300 of ranks=110927rank=44400 of ranks=110927rank=44500 of ranks=110927rank=44600 of ranks=110927rank=44700 of ranks=110927rank=44800 of ranks=110927rank=44900 of ranks=110927rank=45000 of ranks=110927rank=45100 of ranks=110927rank=45200 of ranks=110927rank=45300 of ranks=110927rank=45400 of ranks=110927rank=45500 of ranks=110927rank=45600 of ranks=110927rank=45700 of ranks=110927rank=45800 of ranks=110927rank=45900 of ranks=110927rank=46000 of ranks=110927rank=46100 of ranks=110927rank=46200 of ranks=110927rank=46300 of ranks=110927rank=46400 of ranks=110927rank=46500 of ranks=110927rank=46600 of ranks=110927rank=46700 of ranks=110927rank=46800 of ranks=110927rank=46900 of ranks=110927rank=47000 of ranks=110927rank=47100 of ranks=110927rank=47200 of ranks=110927rank=47300 of ranks=110927rank=47400 of ranks=110927rank=47500 of ranks=110927rank=47600 of ranks=110927rank=47700 of ranks=110927rank=47800 of ranks=110927rank=47900 of ranks=110927rank=48000 of ranks=110927rank=48100 of ranks=110927rank=48200 of ranks=110927rank=48300 of ranks=110927rank=48400 of ranks=110927rank=48500 of ranks=110927rank=48600 of ranks=110927rank=48700 of ranks=110927rank=48800 of ranks=110927rank=48900 of ranks=110927rank=49000 of ranks=110927rank=49100 of ranks=110927rank=49200 of ranks=110927rank=49300 of ranks=110927rank=49400 of ranks=110927rank=49500 of ranks=110927rank=49600 of ranks=110927rank=49700 of ranks=110927rank=49800 of ranks=110927rank=49900 of ranks=110927rank=50000 of ranks=110927rank=50100 of ranks=110927rank=50200 of ranks=110927rank=50300 of ranks=110927rank=50400 of ranks=110927rank=50500 of ranks=110927rank=50600 of ranks=110927rank=50700 of ranks=110927rank=50800 of ranks=110927rank=50900 of ranks=110927rank=51000 of ranks=110927rank=51100 of ranks=110927rank=51200 of ranks=110927rank=51300 of ranks=110927rank=51400 of ranks=110927rank=51500 of ranks=110927rank=51600 of ranks=110927rank=51700 of ranks=110927rank=51800 of ranks=110927rank=51900 of ranks=110927rank=52000 of ranks=110927rank=52100 of ranks=110927rank=52200 of ranks=110927rank=52300 of ranks=110927rank=52400 of ranks=110927rank=52500 of ranks=110927rank=52600 of ranks=110927rank=52700 of ranks=110927rank=52800 of ranks=110927rank=52900 of ranks=110927rank=53000 of ranks=110927rank=53100 of ranks=110927rank=53200 of ranks=110927rank=53300 of ranks=110927rank=53400 of ranks=110927rank=53500 of ranks=110927rank=53600 of ranks=110927rank=53700 of ranks=110927rank=53800 of ranks=110927rank=53900 of ranks=110927rank=54000 of ranks=110927rank=54100 of ranks=110927rank=54200 of ranks=110927rank=54300 of ranks=110927rank=54400 of ranks=110927rank=54500 of ranks=110927rank=54600 of ranks=110927rank=54700 of ranks=110927rank=54800 of ranks=110927rank=54900 of ranks=110927rank=55000 of ranks=110927rank=55100 of ranks=110927rank=55200 of ranks=110927rank=55300 of ranks=110927rank=55400 of ranks=110927rank=55500 of ranks=110927rank=55600 of ranks=110927rank=55700 of ranks=110927rank=55800 of ranks=110927rank=55900 of ranks=110927rank=56000 of ranks=110927rank=56100 of ranks=110927rank=56200 of ranks=110927rank=56300 of ranks=110927rank=56400 of ranks=110927rank=56500 of ranks=110927rank=56600 of ranks=110927rank=56700 of ranks=110927rank=56800 of ranks=110927rank=56900 of ranks=110927rank=57000 of ranks=110927rank=57100 of ranks=110927rank=57200 of ranks=110927rank=57300 of ranks=110927rank=57400 of ranks=110927rank=57500 of ranks=110927rank=57600 of ranks=110927rank=57700 of ranks=110927rank=57800 of ranks=110927rank=57900 of ranks=110927rank=58000 of ranks=110927rank=58100 of ranks=110927rank=58200 of ranks=110927rank=58300 of ranks=110927rank=58400 of ranks=110927rank=58500 of ranks=110927rank=58600 of ranks=110927rank=58700 of ranks=110927rank=58800 of ranks=110927rank=58900 of ranks=110927rank=59000 of ranks=110927rank=59100 of ranks=110927rank=59200 of ranks=110927rank=59300 of ranks=110927rank=59400 of ranks=110927rank=59500 of ranks=110927rank=59600 of ranks=110927rank=59700 of ranks=110927rank=59800 of ranks=110927rank=59900 of ranks=110927rank=60000 of ranks=110927rank=60100 of ranks=110927rank=60200 of ranks=110927rank=60300 of ranks=110927rank=60400 of ranks=110927rank=60500 of ranks=110927rank=60600 of ranks=110927rank=60700 of ranks=110927rank=60800 of ranks=110927rank=60900 of ranks=110927rank=61000 of ranks=110927rank=61100 of ranks=110927rank=61200 of ranks=110927rank=61300 of ranks=110927rank=61400 of ranks=110927rank=61500 of ranks=110927rank=61600 of ranks=110927rank=61700 of ranks=110927rank=61800 of ranks=110927rank=61900 of ranks=110927rank=62000 of ranks=110927rank=62100 of ranks=110927rank=62200 of ranks=110927rank=62300 of ranks=110927rank=62400 of ranks=110927rank=62500 of ranks=110927rank=62600 of ranks=110927rank=62700 of ranks=110927rank=62800 of ranks=110927rank=62900 of ranks=110927rank=63000 of ranks=110927rank=63100 of ranks=110927rank=63200 of ranks=110927rank=63300 of ranks=110927rank=63400 of ranks=110927rank=63500 of ranks=110927rank=63600 of ranks=110927rank=63700 of ranks=110927rank=63800 of ranks=110927rank=63900 of ranks=110927rank=64000 of ranks=110927rank=64100 of ranks=110927rank=64200 of ranks=110927rank=64300 of ranks=110927rank=64400 of ranks=110927rank=64500 of ranks=110927rank=64600 of ranks=110927rank=64700 of ranks=110927rank=64800 of ranks=110927rank=64900 of ranks=110927rank=65000 of ranks=110927rank=65100 of ranks=110927rank=65200 of ranks=110927rank=65300 of ranks=110927rank=65400 of ranks=110927rank=65500 of ranks=110927rank=65600 of ranks=110927rank=65700 of ranks=110927rank=65800 of ranks=110927rank=65900 of ranks=110927rank=66000 of ranks=110927rank=66100 of ranks=110927rank=66200 of ranks=110927rank=66300 of ranks=110927rank=66400 of ranks=110927rank=66500 of ranks=110927rank=66600 of ranks=110927rank=66700 of ranks=110927rank=66800 of ranks=110927rank=66900 of ranks=110927rank=67000 of ranks=110927rank=67100 of ranks=110927rank=67200 of ranks=110927rank=67300 of ranks=110927rank=67400 of ranks=110927rank=67500 of ranks=110927rank=67600 of ranks=110927rank=67700 of ranks=110927rank=67800 of ranks=110927rank=67900 of ranks=110927rank=68000 of ranks=110927rank=68100 of ranks=110927rank=68200 of ranks=110927rank=68300 of ranks=110927rank=68400 of ranks=110927rank=68500 of ranks=110927rank=68600 of ranks=110927rank=68700 of ranks=110927rank=68800 of ranks=110927rank=68900 of ranks=110927rank=69000 of ranks=110927rank=69100 of ranks=110927rank=69200 of ranks=110927rank=69300 of ranks=110927rank=69400 of ranks=110927rank=69500 of ranks=110927rank=69600 of ranks=110927rank=69700 of ranks=110927rank=69800 of ranks=110927rank=69900 of ranks=110927rank=70000 of ranks=110927rank=70100 of ranks=110927rank=70200 of ranks=110927rank=70300 of ranks=110927rank=70400 of ranks=110927rank=70500 of ranks=110927rank=70600 of ranks=110927rank=70700 of ranks=110927rank=70800 of ranks=110927rank=70900 of ranks=110927rank=71000 of ranks=110927rank=71100 of ranks=110927rank=71200 of ranks=110927rank=71300 of ranks=110927rank=71400 of ranks=110927rank=71500 of ranks=110927rank=71600 of ranks=110927rank=71700 of ranks=110927rank=71800 of ranks=110927rank=71900 of ranks=110927rank=72000 of ranks=110927rank=72100 of ranks=110927rank=72200 of ranks=110927rank=72300 of ranks=110927rank=72400 of ranks=110927rank=72500 of ranks=110927rank=72600 of ranks=110927rank=72700 of ranks=110927rank=72800 of ranks=110927rank=72900 of ranks=110927rank=73000 of ranks=110927rank=73100 of ranks=110927rank=73200 of ranks=110927rank=73300 of ranks=110927rank=73400 of ranks=110927rank=73500 of ranks=110927rank=73600 of ranks=110927rank=73700 of ranks=110927rank=73800 of ranks=110927rank=73900 of ranks=110927rank=74000 of ranks=110927rank=74100 of ranks=110927rank=74200 of ranks=110927rank=74300 of ranks=110927rank=74400 of ranks=110927rank=74500 of ranks=110927rank=74600 of ranks=110927rank=74700 of ranks=110927rank=74800 of ranks=110927rank=74900 of ranks=110927rank=75000 of ranks=110927rank=75100 of ranks=110927rank=75200 of ranks=110927rank=75300 of ranks=110927rank=75400 of ranks=110927rank=75500 of ranks=110927rank=75600 of ranks=110927rank=75700 of ranks=110927rank=75800 of ranks=110927rank=75900 of ranks=110927rank=76000 of ranks=110927rank=76100 of ranks=110927rank=76200 of ranks=110927rank=76300 of ranks=110927rank=76400 of ranks=110927rank=76500 of ranks=110927rank=76600 of ranks=110927rank=76700 of ranks=110927rank=76800 of ranks=110927rank=76900 of ranks=110927rank=77000 of ranks=110927rank=77100 of ranks=110927rank=77200 of ranks=110927rank=77300 of ranks=110927rank=77400 of ranks=110927rank=77500 of ranks=110927rank=77600 of ranks=110927rank=77700 of ranks=110927rank=77800 of ranks=110927rank=77900 of ranks=110927rank=78000 of ranks=110927rank=78100 of ranks=110927rank=78200 of ranks=110927rank=78300 of ranks=110927rank=78400 of ranks=110927rank=78500 of ranks=110927rank=78600 of ranks=110927rank=78700 of ranks=110927rank=78800 of ranks=110927rank=78900 of ranks=110927rank=79000 of ranks=110927rank=79100 of ranks=110927rank=79200 of ranks=110927rank=79300 of ranks=110927rank=79400 of ranks=110927rank=79500 of ranks=110927rank=79600 of ranks=110927rank=79700 of ranks=110927rank=79800 of ranks=110927rank=79900 of ranks=110927rank=80000 of ranks=110927rank=80100 of ranks=110927rank=80200 of ranks=110927rank=80300 of ranks=110927rank=80400 of ranks=110927rank=80500 of ranks=110927rank=80600 of ranks=110927rank=80700 of ranks=110927rank=80800 of ranks=110927rank=80900 of ranks=110927rank=81000 of ranks=110927rank=81100 of ranks=110927rank=81200 of ranks=110927rank=81300 of ranks=110927rank=81400 of ranks=110927rank=81500 of ranks=110927rank=81600 of ranks=110927rank=81700 of ranks=110927rank=81800 of ranks=110927rank=81900 of ranks=110927rank=82000 of ranks=110927rank=82100 of ranks=110927rank=82200 of ranks=110927rank=82300 of ranks=110927rank=82400 of ranks=110927rank=82500 of ranks=110927rank=82600 of ranks=110927rank=82700 of ranks=110927rank=82800 of ranks=110927rank=82900 of ranks=110927rank=83000 of ranks=110927rank=83100 of ranks=110927rank=83200 of ranks=110927rank=83300 of ranks=110927rank=83400 of ranks=110927rank=83500 of ranks=110927rank=83600 of ranks=110927rank=83700 of ranks=110927rank=83800 of ranks=110927rank=83900 of ranks=110927rank=84000 of ranks=110927rank=84100 of ranks=110927rank=84200 of ranks=110927rank=84300 of ranks=110927rank=84400 of ranks=110927rank=84500 of ranks=110927rank=84600 of ranks=110927rank=84700 of ranks=110927rank=84800 of ranks=110927rank=84900 of ranks=110927rank=85000 of ranks=110927rank=85100 of ranks=110927rank=85200 of ranks=110927rank=85300 of ranks=110927rank=85400 of ranks=110927rank=85500 of ranks=110927rank=85600 of ranks=110927rank=85700 of ranks=110927rank=85800 of ranks=110927rank=85900 of ranks=110927rank=86000 of ranks=110927rank=86100 of ranks=110927rank=86200 of ranks=110927rank=86300 of ranks=110927rank=86400 of ranks=110927rank=86500 of ranks=110927rank=86600 of ranks=110927rank=86700 of ranks=110927rank=86800 of ranks=110927rank=86900 of ranks=110927rank=87000 of ranks=110927rank=87100 of ranks=110927rank=87200 of ranks=110927rank=87300 of ranks=110927rank=87400 of ranks=110927rank=87500 of ranks=110927rank=87600 of ranks=110927rank=87700 of ranks=110927rank=87800 of ranks=110927rank=87900 of ranks=110927rank=88000 of ranks=110927rank=88100 of ranks=110927rank=88200 of ranks=110927rank=88300 of ranks=110927rank=88400 of ranks=110927rank=88500 of ranks=110927rank=88600 of ranks=110927rank=88700 of ranks=110927rank=88800 of ranks=110927rank=88900 of ranks=110927rank=89000 of ranks=110927rank=89100 of ranks=110927rank=89200 of ranks=110927rank=89300 of ranks=110927rank=89400 of ranks=110927rank=89500 of ranks=110927rank=89600 of ranks=110927rank=89700 of ranks=110927rank=89800 of ranks=110927rank=89900 of ranks=110927rank=90000 of ranks=110927rank=90100 of ranks=110927rank=90200 of ranks=110927rank=90300 of ranks=110927rank=90400 of ranks=110927rank=90500 of ranks=110927rank=90600 of ranks=110927rank=90700 of ranks=110927rank=90800 of ranks=110927rank=90900 of ranks=110927rank=91000 of ranks=110927rank=91100 of ranks=110927rank=91200 of ranks=110927rank=91300 of ranks=110927rank=91400 of ranks=110927rank=91500 of ranks=110927rank=91600 of ranks=110927rank=91700 of ranks=110927rank=91800 of ranks=110927rank=91900 of ranks=110927rank=92000 of ranks=110927rank=92100 of ranks=110927rank=92200 of ranks=110927rank=92300 of ranks=110927rank=92400 of ranks=110927rank=92500 of ranks=110927rank=92600 of ranks=110927rank=92700 of ranks=110927rank=92800 of ranks=110927rank=92900 of ranks=110927rank=93000 of ranks=110927rank=93100 of ranks=110927rank=93200 of ranks=110927rank=93300 of ranks=110927rank=93400 of ranks=110927rank=93500 of ranks=110927rank=93600 of ranks=110927rank=93700 of ranks=110927rank=93800 of ranks=110927rank=93900 of ranks=110927rank=94000 of ranks=110927rank=94100 of ranks=110927rank=94200 of ranks=110927rank=94300 of ranks=110927rank=94400 of ranks=110927rank=94500 of ranks=110927rank=94600 of ranks=110927rank=94700 of ranks=110927rank=94800 of ranks=110927rank=94900 of ranks=110927rank=95000 of ranks=110927rank=95100 of ranks=110927rank=95200 of ranks=110927rank=95300 of ranks=110927rank=95400 of ranks=110927rank=95500 of ranks=110927rank=95600 of ranks=110927rank=95700 of ranks=110927rank=95800 of ranks=110927rank=95900 of ranks=110927rank=96000 of ranks=110927rank=96100 of ranks=110927rank=96200 of ranks=110927rank=96300 of ranks=110927rank=96400 of ranks=110927rank=96500 of ranks=110927rank=96600 of ranks=110927rank=96700 of ranks=110927rank=96800 of ranks=110927rank=96900 of ranks=110927rank=97000 of ranks=110927rank=97100 of ranks=110927rank=97200 of ranks=110927rank=97300 of ranks=110927rank=97400 of ranks=110927rank=97500 of ranks=110927rank=97600 of ranks=110927rank=97700 of ranks=110927rank=97800 of ranks=110927rank=97900 of ranks=110927rank=98000 of ranks=110927rank=98100 of ranks=110927rank=98200 of ranks=110927rank=98300 of ranks=110927rank=98400 of ranks=110927rank=98500 of ranks=110927rank=98600 of ranks=110927rank=98700 of ranks=110927rank=98800 of ranks=110927rank=98900 of ranks=110927rank=99000 of ranks=110927rank=99100 of ranks=110927rank=99200 of ranks=110927rank=99300 of ranks=110927rank=99400 of ranks=110927rank=99500 of ranks=110927rank=99600 of ranks=110927rank=99700 of ranks=110927rank=99800 of ranks=110927rank=99900 of ranks=110927rank=100000 of ranks=110927rank=100100 of ranks=110927rank=100200 of ranks=110927rank=100300 of ranks=110927rank=100400 of ranks=110927rank=100500 of ranks=110927rank=100600 of ranks=110927rank=100700 of ranks=110927rank=100800 of ranks=110927rank=100900 of ranks=110927rank=101000 of ranks=110927rank=101100 of ranks=110927rank=101200 of ranks=110927rank=101300 of ranks=110927rank=101400 of ranks=110927rank=101500 of ranks=110927rank=101600 of ranks=110927rank=101700 of ranks=110927rank=101800 of ranks=110927rank=101900 of ranks=110927rank=102000 of ranks=110927rank=102100 of ranks=110927rank=102200 of ranks=110927rank=102300 of ranks=110927rank=102400 of ranks=110927rank=102500 of ranks=110927rank=102600 of ranks=110927rank=102700 of ranks=110927rank=102800 of ranks=110927rank=102900 of ranks=110927rank=103000 of ranks=110927rank=103100 of ranks=110927rank=103200 of ranks=110927rank=103300 of ranks=110927rank=103400 of ranks=110927rank=103500 of ranks=110927rank=103600 of ranks=110927rank=103700 of ranks=110927rank=103800 of ranks=110927rank=103900 of ranks=110927rank=104000 of ranks=110927rank=104100 of ranks=110927rank=104200 of ranks=110927rank=104300 of ranks=110927rank=104400 of ranks=110927rank=104500 of ranks=110927rank=104600 of ranks=110927rank=104700 of ranks=110927rank=104800 of ranks=110927rank=104900 of ranks=110927rank=105000 of ranks=110927rank=105100 of ranks=110927rank=105200 of ranks=110927rank=105300 of ranks=110927rank=105400 of ranks=110927rank=105500 of ranks=110927rank=105600 of ranks=110927rank=105700 of ranks=110927rank=105800 of ranks=110927rank=105900 of ranks=110927rank=106000 of ranks=110927rank=106100 of ranks=110927rank=106200 of ranks=110927rank=106300 of ranks=110927rank=106400 of ranks=110927rank=106500 of ranks=110927rank=106600 of ranks=110927rank=106700 of ranks=110927rank=106800 of ranks=110927rank=106900 of ranks=110927rank=107000 of ranks=110927rank=107100 of ranks=110927rank=107200 of ranks=110927rank=107300 of ranks=110927rank=107400 of ranks=110927rank=107500 of ranks=110927rank=107600 of ranks=110927rank=107700 of ranks=110927rank=107800 of ranks=110927rank=107900 of ranks=110927rank=108000 of ranks=110927rank=108100 of ranks=110927rank=108200 of ranks=110927rank=108300 of ranks=110927rank=108400 of ranks=110927rank=108500 of ranks=110927rank=108600 of ranks=110927rank=108700 of ranks=110927rank=108800 of ranks=110927rank=108900 of ranks=110927rank=109000 of ranks=110927rank=109100 of ranks=110927rank=109200 of ranks=110927rank=109300 of ranks=110927rank=109400 of ranks=110927rank=109500 of ranks=110927rank=109600 of ranks=110927rank=109700 of ranks=110927rank=109800 of ranks=110927rank=109900 of ranks=110927rank=110000 of ranks=110927rank=110100 of ranks=110927rank=110200 of ranks=110927rank=110300 of ranks=110927rank=110400 of ranks=110927rank=110500 of ranks=110927rank=110600 of ranks=110927rank=110700 of ranks=110927rank=110800 of ranks=110927rank=110900 of ranks=110927

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              95.7885    486   3455     12    498           74.6486
   1 car                    98.2841  49976  29954    340  50316           80.8051
   2 truck                  96.3048   1808   4971     17   1825           70.5851
   3 bus                    93.6578    360   4147      6    366           60.9497
   4 pedestrian             94.8798   4120  11650    139   4259           69.9345

for conf_thresh=0.25, precision=0.90, recall=0.95, F1 score=0.92
for conf_thresh=0.25, TP=54395, FP=6225, FN=2869, average IoU=79.47%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=95.78%
Total detection time: 241 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5467: loss=3.150, avg loss=3.199, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=2.1 seconds, 349888 images, time remaining=2.5 hours
5468: loss=3.224, avg loss=3.201, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=583.8 milliseconds, train=2.1 seconds, 349952 images, time remaining=2.5 hours
5469: loss=3.104, avg loss=3.192, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=2.1 seconds, 350016 images, time remaining=2.5 hours
5470: loss=2.878, avg loss=3.160, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 350080 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a36000000
5471: loss=3.177, avg loss=3.162, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=667.6 milliseconds, train=1.5 seconds, 350144 images, time remaining=2.5 hours
5472: loss=3.526, avg loss=3.198, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 350208 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5473: loss=2.881, avg loss=3.167, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 350272 images, time remaining=2.5 hours
5474: loss=3.650, avg loss=3.215, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 350336 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5475: loss=3.523, avg loss=3.246, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=1.5 seconds, 350400 images, time remaining=2.5 hours
5476: loss=2.863, avg loss=3.208, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=763.1 milliseconds, train=1.5 seconds, 350464 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5477: loss=3.177, avg loss=3.204, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=1.5 seconds, 350528 images, time remaining=2.5 hours
5478: loss=2.808, avg loss=3.165, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=761.6 milliseconds, train=1.5 seconds, 350592 images, time remaining=2.5 hours
5479: loss=3.083, avg loss=3.157, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=920.2 milliseconds, train=1.5 seconds, 350656 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5480: loss=4.062, avg loss=3.247, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 350720 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145d42000000
5481: loss=3.058, avg loss=3.228, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.6 seconds, 350784 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5482: loss=3.071, avg loss=3.213, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.1 seconds, train=1.6 seconds, 350848 images, time remaining=2.5 hours
5483: loss=2.964, avg loss=3.188, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=727.4 milliseconds, train=1.6 seconds, 350912 images, time remaining=2.5 hours
5484: loss=3.160, avg loss=3.185, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=606.7 milliseconds, train=1.6 seconds, 350976 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5485: loss=3.070, avg loss=3.173, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=1.6 seconds, 351040 images, time remaining=2.5 hours
5486: loss=3.792, avg loss=3.235, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 351104 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5487: loss=2.849, avg loss=3.197, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.6 seconds, 351168 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5488: loss=2.679, avg loss=3.145, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=1.6 seconds, 351232 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5489: loss=3.087, avg loss=3.139, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=1.6 seconds, 351296 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5490: loss=2.895, avg loss=3.115, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.0 seconds, train=1.6 seconds, 351360 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x1450b7c00000
5491: loss=2.211, avg loss=3.024, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 351424 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5492: loss=3.302, avg loss=3.052, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 351488 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5493: loss=2.961, avg loss=3.043, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 351552 images, time remaining=2.5 hours
5494: loss=2.987, avg loss=3.037, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=798.2 milliseconds, train=1.5 seconds, 351616 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5495: loss=3.155, avg loss=3.049, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.4 seconds, train=1.5 seconds, 351680 images, time remaining=2.5 hours
5496: loss=3.003, avg loss=3.045, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 351744 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5497: loss=2.618, avg loss=3.002, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.1 seconds, train=1.5 seconds, 351808 images, time remaining=2.5 hours
5498: loss=3.612, avg loss=3.063, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 351872 images, time remaining=2.5 hours
5499: loss=3.742, avg loss=3.131, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=432.9 milliseconds, train=1.6 seconds, 351936 images, time remaining=2.5 hours
5500: loss=3.029, avg loss=3.121, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=536.7 milliseconds, train=1.5 seconds, 352000 images, time remaining=2.5 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a2e000000
5501: loss=3.180, avg loss=3.127, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 352064 images, time remaining=2.5 hours
5502: loss=3.078, avg loss=3.122, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=2.1 seconds, 352128 images, time remaining=2.5 hours
5503: loss=4.025, avg loss=3.212, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=766.1 milliseconds, train=2.1 seconds, 352192 images, time remaining=2.5 hours
5504: loss=3.035, avg loss=3.195, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=889.2 milliseconds, train=2.1 seconds, 352256 images, time remaining=2.5 hours
5505: loss=3.358, avg loss=3.211, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 352320 images, time remaining=2.5 hours
5506: loss=2.602, avg loss=3.150, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 352384 images, time remaining=2.5 hours
5507: loss=2.294, avg loss=3.064, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=2.1 seconds, 352448 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5508: loss=3.007, avg loss=3.059, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 352512 images, time remaining=2.5 hours
5509: loss=2.912, avg loss=3.044, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=924.3 milliseconds, train=2.1 seconds, 352576 images, time remaining=2.5 hours
5510: loss=2.835, avg loss=3.023, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=932.7 milliseconds, train=2.1 seconds, 352640 images, time remaining=2.5 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145c0c000000
5511: loss=3.375, avg loss=3.058, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 352704 images, time remaining=2.5 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5512: loss=4.368, avg loss=3.189, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 352768 images, time remaining=2.4 hours
5513: loss=2.708, avg loss=3.141, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 352832 images, time remaining=2.4 hours
5514: loss=2.538, avg loss=3.081, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 352896 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5515: loss=2.564, avg loss=3.029, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=1.2 seconds, 352960 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5516: loss=3.460, avg loss=3.072, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 353024 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5517: loss=2.777, avg loss=3.043, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=1.2 seconds, 353088 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5518: loss=2.943, avg loss=3.033, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.0 seconds, train=1.2 seconds, 353152 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5519: loss=3.374, avg loss=3.067, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 353216 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5520: loss=2.811, avg loss=3.041, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 353280 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x14471c000000
5521: loss=5.582, avg loss=3.295, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=992.2 milliseconds, train=4.7 seconds, 353344 images, time remaining=2.4 hours
5522: loss=4.073, avg loss=3.373, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=4.6 seconds, 353408 images, time remaining=2.4 hours
5523: loss=3.750, avg loss=3.411, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.7 seconds, train=4.6 seconds, 353472 images, time remaining=2.4 hours
5524: loss=3.486, avg loss=3.418, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 353536 images, time remaining=2.4 hours
5525: loss=3.729, avg loss=3.449, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=4.6 seconds, 353600 images, time remaining=2.4 hours
5526: loss=3.622, avg loss=3.467, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 353664 images, time remaining=2.4 hours
5527: loss=3.804, avg loss=3.500, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 353728 images, time remaining=2.4 hours
5528: loss=4.800, avg loss=3.630, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=4.6 seconds, 353792 images, time remaining=2.4 hours
5529: loss=3.464, avg loss=3.614, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 353856 images, time remaining=2.4 hours
5530: loss=3.614, avg loss=3.614, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=4.7 seconds, 353920 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145822000000
5531: loss=3.622, avg loss=3.615, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 second, train=1.6 seconds, 353984 images, time remaining=2.4 hours
5532: loss=4.084, avg loss=3.661, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=781.4 milliseconds, train=1.6 seconds, 354048 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5533: loss=4.295, avg loss=3.725, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 354112 images, time remaining=2.4 hours
5534: loss=3.290, avg loss=3.681, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=940.3 milliseconds, train=1.7 seconds, 354176 images, time remaining=2.4 hours
5535: loss=3.106, avg loss=3.624, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=747.8 milliseconds, train=1.6 seconds, 354240 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5536: loss=3.286, avg loss=3.590, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=1.6 seconds, 354304 images, time remaining=2.4 hours
5537: loss=3.074, avg loss=3.539, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.7 seconds, 354368 images, time remaining=2.4 hours
5538: loss=2.889, avg loss=3.474, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=734.8 milliseconds, train=1.6 seconds, 354432 images, time remaining=2.4 hours
5539: loss=3.611, avg loss=3.487, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=973.5 milliseconds, train=1.6 seconds, 354496 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5540: loss=3.020, avg loss=3.441, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=1.6 seconds, 354560 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5541: loss=4.954, avg loss=3.592, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=654.3 milliseconds, train=4.1 seconds, 354624 images, time remaining=2.4 hours
5542: loss=5.853, avg loss=3.818, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 354688 images, time remaining=2.4 hours
5543: loss=4.532, avg loss=3.889, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=920.8 milliseconds, train=4.1 seconds, 354752 images, time remaining=2.4 hours
5544: loss=4.669, avg loss=3.967, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 354816 images, time remaining=2.4 hours
5545: loss=3.938, avg loss=3.964, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 354880 images, time remaining=2.4 hours
5546: loss=2.952, avg loss=3.863, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=4.1 seconds, 354944 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5547: loss=3.852, avg loss=3.862, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.7 seconds, train=4.1 seconds, 355008 images, time remaining=2.4 hours
5548: loss=3.098, avg loss=3.786, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 355072 images, time remaining=2.4 hours
5549: loss=3.601, avg loss=3.767, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 355136 images, time remaining=2.4 hours
5550: loss=4.097, avg loss=3.800, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=4.2 seconds, 355200 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5551: loss=4.193, avg loss=3.839, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 355264 images, time remaining=2.4 hours
5552: loss=3.795, avg loss=3.835, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 355328 images, time remaining=2.4 hours
5553: loss=3.824, avg loss=3.834, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 355392 images, time remaining=2.4 hours
5554: loss=4.444, avg loss=3.895, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=851.2 milliseconds, train=5.1 seconds, 355456 images, time remaining=2.4 hours
5555: loss=4.773, avg loss=3.983, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 355520 images, time remaining=2.4 hours
5556: loss=3.256, avg loss=3.910, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=5.1 seconds, 355584 images, time remaining=2.4 hours
5557: loss=4.395, avg loss=3.958, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=5.1 seconds, 355648 images, time remaining=2.4 hours
5558: loss=4.516, avg loss=4.014, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=5.1 seconds, 355712 images, time remaining=2.4 hours
5559: loss=4.130, avg loss=4.026, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 355776 images, time remaining=2.4 hours
5560: loss=4.091, avg loss=4.032, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=5.1 seconds, 355840 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x14640c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5561: loss=3.327, avg loss=3.962, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=1.6 seconds, 355904 images, time remaining=2.4 hours
5562: loss=3.170, avg loss=3.883, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=687.0 milliseconds, train=1.6 seconds, 355968 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5563: loss=3.281, avg loss=3.822, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.6 seconds, train=1.6 seconds, 356032 images, time remaining=2.4 hours
5564: loss=3.343, avg loss=3.775, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 356096 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5565: loss=4.169, avg loss=3.814, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.7 seconds, train=1.6 seconds, 356160 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5566: loss=3.470, avg loss=3.780, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=1.6 seconds, 356224 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5567: loss=3.908, avg loss=3.793, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.4 seconds, train=1.6 seconds, 356288 images, time remaining=2.4 hours
5568: loss=2.833, avg loss=3.697, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 356352 images, time remaining=2.4 hours
5569: loss=3.575, avg loss=3.684, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.7 seconds, 356416 images, time remaining=2.4 hours
5570: loss=3.418, avg loss=3.658, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=501.1 milliseconds, train=1.7 seconds, 356480 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5571: loss=4.217, avg loss=3.714, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 356544 images, time remaining=2.4 hours
5572: loss=4.587, avg loss=3.801, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 356608 images, time remaining=2.4 hours
5573: loss=3.747, avg loss=3.796, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=3.4 seconds, 356672 images, time remaining=2.4 hours
5574: loss=3.526, avg loss=3.769, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=835.2 milliseconds, train=3.4 seconds, 356736 images, time remaining=2.4 hours
5575: loss=3.168, avg loss=3.709, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=3.4 seconds, 356800 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5576: loss=3.299, avg loss=3.668, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=3.4 seconds, 356864 images, time remaining=2.4 hours
5577: loss=3.533, avg loss=3.654, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=3.4 seconds, 356928 images, time remaining=2.4 hours
5578: loss=2.966, avg loss=3.585, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=3.4 seconds, 356992 images, time remaining=2.4 hours
5579: loss=4.503, avg loss=3.677, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=3.4 seconds, 357056 images, time remaining=2.4 hours
5580: loss=3.422, avg loss=3.652, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 357120 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5581: loss=3.276, avg loss=3.614, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 357184 images, time remaining=2.4 hours
5582: loss=4.080, avg loss=3.661, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=2.0 seconds, 357248 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5583: loss=3.358, avg loss=3.630, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=2.0 seconds, 357312 images, time remaining=2.4 hours
5584: loss=3.152, avg loss=3.583, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=904.2 milliseconds, train=2.0 seconds, 357376 images, time remaining=2.4 hours
5585: loss=3.408, avg loss=3.565, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=867.2 milliseconds, train=2.0 seconds, 357440 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5586: loss=3.031, avg loss=3.512, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=2.0 seconds, 357504 images, time remaining=2.4 hours
5587: loss=2.434, avg loss=3.404, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=2.0 seconds, 357568 images, time remaining=2.4 hours
5588: loss=2.866, avg loss=3.350, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=998.6 milliseconds, train=2.0 seconds, 357632 images, time remaining=2.4 hours
5589: loss=3.248, avg loss=3.340, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=2.0 seconds, 357696 images, time remaining=2.4 hours
5590: loss=3.008, avg loss=3.307, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 357760 images, time remaining=2.4 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146498000000
5591: loss=3.082, avg loss=3.284, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=893.7 milliseconds, train=1.2 seconds, 357824 images, time remaining=2.4 hours
5592: loss=3.177, avg loss=3.274, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=936.2 milliseconds, train=1.2 seconds, 357888 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5593: loss=2.624, avg loss=3.209, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 357952 images, time remaining=2.4 hours
5594: loss=3.462, avg loss=3.234, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=862.2 milliseconds, train=1.2 seconds, 358016 images, time remaining=2.4 hours
5595: loss=3.317, avg loss=3.242, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=960.0 milliseconds, train=1.2 seconds, 358080 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5596: loss=3.240, avg loss=3.242, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=1.2 seconds, 358144 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5597: loss=3.477, avg loss=3.266, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 358208 images, time remaining=2.4 hours
5598: loss=3.367, avg loss=3.276, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 358272 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5599: loss=2.626, avg loss=3.211, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 358336 images, time remaining=2.4 hours
5600: loss=3.111, avg loss=3.201, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=947.3 milliseconds, train=1.2 seconds, 358400 images, time remaining=2.4 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x14647e000000
5601: loss=2.648, avg loss=3.146, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=694.0 milliseconds, train=1.4 seconds, 358464 images, time remaining=2.4 hours
5602: loss=3.284, avg loss=3.159, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=832.2 milliseconds, train=1.4 seconds, 358528 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5603: loss=2.872, avg loss=3.131, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=1.4 seconds, 358592 images, time remaining=2.4 hours
5604: loss=2.640, avg loss=3.082, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=922.2 milliseconds, train=1.4 seconds, 358656 images, time remaining=2.4 hours
5605: loss=3.487, avg loss=3.122, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 358720 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5606: loss=3.412, avg loss=3.151, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 358784 images, time remaining=2.4 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5607: loss=2.936, avg loss=3.130, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 358848 images, time remaining=2.4 hours
5608: loss=2.705, avg loss=3.087, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=910.6 milliseconds, train=1.4 seconds, 358912 images, time remaining=2.4 hours
5609: loss=3.019, avg loss=3.080, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 358976 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5610: loss=3.301, avg loss=3.102, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.7 seconds, train=1.4 seconds, 359040 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146368000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5611: loss=2.968, avg loss=3.089, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 359104 images, time remaining=2.3 hours
5612: loss=2.672, avg loss=3.047, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 359168 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5613: loss=3.308, avg loss=3.073, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 359232 images, time remaining=2.3 hours
5614: loss=3.056, avg loss=3.071, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=845.6 milliseconds, train=1.5 seconds, 359296 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5615: loss=2.578, avg loss=3.022, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 359360 images, time remaining=2.3 hours
5616: loss=2.667, avg loss=2.987, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=836.8 milliseconds, train=1.5 seconds, 359424 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5617: loss=2.930, avg loss=2.981, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 359488 images, time remaining=2.3 hours
5618: loss=2.910, avg loss=2.974, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 359552 images, time remaining=2.3 hours
5619: loss=3.044, avg loss=2.981, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=816.2 milliseconds, train=1.5 seconds, 359616 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5620: loss=2.642, avg loss=2.947, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 359680 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x146368000000
5621: loss=2.647, avg loss=2.917, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 359744 images, time remaining=2.3 hours
5622: loss=2.715, avg loss=2.897, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 359808 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5623: loss=2.855, avg loss=2.893, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=1.5 seconds, 359872 images, time remaining=2.3 hours
5624: loss=2.913, avg loss=2.895, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 359936 images, time remaining=2.3 hours
5625: loss=3.347, avg loss=2.940, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=979.5 milliseconds, train=1.5 seconds, 360000 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5626: loss=3.344, avg loss=2.980, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 360064 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5627: loss=3.148, avg loss=2.997, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 360128 images, time remaining=2.3 hours
5628: loss=2.778, avg loss=2.975, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 360192 images, time remaining=2.3 hours
5629: loss=2.969, avg loss=2.975, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=742.8 milliseconds, train=1.5 seconds, 360256 images, time remaining=2.3 hours
5630: loss=3.000, avg loss=2.977, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.5 seconds, 360320 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5631: loss=3.926, avg loss=3.072, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=535.9 milliseconds, train=5.1 seconds, 360384 images, time remaining=2.3 hours
5632: loss=4.564, avg loss=3.221, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=5.0 seconds, 360448 images, time remaining=2.3 hours
5633: loss=4.217, avg loss=3.321, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 360512 images, time remaining=2.3 hours
5634: loss=3.601, avg loss=3.349, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=5.1 seconds, 360576 images, time remaining=2.3 hours
5635: loss=3.709, avg loss=3.385, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=5.1 seconds, 360640 images, time remaining=2.3 hours
5636: loss=4.252, avg loss=3.472, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 360704 images, time remaining=2.3 hours
5637: loss=5.367, avg loss=3.661, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 360768 images, time remaining=2.3 hours
5638: loss=4.538, avg loss=3.749, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.1 seconds, train=5.0 seconds, 360832 images, time remaining=2.3 hours
5639: loss=3.883, avg loss=3.762, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.7 seconds, train=5.0 seconds, 360896 images, time remaining=2.3 hours
5640: loss=4.716, avg loss=3.858, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=5.1 seconds, 360960 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5641: loss=3.054, avg loss=3.777, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=729.0 milliseconds, train=2.1 seconds, 361024 images, time remaining=2.3 hours
5642: loss=3.244, avg loss=3.724, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 361088 images, time remaining=2.3 hours
5643: loss=2.658, avg loss=3.617, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=744.6 milliseconds, train=2.1 seconds, 361152 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5644: loss=3.503, avg loss=3.606, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 361216 images, time remaining=2.3 hours
5645: loss=2.814, avg loss=3.527, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=817.3 milliseconds, train=2.1 seconds, 361280 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5646: loss=3.148, avg loss=3.489, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=2.1 seconds, 361344 images, time remaining=2.3 hours
5647: loss=3.096, avg loss=3.450, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 361408 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5648: loss=3.793, avg loss=3.484, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.6 seconds, train=2.1 seconds, 361472 images, time remaining=2.3 hours
5649: loss=2.900, avg loss=3.426, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=566.8 milliseconds, train=2.1 seconds, 361536 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5650: loss=4.383, avg loss=3.521, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 361600 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5651: loss=3.383, avg loss=3.507, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 361664 images, time remaining=2.3 hours
5652: loss=2.860, avg loss=3.443, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 361728 images, time remaining=2.3 hours
5653: loss=2.983, avg loss=3.397, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 361792 images, time remaining=2.3 hours
5654: loss=3.297, avg loss=3.387, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=893.5 milliseconds, train=3.4 seconds, 361856 images, time remaining=2.3 hours
5655: loss=3.808, avg loss=3.429, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 361920 images, time remaining=2.3 hours
5656: loss=3.844, avg loss=3.470, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=3.3 seconds, 361984 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5657: loss=2.902, avg loss=3.414, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.5 seconds, train=3.3 seconds, 362048 images, time remaining=2.3 hours
5658: loss=2.782, avg loss=3.350, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=906.8 milliseconds, train=3.4 seconds, 362112 images, time remaining=2.3 hours
5659: loss=3.306, avg loss=3.346, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 362176 images, time remaining=2.3 hours
5660: loss=2.813, avg loss=3.293, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 362240 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145765800000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5661: loss=3.172, avg loss=3.281, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 362304 images, time remaining=2.3 hours
5662: loss=2.934, avg loss=3.246, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 362368 images, time remaining=2.3 hours
5663: loss=3.154, avg loss=3.237, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 362432 images, time remaining=2.3 hours
5664: loss=3.541, avg loss=3.267, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=682.6 milliseconds, train=1.5 seconds, 362496 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5665: loss=2.686, avg loss=3.209, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 362560 images, time remaining=2.3 hours
5666: loss=2.463, avg loss=3.134, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=925.3 milliseconds, train=1.5 seconds, 362624 images, time remaining=2.3 hours
5667: loss=2.929, avg loss=3.114, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 362688 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5668: loss=2.386, avg loss=3.041, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 362752 images, time remaining=2.3 hours
5669: loss=2.891, avg loss=3.026, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=579.1 milliseconds, train=1.5 seconds, 362816 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5670: loss=3.143, avg loss=3.038, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=1.5 seconds, 362880 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5671: loss=4.190, avg loss=3.153, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 362944 images, time remaining=2.3 hours
5672: loss=4.203, avg loss=3.258, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=987.0 milliseconds, train=3.9 seconds, 363008 images, time remaining=2.3 hours
5673: loss=4.549, avg loss=3.387, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 363072 images, time remaining=2.3 hours
5674: loss=3.535, avg loss=3.402, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=3.9 seconds, 363136 images, time remaining=2.3 hours
5675: loss=3.112, avg loss=3.373, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=3.9 seconds, 363200 images, time remaining=2.3 hours
5676: loss=3.389, avg loss=3.375, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=867.0 milliseconds, train=3.9 seconds, 363264 images, time remaining=2.3 hours
5677: loss=3.514, avg loss=3.389, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 363328 images, time remaining=2.3 hours
5678: loss=3.821, avg loss=3.432, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 363392 images, time remaining=2.3 hours
5679: loss=4.344, avg loss=3.523, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=456.4 milliseconds, train=3.9 seconds, 363456 images, time remaining=2.3 hours
5680: loss=4.180, avg loss=3.589, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=674.2 milliseconds, train=3.9 seconds, 363520 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5681: loss=3.661, avg loss=3.596, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 363584 images, time remaining=2.3 hours
5682: loss=3.331, avg loss=3.570, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=730.4 milliseconds, train=3.9 seconds, 363648 images, time remaining=2.3 hours
5683: loss=3.465, avg loss=3.559, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=962.7 milliseconds, train=3.9 seconds, 363712 images, time remaining=2.3 hours
5684: loss=3.023, avg loss=3.506, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=3.9 seconds, 363776 images, time remaining=2.3 hours
5685: loss=3.662, avg loss=3.521, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=3.9 seconds, 363840 images, time remaining=2.3 hours
5686: loss=3.222, avg loss=3.491, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 363904 images, time remaining=2.3 hours
5687: loss=4.029, avg loss=3.545, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 363968 images, time remaining=2.3 hours
5688: loss=4.040, avg loss=3.595, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=3.9 seconds, 364032 images, time remaining=2.3 hours
5689: loss=3.317, avg loss=3.567, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 364096 images, time remaining=2.3 hours
5690: loss=3.833, avg loss=3.593, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 364160 images, time remaining=2.3 hours
Resizing, random_coef=1.40, batch=4, 1152x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5691: loss=3.119, avg loss=3.546, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=769.3 milliseconds, train=3.6 seconds, 364224 images, time remaining=2.3 hours
5692: loss=3.154, avg loss=3.507, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.1 seconds, train=3.6 seconds, 364288 images, time remaining=2.3 hours
5693: loss=2.861, avg loss=3.442, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 364352 images, time remaining=2.3 hours
5694: loss=4.460, avg loss=3.544, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=3.7 seconds, 364416 images, time remaining=2.3 hours
5695: loss=3.389, avg loss=3.529, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=3.6 seconds, 364480 images, time remaining=2.3 hours
5696: loss=3.230, avg loss=3.499, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 364544 images, time remaining=2.3 hours
5697: loss=3.413, avg loss=3.490, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=3.7 seconds, 364608 images, time remaining=2.3 hours
5698: loss=2.948, avg loss=3.436, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=3.6 seconds, 364672 images, time remaining=2.3 hours
5699: loss=3.516, avg loss=3.444, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 364736 images, time remaining=2.3 hours
5700: loss=2.344, avg loss=3.334, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 364800 images, time remaining=2.3 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x144e50c00000
5701: loss=3.702, avg loss=3.371, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.4 seconds, 364864 images, time remaining=2.3 hours
5702: loss=3.482, avg loss=3.382, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=763.5 milliseconds, train=1.4 seconds, 364928 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5703: loss=3.716, avg loss=3.415, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=1.4 seconds, 364992 images, time remaining=2.3 hours
5704: loss=3.306, avg loss=3.404, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=778.5 milliseconds, train=1.4 seconds, 365056 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5705: loss=3.444, avg loss=3.408, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 365120 images, time remaining=2.3 hours
5706: loss=3.044, avg loss=3.372, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=978.6 milliseconds, train=1.4 seconds, 365184 images, time remaining=2.3 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5707: loss=3.743, avg loss=3.409, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.3 seconds, train=1.4 seconds, 365248 images, time remaining=2.3 hours
5708: loss=3.383, avg loss=3.406, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=704.4 milliseconds, train=1.4 seconds, 365312 images, time remaining=2.2 hours
5709: loss=2.491, avg loss=3.315, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=494.7 milliseconds, train=1.4 seconds, 365376 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5710: loss=3.129, avg loss=3.296, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=1.4 seconds, 365440 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5711: loss=4.005, avg loss=3.367, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=3.9 seconds, 365504 images, time remaining=2.2 hours
5712: loss=4.305, avg loss=3.461, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=685.3 milliseconds, train=3.9 seconds, 365568 images, time remaining=2.2 hours
5713: loss=3.700, avg loss=3.485, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=998.3 milliseconds, train=3.9 seconds, 365632 images, time remaining=2.2 hours
5714: loss=4.157, avg loss=3.552, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 365696 images, time remaining=2.2 hours
5715: loss=3.157, avg loss=3.513, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 365760 images, time remaining=2.2 hours
5716: loss=3.230, avg loss=3.484, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 365824 images, time remaining=2.2 hours
5717: loss=3.136, avg loss=3.449, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=624.9 milliseconds, train=3.9 seconds, 365888 images, time remaining=2.2 hours
5718: loss=3.255, avg loss=3.430, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 365952 images, time remaining=2.2 hours
5719: loss=4.420, avg loss=3.529, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 366016 images, time remaining=2.2 hours
5720: loss=2.885, avg loss=3.465, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 366080 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x146480000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5721: loss=4.257, avg loss=3.544, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 366144 images, time remaining=2.2 hours
5722: loss=3.914, avg loss=3.581, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=497.8 milliseconds, train=1.4 seconds, 366208 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5723: loss=3.209, avg loss=3.544, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.4 seconds, 366272 images, time remaining=2.2 hours
5724: loss=3.467, avg loss=3.536, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=636.0 milliseconds, train=1.4 seconds, 366336 images, time remaining=2.2 hours
5725: loss=2.660, avg loss=3.448, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=969.9 milliseconds, train=1.4 seconds, 366400 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5726: loss=2.857, avg loss=3.389, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 366464 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5727: loss=3.560, avg loss=3.406, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=1.4 seconds, 366528 images, time remaining=2.2 hours
5728: loss=4.407, avg loss=3.506, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=744.7 milliseconds, train=1.4 seconds, 366592 images, time remaining=2.2 hours
5729: loss=3.480, avg loss=3.504, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=786.5 milliseconds, train=1.4 seconds, 366656 images, time remaining=2.2 hours
5730: loss=3.769, avg loss=3.530, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 366720 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x14647c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5731: loss=4.161, avg loss=3.593, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 366784 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5732: loss=3.258, avg loss=3.560, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 366848 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5733: loss=3.971, avg loss=3.601, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=1.5 seconds, 366912 images, time remaining=2.2 hours
5734: loss=3.262, avg loss=3.567, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 366976 images, time remaining=2.2 hours
5735: loss=3.388, avg loss=3.549, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 367040 images, time remaining=2.2 hours
5736: loss=2.588, avg loss=3.453, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=691.5 milliseconds, train=1.5 seconds, 367104 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5737: loss=3.101, avg loss=3.418, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=1.5 seconds, 367168 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5738: loss=3.295, avg loss=3.405, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=1.5 seconds, 367232 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5739: loss=3.045, avg loss=3.369, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 367296 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5740: loss=3.653, avg loss=3.398, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=1.5 seconds, 367360 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5741: loss=4.892, avg loss=3.547, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 367424 images, time remaining=2.2 hours
5742: loss=4.924, avg loss=3.685, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=889.5 milliseconds, train=4.6 seconds, 367488 images, time remaining=2.2 hours
5743: loss=3.883, avg loss=3.705, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 367552 images, time remaining=2.2 hours
5744: loss=3.232, avg loss=3.658, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 367616 images, time remaining=2.2 hours
5745: loss=3.650, avg loss=3.657, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 367680 images, time remaining=2.2 hours
5746: loss=3.237, avg loss=3.615, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=4.6 seconds, 367744 images, time remaining=2.2 hours
5747: loss=3.402, avg loss=3.594, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 367808 images, time remaining=2.2 hours
5748: loss=4.549, avg loss=3.689, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=4.6 seconds, 367872 images, time remaining=2.2 hours
5749: loss=3.778, avg loss=3.698, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=4.6 seconds, 367936 images, time remaining=2.2 hours
5750: loss=3.545, avg loss=3.683, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.2 seconds, train=4.6 seconds, 368000 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145704000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5751: loss=4.028, avg loss=3.717, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=1.5 seconds, 368064 images, time remaining=2.2 hours
5752: loss=3.120, avg loss=3.658, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=650.0 milliseconds, train=1.5 seconds, 368128 images, time remaining=2.2 hours
5753: loss=3.774, avg loss=3.669, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=993.7 milliseconds, train=1.6 seconds, 368192 images, time remaining=2.2 hours
5754: loss=3.286, avg loss=3.631, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 368256 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5755: loss=3.412, avg loss=3.609, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 368320 images, time remaining=2.2 hours
5756: loss=2.687, avg loss=3.517, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=843.7 milliseconds, train=1.5 seconds, 368384 images, time remaining=2.2 hours
5757: loss=2.850, avg loss=3.450, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=735.3 milliseconds, train=1.5 seconds, 368448 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5758: loss=3.014, avg loss=3.407, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 368512 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5759: loss=2.950, avg loss=3.361, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 368576 images, time remaining=2.2 hours
5760: loss=3.520, avg loss=3.377, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.5 seconds, 368640 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5761: loss=6.189, avg loss=3.658, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=562.3 milliseconds, train=4.6 seconds, 368704 images, time remaining=2.2 hours
5762: loss=4.657, avg loss=3.758, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=4.5 seconds, 368768 images, time remaining=2.2 hours
5763: loss=3.894, avg loss=3.772, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.9 seconds, train=4.5 seconds, 368832 images, time remaining=2.2 hours
5764: loss=4.089, avg loss=3.803, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=729.8 milliseconds, train=4.5 seconds, 368896 images, time remaining=2.2 hours
5765: loss=3.153, avg loss=3.738, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=4.5 seconds, 368960 images, time remaining=2.2 hours
5766: loss=3.661, avg loss=3.731, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.6 seconds, train=4.5 seconds, 369024 images, time remaining=2.2 hours
5767: loss=4.152, avg loss=3.773, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=724.4 milliseconds, train=4.5 seconds, 369088 images, time remaining=2.2 hours
5768: loss=3.600, avg loss=3.755, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=4.5 seconds, 369152 images, time remaining=2.2 hours
5769: loss=3.392, avg loss=3.719, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=4.5 seconds, 369216 images, time remaining=2.2 hours
5770: loss=4.621, avg loss=3.809, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=668.3 milliseconds, train=4.5 seconds, 369280 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5771: loss=4.233, avg loss=3.852, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=2.0 seconds, 369344 images, time remaining=2.2 hours
5772: loss=3.460, avg loss=3.812, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=2.0 seconds, 369408 images, time remaining=2.2 hours
5773: loss=3.806, avg loss=3.812, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=673.6 milliseconds, train=2.0 seconds, 369472 images, time remaining=2.2 hours
5774: loss=3.547, avg loss=3.785, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=621.2 milliseconds, train=2.0 seconds, 369536 images, time remaining=2.2 hours
5775: loss=3.401, avg loss=3.747, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 369600 images, time remaining=2.2 hours
5776: loss=3.367, avg loss=3.709, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=718.4 milliseconds, train=2.0 seconds, 369664 images, time remaining=2.2 hours
5777: loss=2.494, avg loss=3.587, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=2.0 seconds, 369728 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5778: loss=3.037, avg loss=3.532, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.0 seconds, train=2.0 seconds, 369792 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5779: loss=3.718, avg loss=3.551, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=2.0 seconds, 369856 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5780: loss=2.718, avg loss=3.468, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=1.9 seconds, 369920 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145fe2000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5781: loss=3.673, avg loss=3.488, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 369984 images, time remaining=2.2 hours
5782: loss=3.335, avg loss=3.473, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=907.3 milliseconds, train=1.5 seconds, 370048 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5783: loss=3.305, avg loss=3.456, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.0 seconds, train=1.5 seconds, 370112 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5784: loss=3.045, avg loss=3.415, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=1.5 seconds, 370176 images, time remaining=2.2 hours
5785: loss=4.024, avg loss=3.476, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 370240 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5786: loss=3.187, avg loss=3.447, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=1.5 seconds, 370304 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5787: loss=2.663, avg loss=3.369, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.8 seconds, train=1.5 seconds, 370368 images, time remaining=2.2 hours
5788: loss=2.804, avg loss=3.312, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 370432 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5789: loss=2.815, avg loss=3.262, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=1.5 seconds, 370496 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5790: loss=2.932, avg loss=3.229, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 370560 images, time remaining=2.2 hours
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5791: loss=4.737, avg loss=3.380, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.4 seconds, train=5.1 seconds, 370624 images, time remaining=2.2 hours
5792: loss=4.819, avg loss=3.524, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=799.5 milliseconds, train=5.0 seconds, 370688 images, time remaining=2.2 hours
5793: loss=3.361, avg loss=3.508, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=5.0 seconds, 370752 images, time remaining=2.2 hours
5794: loss=3.692, avg loss=3.526, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=5.1 seconds, 370816 images, time remaining=2.2 hours
5795: loss=5.389, avg loss=3.712, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.6 seconds, train=5.0 seconds, 370880 images, time remaining=2.2 hours
5796: loss=4.083, avg loss=3.750, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=5.0 seconds, 370944 images, time remaining=2.2 hours
5797: loss=3.923, avg loss=3.767, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.8 seconds, train=5.0 seconds, 371008 images, time remaining=2.2 hours
5798: loss=4.448, avg loss=3.835, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=5.0 seconds, 371072 images, time remaining=2.2 hours
5799: loss=3.622, avg loss=3.814, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.5 seconds, train=5.0 seconds, 371136 images, time remaining=2.2 hours
5800: loss=3.642, avg loss=3.796, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.5 seconds, train=5.0 seconds, 371200 images, time remaining=2.2 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145f4e000000
5801: loss=3.903, avg loss=3.807, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=866.7 milliseconds, train=1.4 seconds, 371264 images, time remaining=2.2 hours
5802: loss=4.340, avg loss=3.860, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=849.1 milliseconds, train=1.4 seconds, 371328 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5803: loss=3.527, avg loss=3.827, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 371392 images, time remaining=2.2 hours
5804: loss=3.250, avg loss=3.769, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 371456 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5805: loss=3.115, avg loss=3.704, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.8 seconds, train=1.4 seconds, 371520 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5806: loss=3.532, avg loss=3.687, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 371584 images, time remaining=2.2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5807: loss=2.753, avg loss=3.593, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 371648 images, time remaining=2.1 hours
5808: loss=3.651, avg loss=3.599, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=769.8 milliseconds, train=1.4 seconds, 371712 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5809: loss=2.988, avg loss=3.538, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=1.4 seconds, 371776 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5810: loss=3.625, avg loss=3.547, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=1.4 seconds, 371840 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5811: loss=7.474, avg loss=3.939, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.7 seconds, train=4.7 seconds, 371904 images, time remaining=2.1 hours
5812: loss=5.921, avg loss=4.138, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=4.6 seconds, 371968 images, time remaining=2.1 hours
5813: loss=4.163, avg loss=4.140, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=4.6 seconds, 372032 images, time remaining=2.1 hours
5814: loss=4.066, avg loss=4.133, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 372096 images, time remaining=2.1 hours
5815: loss=3.724, avg loss=4.092, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.7 seconds, train=4.7 seconds, 372160 images, time remaining=2.1 hours
5816: loss=3.815, avg loss=4.064, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 372224 images, time remaining=2.1 hours
5817: loss=3.976, avg loss=4.055, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=4.6 seconds, 372288 images, time remaining=2.1 hours
5818: loss=4.457, avg loss=4.096, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.2 seconds, train=4.6 seconds, 372352 images, time remaining=2.1 hours
5819: loss=4.043, avg loss=4.090, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=674.1 milliseconds, train=4.6 seconds, 372416 images, time remaining=2.1 hours
5820: loss=5.646, avg loss=4.246, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.5 seconds, train=4.6 seconds, 372480 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5821: loss=4.472, avg loss=4.269, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=5.3 seconds, train=3.9 seconds, 372544 images, time remaining=2.1 hours
5822: loss=3.258, avg loss=4.167, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.2 seconds, train=3.9 seconds, 372608 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5823: loss=3.332, avg loss=4.084, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=5.3 seconds, train=3.9 seconds, 372672 images, time remaining=2.1 hours
5824: loss=3.994, avg loss=4.075, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 372736 images, time remaining=2.1 hours
5825: loss=4.356, avg loss=4.103, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.1 seconds, train=3.9 seconds, 372800 images, time remaining=2.1 hours
5826: loss=4.374, avg loss=4.130, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=807.2 milliseconds, train=3.9 seconds, 372864 images, time remaining=2.1 hours
5827: loss=3.538, avg loss=4.071, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 372928 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5828: loss=3.980, avg loss=4.062, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.4 seconds, train=3.9 seconds, 372992 images, time remaining=2.1 hours
5829: loss=3.916, avg loss=4.047, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.5 seconds, train=3.9 seconds, 373056 images, time remaining=2.1 hours
5830: loss=3.532, avg loss=3.996, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 373120 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x144e02e00000
5831: loss=3.164, avg loss=3.913, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=854.5 milliseconds, train=1.2 seconds, 373184 images, time remaining=2.1 hours
5832: loss=3.630, avg loss=3.884, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=829.3 milliseconds, train=1.2 seconds, 373248 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5833: loss=3.973, avg loss=3.893, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.4 seconds, train=1.2 seconds, 373312 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5834: loss=2.967, avg loss=3.801, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 373376 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5835: loss=2.961, avg loss=3.717, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=1.2 seconds, 373440 images, time remaining=2.1 hours
5836: loss=3.040, avg loss=3.649, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=724.0 milliseconds, train=1.2 seconds, 373504 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5837: loss=3.053, avg loss=3.589, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=5.7 seconds, train=1.2 seconds, 373568 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5838: loss=3.212, avg loss=3.552, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.1 seconds, train=1.2 seconds, 373632 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5839: loss=3.423, avg loss=3.539, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.3 seconds, train=1.2 seconds, 373696 images, time remaining=2.1 hours
5840: loss=3.850, avg loss=3.570, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=836.2 milliseconds, train=1.2 seconds, 373760 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5841: loss=5.087, avg loss=3.722, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.4 seconds, train=5.0 seconds, 373824 images, time remaining=2.1 hours
5842: loss=5.416, avg loss=3.891, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.2 seconds, train=5.0 seconds, 373888 images, time remaining=2.1 hours
5843: loss=5.096, avg loss=4.012, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.9 seconds, train=5.0 seconds, 373952 images, time remaining=2.1 hours
5844: loss=3.687, avg loss=3.979, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.0 seconds, train=5.0 seconds, 374016 images, time remaining=2.1 hours
5845: loss=4.431, avg loss=4.024, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=944.6 milliseconds, train=5.0 seconds, 374080 images, time remaining=2.1 hours
5846: loss=4.041, avg loss=4.026, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=5.0 seconds, 374144 images, time remaining=2.1 hours
5847: loss=3.094, avg loss=3.933, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.0 seconds, train=5.0 seconds, 374208 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5848: loss=4.128, avg loss=3.952, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=5.3 seconds, train=5.0 seconds, 374272 images, time remaining=2.1 hours
5849: loss=4.460, avg loss=4.003, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=670.5 milliseconds, train=5.0 seconds, 374336 images, time remaining=2.1 hours
5850: loss=4.877, avg loss=4.090, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=2.3 seconds, train=5.0 seconds, 374400 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14644c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5851: loss=5.772, avg loss=4.259, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.9 seconds, train=1.2 seconds, 374464 images, time remaining=2.1 hours
5852: loss=5.243, avg loss=4.357, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 374528 images, time remaining=2.1 hours
5853: loss=5.042, avg loss=4.426, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=941.6 milliseconds, train=1.2 seconds, 374592 images, time remaining=2.1 hours
5854: loss=4.423, avg loss=4.425, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=749.6 milliseconds, train=1.2 seconds, 374656 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5855: loss=3.749, avg loss=4.358, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 374720 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5856: loss=3.494, avg loss=4.271, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 374784 images, time remaining=2.1 hours
5857: loss=3.811, avg loss=4.225, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=1.2 seconds, 374848 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5858: loss=3.654, avg loss=4.168, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 374912 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5859: loss=3.611, avg loss=4.112, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.4 seconds, train=1.2 seconds, 374976 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5860: loss=3.463, avg loss=4.048, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 375040 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5861: loss=5.453, avg loss=4.188, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.1 seconds, train=3.8 seconds, 375104 images, time remaining=2.1 hours
5862: loss=6.167, avg loss=4.386, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=3.8 seconds, 375168 images, time remaining=2.1 hours
5863: loss=5.093, avg loss=4.457, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=3.7 seconds, train=3.8 seconds, 375232 images, time remaining=2.1 hours
5864: loss=4.878, avg loss=4.499, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.4 seconds, train=3.8 seconds, 375296 images, time remaining=2.1 hours
5865: loss=4.295, avg loss=4.478, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.3 seconds, train=3.8 seconds, 375360 images, time remaining=2.1 hours
5866: loss=3.082, avg loss=4.339, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=923.5 milliseconds, train=3.8 seconds, 375424 images, time remaining=2.1 hours
5867: loss=3.594, avg loss=4.264, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.6 seconds, train=3.8 seconds, 375488 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5868: loss=3.424, avg loss=4.180, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.2 seconds, train=3.7 seconds, 375552 images, time remaining=2.1 hours
5869: loss=4.509, avg loss=4.213, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.2 seconds, train=3.8 seconds, 375616 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5870: loss=4.584, avg loss=4.250, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.1 seconds, train=3.8 seconds, 375680 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5871: loss=4.369, avg loss=4.262, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=4.0 seconds, train=2.1 seconds, 375744 images, time remaining=2.1 hours
5872: loss=4.676, avg loss=4.304, last=95.78%, best=95.78%, next=5872, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 375808 images, time remaining=2.1 hours
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=96701, unique_truth_count=57264
rank=0 of ranks=96701rank=100 of ranks=96701rank=200 of ranks=96701rank=300 of ranks=96701rank=400 of ranks=96701rank=500 of ranks=96701rank=600 of ranks=96701rank=700 of ranks=96701rank=800 of ranks=96701rank=900 of ranks=96701rank=1000 of ranks=96701rank=1100 of ranks=96701rank=1200 of ranks=96701rank=1300 of ranks=96701rank=1400 of ranks=96701rank=1500 of ranks=96701rank=1600 of ranks=96701rank=1700 of ranks=96701rank=1800 of ranks=96701rank=1900 of ranks=96701rank=2000 of ranks=96701rank=2100 of ranks=96701rank=2200 of ranks=96701rank=2300 of ranks=96701rank=2400 of ranks=96701rank=2500 of ranks=96701rank=2600 of ranks=96701rank=2700 of ranks=96701rank=2800 of ranks=96701rank=2900 of ranks=96701rank=3000 of ranks=96701rank=3100 of ranks=96701rank=3200 of ranks=96701rank=3300 of ranks=96701rank=3400 of ranks=96701rank=3500 of ranks=96701rank=3600 of ranks=96701rank=3700 of ranks=96701rank=3800 of ranks=96701rank=3900 of ranks=96701rank=4000 of ranks=96701rank=4100 of ranks=96701rank=4200 of ranks=96701rank=4300 of ranks=96701rank=4400 of ranks=96701rank=4500 of ranks=96701rank=4600 of ranks=96701rank=4700 of ranks=96701rank=4800 of ranks=96701rank=4900 of ranks=96701rank=5000 of ranks=96701rank=5100 of ranks=96701rank=5200 of ranks=96701rank=5300 of ranks=96701rank=5400 of ranks=96701rank=5500 of ranks=96701rank=5600 of ranks=96701rank=5700 of ranks=96701rank=5800 of ranks=96701rank=5900 of ranks=96701rank=6000 of ranks=96701rank=6100 of ranks=96701rank=6200 of ranks=96701rank=6300 of ranks=96701rank=6400 of ranks=96701rank=6500 of ranks=96701rank=6600 of ranks=96701rank=6700 of ranks=96701rank=6800 of ranks=96701rank=6900 of ranks=96701rank=7000 of ranks=96701rank=7100 of ranks=96701rank=7200 of ranks=96701rank=7300 of ranks=96701rank=7400 of ranks=96701rank=7500 of ranks=96701rank=7600 of ranks=96701rank=7700 of ranks=96701rank=7800 of ranks=96701rank=7900 of ranks=96701rank=8000 of ranks=96701rank=8100 of ranks=96701rank=8200 of ranks=96701rank=8300 of ranks=96701rank=8400 of ranks=96701rank=8500 of ranks=96701rank=8600 of ranks=96701rank=8700 of ranks=96701rank=8800 of ranks=96701rank=8900 of ranks=96701rank=9000 of ranks=96701rank=9100 of ranks=96701rank=9200 of ranks=96701rank=9300 of ranks=96701rank=9400 of ranks=96701rank=9500 of ranks=96701rank=9600 of ranks=96701rank=9700 of ranks=96701rank=9800 of ranks=96701rank=9900 of ranks=96701rank=10000 of ranks=96701rank=10100 of ranks=96701rank=10200 of ranks=96701rank=10300 of ranks=96701rank=10400 of ranks=96701rank=10500 of ranks=96701rank=10600 of ranks=96701rank=10700 of ranks=96701rank=10800 of ranks=96701rank=10900 of ranks=96701rank=11000 of ranks=96701rank=11100 of ranks=96701rank=11200 of ranks=96701rank=11300 of ranks=96701rank=11400 of ranks=96701rank=11500 of ranks=96701rank=11600 of ranks=96701rank=11700 of ranks=96701rank=11800 of ranks=96701rank=11900 of ranks=96701rank=12000 of ranks=96701rank=12100 of ranks=96701rank=12200 of ranks=96701rank=12300 of ranks=96701rank=12400 of ranks=96701rank=12500 of ranks=96701rank=12600 of ranks=96701rank=12700 of ranks=96701rank=12800 of ranks=96701rank=12900 of ranks=96701rank=13000 of ranks=96701rank=13100 of ranks=96701rank=13200 of ranks=96701rank=13300 of ranks=96701rank=13400 of ranks=96701rank=13500 of ranks=96701rank=13600 of ranks=96701rank=13700 of ranks=96701rank=13800 of ranks=96701rank=13900 of ranks=96701rank=14000 of ranks=96701rank=14100 of ranks=96701rank=14200 of ranks=96701rank=14300 of ranks=96701rank=14400 of ranks=96701rank=14500 of ranks=96701rank=14600 of ranks=96701rank=14700 of ranks=96701rank=14800 of ranks=96701rank=14900 of ranks=96701rank=15000 of ranks=96701rank=15100 of ranks=96701rank=15200 of ranks=96701rank=15300 of ranks=96701rank=15400 of ranks=96701rank=15500 of ranks=96701rank=15600 of ranks=96701rank=15700 of ranks=96701rank=15800 of ranks=96701rank=15900 of ranks=96701rank=16000 of ranks=96701rank=16100 of ranks=96701rank=16200 of ranks=96701rank=16300 of ranks=96701rank=16400 of ranks=96701rank=16500 of ranks=96701rank=16600 of ranks=96701rank=16700 of ranks=96701rank=16800 of ranks=96701rank=16900 of ranks=96701rank=17000 of ranks=96701rank=17100 of ranks=96701rank=17200 of ranks=96701rank=17300 of ranks=96701rank=17400 of ranks=96701rank=17500 of ranks=96701rank=17600 of ranks=96701rank=17700 of ranks=96701rank=17800 of ranks=96701rank=17900 of ranks=96701rank=18000 of ranks=96701rank=18100 of ranks=96701rank=18200 of ranks=96701rank=18300 of ranks=96701rank=18400 of ranks=96701rank=18500 of ranks=96701rank=18600 of ranks=96701rank=18700 of ranks=96701rank=18800 of ranks=96701rank=18900 of ranks=96701rank=19000 of ranks=96701rank=19100 of ranks=96701rank=19200 of ranks=96701rank=19300 of ranks=96701rank=19400 of ranks=96701rank=19500 of ranks=96701rank=19600 of ranks=96701rank=19700 of ranks=96701rank=19800 of ranks=96701rank=19900 of ranks=96701rank=20000 of ranks=96701rank=20100 of ranks=96701rank=20200 of ranks=96701rank=20300 of ranks=96701rank=20400 of ranks=96701rank=20500 of ranks=96701rank=20600 of ranks=96701rank=20700 of ranks=96701rank=20800 of ranks=96701rank=20900 of ranks=96701rank=21000 of ranks=96701rank=21100 of ranks=96701rank=21200 of ranks=96701rank=21300 of ranks=96701rank=21400 of ranks=96701rank=21500 of ranks=96701rank=21600 of ranks=96701rank=21700 of ranks=96701rank=21800 of ranks=96701rank=21900 of ranks=96701rank=22000 of ranks=96701rank=22100 of ranks=96701rank=22200 of ranks=96701rank=22300 of ranks=96701rank=22400 of ranks=96701rank=22500 of ranks=96701rank=22600 of ranks=96701rank=22700 of ranks=96701rank=22800 of ranks=96701rank=22900 of ranks=96701rank=23000 of ranks=96701rank=23100 of ranks=96701rank=23200 of ranks=96701rank=23300 of ranks=96701rank=23400 of ranks=96701rank=23500 of ranks=96701rank=23600 of ranks=96701rank=23700 of ranks=96701rank=23800 of ranks=96701rank=23900 of ranks=96701rank=24000 of ranks=96701rank=24100 of ranks=96701rank=24200 of ranks=96701rank=24300 of ranks=96701rank=24400 of ranks=96701rank=24500 of ranks=96701rank=24600 of ranks=96701rank=24700 of ranks=96701rank=24800 of ranks=96701rank=24900 of ranks=96701rank=25000 of ranks=96701rank=25100 of ranks=96701rank=25200 of ranks=96701rank=25300 of ranks=96701rank=25400 of ranks=96701rank=25500 of ranks=96701rank=25600 of ranks=96701rank=25700 of ranks=96701rank=25800 of ranks=96701rank=25900 of ranks=96701rank=26000 of ranks=96701rank=26100 of ranks=96701rank=26200 of ranks=96701rank=26300 of ranks=96701rank=26400 of ranks=96701rank=26500 of ranks=96701rank=26600 of ranks=96701rank=26700 of ranks=96701rank=26800 of ranks=96701rank=26900 of ranks=96701rank=27000 of ranks=96701rank=27100 of ranks=96701rank=27200 of ranks=96701rank=27300 of ranks=96701rank=27400 of ranks=96701rank=27500 of ranks=96701rank=27600 of ranks=96701rank=27700 of ranks=96701rank=27800 of ranks=96701rank=27900 of ranks=96701rank=28000 of ranks=96701rank=28100 of ranks=96701rank=28200 of ranks=96701rank=28300 of ranks=96701rank=28400 of ranks=96701rank=28500 of ranks=96701rank=28600 of ranks=96701rank=28700 of ranks=96701rank=28800 of ranks=96701rank=28900 of ranks=96701rank=29000 of ranks=96701rank=29100 of ranks=96701rank=29200 of ranks=96701rank=29300 of ranks=96701rank=29400 of ranks=96701rank=29500 of ranks=96701rank=29600 of ranks=96701rank=29700 of ranks=96701rank=29800 of ranks=96701rank=29900 of ranks=96701rank=30000 of ranks=96701rank=30100 of ranks=96701rank=30200 of ranks=96701rank=30300 of ranks=96701rank=30400 of ranks=96701rank=30500 of ranks=96701rank=30600 of ranks=96701rank=30700 of ranks=96701rank=30800 of ranks=96701rank=30900 of ranks=96701rank=31000 of ranks=96701rank=31100 of ranks=96701rank=31200 of ranks=96701rank=31300 of ranks=96701rank=31400 of ranks=96701rank=31500 of ranks=96701rank=31600 of ranks=96701rank=31700 of ranks=96701rank=31800 of ranks=96701rank=31900 of ranks=96701rank=32000 of ranks=96701rank=32100 of ranks=96701rank=32200 of ranks=96701rank=32300 of ranks=96701rank=32400 of ranks=96701rank=32500 of ranks=96701rank=32600 of ranks=96701rank=32700 of ranks=96701rank=32800 of ranks=96701rank=32900 of ranks=96701rank=33000 of ranks=96701rank=33100 of ranks=96701rank=33200 of ranks=96701rank=33300 of ranks=96701rank=33400 of ranks=96701rank=33500 of ranks=96701rank=33600 of ranks=96701rank=33700 of ranks=96701rank=33800 of ranks=96701rank=33900 of ranks=96701rank=34000 of ranks=96701rank=34100 of ranks=96701rank=34200 of ranks=96701rank=34300 of ranks=96701rank=34400 of ranks=96701rank=34500 of ranks=96701rank=34600 of ranks=96701rank=34700 of ranks=96701rank=34800 of ranks=96701rank=34900 of ranks=96701rank=35000 of ranks=96701rank=35100 of ranks=96701rank=35200 of ranks=96701rank=35300 of ranks=96701rank=35400 of ranks=96701rank=35500 of ranks=96701rank=35600 of ranks=96701rank=35700 of ranks=96701rank=35800 of ranks=96701rank=35900 of ranks=96701rank=36000 of ranks=96701rank=36100 of ranks=96701rank=36200 of ranks=96701rank=36300 of ranks=96701rank=36400 of ranks=96701rank=36500 of ranks=96701rank=36600 of ranks=96701rank=36700 of ranks=96701rank=36800 of ranks=96701rank=36900 of ranks=96701rank=37000 of ranks=96701rank=37100 of ranks=96701rank=37200 of ranks=96701rank=37300 of ranks=96701rank=37400 of ranks=96701rank=37500 of ranks=96701rank=37600 of ranks=96701rank=37700 of ranks=96701rank=37800 of ranks=96701rank=37900 of ranks=96701rank=38000 of ranks=96701rank=38100 of ranks=96701rank=38200 of ranks=96701rank=38300 of ranks=96701rank=38400 of ranks=96701rank=38500 of ranks=96701rank=38600 of ranks=96701rank=38700 of ranks=96701rank=38800 of ranks=96701rank=38900 of ranks=96701rank=39000 of ranks=96701rank=39100 of ranks=96701rank=39200 of ranks=96701rank=39300 of ranks=96701rank=39400 of ranks=96701rank=39500 of ranks=96701rank=39600 of ranks=96701rank=39700 of ranks=96701rank=39800 of ranks=96701rank=39900 of ranks=96701rank=40000 of ranks=96701rank=40100 of ranks=96701rank=40200 of ranks=96701rank=40300 of ranks=96701rank=40400 of ranks=96701rank=40500 of ranks=96701rank=40600 of ranks=96701rank=40700 of ranks=96701rank=40800 of ranks=96701rank=40900 of ranks=96701rank=41000 of ranks=96701rank=41100 of ranks=96701rank=41200 of ranks=96701rank=41300 of ranks=96701rank=41400 of ranks=96701rank=41500 of ranks=96701rank=41600 of ranks=96701rank=41700 of ranks=96701rank=41800 of ranks=96701rank=41900 of ranks=96701rank=42000 of ranks=96701rank=42100 of ranks=96701rank=42200 of ranks=96701rank=42300 of ranks=96701rank=42400 of ranks=96701rank=42500 of ranks=96701rank=42600 of ranks=96701rank=42700 of ranks=96701rank=42800 of ranks=96701rank=42900 of ranks=96701rank=43000 of ranks=96701rank=43100 of ranks=96701rank=43200 of ranks=96701rank=43300 of ranks=96701rank=43400 of ranks=96701rank=43500 of ranks=96701rank=43600 of ranks=96701rank=43700 of ranks=96701rank=43800 of ranks=96701rank=43900 of ranks=96701rank=44000 of ranks=96701rank=44100 of ranks=96701rank=44200 of ranks=96701rank=44300 of ranks=96701rank=44400 of ranks=96701rank=44500 of ranks=96701rank=44600 of ranks=96701rank=44700 of ranks=96701rank=44800 of ranks=96701rank=44900 of ranks=96701rank=45000 of ranks=96701rank=45100 of ranks=96701rank=45200 of ranks=96701rank=45300 of ranks=96701rank=45400 of ranks=96701rank=45500 of ranks=96701rank=45600 of ranks=96701rank=45700 of ranks=96701rank=45800 of ranks=96701rank=45900 of ranks=96701rank=46000 of ranks=96701rank=46100 of ranks=96701rank=46200 of ranks=96701rank=46300 of ranks=96701rank=46400 of ranks=96701rank=46500 of ranks=96701rank=46600 of ranks=96701rank=46700 of ranks=96701rank=46800 of ranks=96701rank=46900 of ranks=96701rank=47000 of ranks=96701rank=47100 of ranks=96701rank=47200 of ranks=96701rank=47300 of ranks=96701rank=47400 of ranks=96701rank=47500 of ranks=96701rank=47600 of ranks=96701rank=47700 of ranks=96701rank=47800 of ranks=96701rank=47900 of ranks=96701rank=48000 of ranks=96701rank=48100 of ranks=96701rank=48200 of ranks=96701rank=48300 of ranks=96701rank=48400 of ranks=96701rank=48500 of ranks=96701rank=48600 of ranks=96701rank=48700 of ranks=96701rank=48800 of ranks=96701rank=48900 of ranks=96701rank=49000 of ranks=96701rank=49100 of ranks=96701rank=49200 of ranks=96701rank=49300 of ranks=96701rank=49400 of ranks=96701rank=49500 of ranks=96701rank=49600 of ranks=96701rank=49700 of ranks=96701rank=49800 of ranks=96701rank=49900 of ranks=96701rank=50000 of ranks=96701rank=50100 of ranks=96701rank=50200 of ranks=96701rank=50300 of ranks=96701rank=50400 of ranks=96701rank=50500 of ranks=96701rank=50600 of ranks=96701rank=50700 of ranks=96701rank=50800 of ranks=96701rank=50900 of ranks=96701rank=51000 of ranks=96701rank=51100 of ranks=96701rank=51200 of ranks=96701rank=51300 of ranks=96701rank=51400 of ranks=96701rank=51500 of ranks=96701rank=51600 of ranks=96701rank=51700 of ranks=96701rank=51800 of ranks=96701rank=51900 of ranks=96701rank=52000 of ranks=96701rank=52100 of ranks=96701rank=52200 of ranks=96701rank=52300 of ranks=96701rank=52400 of ranks=96701rank=52500 of ranks=96701rank=52600 of ranks=96701rank=52700 of ranks=96701rank=52800 of ranks=96701rank=52900 of ranks=96701rank=53000 of ranks=96701rank=53100 of ranks=96701rank=53200 of ranks=96701rank=53300 of ranks=96701rank=53400 of ranks=96701rank=53500 of ranks=96701rank=53600 of ranks=96701rank=53700 of ranks=96701rank=53800 of ranks=96701rank=53900 of ranks=96701rank=54000 of ranks=96701rank=54100 of ranks=96701rank=54200 of ranks=96701rank=54300 of ranks=96701rank=54400 of ranks=96701rank=54500 of ranks=96701rank=54600 of ranks=96701rank=54700 of ranks=96701rank=54800 of ranks=96701rank=54900 of ranks=96701rank=55000 of ranks=96701rank=55100 of ranks=96701rank=55200 of ranks=96701rank=55300 of ranks=96701rank=55400 of ranks=96701rank=55500 of ranks=96701rank=55600 of ranks=96701rank=55700 of ranks=96701rank=55800 of ranks=96701rank=55900 of ranks=96701rank=56000 of ranks=96701rank=56100 of ranks=96701rank=56200 of ranks=96701rank=56300 of ranks=96701rank=56400 of ranks=96701rank=56500 of ranks=96701rank=56600 of ranks=96701rank=56700 of ranks=96701rank=56800 of ranks=96701rank=56900 of ranks=96701rank=57000 of ranks=96701rank=57100 of ranks=96701rank=57200 of ranks=96701rank=57300 of ranks=96701rank=57400 of ranks=96701rank=57500 of ranks=96701rank=57600 of ranks=96701rank=57700 of ranks=96701rank=57800 of ranks=96701rank=57900 of ranks=96701rank=58000 of ranks=96701rank=58100 of ranks=96701rank=58200 of ranks=96701rank=58300 of ranks=96701rank=58400 of ranks=96701rank=58500 of ranks=96701rank=58600 of ranks=96701rank=58700 of ranks=96701rank=58800 of ranks=96701rank=58900 of ranks=96701rank=59000 of ranks=96701rank=59100 of ranks=96701rank=59200 of ranks=96701rank=59300 of ranks=96701rank=59400 of ranks=96701rank=59500 of ranks=96701rank=59600 of ranks=96701rank=59700 of ranks=96701rank=59800 of ranks=96701rank=59900 of ranks=96701rank=60000 of ranks=96701rank=60100 of ranks=96701rank=60200 of ranks=96701rank=60300 of ranks=96701rank=60400 of ranks=96701rank=60500 of ranks=96701rank=60600 of ranks=96701rank=60700 of ranks=96701rank=60800 of ranks=96701rank=60900 of ranks=96701rank=61000 of ranks=96701rank=61100 of ranks=96701rank=61200 of ranks=96701rank=61300 of ranks=96701rank=61400 of ranks=96701rank=61500 of ranks=96701rank=61600 of ranks=96701rank=61700 of ranks=96701rank=61800 of ranks=96701rank=61900 of ranks=96701rank=62000 of ranks=96701rank=62100 of ranks=96701rank=62200 of ranks=96701rank=62300 of ranks=96701rank=62400 of ranks=96701rank=62500 of ranks=96701rank=62600 of ranks=96701rank=62700 of ranks=96701rank=62800 of ranks=96701rank=62900 of ranks=96701rank=63000 of ranks=96701rank=63100 of ranks=96701rank=63200 of ranks=96701rank=63300 of ranks=96701rank=63400 of ranks=96701rank=63500 of ranks=96701rank=63600 of ranks=96701rank=63700 of ranks=96701rank=63800 of ranks=96701rank=63900 of ranks=96701rank=64000 of ranks=96701rank=64100 of ranks=96701rank=64200 of ranks=96701rank=64300 of ranks=96701rank=64400 of ranks=96701rank=64500 of ranks=96701rank=64600 of ranks=96701rank=64700 of ranks=96701rank=64800 of ranks=96701rank=64900 of ranks=96701rank=65000 of ranks=96701rank=65100 of ranks=96701rank=65200 of ranks=96701rank=65300 of ranks=96701rank=65400 of ranks=96701rank=65500 of ranks=96701rank=65600 of ranks=96701rank=65700 of ranks=96701rank=65800 of ranks=96701rank=65900 of ranks=96701rank=66000 of ranks=96701rank=66100 of ranks=96701rank=66200 of ranks=96701rank=66300 of ranks=96701rank=66400 of ranks=96701rank=66500 of ranks=96701rank=66600 of ranks=96701rank=66700 of ranks=96701rank=66800 of ranks=96701rank=66900 of ranks=96701rank=67000 of ranks=96701rank=67100 of ranks=96701rank=67200 of ranks=96701rank=67300 of ranks=96701rank=67400 of ranks=96701rank=67500 of ranks=96701rank=67600 of ranks=96701rank=67700 of ranks=96701rank=67800 of ranks=96701rank=67900 of ranks=96701rank=68000 of ranks=96701rank=68100 of ranks=96701rank=68200 of ranks=96701rank=68300 of ranks=96701rank=68400 of ranks=96701rank=68500 of ranks=96701rank=68600 of ranks=96701rank=68700 of ranks=96701rank=68800 of ranks=96701rank=68900 of ranks=96701rank=69000 of ranks=96701rank=69100 of ranks=96701rank=69200 of ranks=96701rank=69300 of ranks=96701rank=69400 of ranks=96701rank=69500 of ranks=96701rank=69600 of ranks=96701rank=69700 of ranks=96701rank=69800 of ranks=96701rank=69900 of ranks=96701rank=70000 of ranks=96701rank=70100 of ranks=96701rank=70200 of ranks=96701rank=70300 of ranks=96701rank=70400 of ranks=96701rank=70500 of ranks=96701rank=70600 of ranks=96701rank=70700 of ranks=96701rank=70800 of ranks=96701rank=70900 of ranks=96701rank=71000 of ranks=96701rank=71100 of ranks=96701rank=71200 of ranks=96701rank=71300 of ranks=96701rank=71400 of ranks=96701rank=71500 of ranks=96701rank=71600 of ranks=96701rank=71700 of ranks=96701rank=71800 of ranks=96701rank=71900 of ranks=96701rank=72000 of ranks=96701rank=72100 of ranks=96701rank=72200 of ranks=96701rank=72300 of ranks=96701rank=72400 of ranks=96701rank=72500 of ranks=96701rank=72600 of ranks=96701rank=72700 of ranks=96701rank=72800 of ranks=96701rank=72900 of ranks=96701rank=73000 of ranks=96701rank=73100 of ranks=96701rank=73200 of ranks=96701rank=73300 of ranks=96701rank=73400 of ranks=96701rank=73500 of ranks=96701rank=73600 of ranks=96701rank=73700 of ranks=96701rank=73800 of ranks=96701rank=73900 of ranks=96701rank=74000 of ranks=96701rank=74100 of ranks=96701rank=74200 of ranks=96701rank=74300 of ranks=96701rank=74400 of ranks=96701rank=74500 of ranks=96701rank=74600 of ranks=96701rank=74700 of ranks=96701rank=74800 of ranks=96701rank=74900 of ranks=96701rank=75000 of ranks=96701rank=75100 of ranks=96701rank=75200 of ranks=96701rank=75300 of ranks=96701rank=75400 of ranks=96701rank=75500 of ranks=96701rank=75600 of ranks=96701rank=75700 of ranks=96701rank=75800 of ranks=96701rank=75900 of ranks=96701rank=76000 of ranks=96701rank=76100 of ranks=96701rank=76200 of ranks=96701rank=76300 of ranks=96701rank=76400 of ranks=96701rank=76500 of ranks=96701rank=76600 of ranks=96701rank=76700 of ranks=96701rank=76800 of ranks=96701rank=76900 of ranks=96701rank=77000 of ranks=96701rank=77100 of ranks=96701rank=77200 of ranks=96701rank=77300 of ranks=96701rank=77400 of ranks=96701rank=77500 of ranks=96701rank=77600 of ranks=96701rank=77700 of ranks=96701rank=77800 of ranks=96701rank=77900 of ranks=96701rank=78000 of ranks=96701rank=78100 of ranks=96701rank=78200 of ranks=96701rank=78300 of ranks=96701rank=78400 of ranks=96701rank=78500 of ranks=96701rank=78600 of ranks=96701rank=78700 of ranks=96701rank=78800 of ranks=96701rank=78900 of ranks=96701rank=79000 of ranks=96701rank=79100 of ranks=96701rank=79200 of ranks=96701rank=79300 of ranks=96701rank=79400 of ranks=96701rank=79500 of ranks=96701rank=79600 of ranks=96701rank=79700 of ranks=96701rank=79800 of ranks=96701rank=79900 of ranks=96701rank=80000 of ranks=96701rank=80100 of ranks=96701rank=80200 of ranks=96701rank=80300 of ranks=96701rank=80400 of ranks=96701rank=80500 of ranks=96701rank=80600 of ranks=96701rank=80700 of ranks=96701rank=80800 of ranks=96701rank=80900 of ranks=96701rank=81000 of ranks=96701rank=81100 of ranks=96701rank=81200 of ranks=96701rank=81300 of ranks=96701rank=81400 of ranks=96701rank=81500 of ranks=96701rank=81600 of ranks=96701rank=81700 of ranks=96701rank=81800 of ranks=96701rank=81900 of ranks=96701rank=82000 of ranks=96701rank=82100 of ranks=96701rank=82200 of ranks=96701rank=82300 of ranks=96701rank=82400 of ranks=96701rank=82500 of ranks=96701rank=82600 of ranks=96701rank=82700 of ranks=96701rank=82800 of ranks=96701rank=82900 of ranks=96701rank=83000 of ranks=96701rank=83100 of ranks=96701rank=83200 of ranks=96701rank=83300 of ranks=96701rank=83400 of ranks=96701rank=83500 of ranks=96701rank=83600 of ranks=96701rank=83700 of ranks=96701rank=83800 of ranks=96701rank=83900 of ranks=96701rank=84000 of ranks=96701rank=84100 of ranks=96701rank=84200 of ranks=96701rank=84300 of ranks=96701rank=84400 of ranks=96701rank=84500 of ranks=96701rank=84600 of ranks=96701rank=84700 of ranks=96701rank=84800 of ranks=96701rank=84900 of ranks=96701rank=85000 of ranks=96701rank=85100 of ranks=96701rank=85200 of ranks=96701rank=85300 of ranks=96701rank=85400 of ranks=96701rank=85500 of ranks=96701rank=85600 of ranks=96701rank=85700 of ranks=96701rank=85800 of ranks=96701rank=85900 of ranks=96701rank=86000 of ranks=96701rank=86100 of ranks=96701rank=86200 of ranks=96701rank=86300 of ranks=96701rank=86400 of ranks=96701rank=86500 of ranks=96701rank=86600 of ranks=96701rank=86700 of ranks=96701rank=86800 of ranks=96701rank=86900 of ranks=96701rank=87000 of ranks=96701rank=87100 of ranks=96701rank=87200 of ranks=96701rank=87300 of ranks=96701rank=87400 of ranks=96701rank=87500 of ranks=96701rank=87600 of ranks=96701rank=87700 of ranks=96701rank=87800 of ranks=96701rank=87900 of ranks=96701rank=88000 of ranks=96701rank=88100 of ranks=96701rank=88200 of ranks=96701rank=88300 of ranks=96701rank=88400 of ranks=96701rank=88500 of ranks=96701rank=88600 of ranks=96701rank=88700 of ranks=96701rank=88800 of ranks=96701rank=88900 of ranks=96701rank=89000 of ranks=96701rank=89100 of ranks=96701rank=89200 of ranks=96701rank=89300 of ranks=96701rank=89400 of ranks=96701rank=89500 of ranks=96701rank=89600 of ranks=96701rank=89700 of ranks=96701rank=89800 of ranks=96701rank=89900 of ranks=96701rank=90000 of ranks=96701rank=90100 of ranks=96701rank=90200 of ranks=96701rank=90300 of ranks=96701rank=90400 of ranks=96701rank=90500 of ranks=96701rank=90600 of ranks=96701rank=90700 of ranks=96701rank=90800 of ranks=96701rank=90900 of ranks=96701rank=91000 of ranks=96701rank=91100 of ranks=96701rank=91200 of ranks=96701rank=91300 of ranks=96701rank=91400 of ranks=96701rank=91500 of ranks=96701rank=91600 of ranks=96701rank=91700 of ranks=96701rank=91800 of ranks=96701rank=91900 of ranks=96701rank=92000 of ranks=96701rank=92100 of ranks=96701rank=92200 of ranks=96701rank=92300 of ranks=96701rank=92400 of ranks=96701rank=92500 of ranks=96701rank=92600 of ranks=96701rank=92700 of ranks=96701rank=92800 of ranks=96701rank=92900 of ranks=96701rank=93000 of ranks=96701rank=93100 of ranks=96701rank=93200 of ranks=96701rank=93300 of ranks=96701rank=93400 of ranks=96701rank=93500 of ranks=96701rank=93600 of ranks=96701rank=93700 of ranks=96701rank=93800 of ranks=96701rank=93900 of ranks=96701rank=94000 of ranks=96701rank=94100 of ranks=96701rank=94200 of ranks=96701rank=94300 of ranks=96701rank=94400 of ranks=96701rank=94500 of ranks=96701rank=94600 of ranks=96701rank=94700 of ranks=96701rank=94800 of ranks=96701rank=94900 of ranks=96701rank=95000 of ranks=96701rank=95100 of ranks=96701rank=95200 of ranks=96701rank=95300 of ranks=96701rank=95400 of ranks=96701rank=95500 of ranks=96701rank=95600 of ranks=96701rank=95700 of ranks=96701rank=95800 of ranks=96701rank=95900 of ranks=96701rank=96000 of ranks=96701rank=96100 of ranks=96701rank=96200 of ranks=96701rank=96300 of ranks=96701rank=96400 of ranks=96701rank=96500 of ranks=96701rank=96600 of ranks=96701rank=96700 of ranks=96701

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              91.5325    478   3844     20    498           71.6739
   1 car                    97.4756  49660  19928    656  50316           84.0968
   2 truck                  94.8298   1798   6275     27   1825           64.5081
   3 bus                    91.4495    360   3560      6    366           58.5278
   4 pedestrian             90.4725   3978   6820    281   4259           72.5546

for conf_thresh=0.25, precision=0.94, recall=0.91, F1 score=0.92
for conf_thresh=0.25, TP=51988, FP=3352, FN=5276, average IoU=82.31%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=93.15%
Total detection time: 212 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5873: loss=3.402, avg loss=4.213, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.2 seconds, train=2.1 seconds, 375872 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5874: loss=3.715, avg loss=4.164, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=2.1 seconds, 375936 images, time remaining=2.1 hours
5875: loss=3.605, avg loss=4.108, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=2.1 seconds, 376000 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5876: loss=2.920, avg loss=3.989, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.7 seconds, train=2.1 seconds, 376064 images, time remaining=2.1 hours
5877: loss=4.025, avg loss=3.993, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=840.8 milliseconds, train=2.1 seconds, 376128 images, time remaining=2.1 hours
5878: loss=3.804, avg loss=3.974, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 376192 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5879: loss=3.520, avg loss=3.928, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=2.1 seconds, 376256 images, time remaining=2.1 hours
5880: loss=3.907, avg loss=3.926, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 376320 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5881: loss=6.923, avg loss=4.226, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=4.7 seconds, 376384 images, time remaining=2.1 hours
5882: loss=4.858, avg loss=4.289, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=4.6 seconds, 376448 images, time remaining=2.1 hours
5883: loss=3.592, avg loss=4.219, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 376512 images, time remaining=2.1 hours
5884: loss=4.573, avg loss=4.255, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=4.6 seconds, 376576 images, time remaining=2.1 hours
5885: loss=4.342, avg loss=4.263, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 376640 images, time remaining=2.1 hours
5886: loss=4.314, avg loss=4.268, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=694.0 milliseconds, train=4.7 seconds, 376704 images, time remaining=2.1 hours
5887: loss=5.021, avg loss=4.344, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=4.7 seconds, 376768 images, time remaining=2.1 hours
5888: loss=3.666, avg loss=4.276, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.7 seconds, train=4.6 seconds, 376832 images, time remaining=2.1 hours
5889: loss=4.890, avg loss=4.337, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=741.9 milliseconds, train=4.7 seconds, 376896 images, time remaining=2.1 hours
5890: loss=3.635, avg loss=4.267, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=971.8 milliseconds, train=4.6 seconds, 376960 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
5891: loss=3.669, avg loss=4.207, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=2.1 seconds, 377024 images, time remaining=2.1 hours
5892: loss=2.995, avg loss=4.086, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=544.3 milliseconds, train=2.1 seconds, 377088 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5893: loss=3.415, avg loss=4.019, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.0 seconds, train=2.1 seconds, 377152 images, time remaining=2.1 hours
5894: loss=3.436, avg loss=3.961, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=960.2 milliseconds, train=2.1 seconds, 377216 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5895: loss=3.892, avg loss=3.954, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=2.1 seconds, 377280 images, time remaining=2.1 hours
5896: loss=3.409, avg loss=3.899, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 377344 images, time remaining=2.1 hours
5897: loss=2.743, avg loss=3.784, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=983.2 milliseconds, train=2.1 seconds, 377408 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5898: loss=3.143, avg loss=3.720, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.8 seconds, train=2.1 seconds, 377472 images, time remaining=2.1 hours
5899: loss=2.546, avg loss=3.602, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 377536 images, time remaining=2.1 hours
5900: loss=4.053, avg loss=3.648, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=966.1 milliseconds, train=2.1 seconds, 377600 images, time remaining=2.1 hours
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145c52000000
5901: loss=3.437, avg loss=3.626, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=555.9 milliseconds, train=1.7 seconds, 377664 images, time remaining=2.1 hours
5902: loss=3.188, avg loss=3.583, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=762.6 milliseconds, train=1.7 seconds, 377728 images, time remaining=2.1 hours
5903: loss=2.940, avg loss=3.518, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=877.2 milliseconds, train=1.6 seconds, 377792 images, time remaining=2.1 hours
5904: loss=3.492, avg loss=3.516, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=654.3 milliseconds, train=1.6 seconds, 377856 images, time remaining=2.1 hours
5905: loss=2.615, avg loss=3.426, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 377920 images, time remaining=2.1 hours
5906: loss=2.853, avg loss=3.368, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=719.1 milliseconds, train=1.6 seconds, 377984 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5907: loss=3.139, avg loss=3.345, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.7 seconds, train=1.6 seconds, 378048 images, time remaining=2.1 hours
5908: loss=2.991, avg loss=3.310, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 378112 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5909: loss=2.801, avg loss=3.259, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.2 seconds, train=1.6 seconds, 378176 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5910: loss=2.746, avg loss=3.208, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 378240 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5911: loss=3.970, avg loss=3.284, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 378304 images, time remaining=2.1 hours
5912: loss=3.903, avg loss=3.346, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=4.1 seconds, 378368 images, time remaining=2.1 hours
5913: loss=3.991, avg loss=3.410, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.9 seconds, train=4.1 seconds, 378432 images, time remaining=2.1 hours
5914: loss=3.324, avg loss=3.402, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=872.0 milliseconds, train=4.1 seconds, 378496 images, time remaining=2.1 hours
5915: loss=2.936, avg loss=3.355, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=4.1 seconds, 378560 images, time remaining=2.1 hours
5916: loss=3.404, avg loss=3.360, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=4.1 seconds, 378624 images, time remaining=2.1 hours
5917: loss=3.420, avg loss=3.366, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=735.4 milliseconds, train=4.1 seconds, 378688 images, time remaining=2.1 hours
5918: loss=3.099, avg loss=3.339, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.6 seconds, train=4.1 seconds, 378752 images, time remaining=2.1 hours
5919: loss=3.101, avg loss=3.316, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.9 seconds, train=4.1 seconds, 378816 images, time remaining=2.1 hours
5920: loss=3.319, avg loss=3.316, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 378880 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x144e0fc00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5921: loss=3.530, avg loss=3.337, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=1.2 seconds, 378944 images, time remaining=2.1 hours
5922: loss=3.664, avg loss=3.370, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=861.6 milliseconds, train=1.2 seconds, 379008 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5923: loss=3.437, avg loss=3.377, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 379072 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5924: loss=3.520, avg loss=3.391, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 379136 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5925: loss=3.271, avg loss=3.379, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.0 seconds, train=1.2 seconds, 379200 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5926: loss=3.237, avg loss=3.365, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.0 seconds, train=1.2 seconds, 379264 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5927: loss=2.814, avg loss=3.310, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=1.2 seconds, 379328 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5928: loss=2.854, avg loss=3.264, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.0 seconds, train=1.2 seconds, 379392 images, time remaining=2.1 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5929: loss=2.853, avg loss=3.223, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=1.2 seconds, 379456 images, time remaining=2.1 hours
5930: loss=3.864, avg loss=3.287, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=834.5 milliseconds, train=1.2 seconds, 379520 images, time remaining=2.1 hours
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5931: loss=4.859, avg loss=3.444, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.1 seconds, train=3.9 seconds, 379584 images, time remaining=2.1 hours
5932: loss=4.037, avg loss=3.504, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 379648 images, time remaining=2 hours
5933: loss=4.306, avg loss=3.584, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=714.3 milliseconds, train=3.9 seconds, 379712 images, time remaining=2 hours
5934: loss=3.647, avg loss=3.590, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 379776 images, time remaining=2 hours
5935: loss=3.281, avg loss=3.559, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=712.6 milliseconds, train=3.9 seconds, 379840 images, time remaining=2 hours
5936: loss=3.380, avg loss=3.541, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=767.9 milliseconds, train=3.9 seconds, 379904 images, time remaining=2 hours
5937: loss=3.258, avg loss=3.513, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=603.7 milliseconds, train=3.9 seconds, 379968 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5938: loss=3.698, avg loss=3.531, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.6 seconds, train=3.8 seconds, 380032 images, time remaining=2 hours
5939: loss=3.826, avg loss=3.561, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 380096 images, time remaining=2 hours
5940: loss=3.609, avg loss=3.566, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=3.9 seconds, 380160 images, time remaining=2 hours
Resizing, random_coef=1.40, batch=4, 1152x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5941: loss=4.151, avg loss=3.624, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 380224 images, time remaining=2 hours
5942: loss=3.752, avg loss=3.637, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=3.6 seconds, 380288 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5943: loss=4.031, avg loss=3.676, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.1 seconds, train=3.6 seconds, 380352 images, time remaining=2 hours
5944: loss=3.735, avg loss=3.682, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 380416 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5945: loss=3.360, avg loss=3.650, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=5.3 seconds, train=3.6 seconds, 380480 images, time remaining=2 hours
5946: loss=3.701, avg loss=3.655, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=568.9 milliseconds, train=3.6 seconds, 380544 images, time remaining=2 hours
5947: loss=3.625, avg loss=3.652, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=3.7 seconds, 380608 images, time remaining=2 hours
5948: loss=3.788, avg loss=3.666, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=3.6 seconds, 380672 images, time remaining=2 hours
5949: loss=3.365, avg loss=3.636, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.6 seconds, 380736 images, time remaining=2 hours
5950: loss=3.503, avg loss=3.622, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=552.0 milliseconds, train=3.6 seconds, 380800 images, time remaining=2 hours
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5951: loss=3.416, avg loss=3.602, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=854.5 milliseconds, train=4.4 seconds, 380864 images, time remaining=2 hours
5952: loss=4.719, avg loss=3.713, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=634.1 milliseconds, train=4.4 seconds, 380928 images, time remaining=2 hours
5953: loss=3.511, avg loss=3.693, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=4.4 seconds, 380992 images, time remaining=2 hours
5954: loss=3.602, avg loss=3.684, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=4.4 seconds, 381056 images, time remaining=2 hours
5955: loss=3.235, avg loss=3.639, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=737.3 milliseconds, train=4.5 seconds, 381120 images, time remaining=2 hours
5956: loss=3.057, avg loss=3.581, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=931.6 milliseconds, train=4.4 seconds, 381184 images, time remaining=2 hours
5957: loss=3.758, avg loss=3.599, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=754.0 milliseconds, train=4.4 seconds, 381248 images, time remaining=2 hours
5958: loss=3.260, avg loss=3.565, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=779.1 milliseconds, train=4.4 seconds, 381312 images, time remaining=2 hours
5959: loss=3.307, avg loss=3.539, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.2 seconds, train=4.4 seconds, 381376 images, time remaining=2 hours
5960: loss=3.470, avg loss=3.532, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.0 seconds, train=4.4 seconds, 381440 images, time remaining=2 hours
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463a0000000
5961: loss=3.872, avg loss=3.566, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 381504 images, time remaining=2 hours
5962: loss=4.361, avg loss=3.646, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 381568 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5963: loss=3.112, avg loss=3.592, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 381632 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5964: loss=3.513, avg loss=3.584, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.2 seconds, train=1.2 seconds, 381696 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5965: loss=3.182, avg loss=3.544, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 381760 images, time remaining=2 hours
5966: loss=3.082, avg loss=3.498, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=804.8 milliseconds, train=1.2 seconds, 381824 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5967: loss=3.924, avg loss=3.541, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=1.2 seconds, 381888 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5968: loss=4.032, avg loss=3.590, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 381952 images, time remaining=2 hours
5969: loss=3.390, avg loss=3.570, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 382016 images, time remaining=2 hours
5970: loss=3.136, avg loss=3.526, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 382080 images, time remaining=2 hours
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146470000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5971: loss=4.294, avg loss=3.603, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.8 seconds, train=1.7 seconds, 382144 images, time remaining=2 hours
5972: loss=2.989, avg loss=3.542, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=747.1 milliseconds, train=1.6 seconds, 382208 images, time remaining=2 hours
5973: loss=3.173, avg loss=3.505, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=654.8 milliseconds, train=1.6 seconds, 382272 images, time remaining=2 hours
5974: loss=2.727, avg loss=3.427, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=617.8 milliseconds, train=1.6 seconds, 382336 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5975: loss=3.591, avg loss=3.443, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.2 seconds, train=1.7 seconds, 382400 images, time remaining=2 hours
5976: loss=2.965, avg loss=3.396, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=761.0 milliseconds, train=1.7 seconds, 382464 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5977: loss=3.083, avg loss=3.364, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=1.6 seconds, 382528 images, time remaining=2 hours
5978: loss=3.133, avg loss=3.341, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=495.8 milliseconds, train=1.7 seconds, 382592 images, time remaining=2 hours
5979: loss=3.176, avg loss=3.325, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 382656 images, time remaining=2 hours
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5980: loss=2.860, avg loss=3.278, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=1.6 seconds, 382720 images, time remaining=2 hours
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5981: loss=3.771, avg loss=3.327, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=607.9 milliseconds, train=3.3 seconds, 382784 images, time remaining=2 hours
5982: loss=3.184, avg loss=3.313, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 382848 images, time remaining=119.9 minutes
5983: loss=3.672, avg loss=3.349, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 382912 images, time remaining=119.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5984: loss=3.114, avg loss=3.326, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.8 seconds, train=3.3 seconds, 382976 images, time remaining=119.8 minutes
5985: loss=2.944, avg loss=3.287, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=789.5 milliseconds, train=3.3 seconds, 383040 images, time remaining=119.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5986: loss=2.975, avg loss=3.256, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.0 seconds, train=3.3 seconds, 383104 images, time remaining=119.7 minutes
5987: loss=3.772, avg loss=3.308, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 383168 images, time remaining=119.7 minutes
5988: loss=2.891, avg loss=3.266, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 383232 images, time remaining=119.6 minutes
5989: loss=3.407, avg loss=3.280, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 383296 images, time remaining=119.5 minutes
5990: loss=3.389, avg loss=3.291, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 383360 images, time remaining=119.5 minutes
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
5991: loss=3.114, avg loss=3.273, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=883.8 milliseconds, train=3.3 seconds, 383424 images, time remaining=119.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5992: loss=2.464, avg loss=3.192, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.0 seconds, train=3.3 seconds, 383488 images, time remaining=119.4 minutes
5993: loss=2.769, avg loss=3.150, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 383552 images, time remaining=119.3 minutes
5994: loss=3.613, avg loss=3.196, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 383616 images, time remaining=119.2 minutes
5995: loss=2.669, avg loss=3.144, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 383680 images, time remaining=119.2 minutes
5996: loss=3.046, avg loss=3.134, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=3.4 seconds, 383744 images, time remaining=119.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
5997: loss=2.806, avg loss=3.101, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.0 seconds, train=3.3 seconds, 383808 images, time remaining=119.1 minutes
5998: loss=3.115, avg loss=3.102, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 383872 images, time remaining=119 minutes
5999: loss=3.623, avg loss=3.155, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 383936 images, time remaining=118.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6000: loss=2.458, avg loss=3.085, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.9 seconds, train=3.3 seconds, 384000 images, time remaining=118.9 minutes
Saving weights to /workspace/.cache/splits/combined_6000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6001: loss=2.769, avg loss=3.053, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.2 seconds, train=4.1 seconds, 384064 images, time remaining=118.8 minutes
6002: loss=2.277, avg loss=2.976, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=4.1 seconds, 384128 images, time remaining=118.8 minutes
6003: loss=3.159, avg loss=2.994, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.5 seconds, train=4.1 seconds, 384192 images, time remaining=118.8 minutes
6004: loss=3.617, avg loss=3.056, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 384256 images, time remaining=118.7 minutes
6005: loss=2.957, avg loss=3.046, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=4.1 seconds, 384320 images, time remaining=118.6 minutes
6006: loss=3.440, avg loss=3.086, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=478.1 milliseconds, train=4.1 seconds, 384384 images, time remaining=118.6 minutes
6007: loss=3.096, avg loss=3.087, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=4.1 seconds, 384448 images, time remaining=118.5 minutes
6008: loss=2.638, avg loss=3.042, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 384512 images, time remaining=118.4 minutes
6009: loss=3.352, avg loss=3.073, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=4.1 seconds, 384576 images, time remaining=118.4 minutes
6010: loss=2.983, avg loss=3.064, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=4.1 seconds, 384640 images, time remaining=118.3 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6011: loss=2.910, avg loss=3.049, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=3.4 seconds, 384704 images, time remaining=118.3 minutes
6012: loss=2.763, avg loss=3.020, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=715.4 milliseconds, train=3.4 seconds, 384768 images, time remaining=118.2 minutes
6013: loss=3.098, avg loss=3.028, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=3.4 seconds, 384832 images, time remaining=118.2 minutes
6014: loss=3.041, avg loss=3.029, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.4 seconds, 384896 images, time remaining=118.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6015: loss=3.490, avg loss=3.075, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.7 seconds, train=3.4 seconds, 384960 images, time remaining=118 minutes
6016: loss=2.673, avg loss=3.035, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=955.3 milliseconds, train=3.4 seconds, 385024 images, time remaining=118 minutes
6017: loss=2.774, avg loss=3.009, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=3.4 seconds, 385088 images, time remaining=117.9 minutes
6018: loss=2.748, avg loss=2.983, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=407.7 milliseconds, train=3.4 seconds, 385152 images, time remaining=117.9 minutes
6019: loss=2.973, avg loss=2.982, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 385216 images, time remaining=117.8 minutes
6020: loss=3.067, avg loss=2.990, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=3.4 seconds, 385280 images, time remaining=117.8 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1458a8000000
6021: loss=3.569, avg loss=3.048, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=951.0 milliseconds, train=1.2 seconds, 385344 images, time remaining=117.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6022: loss=2.865, avg loss=3.030, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=5.3 seconds, train=1.2 seconds, 385408 images, time remaining=117.6 minutes
6023: loss=2.490, avg loss=2.976, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=644.8 milliseconds, train=1.2 seconds, 385472 images, time remaining=117.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6024: loss=3.209, avg loss=2.999, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=1.2 seconds, 385536 images, time remaining=117.5 minutes
6025: loss=2.701, avg loss=2.969, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=749.2 milliseconds, train=1.2 seconds, 385600 images, time remaining=117.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6026: loss=3.231, avg loss=2.995, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.4 seconds, train=1.2 seconds, 385664 images, time remaining=117.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6027: loss=2.749, avg loss=2.971, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 385728 images, time remaining=117.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6028: loss=3.152, avg loss=2.989, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.1 seconds, train=1.2 seconds, 385792 images, time remaining=117.2 minutes
6029: loss=2.903, avg loss=2.980, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=1.3 seconds, 385856 images, time remaining=117.2 minutes
6030: loss=3.098, avg loss=2.992, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=464.0 milliseconds, train=1.2 seconds, 385920 images, time remaining=117.1 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6031: loss=5.987, avg loss=3.292, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=5.1 seconds, 385984 images, time remaining=117.1 minutes
6032: loss=4.750, avg loss=3.437, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 386048 images, time remaining=117 minutes
6033: loss=3.810, avg loss=3.475, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.0 seconds, train=5.1 seconds, 386112 images, time remaining=117 minutes
6034: loss=3.505, avg loss=3.478, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.8 seconds, train=5.1 seconds, 386176 images, time remaining=116.9 minutes
6035: loss=3.082, avg loss=3.438, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=5.0 seconds, 386240 images, time remaining=116.9 minutes
6036: loss=4.592, avg loss=3.553, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=5.1 seconds, 386304 images, time remaining=116.8 minutes
6037: loss=3.396, avg loss=3.538, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=5.0 seconds, 386368 images, time remaining=116.8 minutes
6038: loss=3.504, avg loss=3.534, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=5.0 seconds, 386432 images, time remaining=116.7 minutes
6039: loss=3.870, avg loss=3.568, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.5 seconds, train=5.1 seconds, 386496 images, time remaining=116.7 minutes
6040: loss=3.085, avg loss=3.520, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=699.7 milliseconds, train=5.0 seconds, 386560 images, time remaining=116.6 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6041: loss=3.142, avg loss=3.482, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 386624 images, time remaining=116.6 minutes
6042: loss=3.154, avg loss=3.449, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=581.5 milliseconds, train=2.1 seconds, 386688 images, time remaining=116.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6043: loss=4.647, avg loss=3.569, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=2.1 seconds, 386752 images, time remaining=116.4 minutes
6044: loss=2.915, avg loss=3.503, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=679.0 milliseconds, train=2.1 seconds, 386816 images, time remaining=116.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6045: loss=2.824, avg loss=3.435, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.9 seconds, train=2.1 seconds, 386880 images, time remaining=116.3 minutes
6046: loss=2.756, avg loss=3.367, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=684.2 milliseconds, train=2.1 seconds, 386944 images, time remaining=116.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6047: loss=2.746, avg loss=3.305, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.8 seconds, train=2.1 seconds, 387008 images, time remaining=116.2 minutes
6048: loss=2.343, avg loss=3.209, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 387072 images, time remaining=116.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6049: loss=2.741, avg loss=3.162, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=2.1 seconds, 387136 images, time remaining=116.1 minutes
6050: loss=3.308, avg loss=3.177, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 387200 images, time remaining=116 minutes
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6051: loss=3.592, avg loss=3.218, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 387264 images, time remaining=115.9 minutes
6052: loss=2.471, avg loss=3.144, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=655.9 milliseconds, train=3.2 seconds, 387328 images, time remaining=115.9 minutes
6053: loss=3.217, avg loss=3.151, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=3.2 seconds, 387392 images, time remaining=115.8 minutes
6054: loss=3.156, avg loss=3.151, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=665.8 milliseconds, train=3.2 seconds, 387456 images, time remaining=115.8 minutes
6055: loss=2.773, avg loss=3.114, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=549.8 milliseconds, train=3.2 seconds, 387520 images, time remaining=115.7 minutes
6056: loss=2.161, avg loss=3.018, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=992.3 milliseconds, train=3.2 seconds, 387584 images, time remaining=115.6 minutes
6057: loss=2.783, avg loss=2.995, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.2 seconds, train=3.2 seconds, 387648 images, time remaining=115.6 minutes
6058: loss=2.936, avg loss=2.989, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.2 seconds, 387712 images, time remaining=115.5 minutes
6059: loss=2.738, avg loss=2.964, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=3.2 seconds, 387776 images, time remaining=115.4 minutes
6060: loss=3.102, avg loss=2.978, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=529.9 milliseconds, train=3.2 seconds, 387840 images, time remaining=115.4 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6061: loss=4.133, avg loss=3.093, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=840.7 milliseconds, train=4.6 seconds, 387904 images, time remaining=115.3 minutes
6062: loss=3.279, avg loss=3.112, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 387968 images, time remaining=115.3 minutes
6063: loss=3.982, avg loss=3.199, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 388032 images, time remaining=115.2 minutes
6064: loss=4.019, avg loss=3.281, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=4.7 seconds, 388096 images, time remaining=115.2 minutes
6065: loss=3.065, avg loss=3.259, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 388160 images, time remaining=115.1 minutes
6066: loss=3.205, avg loss=3.254, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=862.0 milliseconds, train=4.6 seconds, 388224 images, time remaining=115.1 minutes
6067: loss=3.307, avg loss=3.259, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=4.6 seconds, 388288 images, time remaining=115 minutes
6068: loss=4.032, avg loss=3.337, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=4.7 seconds, 388352 images, time remaining=115 minutes
6069: loss=4.498, avg loss=3.453, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=4.6 seconds, 388416 images, time remaining=114.9 minutes
6070: loss=3.635, avg loss=3.471, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=4.6 seconds, train=4.6 seconds, 388480 images, time remaining=114.9 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1459c2000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6071: loss=3.062, avg loss=3.430, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.6 seconds, train=1.2 seconds, 388544 images, time remaining=114.8 minutes
6072: loss=4.159, avg loss=3.503, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=1.2 seconds, 388608 images, time remaining=114.7 minutes
6073: loss=3.837, avg loss=3.536, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=523.5 milliseconds, train=1.2 seconds, 388672 images, time remaining=114.7 minutes
6074: loss=2.576, avg loss=3.440, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=820.9 milliseconds, train=1.2 seconds, 388736 images, time remaining=114.6 minutes
6075: loss=3.430, avg loss=3.439, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=850.7 milliseconds, train=1.2 seconds, 388800 images, time remaining=114.5 minutes
6076: loss=3.086, avg loss=3.404, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=824.7 milliseconds, train=1.2 seconds, 388864 images, time remaining=114.4 minutes
6077: loss=3.361, avg loss=3.400, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.2 seconds, 388928 images, time remaining=114.4 minutes
6078: loss=2.882, avg loss=3.348, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=705.3 milliseconds, train=1.2 seconds, 388992 images, time remaining=114.3 minutes
6079: loss=2.583, avg loss=3.271, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=600.3 milliseconds, train=1.2 seconds, 389056 images, time remaining=114.2 minutes
6080: loss=4.207, avg loss=3.365, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=850.1 milliseconds, train=1.2 seconds, 389120 images, time remaining=114.2 minutes
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6081: loss=4.113, avg loss=3.440, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=592.7 milliseconds, train=3.3 seconds, 389184 images, time remaining=114.1 minutes
6082: loss=3.962, avg loss=3.492, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=3.3 seconds, 389248 images, time remaining=114.1 minutes
6083: loss=3.329, avg loss=3.476, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=781.5 milliseconds, train=3.3 seconds, 389312 images, time remaining=114 minutes
6084: loss=3.277, avg loss=3.456, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 389376 images, time remaining=113.9 minutes
6085: loss=3.305, avg loss=3.441, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.8 seconds, train=3.4 seconds, 389440 images, time remaining=113.9 minutes
6086: loss=3.001, avg loss=3.397, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=643.3 milliseconds, train=3.3 seconds, 389504 images, time remaining=113.8 minutes
6087: loss=2.942, avg loss=3.351, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 389568 images, time remaining=113.7 minutes
6088: loss=3.276, avg loss=3.344, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=577.1 milliseconds, train=3.4 seconds, 389632 images, time remaining=113.7 minutes
6089: loss=3.355, avg loss=3.345, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=663.1 milliseconds, train=3.3 seconds, 389696 images, time remaining=113.6 minutes
6090: loss=3.378, avg loss=3.348, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 389760 images, time remaining=113.6 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6091: loss=3.332, avg loss=3.347, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=722.6 milliseconds, train=3.9 seconds, 389824 images, time remaining=113.5 minutes
6092: loss=3.209, avg loss=3.333, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 389888 images, time remaining=113.5 minutes
6093: loss=3.265, avg loss=3.326, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=586.9 milliseconds, train=3.9 seconds, 389952 images, time remaining=113.4 minutes
6094: loss=3.239, avg loss=3.317, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.9 seconds, 390016 images, time remaining=113.3 minutes
6095: loss=3.439, avg loss=3.329, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 390080 images, time remaining=113.3 minutes
6096: loss=2.967, avg loss=3.293, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.9 seconds, 390144 images, time remaining=113.2 minutes
6097: loss=3.578, avg loss=3.322, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.9 seconds, 390208 images, time remaining=113.2 minutes
6098: loss=3.733, avg loss=3.363, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=899.7 milliseconds, train=3.9 seconds, 390272 images, time remaining=113.1 minutes
6099: loss=3.100, avg loss=3.336, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.7 seconds, train=3.9 seconds, 390336 images, time remaining=113.1 minutes
6100: loss=2.739, avg loss=3.277, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=590.6 milliseconds, train=3.9 seconds, 390400 images, time remaining=113 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145752600000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6101: loss=2.929, avg loss=3.242, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 390464 images, time remaining=112.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6102: loss=2.577, avg loss=3.175, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 390528 images, time remaining=112.9 minutes
6103: loss=2.921, avg loss=3.150, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=447.7 milliseconds, train=1.2 seconds, 390592 images, time remaining=112.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6104: loss=2.700, avg loss=3.105, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=1.2 seconds, 390656 images, time remaining=112.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6105: loss=3.202, avg loss=3.115, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 390720 images, time remaining=112.7 minutes
6106: loss=3.498, avg loss=3.153, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=650.8 milliseconds, train=1.2 seconds, 390784 images, time remaining=112.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6107: loss=2.723, avg loss=3.110, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=1.2 seconds, 390848 images, time remaining=112.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6108: loss=3.182, avg loss=3.117, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=1.2 seconds, 390912 images, time remaining=112.5 minutes
6109: loss=3.212, avg loss=3.127, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=591.8 milliseconds, train=1.2 seconds, 390976 images, time remaining=112.4 minutes
6110: loss=2.646, avg loss=3.079, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=646.9 milliseconds, train=1.2 seconds, 391040 images, time remaining=112.3 minutes
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6111: loss=3.044, avg loss=3.075, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=2.0 seconds, 391104 images, time remaining=112.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6112: loss=3.222, avg loss=3.090, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.5 seconds, train=2.0 seconds, 391168 images, time remaining=112.2 minutes
6113: loss=2.755, avg loss=3.056, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=683.8 milliseconds, train=2.0 seconds, 391232 images, time remaining=112.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6114: loss=2.551, avg loss=3.006, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=2.0 seconds, 391296 images, time remaining=112.1 minutes
6115: loss=2.452, avg loss=2.950, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=616.5 milliseconds, train=2.0 seconds, 391360 images, time remaining=112 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6116: loss=3.044, avg loss=2.960, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=1.9 seconds, 391424 images, time remaining=111.9 minutes
6117: loss=2.059, avg loss=2.870, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=2.0 seconds, 391488 images, time remaining=111.9 minutes
6118: loss=2.644, avg loss=2.847, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=488.7 milliseconds, train=2.0 seconds, 391552 images, time remaining=111.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6119: loss=2.948, avg loss=2.857, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.8 seconds, train=2.0 seconds, 391616 images, time remaining=111.8 minutes
6120: loss=3.343, avg loss=2.906, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=579.1 milliseconds, train=2.0 seconds, 391680 images, time remaining=111.7 minutes
Resizing, random_coef=1.40, batch=4, 832x672
GPU #0: allocating workspace: 418.6 MiB begins at 0x14641a000000
6121: loss=3.260, avg loss=2.941, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=1.6 seconds, 391744 images, time remaining=111.6 minutes
6122: loss=3.213, avg loss=2.968, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=901.4 milliseconds, train=1.6 seconds, 391808 images, time remaining=111.5 minutes
6123: loss=2.374, avg loss=2.909, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=757.1 milliseconds, train=1.6 seconds, 391872 images, time remaining=111.5 minutes
6124: loss=3.099, avg loss=2.928, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=525.8 milliseconds, train=1.6 seconds, 391936 images, time remaining=111.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6125: loss=2.335, avg loss=2.869, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=1.6 seconds, 392000 images, time remaining=111.3 minutes
6126: loss=2.568, avg loss=2.839, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=928.0 milliseconds, train=1.6 seconds, 392064 images, time remaining=111.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6127: loss=2.966, avg loss=2.851, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.6 seconds, train=1.6 seconds, 392128 images, time remaining=111.2 minutes
6128: loss=2.811, avg loss=2.847, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=743.5 milliseconds, train=1.6 seconds, 392192 images, time remaining=111.1 minutes
6129: loss=2.866, avg loss=2.849, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=499.5 milliseconds, train=1.6 seconds, 392256 images, time remaining=111.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6130: loss=2.505, avg loss=2.815, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=1.6 seconds, 392320 images, time remaining=111 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14641c000000
6131: loss=3.190, avg loss=2.852, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=577.6 milliseconds, train=1.5 seconds, 392384 images, time remaining=110.9 minutes
6132: loss=3.263, avg loss=2.893, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=692.5 milliseconds, train=1.5 seconds, 392448 images, time remaining=110.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6133: loss=2.437, avg loss=2.848, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=1.5 seconds, 392512 images, time remaining=110.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6134: loss=2.636, avg loss=2.827, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=1.5 seconds, 392576 images, time remaining=110.7 minutes
6135: loss=3.328, avg loss=2.877, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.0 seconds, train=1.6 seconds, 392640 images, time remaining=110.7 minutes
6136: loss=3.229, avg loss=2.912, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=795.0 milliseconds, train=1.5 seconds, 392704 images, time remaining=110.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6137: loss=2.463, avg loss=2.867, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 392768 images, time remaining=110.5 minutes
6138: loss=3.494, avg loss=2.930, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=788.1 milliseconds, train=1.5 seconds, 392832 images, time remaining=110.4 minutes
6139: loss=3.171, avg loss=2.954, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=656.5 milliseconds, train=1.5 seconds, 392896 images, time remaining=110.4 minutes
6140: loss=2.437, avg loss=2.902, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=718.4 milliseconds, train=1.5 seconds, 392960 images, time remaining=110.3 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6141: loss=2.829, avg loss=2.895, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=506.4 milliseconds, train=3.6 seconds, 393024 images, time remaining=110.3 minutes
6142: loss=3.785, avg loss=2.984, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=506.7 milliseconds, train=3.6 seconds, 393088 images, time remaining=110.2 minutes
6143: loss=3.437, avg loss=3.029, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 393152 images, time remaining=110.1 minutes
6144: loss=3.631, avg loss=3.089, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=758.8 milliseconds, train=3.6 seconds, 393216 images, time remaining=110.1 minutes
6145: loss=2.972, avg loss=3.078, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 393280 images, time remaining=110 minutes
6146: loss=2.107, avg loss=2.981, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.3 seconds, train=3.6 seconds, 393344 images, time remaining=110 minutes
6147: loss=3.751, avg loss=3.058, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.6 seconds, 393408 images, time remaining=109.9 minutes
6148: loss=3.064, avg loss=3.058, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.6 seconds, 393472 images, time remaining=109.8 minutes
6149: loss=2.329, avg loss=2.985, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=827.2 milliseconds, train=3.6 seconds, 393536 images, time remaining=109.8 minutes
6150: loss=2.917, avg loss=2.979, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=821.7 milliseconds, train=3.6 seconds, 393600 images, time remaining=109.7 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145bd8000000
6151: loss=3.282, avg loss=3.009, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=594.0 milliseconds, train=1.6 seconds, 393664 images, time remaining=109.7 minutes
6152: loss=3.599, avg loss=3.068, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=615.6 milliseconds, train=1.7 seconds, 393728 images, time remaining=109.6 minutes
6153: loss=2.618, avg loss=3.023, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=535.0 milliseconds, train=1.6 seconds, 393792 images, time remaining=109.5 minutes
6154: loss=2.491, avg loss=2.970, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=580.6 milliseconds, train=1.6 seconds, 393856 images, time remaining=109.5 minutes
6155: loss=3.229, avg loss=2.996, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.7 seconds, 393920 images, time remaining=109.4 minutes
6156: loss=2.679, avg loss=2.964, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=1.6 seconds, 393984 images, time remaining=109.3 minutes
6157: loss=2.825, avg loss=2.950, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=459.3 milliseconds, train=1.7 seconds, 394048 images, time remaining=109.3 minutes
6158: loss=3.427, avg loss=2.998, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.6 seconds, 394112 images, time remaining=109.2 minutes
6159: loss=2.101, avg loss=2.908, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=745.8 milliseconds, train=1.6 seconds, 394176 images, time remaining=109.1 minutes
6160: loss=2.851, avg loss=2.903, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=924.2 milliseconds, train=1.6 seconds, 394240 images, time remaining=109 minutes
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x145bde000000
6161: loss=2.614, avg loss=2.874, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=576.1 milliseconds, train=1.4 seconds, 394304 images, time remaining=109 minutes
6162: loss=2.546, avg loss=2.841, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=738.5 milliseconds, train=1.4 seconds, 394368 images, time remaining=108.9 minutes
6163: loss=3.173, avg loss=2.874, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=542.1 milliseconds, train=1.4 seconds, 394432 images, time remaining=108.8 minutes
6164: loss=2.632, avg loss=2.850, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=519.5 milliseconds, train=1.4 seconds, 394496 images, time remaining=108.8 minutes
6165: loss=2.831, avg loss=2.848, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=551.6 milliseconds, train=1.4 seconds, 394560 images, time remaining=108.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6166: loss=2.278, avg loss=2.791, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.0 seconds, train=1.4 seconds, 394624 images, time remaining=108.7 minutes
6167: loss=2.593, avg loss=2.771, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=906.9 milliseconds, train=1.4 seconds, 394688 images, time remaining=108.6 minutes
6168: loss=2.694, avg loss=2.763, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=717.0 milliseconds, train=1.4 seconds, 394752 images, time remaining=108.5 minutes
6169: loss=2.138, avg loss=2.701, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 394816 images, time remaining=108.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6170: loss=3.236, avg loss=2.754, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=1.4 seconds, 394880 images, time remaining=108.4 minutes
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6171: loss=3.290, avg loss=2.808, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.2 seconds, train=3.5 seconds, 394944 images, time remaining=108.3 minutes
6172: loss=3.398, avg loss=2.867, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=863.6 milliseconds, train=3.5 seconds, 395008 images, time remaining=108.3 minutes
6173: loss=2.965, avg loss=2.877, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=3.5 seconds, 395072 images, time remaining=108.2 minutes
6174: loss=3.869, avg loss=2.976, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=731.0 milliseconds, train=3.5 seconds, 395136 images, time remaining=108.1 minutes
6175: loss=3.408, avg loss=3.019, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=3.5 seconds, 395200 images, time remaining=108.1 minutes
6176: loss=2.358, avg loss=2.953, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=613.6 milliseconds, train=3.5 seconds, 395264 images, time remaining=108 minutes
6177: loss=2.455, avg loss=2.903, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.5 seconds, 395328 images, time remaining=108 minutes
6178: loss=2.972, avg loss=2.910, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=714.2 milliseconds, train=3.5 seconds, 395392 images, time remaining=107.9 minutes
6179: loss=2.514, avg loss=2.870, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=3.5 seconds, 395456 images, time remaining=107.8 minutes
6180: loss=2.870, avg loss=2.870, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.4 seconds, train=3.5 seconds, 395520 images, time remaining=107.8 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145cc6000000
6181: loss=2.555, avg loss=2.839, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=895.5 milliseconds, train=1.6 seconds, 395584 images, time remaining=107.7 minutes
6182: loss=3.211, avg loss=2.876, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=608.0 milliseconds, train=1.5 seconds, 395648 images, time remaining=107.7 minutes
6183: loss=2.755, avg loss=2.864, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=531.1 milliseconds, train=1.5 seconds, 395712 images, time remaining=107.6 minutes
6184: loss=2.732, avg loss=2.851, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 395776 images, time remaining=107.5 minutes
6185: loss=2.593, avg loss=2.825, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=743.9 milliseconds, train=1.5 seconds, 395840 images, time remaining=107.5 minutes
6186: loss=2.625, avg loss=2.805, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=632.8 milliseconds, train=1.5 seconds, 395904 images, time remaining=107.4 minutes
6187: loss=3.207, avg loss=2.845, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=553.9 milliseconds, train=1.5 seconds, 395968 images, time remaining=107.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6188: loss=2.197, avg loss=2.780, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=1.5 seconds, 396032 images, time remaining=107.2 minutes
6189: loss=3.017, avg loss=2.804, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=914.1 milliseconds, train=1.5 seconds, 396096 images, time remaining=107.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6190: loss=2.787, avg loss=2.802, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.1 seconds, train=1.5 seconds, 396160 images, time remaining=107.1 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146362000000
6191: loss=3.365, avg loss=2.859, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=630.5 milliseconds, train=1.6 seconds, 396224 images, time remaining=107.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6192: loss=2.637, avg loss=2.836, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.8 seconds, train=1.6 seconds, 396288 images, time remaining=107 minutes
6193: loss=3.264, avg loss=2.879, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=646.7 milliseconds, train=1.6 seconds, 396352 images, time remaining=106.9 minutes
6194: loss=2.829, avg loss=2.874, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=585.2 milliseconds, train=1.6 seconds, 396416 images, time remaining=106.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6195: loss=2.710, avg loss=2.858, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=1.6 seconds, 396480 images, time remaining=106.8 minutes
6196: loss=3.129, avg loss=2.885, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=471.0 milliseconds, train=1.6 seconds, 396544 images, time remaining=106.7 minutes
6197: loss=2.608, avg loss=2.857, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=907.5 milliseconds, train=1.6 seconds, 396608 images, time remaining=106.7 minutes
6198: loss=2.393, avg loss=2.811, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=885.7 milliseconds, train=1.6 seconds, 396672 images, time remaining=106.6 minutes
6199: loss=2.962, avg loss=2.826, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=596.7 milliseconds, train=1.6 seconds, 396736 images, time remaining=106.5 minutes
6200: loss=2.133, avg loss=2.757, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=489.1 milliseconds, train=1.6 seconds, 396800 images, time remaining=106.4 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1463b0000000
6201: loss=3.441, avg loss=2.825, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=682.3 milliseconds, train=1.4 seconds, 396864 images, time remaining=106.4 minutes
6202: loss=3.027, avg loss=2.845, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=592.4 milliseconds, train=1.4 seconds, 396928 images, time remaining=106.3 minutes
6203: loss=2.710, avg loss=2.832, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=444.5 milliseconds, train=1.4 seconds, 396992 images, time remaining=106.2 minutes
6204: loss=3.369, avg loss=2.885, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=754.3 milliseconds, train=1.4 seconds, 397056 images, time remaining=106.2 minutes
6205: loss=2.629, avg loss=2.860, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=609.3 milliseconds, train=1.4 seconds, 397120 images, time remaining=106.1 minutes
6206: loss=2.497, avg loss=2.823, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=912.8 milliseconds, train=1.4 seconds, 397184 images, time remaining=106 minutes
6207: loss=3.034, avg loss=2.845, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=528.0 milliseconds, train=1.4 seconds, 397248 images, time remaining=106 minutes
6208: loss=2.605, avg loss=2.821, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 397312 images, time remaining=105.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6209: loss=3.080, avg loss=2.847, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.2 seconds, train=1.4 seconds, 397376 images, time remaining=105.8 minutes
6210: loss=2.524, avg loss=2.814, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=681.5 milliseconds, train=1.4 seconds, 397440 images, time remaining=105.8 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146464000000
6211: loss=3.130, avg loss=2.846, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=1.5 seconds, 397504 images, time remaining=105.7 minutes
6212: loss=2.697, avg loss=2.831, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=886.0 milliseconds, train=1.5 seconds, 397568 images, time remaining=105.6 minutes
6213: loss=2.503, avg loss=2.798, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=549.2 milliseconds, train=1.5 seconds, 397632 images, time remaining=105.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6214: loss=3.233, avg loss=2.842, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=1.5 seconds, 397696 images, time remaining=105.5 minutes
6215: loss=2.371, avg loss=2.795, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=598.0 milliseconds, train=1.6 seconds, 397760 images, time remaining=105.4 minutes
6216: loss=2.404, avg loss=2.756, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=1.5 seconds, 397824 images, time remaining=105.4 minutes
6217: loss=3.117, avg loss=2.792, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=552.4 milliseconds, train=1.6 seconds, 397888 images, time remaining=105.3 minutes
6218: loss=2.746, avg loss=2.787, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.6 seconds, 397952 images, time remaining=105.2 minutes
6219: loss=2.626, avg loss=2.771, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=539.0 milliseconds, train=1.5 seconds, 398016 images, time remaining=105.2 minutes
6220: loss=2.932, avg loss=2.787, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=1.5 seconds, 398080 images, time remaining=105.1 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6221: loss=3.446, avg loss=2.853, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=597.3 milliseconds, train=4.6 seconds, 398144 images, time remaining=105.1 minutes
6222: loss=3.343, avg loss=2.902, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=880.4 milliseconds, train=4.7 seconds, 398208 images, time remaining=105 minutes
6223: loss=3.319, avg loss=2.944, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=4.6 seconds, 398272 images, time remaining=104.9 minutes
6224: loss=3.493, avg loss=2.999, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=4.6 seconds, 398336 images, time remaining=104.9 minutes
6225: loss=3.417, avg loss=3.040, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 398400 images, time remaining=104.8 minutes
6226: loss=2.998, avg loss=3.036, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=4.6 seconds, 398464 images, time remaining=104.8 minutes
6227: loss=3.449, avg loss=3.077, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.9 seconds, train=4.6 seconds, 398528 images, time remaining=104.7 minutes
6228: loss=2.995, avg loss=3.069, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 398592 images, time remaining=104.7 minutes
6229: loss=3.590, avg loss=3.121, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 398656 images, time remaining=104.6 minutes
6230: loss=2.635, avg loss=3.073, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=4.6 seconds, 398720 images, time remaining=104.6 minutes
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6231: loss=2.555, avg loss=3.021, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.0 seconds, train=3.2 seconds, 398784 images, time remaining=104.5 minutes
6232: loss=2.846, avg loss=3.003, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.3 seconds, 398848 images, time remaining=104.5 minutes
6233: loss=2.514, avg loss=2.954, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=770.3 milliseconds, train=3.3 seconds, 398912 images, time remaining=104.4 minutes
6234: loss=2.585, avg loss=2.917, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=814.9 milliseconds, train=3.2 seconds, 398976 images, time remaining=104.3 minutes
6235: loss=2.900, avg loss=2.916, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 399040 images, time remaining=104.3 minutes
6236: loss=2.662, avg loss=2.890, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=771.1 milliseconds, train=3.3 seconds, 399104 images, time remaining=104.2 minutes
6237: loss=2.810, avg loss=2.882, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=526.7 milliseconds, train=3.3 seconds, 399168 images, time remaining=104.2 minutes
6238: loss=3.329, avg loss=2.927, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=842.2 milliseconds, train=3.3 seconds, 399232 images, time remaining=104.1 minutes
6239: loss=2.020, avg loss=2.836, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=700.2 milliseconds, train=3.3 seconds, 399296 images, time remaining=104 minutes
6240: loss=3.002, avg loss=2.853, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=900.7 milliseconds, train=3.3 seconds, 399360 images, time remaining=104 minutes
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6241: loss=2.615, avg loss=2.829, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.6 seconds, train=4.1 seconds, 399424 images, time remaining=103.9 minutes
6242: loss=3.474, avg loss=2.894, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=647.4 milliseconds, train=4.1 seconds, 399488 images, time remaining=103.9 minutes
6243: loss=3.260, avg loss=2.930, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=885.3 milliseconds, train=4.1 seconds, 399552 images, time remaining=103.8 minutes
6244: loss=3.188, avg loss=2.956, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 399616 images, time remaining=103.8 minutes
6245: loss=3.441, avg loss=3.005, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=4.1 seconds, 399680 images, time remaining=103.7 minutes
6246: loss=2.644, avg loss=2.969, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=4.1 seconds, 399744 images, time remaining=103.6 minutes
6247: loss=3.301, avg loss=3.002, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.5 seconds, train=4.1 seconds, 399808 images, time remaining=103.6 minutes
6248: loss=3.114, avg loss=3.013, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 399872 images, time remaining=103.5 minutes
6249: loss=3.180, avg loss=3.030, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=4.1 seconds, 399936 images, time remaining=103.5 minutes
6250: loss=2.895, avg loss=3.016, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=779.7 milliseconds, train=4.1 seconds, 400000 images, time remaining=103.4 minutes
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6251: loss=2.489, avg loss=2.964, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.3 seconds, 400064 images, time remaining=103.4 minutes
6252: loss=2.811, avg loss=2.948, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=685.2 milliseconds, train=3.3 seconds, 400128 images, time remaining=103.3 minutes
6253: loss=2.811, avg loss=2.934, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.1 seconds, train=3.3 seconds, 400192 images, time remaining=103.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6254: loss=2.865, avg loss=2.928, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.3 seconds, train=3.3 seconds, 400256 images, time remaining=103.2 minutes
6255: loss=2.722, avg loss=2.907, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=773.3 milliseconds, train=3.3 seconds, 400320 images, time remaining=103.1 minutes
6256: loss=2.926, avg loss=2.909, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.1 seconds, train=3.2 seconds, 400384 images, time remaining=103.1 minutes
6257: loss=3.198, avg loss=2.938, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=501.3 milliseconds, train=3.3 seconds, 400448 images, time remaining=103 minutes
6258: loss=3.046, avg loss=2.949, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=540.5 milliseconds, train=3.3 seconds, 400512 images, time remaining=103 minutes
6259: loss=2.349, avg loss=2.889, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 400576 images, time remaining=102.9 minutes
6260: loss=2.916, avg loss=2.891, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=759.9 milliseconds, train=3.3 seconds, 400640 images, time remaining=102.8 minutes
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6261: loss=2.702, avg loss=2.872, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.4 seconds, train=3.2 seconds, 400704 images, time remaining=102.8 minutes
6262: loss=2.423, avg loss=2.827, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.5 seconds, train=3.3 seconds, 400768 images, time remaining=102.7 minutes
6263: loss=2.945, avg loss=2.839, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=642.8 milliseconds, train=3.3 seconds, 400832 images, time remaining=102.6 minutes
6264: loss=3.091, avg loss=2.864, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 400896 images, time remaining=102.6 minutes
6265: loss=3.097, avg loss=2.888, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.8 seconds, train=3.3 seconds, 400960 images, time remaining=102.5 minutes
6266: loss=2.624, avg loss=2.861, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 401024 images, time remaining=102.5 minutes
6267: loss=2.082, avg loss=2.783, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.6 seconds, train=3.3 seconds, 401088 images, time remaining=102.4 minutes
6268: loss=2.966, avg loss=2.802, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=842.2 milliseconds, train=3.3 seconds, 401152 images, time remaining=102.3 minutes
6269: loss=3.231, avg loss=2.845, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.7 seconds, train=3.3 seconds, 401216 images, time remaining=102.3 minutes
6270: loss=3.749, avg loss=2.935, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=469.8 milliseconds, train=3.3 seconds, 401280 images, time remaining=102.2 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6271: loss=3.026, avg loss=2.944, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.2 seconds, train=3.4 seconds, 401344 images, time remaining=102.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6272: loss=2.970, avg loss=2.947, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=3.6 seconds, train=3.4 seconds, 401408 images, time remaining=102.1 minutes
6273: loss=2.638, avg loss=2.916, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=599.9 milliseconds, train=3.4 seconds, 401472 images, time remaining=102.1 minutes
6274: loss=3.155, avg loss=2.940, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=893.9 milliseconds, train=3.4 seconds, 401536 images, time remaining=102 minutes
6275: loss=3.006, avg loss=2.946, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=2.2 seconds, train=3.4 seconds, 401600 images, time remaining=102 minutes
6276: loss=2.440, avg loss=2.896, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=720.9 milliseconds, train=3.4 seconds, 401664 images, time remaining=101.9 minutes
6277: loss=2.844, avg loss=2.891, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=1.3 seconds, train=3.4 seconds, 401728 images, time remaining=101.8 minutes
6278: loss=3.136, avg loss=2.915, last=93.15%, best=95.78%, next=6278, rate=0.00130000, load 64=532.6 milliseconds, train=3.4 seconds, 401792 images, time remaining=101.8 minutes
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=109317, unique_truth_count=57264
rank=0 of ranks=109317rank=100 of ranks=109317rank=200 of ranks=109317rank=300 of ranks=109317rank=400 of ranks=109317rank=500 of ranks=109317rank=600 of ranks=109317rank=700 of ranks=109317rank=800 of ranks=109317rank=900 of ranks=109317rank=1000 of ranks=109317rank=1100 of ranks=109317rank=1200 of ranks=109317rank=1300 of ranks=109317rank=1400 of ranks=109317rank=1500 of ranks=109317rank=1600 of ranks=109317rank=1700 of ranks=109317rank=1800 of ranks=109317rank=1900 of ranks=109317rank=2000 of ranks=109317rank=2100 of ranks=109317rank=2200 of ranks=109317rank=2300 of ranks=109317rank=2400 of ranks=109317rank=2500 of ranks=109317rank=2600 of ranks=109317rank=2700 of ranks=109317rank=2800 of ranks=109317rank=2900 of ranks=109317rank=3000 of ranks=109317rank=3100 of ranks=109317rank=3200 of ranks=109317rank=3300 of ranks=109317rank=3400 of ranks=109317rank=3500 of ranks=109317rank=3600 of ranks=109317rank=3700 of ranks=109317rank=3800 of ranks=109317rank=3900 of ranks=109317rank=4000 of ranks=109317rank=4100 of ranks=109317rank=4200 of ranks=109317rank=4300 of ranks=109317rank=4400 of ranks=109317rank=4500 of ranks=109317rank=4600 of ranks=109317rank=4700 of ranks=109317rank=4800 of ranks=109317rank=4900 of ranks=109317rank=5000 of ranks=109317rank=5100 of ranks=109317rank=5200 of ranks=109317rank=5300 of ranks=109317rank=5400 of ranks=109317rank=5500 of ranks=109317rank=5600 of ranks=109317rank=5700 of ranks=109317rank=5800 of ranks=109317rank=5900 of ranks=109317rank=6000 of ranks=109317rank=6100 of ranks=109317rank=6200 of ranks=109317rank=6300 of ranks=109317rank=6400 of ranks=109317rank=6500 of ranks=109317rank=6600 of ranks=109317rank=6700 of ranks=109317rank=6800 of ranks=109317rank=6900 of ranks=109317rank=7000 of ranks=109317rank=7100 of ranks=109317rank=7200 of ranks=109317rank=7300 of ranks=109317rank=7400 of ranks=109317rank=7500 of ranks=109317rank=7600 of ranks=109317rank=7700 of ranks=109317rank=7800 of ranks=109317rank=7900 of ranks=109317rank=8000 of ranks=109317rank=8100 of ranks=109317rank=8200 of ranks=109317rank=8300 of ranks=109317rank=8400 of ranks=109317rank=8500 of ranks=109317rank=8600 of ranks=109317rank=8700 of ranks=109317rank=8800 of ranks=109317rank=8900 of ranks=109317rank=9000 of ranks=109317rank=9100 of ranks=109317rank=9200 of ranks=109317rank=9300 of ranks=109317rank=9400 of ranks=109317rank=9500 of ranks=109317rank=9600 of ranks=109317rank=9700 of ranks=109317rank=9800 of ranks=109317rank=9900 of ranks=109317rank=10000 of ranks=109317rank=10100 of ranks=109317rank=10200 of ranks=109317rank=10300 of ranks=109317rank=10400 of ranks=109317rank=10500 of ranks=109317rank=10600 of ranks=109317rank=10700 of ranks=109317rank=10800 of ranks=109317rank=10900 of ranks=109317rank=11000 of ranks=109317rank=11100 of ranks=109317rank=11200 of ranks=109317rank=11300 of ranks=109317rank=11400 of ranks=109317rank=11500 of ranks=109317rank=11600 of ranks=109317rank=11700 of ranks=109317rank=11800 of ranks=109317rank=11900 of ranks=109317rank=12000 of ranks=109317rank=12100 of ranks=109317rank=12200 of ranks=109317rank=12300 of ranks=109317rank=12400 of ranks=109317rank=12500 of ranks=109317rank=12600 of ranks=109317rank=12700 of ranks=109317rank=12800 of ranks=109317rank=12900 of ranks=109317rank=13000 of ranks=109317rank=13100 of ranks=109317rank=13200 of ranks=109317rank=13300 of ranks=109317rank=13400 of ranks=109317rank=13500 of ranks=109317rank=13600 of ranks=109317rank=13700 of ranks=109317rank=13800 of ranks=109317rank=13900 of ranks=109317rank=14000 of ranks=109317rank=14100 of ranks=109317rank=14200 of ranks=109317rank=14300 of ranks=109317rank=14400 of ranks=109317rank=14500 of ranks=109317rank=14600 of ranks=109317rank=14700 of ranks=109317rank=14800 of ranks=109317rank=14900 of ranks=109317rank=15000 of ranks=109317rank=15100 of ranks=109317rank=15200 of ranks=109317rank=15300 of ranks=109317rank=15400 of ranks=109317rank=15500 of ranks=109317rank=15600 of ranks=109317rank=15700 of ranks=109317rank=15800 of ranks=109317rank=15900 of ranks=109317rank=16000 of ranks=109317rank=16100 of ranks=109317rank=16200 of ranks=109317rank=16300 of ranks=109317rank=16400 of ranks=109317rank=16500 of ranks=109317rank=16600 of ranks=109317rank=16700 of ranks=109317rank=16800 of ranks=109317rank=16900 of ranks=109317rank=17000 of ranks=109317rank=17100 of ranks=109317rank=17200 of ranks=109317rank=17300 of ranks=109317rank=17400 of ranks=109317rank=17500 of ranks=109317rank=17600 of ranks=109317rank=17700 of ranks=109317rank=17800 of ranks=109317rank=17900 of ranks=109317rank=18000 of ranks=109317rank=18100 of ranks=109317rank=18200 of ranks=109317rank=18300 of ranks=109317rank=18400 of ranks=109317rank=18500 of ranks=109317rank=18600 of ranks=109317rank=18700 of ranks=109317rank=18800 of ranks=109317rank=18900 of ranks=109317rank=19000 of ranks=109317rank=19100 of ranks=109317rank=19200 of ranks=109317rank=19300 of ranks=109317rank=19400 of ranks=109317rank=19500 of ranks=109317rank=19600 of ranks=109317rank=19700 of ranks=109317rank=19800 of ranks=109317rank=19900 of ranks=109317rank=20000 of ranks=109317rank=20100 of ranks=109317rank=20200 of ranks=109317rank=20300 of ranks=109317rank=20400 of ranks=109317rank=20500 of ranks=109317rank=20600 of ranks=109317rank=20700 of ranks=109317rank=20800 of ranks=109317rank=20900 of ranks=109317rank=21000 of ranks=109317rank=21100 of ranks=109317rank=21200 of ranks=109317rank=21300 of ranks=109317rank=21400 of ranks=109317rank=21500 of ranks=109317rank=21600 of ranks=109317rank=21700 of ranks=109317rank=21800 of ranks=109317rank=21900 of ranks=109317rank=22000 of ranks=109317rank=22100 of ranks=109317rank=22200 of ranks=109317rank=22300 of ranks=109317rank=22400 of ranks=109317rank=22500 of ranks=109317rank=22600 of ranks=109317rank=22700 of ranks=109317rank=22800 of ranks=109317rank=22900 of ranks=109317rank=23000 of ranks=109317rank=23100 of ranks=109317rank=23200 of ranks=109317rank=23300 of ranks=109317rank=23400 of ranks=109317rank=23500 of ranks=109317rank=23600 of ranks=109317rank=23700 of ranks=109317rank=23800 of ranks=109317rank=23900 of ranks=109317rank=24000 of ranks=109317rank=24100 of ranks=109317rank=24200 of ranks=109317rank=24300 of ranks=109317rank=24400 of ranks=109317rank=24500 of ranks=109317rank=24600 of ranks=109317rank=24700 of ranks=109317rank=24800 of ranks=109317rank=24900 of ranks=109317rank=25000 of ranks=109317rank=25100 of ranks=109317rank=25200 of ranks=109317rank=25300 of ranks=109317rank=25400 of ranks=109317rank=25500 of ranks=109317rank=25600 of ranks=109317rank=25700 of ranks=109317rank=25800 of ranks=109317rank=25900 of ranks=109317rank=26000 of ranks=109317rank=26100 of ranks=109317rank=26200 of ranks=109317rank=26300 of ranks=109317rank=26400 of ranks=109317rank=26500 of ranks=109317rank=26600 of ranks=109317rank=26700 of ranks=109317rank=26800 of ranks=109317rank=26900 of ranks=109317rank=27000 of ranks=109317rank=27100 of ranks=109317rank=27200 of ranks=109317rank=27300 of ranks=109317rank=27400 of ranks=109317rank=27500 of ranks=109317rank=27600 of ranks=109317rank=27700 of ranks=109317rank=27800 of ranks=109317rank=27900 of ranks=109317rank=28000 of ranks=109317rank=28100 of ranks=109317rank=28200 of ranks=109317rank=28300 of ranks=109317rank=28400 of ranks=109317rank=28500 of ranks=109317rank=28600 of ranks=109317rank=28700 of ranks=109317rank=28800 of ranks=109317rank=28900 of ranks=109317rank=29000 of ranks=109317rank=29100 of ranks=109317rank=29200 of ranks=109317rank=29300 of ranks=109317rank=29400 of ranks=109317rank=29500 of ranks=109317rank=29600 of ranks=109317rank=29700 of ranks=109317rank=29800 of ranks=109317rank=29900 of ranks=109317rank=30000 of ranks=109317rank=30100 of ranks=109317rank=30200 of ranks=109317rank=30300 of ranks=109317rank=30400 of ranks=109317rank=30500 of ranks=109317rank=30600 of ranks=109317rank=30700 of ranks=109317rank=30800 of ranks=109317rank=30900 of ranks=109317rank=31000 of ranks=109317rank=31100 of ranks=109317rank=31200 of ranks=109317rank=31300 of ranks=109317rank=31400 of ranks=109317rank=31500 of ranks=109317rank=31600 of ranks=109317rank=31700 of ranks=109317rank=31800 of ranks=109317rank=31900 of ranks=109317rank=32000 of ranks=109317rank=32100 of ranks=109317rank=32200 of ranks=109317rank=32300 of ranks=109317rank=32400 of ranks=109317rank=32500 of ranks=109317rank=32600 of ranks=109317rank=32700 of ranks=109317rank=32800 of ranks=109317rank=32900 of ranks=109317rank=33000 of ranks=109317rank=33100 of ranks=109317rank=33200 of ranks=109317rank=33300 of ranks=109317rank=33400 of ranks=109317rank=33500 of ranks=109317rank=33600 of ranks=109317rank=33700 of ranks=109317rank=33800 of ranks=109317rank=33900 of ranks=109317rank=34000 of ranks=109317rank=34100 of ranks=109317rank=34200 of ranks=109317rank=34300 of ranks=109317rank=34400 of ranks=109317rank=34500 of ranks=109317rank=34600 of ranks=109317rank=34700 of ranks=109317rank=34800 of ranks=109317rank=34900 of ranks=109317rank=35000 of ranks=109317rank=35100 of ranks=109317rank=35200 of ranks=109317rank=35300 of ranks=109317rank=35400 of ranks=109317rank=35500 of ranks=109317rank=35600 of ranks=109317rank=35700 of ranks=109317rank=35800 of ranks=109317rank=35900 of ranks=109317rank=36000 of ranks=109317rank=36100 of ranks=109317rank=36200 of ranks=109317rank=36300 of ranks=109317rank=36400 of ranks=109317rank=36500 of ranks=109317rank=36600 of ranks=109317rank=36700 of ranks=109317rank=36800 of ranks=109317rank=36900 of ranks=109317rank=37000 of ranks=109317rank=37100 of ranks=109317rank=37200 of ranks=109317rank=37300 of ranks=109317rank=37400 of ranks=109317rank=37500 of ranks=109317rank=37600 of ranks=109317rank=37700 of ranks=109317rank=37800 of ranks=109317rank=37900 of ranks=109317rank=38000 of ranks=109317rank=38100 of ranks=109317rank=38200 of ranks=109317rank=38300 of ranks=109317rank=38400 of ranks=109317rank=38500 of ranks=109317rank=38600 of ranks=109317rank=38700 of ranks=109317rank=38800 of ranks=109317rank=38900 of ranks=109317rank=39000 of ranks=109317rank=39100 of ranks=109317rank=39200 of ranks=109317rank=39300 of ranks=109317rank=39400 of ranks=109317rank=39500 of ranks=109317rank=39600 of ranks=109317rank=39700 of ranks=109317rank=39800 of ranks=109317rank=39900 of ranks=109317rank=40000 of ranks=109317rank=40100 of ranks=109317rank=40200 of ranks=109317rank=40300 of ranks=109317rank=40400 of ranks=109317rank=40500 of ranks=109317rank=40600 of ranks=109317rank=40700 of ranks=109317rank=40800 of ranks=109317rank=40900 of ranks=109317rank=41000 of ranks=109317rank=41100 of ranks=109317rank=41200 of ranks=109317rank=41300 of ranks=109317rank=41400 of ranks=109317rank=41500 of ranks=109317rank=41600 of ranks=109317rank=41700 of ranks=109317rank=41800 of ranks=109317rank=41900 of ranks=109317rank=42000 of ranks=109317rank=42100 of ranks=109317rank=42200 of ranks=109317rank=42300 of ranks=109317rank=42400 of ranks=109317rank=42500 of ranks=109317rank=42600 of ranks=109317rank=42700 of ranks=109317rank=42800 of ranks=109317rank=42900 of ranks=109317rank=43000 of ranks=109317rank=43100 of ranks=109317rank=43200 of ranks=109317rank=43300 of ranks=109317rank=43400 of ranks=109317rank=43500 of ranks=109317rank=43600 of ranks=109317rank=43700 of ranks=109317rank=43800 of ranks=109317rank=43900 of ranks=109317rank=44000 of ranks=109317rank=44100 of ranks=109317rank=44200 of ranks=109317rank=44300 of ranks=109317rank=44400 of ranks=109317rank=44500 of ranks=109317rank=44600 of ranks=109317rank=44700 of ranks=109317rank=44800 of ranks=109317rank=44900 of ranks=109317rank=45000 of ranks=109317rank=45100 of ranks=109317rank=45200 of ranks=109317rank=45300 of ranks=109317rank=45400 of ranks=109317rank=45500 of ranks=109317rank=45600 of ranks=109317rank=45700 of ranks=109317rank=45800 of ranks=109317rank=45900 of ranks=109317rank=46000 of ranks=109317rank=46100 of ranks=109317rank=46200 of ranks=109317rank=46300 of ranks=109317rank=46400 of ranks=109317rank=46500 of ranks=109317rank=46600 of ranks=109317rank=46700 of ranks=109317rank=46800 of ranks=109317rank=46900 of ranks=109317rank=47000 of ranks=109317rank=47100 of ranks=109317rank=47200 of ranks=109317rank=47300 of ranks=109317rank=47400 of ranks=109317rank=47500 of ranks=109317rank=47600 of ranks=109317rank=47700 of ranks=109317rank=47800 of ranks=109317rank=47900 of ranks=109317rank=48000 of ranks=109317rank=48100 of ranks=109317rank=48200 of ranks=109317rank=48300 of ranks=109317rank=48400 of ranks=109317rank=48500 of ranks=109317rank=48600 of ranks=109317rank=48700 of ranks=109317rank=48800 of ranks=109317rank=48900 of ranks=109317rank=49000 of ranks=109317rank=49100 of ranks=109317rank=49200 of ranks=109317rank=49300 of ranks=109317rank=49400 of ranks=109317rank=49500 of ranks=109317rank=49600 of ranks=109317rank=49700 of ranks=109317rank=49800 of ranks=109317rank=49900 of ranks=109317rank=50000 of ranks=109317rank=50100 of ranks=109317rank=50200 of ranks=109317rank=50300 of ranks=109317rank=50400 of ranks=109317rank=50500 of ranks=109317rank=50600 of ranks=109317rank=50700 of ranks=109317rank=50800 of ranks=109317rank=50900 of ranks=109317rank=51000 of ranks=109317rank=51100 of ranks=109317rank=51200 of ranks=109317rank=51300 of ranks=109317rank=51400 of ranks=109317rank=51500 of ranks=109317rank=51600 of ranks=109317rank=51700 of ranks=109317rank=51800 of ranks=109317rank=51900 of ranks=109317rank=52000 of ranks=109317rank=52100 of ranks=109317rank=52200 of ranks=109317rank=52300 of ranks=109317rank=52400 of ranks=109317rank=52500 of ranks=109317rank=52600 of ranks=109317rank=52700 of ranks=109317rank=52800 of ranks=109317rank=52900 of ranks=109317rank=53000 of ranks=109317rank=53100 of ranks=109317rank=53200 of ranks=109317rank=53300 of ranks=109317rank=53400 of ranks=109317rank=53500 of ranks=109317rank=53600 of ranks=109317rank=53700 of ranks=109317rank=53800 of ranks=109317rank=53900 of ranks=109317rank=54000 of ranks=109317rank=54100 of ranks=109317rank=54200 of ranks=109317rank=54300 of ranks=109317rank=54400 of ranks=109317rank=54500 of ranks=109317rank=54600 of ranks=109317rank=54700 of ranks=109317rank=54800 of ranks=109317rank=54900 of ranks=109317rank=55000 of ranks=109317rank=55100 of ranks=109317rank=55200 of ranks=109317rank=55300 of ranks=109317rank=55400 of ranks=109317rank=55500 of ranks=109317rank=55600 of ranks=109317rank=55700 of ranks=109317rank=55800 of ranks=109317rank=55900 of ranks=109317rank=56000 of ranks=109317rank=56100 of ranks=109317rank=56200 of ranks=109317rank=56300 of ranks=109317rank=56400 of ranks=109317rank=56500 of ranks=109317rank=56600 of ranks=109317rank=56700 of ranks=109317rank=56800 of ranks=109317rank=56900 of ranks=109317rank=57000 of ranks=109317rank=57100 of ranks=109317rank=57200 of ranks=109317rank=57300 of ranks=109317rank=57400 of ranks=109317rank=57500 of ranks=109317rank=57600 of ranks=109317rank=57700 of ranks=109317rank=57800 of ranks=109317rank=57900 of ranks=109317rank=58000 of ranks=109317rank=58100 of ranks=109317rank=58200 of ranks=109317rank=58300 of ranks=109317rank=58400 of ranks=109317rank=58500 of ranks=109317rank=58600 of ranks=109317rank=58700 of ranks=109317rank=58800 of ranks=109317rank=58900 of ranks=109317rank=59000 of ranks=109317rank=59100 of ranks=109317rank=59200 of ranks=109317rank=59300 of ranks=109317rank=59400 of ranks=109317rank=59500 of ranks=109317rank=59600 of ranks=109317rank=59700 of ranks=109317rank=59800 of ranks=109317rank=59900 of ranks=109317rank=60000 of ranks=109317rank=60100 of ranks=109317rank=60200 of ranks=109317rank=60300 of ranks=109317rank=60400 of ranks=109317rank=60500 of ranks=109317rank=60600 of ranks=109317rank=60700 of ranks=109317rank=60800 of ranks=109317rank=60900 of ranks=109317rank=61000 of ranks=109317rank=61100 of ranks=109317rank=61200 of ranks=109317rank=61300 of ranks=109317rank=61400 of ranks=109317rank=61500 of ranks=109317rank=61600 of ranks=109317rank=61700 of ranks=109317rank=61800 of ranks=109317rank=61900 of ranks=109317rank=62000 of ranks=109317rank=62100 of ranks=109317rank=62200 of ranks=109317rank=62300 of ranks=109317rank=62400 of ranks=109317rank=62500 of ranks=109317rank=62600 of ranks=109317rank=62700 of ranks=109317rank=62800 of ranks=109317rank=62900 of ranks=109317rank=63000 of ranks=109317rank=63100 of ranks=109317rank=63200 of ranks=109317rank=63300 of ranks=109317rank=63400 of ranks=109317rank=63500 of ranks=109317rank=63600 of ranks=109317rank=63700 of ranks=109317rank=63800 of ranks=109317rank=63900 of ranks=109317rank=64000 of ranks=109317rank=64100 of ranks=109317rank=64200 of ranks=109317rank=64300 of ranks=109317rank=64400 of ranks=109317rank=64500 of ranks=109317rank=64600 of ranks=109317rank=64700 of ranks=109317rank=64800 of ranks=109317rank=64900 of ranks=109317rank=65000 of ranks=109317rank=65100 of ranks=109317rank=65200 of ranks=109317rank=65300 of ranks=109317rank=65400 of ranks=109317rank=65500 of ranks=109317rank=65600 of ranks=109317rank=65700 of ranks=109317rank=65800 of ranks=109317rank=65900 of ranks=109317rank=66000 of ranks=109317rank=66100 of ranks=109317rank=66200 of ranks=109317rank=66300 of ranks=109317rank=66400 of ranks=109317rank=66500 of ranks=109317rank=66600 of ranks=109317rank=66700 of ranks=109317rank=66800 of ranks=109317rank=66900 of ranks=109317rank=67000 of ranks=109317rank=67100 of ranks=109317rank=67200 of ranks=109317rank=67300 of ranks=109317rank=67400 of ranks=109317rank=67500 of ranks=109317rank=67600 of ranks=109317rank=67700 of ranks=109317rank=67800 of ranks=109317rank=67900 of ranks=109317rank=68000 of ranks=109317rank=68100 of ranks=109317rank=68200 of ranks=109317rank=68300 of ranks=109317rank=68400 of ranks=109317rank=68500 of ranks=109317rank=68600 of ranks=109317rank=68700 of ranks=109317rank=68800 of ranks=109317rank=68900 of ranks=109317rank=69000 of ranks=109317rank=69100 of ranks=109317rank=69200 of ranks=109317rank=69300 of ranks=109317rank=69400 of ranks=109317rank=69500 of ranks=109317rank=69600 of ranks=109317rank=69700 of ranks=109317rank=69800 of ranks=109317rank=69900 of ranks=109317rank=70000 of ranks=109317rank=70100 of ranks=109317rank=70200 of ranks=109317rank=70300 of ranks=109317rank=70400 of ranks=109317rank=70500 of ranks=109317rank=70600 of ranks=109317rank=70700 of ranks=109317rank=70800 of ranks=109317rank=70900 of ranks=109317rank=71000 of ranks=109317rank=71100 of ranks=109317rank=71200 of ranks=109317rank=71300 of ranks=109317rank=71400 of ranks=109317rank=71500 of ranks=109317rank=71600 of ranks=109317rank=71700 of ranks=109317rank=71800 of ranks=109317rank=71900 of ranks=109317rank=72000 of ranks=109317rank=72100 of ranks=109317rank=72200 of ranks=109317rank=72300 of ranks=109317rank=72400 of ranks=109317rank=72500 of ranks=109317rank=72600 of ranks=109317rank=72700 of ranks=109317rank=72800 of ranks=109317rank=72900 of ranks=109317rank=73000 of ranks=109317rank=73100 of ranks=109317rank=73200 of ranks=109317rank=73300 of ranks=109317rank=73400 of ranks=109317rank=73500 of ranks=109317rank=73600 of ranks=109317rank=73700 of ranks=109317rank=73800 of ranks=109317rank=73900 of ranks=109317rank=74000 of ranks=109317rank=74100 of ranks=109317rank=74200 of ranks=109317rank=74300 of ranks=109317rank=74400 of ranks=109317rank=74500 of ranks=109317rank=74600 of ranks=109317rank=74700 of ranks=109317rank=74800 of ranks=109317rank=74900 of ranks=109317rank=75000 of ranks=109317rank=75100 of ranks=109317rank=75200 of ranks=109317rank=75300 of ranks=109317rank=75400 of ranks=109317rank=75500 of ranks=109317rank=75600 of ranks=109317rank=75700 of ranks=109317rank=75800 of ranks=109317rank=75900 of ranks=109317rank=76000 of ranks=109317rank=76100 of ranks=109317rank=76200 of ranks=109317rank=76300 of ranks=109317rank=76400 of ranks=109317rank=76500 of ranks=109317rank=76600 of ranks=109317rank=76700 of ranks=109317rank=76800 of ranks=109317rank=76900 of ranks=109317rank=77000 of ranks=109317rank=77100 of ranks=109317rank=77200 of ranks=109317rank=77300 of ranks=109317rank=77400 of ranks=109317rank=77500 of ranks=109317rank=77600 of ranks=109317rank=77700 of ranks=109317rank=77800 of ranks=109317rank=77900 of ranks=109317rank=78000 of ranks=109317rank=78100 of ranks=109317rank=78200 of ranks=109317rank=78300 of ranks=109317rank=78400 of ranks=109317rank=78500 of ranks=109317rank=78600 of ranks=109317rank=78700 of ranks=109317rank=78800 of ranks=109317rank=78900 of ranks=109317rank=79000 of ranks=109317rank=79100 of ranks=109317rank=79200 of ranks=109317rank=79300 of ranks=109317rank=79400 of ranks=109317rank=79500 of ranks=109317rank=79600 of ranks=109317rank=79700 of ranks=109317rank=79800 of ranks=109317rank=79900 of ranks=109317rank=80000 of ranks=109317rank=80100 of ranks=109317rank=80200 of ranks=109317rank=80300 of ranks=109317rank=80400 of ranks=109317rank=80500 of ranks=109317rank=80600 of ranks=109317rank=80700 of ranks=109317rank=80800 of ranks=109317rank=80900 of ranks=109317rank=81000 of ranks=109317rank=81100 of ranks=109317rank=81200 of ranks=109317rank=81300 of ranks=109317rank=81400 of ranks=109317rank=81500 of ranks=109317rank=81600 of ranks=109317rank=81700 of ranks=109317rank=81800 of ranks=109317rank=81900 of ranks=109317rank=82000 of ranks=109317rank=82100 of ranks=109317rank=82200 of ranks=109317rank=82300 of ranks=109317rank=82400 of ranks=109317rank=82500 of ranks=109317rank=82600 of ranks=109317rank=82700 of ranks=109317rank=82800 of ranks=109317rank=82900 of ranks=109317rank=83000 of ranks=109317rank=83100 of ranks=109317rank=83200 of ranks=109317rank=83300 of ranks=109317rank=83400 of ranks=109317rank=83500 of ranks=109317rank=83600 of ranks=109317rank=83700 of ranks=109317rank=83800 of ranks=109317rank=83900 of ranks=109317rank=84000 of ranks=109317rank=84100 of ranks=109317rank=84200 of ranks=109317rank=84300 of ranks=109317rank=84400 of ranks=109317rank=84500 of ranks=109317rank=84600 of ranks=109317rank=84700 of ranks=109317rank=84800 of ranks=109317rank=84900 of ranks=109317rank=85000 of ranks=109317rank=85100 of ranks=109317rank=85200 of ranks=109317rank=85300 of ranks=109317rank=85400 of ranks=109317rank=85500 of ranks=109317rank=85600 of ranks=109317rank=85700 of ranks=109317rank=85800 of ranks=109317rank=85900 of ranks=109317rank=86000 of ranks=109317rank=86100 of ranks=109317rank=86200 of ranks=109317rank=86300 of ranks=109317rank=86400 of ranks=109317rank=86500 of ranks=109317rank=86600 of ranks=109317rank=86700 of ranks=109317rank=86800 of ranks=109317rank=86900 of ranks=109317rank=87000 of ranks=109317rank=87100 of ranks=109317rank=87200 of ranks=109317rank=87300 of ranks=109317rank=87400 of ranks=109317rank=87500 of ranks=109317rank=87600 of ranks=109317rank=87700 of ranks=109317rank=87800 of ranks=109317rank=87900 of ranks=109317rank=88000 of ranks=109317rank=88100 of ranks=109317rank=88200 of ranks=109317rank=88300 of ranks=109317rank=88400 of ranks=109317rank=88500 of ranks=109317rank=88600 of ranks=109317rank=88700 of ranks=109317rank=88800 of ranks=109317rank=88900 of ranks=109317rank=89000 of ranks=109317rank=89100 of ranks=109317rank=89200 of ranks=109317rank=89300 of ranks=109317rank=89400 of ranks=109317rank=89500 of ranks=109317rank=89600 of ranks=109317rank=89700 of ranks=109317rank=89800 of ranks=109317rank=89900 of ranks=109317rank=90000 of ranks=109317rank=90100 of ranks=109317rank=90200 of ranks=109317rank=90300 of ranks=109317rank=90400 of ranks=109317rank=90500 of ranks=109317rank=90600 of ranks=109317rank=90700 of ranks=109317rank=90800 of ranks=109317rank=90900 of ranks=109317rank=91000 of ranks=109317rank=91100 of ranks=109317rank=91200 of ranks=109317rank=91300 of ranks=109317rank=91400 of ranks=109317rank=91500 of ranks=109317rank=91600 of ranks=109317rank=91700 of ranks=109317rank=91800 of ranks=109317rank=91900 of ranks=109317rank=92000 of ranks=109317rank=92100 of ranks=109317rank=92200 of ranks=109317rank=92300 of ranks=109317rank=92400 of ranks=109317rank=92500 of ranks=109317rank=92600 of ranks=109317rank=92700 of ranks=109317rank=92800 of ranks=109317rank=92900 of ranks=109317rank=93000 of ranks=109317rank=93100 of ranks=109317rank=93200 of ranks=109317rank=93300 of ranks=109317rank=93400 of ranks=109317rank=93500 of ranks=109317rank=93600 of ranks=109317rank=93700 of ranks=109317rank=93800 of ranks=109317rank=93900 of ranks=109317rank=94000 of ranks=109317rank=94100 of ranks=109317rank=94200 of ranks=109317rank=94300 of ranks=109317rank=94400 of ranks=109317rank=94500 of ranks=109317rank=94600 of ranks=109317rank=94700 of ranks=109317rank=94800 of ranks=109317rank=94900 of ranks=109317rank=95000 of ranks=109317rank=95100 of ranks=109317rank=95200 of ranks=109317rank=95300 of ranks=109317rank=95400 of ranks=109317rank=95500 of ranks=109317rank=95600 of ranks=109317rank=95700 of ranks=109317rank=95800 of ranks=109317rank=95900 of ranks=109317rank=96000 of ranks=109317rank=96100 of ranks=109317rank=96200 of ranks=109317rank=96300 of ranks=109317rank=96400 of ranks=109317rank=96500 of ranks=109317rank=96600 of ranks=109317rank=96700 of ranks=109317rank=96800 of ranks=109317rank=96900 of ranks=109317rank=97000 of ranks=109317rank=97100 of ranks=109317rank=97200 of ranks=109317rank=97300 of ranks=109317rank=97400 of ranks=109317rank=97500 of ranks=109317rank=97600 of ranks=109317rank=97700 of ranks=109317rank=97800 of ranks=109317rank=97900 of ranks=109317rank=98000 of ranks=109317rank=98100 of ranks=109317rank=98200 of ranks=109317rank=98300 of ranks=109317rank=98400 of ranks=109317rank=98500 of ranks=109317rank=98600 of ranks=109317rank=98700 of ranks=109317rank=98800 of ranks=109317rank=98900 of ranks=109317rank=99000 of ranks=109317rank=99100 of ranks=109317rank=99200 of ranks=109317rank=99300 of ranks=109317rank=99400 of ranks=109317rank=99500 of ranks=109317rank=99600 of ranks=109317rank=99700 of ranks=109317rank=99800 of ranks=109317rank=99900 of ranks=109317rank=100000 of ranks=109317rank=100100 of ranks=109317rank=100200 of ranks=109317rank=100300 of ranks=109317rank=100400 of ranks=109317rank=100500 of ranks=109317rank=100600 of ranks=109317rank=100700 of ranks=109317rank=100800 of ranks=109317rank=100900 of ranks=109317rank=101000 of ranks=109317rank=101100 of ranks=109317rank=101200 of ranks=109317rank=101300 of ranks=109317rank=101400 of ranks=109317rank=101500 of ranks=109317rank=101600 of ranks=109317rank=101700 of ranks=109317rank=101800 of ranks=109317rank=101900 of ranks=109317rank=102000 of ranks=109317rank=102100 of ranks=109317rank=102200 of ranks=109317rank=102300 of ranks=109317rank=102400 of ranks=109317rank=102500 of ranks=109317rank=102600 of ranks=109317rank=102700 of ranks=109317rank=102800 of ranks=109317rank=102900 of ranks=109317rank=103000 of ranks=109317rank=103100 of ranks=109317rank=103200 of ranks=109317rank=103300 of ranks=109317rank=103400 of ranks=109317rank=103500 of ranks=109317rank=103600 of ranks=109317rank=103700 of ranks=109317rank=103800 of ranks=109317rank=103900 of ranks=109317rank=104000 of ranks=109317rank=104100 of ranks=109317rank=104200 of ranks=109317rank=104300 of ranks=109317rank=104400 of ranks=109317rank=104500 of ranks=109317rank=104600 of ranks=109317rank=104700 of ranks=109317rank=104800 of ranks=109317rank=104900 of ranks=109317rank=105000 of ranks=109317rank=105100 of ranks=109317rank=105200 of ranks=109317rank=105300 of ranks=109317rank=105400 of ranks=109317rank=105500 of ranks=109317rank=105600 of ranks=109317rank=105700 of ranks=109317rank=105800 of ranks=109317rank=105900 of ranks=109317rank=106000 of ranks=109317rank=106100 of ranks=109317rank=106200 of ranks=109317rank=106300 of ranks=109317rank=106400 of ranks=109317rank=106500 of ranks=109317rank=106600 of ranks=109317rank=106700 of ranks=109317rank=106800 of ranks=109317rank=106900 of ranks=109317rank=107000 of ranks=109317rank=107100 of ranks=109317rank=107200 of ranks=109317rank=107300 of ranks=109317rank=107400 of ranks=109317rank=107500 of ranks=109317rank=107600 of ranks=109317rank=107700 of ranks=109317rank=107800 of ranks=109317rank=107900 of ranks=109317rank=108000 of ranks=109317rank=108100 of ranks=109317rank=108200 of ranks=109317rank=108300 of ranks=109317rank=108400 of ranks=109317rank=108500 of ranks=109317rank=108600 of ranks=109317rank=108700 of ranks=109317rank=108800 of ranks=109317rank=108900 of ranks=109317rank=109000 of ranks=109317rank=109100 of ranks=109317rank=109200 of ranks=109317rank=109300 of ranks=109317

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              95.4739    484   3030     14    498           73.3291
   1 car                    98.5004  49997  30976    319  50316           80.7241
   2 truck                  97.3071   1812   7413     13   1825           71.2238
   3 bus                    94.6270    359   3329      7    366           68.2324
   4 pedestrian             95.4578   4129   7788    130   4259           72.2283

for conf_thresh=0.25, precision=0.90, recall=0.96, F1 score=0.93
for conf_thresh=0.25, TP=54912, FP=6166, FN=2352, average IoU=79.64%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=96.27%
Total detection time: 146 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
6279: loss=2.867, avg loss=2.910, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=827.4 milliseconds, train=2.1 seconds, 401856 images, time remaining=102.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6280: loss=2.471, avg loss=2.866, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 401920 images, time remaining=102.3 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14647a000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6281: loss=3.181, avg loss=2.898, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.4 seconds, train=1.2 seconds, 401984 images, time remaining=102.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6282: loss=3.244, avg loss=2.932, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.5 seconds, train=1.2 seconds, 402048 images, time remaining=102.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6283: loss=3.684, avg loss=3.008, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.5 seconds, train=1.2 seconds, 402112 images, time remaining=102.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6284: loss=2.405, avg loss=2.947, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 402176 images, time remaining=102.1 minutes
6285: loss=3.518, avg loss=3.004, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=841.1 milliseconds, train=1.2 seconds, 402240 images, time remaining=102 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6286: loss=3.181, avg loss=3.022, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=1.2 seconds, 402304 images, time remaining=101.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6287: loss=2.943, avg loss=3.014, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.3 seconds, train=1.2 seconds, 402368 images, time remaining=101.8 minutes
6288: loss=2.910, avg loss=3.004, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=652.6 milliseconds, train=1.2 seconds, 402432 images, time remaining=101.8 minutes
6289: loss=2.682, avg loss=2.972, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=800.3 milliseconds, train=1.2 seconds, 402496 images, time remaining=101.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6290: loss=2.683, avg loss=2.943, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.4 seconds, train=1.2 seconds, 402560 images, time remaining=101.6 minutes
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6291: loss=2.969, avg loss=2.945, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.7 seconds, train=3.3 seconds, 402624 images, time remaining=101.6 minutes
6292: loss=3.482, avg loss=2.999, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.0 seconds, train=3.3 seconds, 402688 images, time remaining=101.5 minutes
6293: loss=3.551, avg loss=3.054, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 402752 images, time remaining=101.5 minutes
6294: loss=2.961, avg loss=3.045, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.5 seconds, train=3.3 seconds, 402816 images, time remaining=101.4 minutes
6295: loss=2.626, avg loss=3.003, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=893.7 milliseconds, train=3.3 seconds, 402880 images, time remaining=101.3 minutes
6296: loss=2.420, avg loss=2.945, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.9 seconds, train=3.3 seconds, 402944 images, time remaining=101.3 minutes
6297: loss=2.422, avg loss=2.892, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=3.3 seconds, 403008 images, time remaining=101.2 minutes
6298: loss=2.180, avg loss=2.821, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.6 seconds, train=3.2 seconds, 403072 images, time remaining=101.2 minutes
6299: loss=3.535, avg loss=2.893, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.3 seconds, train=3.3 seconds, 403136 images, time remaining=101.1 minutes
6300: loss=3.539, avg loss=2.957, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=3.3 seconds, 403200 images, time remaining=101 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x14602a000000
6301: loss=3.091, avg loss=2.971, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=517.2 milliseconds, train=1.4 seconds, 403264 images, time remaining=101 minutes
6302: loss=3.156, avg loss=2.989, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=672.5 milliseconds, train=1.4 seconds, 403328 images, time remaining=100.9 minutes
6303: loss=2.316, avg loss=2.922, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=933.9 milliseconds, train=1.4 seconds, 403392 images, time remaining=100.8 minutes
6304: loss=2.772, avg loss=2.907, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=1.5 seconds, 403456 images, time remaining=100.8 minutes
6305: loss=3.067, avg loss=2.923, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=437.9 milliseconds, train=1.5 seconds, 403520 images, time remaining=100.7 minutes
6306: loss=2.720, avg loss=2.903, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=636.7 milliseconds, train=1.4 seconds, 403584 images, time remaining=100.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6307: loss=2.615, avg loss=2.874, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.4 seconds, train=1.4 seconds, 403648 images, time remaining=100.6 minutes
6308: loss=2.685, avg loss=2.855, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 403712 images, time remaining=100.5 minutes
6309: loss=2.832, avg loss=2.853, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.1 seconds, train=1.4 seconds, 403776 images, time remaining=100.4 minutes
6310: loss=2.910, avg loss=2.858, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=1.4 seconds, 403840 images, time remaining=100.3 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6311: loss=4.374, avg loss=3.010, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.3 seconds, train=5.1 seconds, 403904 images, time remaining=100.3 minutes
6312: loss=3.895, avg loss=3.098, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.1 seconds, train=5.1 seconds, 403968 images, time remaining=100.3 minutes
6313: loss=3.851, avg loss=3.174, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.9 seconds, train=5.0 seconds, 404032 images, time remaining=100.2 minutes
6314: loss=3.889, avg loss=3.245, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=924.2 milliseconds, train=5.0 seconds, 404096 images, time remaining=100.2 minutes
6315: loss=2.890, avg loss=3.210, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=724.2 milliseconds, train=5.0 seconds, 404160 images, time remaining=100.1 minutes
6316: loss=3.709, avg loss=3.260, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=906.4 milliseconds, train=5.1 seconds, 404224 images, time remaining=100.1 minutes
6317: loss=5.606, avg loss=3.494, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.6 seconds, train=5.1 seconds, 404288 images, time remaining=100 minutes
6318: loss=3.768, avg loss=3.522, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.0 seconds, train=5.1 seconds, 404352 images, time remaining=100 minutes
6319: loss=3.905, avg loss=3.560, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=765.3 milliseconds, train=5.1 seconds, 404416 images, time remaining=99.9 minutes
6320: loss=3.455, avg loss=3.550, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=962.2 milliseconds, train=5.0 seconds, 404480 images, time remaining=99.8 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6321: loss=3.052, avg loss=3.500, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=938.2 milliseconds, train=3.6 seconds, 404544 images, time remaining=99.8 minutes
6322: loss=3.436, avg loss=3.493, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=460.0 milliseconds, train=3.6 seconds, 404608 images, time remaining=99.7 minutes
6323: loss=2.875, avg loss=3.432, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=668.4 milliseconds, train=3.6 seconds, 404672 images, time remaining=99.7 minutes
6324: loss=3.528, avg loss=3.441, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.9 seconds, train=3.6 seconds, 404736 images, time remaining=99.6 minutes
6325: loss=2.746, avg loss=3.372, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=963.7 milliseconds, train=3.6 seconds, 404800 images, time remaining=99.6 minutes
6326: loss=3.017, avg loss=3.336, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.7 seconds, train=3.6 seconds, 404864 images, time remaining=99.5 minutes
6327: loss=3.273, avg loss=3.330, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 404928 images, time remaining=99.4 minutes
6328: loss=2.730, avg loss=3.270, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.3 seconds, train=3.6 seconds, 404992 images, time remaining=99.4 minutes
6329: loss=3.081, avg loss=3.251, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.4 seconds, train=3.6 seconds, 405056 images, time remaining=99.3 minutes
6330: loss=2.929, avg loss=3.219, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=3.6 seconds, 405120 images, time remaining=99.3 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6331: loss=2.624, avg loss=3.159, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.6 seconds, train=2.1 seconds, 405184 images, time remaining=99.2 minutes
6332: loss=2.471, avg loss=3.091, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=2.1 seconds, 405248 images, time remaining=99.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6333: loss=3.187, avg loss=3.100, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.3 seconds, train=2.1 seconds, 405312 images, time remaining=99.1 minutes
6334: loss=2.320, avg loss=3.022, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=771.0 milliseconds, train=2.1 seconds, 405376 images, time remaining=99 minutes
6335: loss=3.442, avg loss=3.064, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=2.1 seconds, 405440 images, time remaining=98.9 minutes
6336: loss=2.354, avg loss=2.993, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=749.6 milliseconds, train=2.1 seconds, 405504 images, time remaining=98.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6337: loss=2.770, avg loss=2.971, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=2.1 seconds, 405568 images, time remaining=98.8 minutes
6338: loss=2.894, avg loss=2.963, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.3 seconds, train=2.1 seconds, 405632 images, time remaining=98.8 minutes
6339: loss=2.288, avg loss=2.896, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=601.7 milliseconds, train=2.1 seconds, 405696 images, time remaining=98.7 minutes
6340: loss=2.778, avg loss=2.884, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.8 seconds, train=2.1 seconds, 405760 images, time remaining=98.6 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6341: loss=2.842, avg loss=2.880, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.7 seconds, train=3.4 seconds, 405824 images, time remaining=98.6 minutes
6342: loss=2.638, avg loss=2.855, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 405888 images, time remaining=98.5 minutes
6343: loss=2.703, avg loss=2.840, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 405952 images, time remaining=98.4 minutes
6344: loss=3.039, avg loss=2.860, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.1 seconds, train=3.4 seconds, 406016 images, time remaining=98.4 minutes
6345: loss=2.988, avg loss=2.873, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=780.1 milliseconds, train=3.4 seconds, 406080 images, time remaining=98.3 minutes
6346: loss=2.683, avg loss=2.854, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.8 seconds, train=3.4 seconds, 406144 images, time remaining=98.3 minutes
6347: loss=3.394, avg loss=2.908, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=592.5 milliseconds, train=3.4 seconds, 406208 images, time remaining=98.2 minutes
6348: loss=3.124, avg loss=2.930, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=735.7 milliseconds, train=3.4 seconds, 406272 images, time remaining=98.1 minutes
6349: loss=2.786, avg loss=2.915, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=3.4 seconds, 406336 images, time remaining=98.1 minutes
6350: loss=2.576, avg loss=2.881, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=710.9 milliseconds, train=3.4 seconds, 406400 images, time remaining=98 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6351: loss=3.852, avg loss=2.978, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=695.2 milliseconds, train=4.7 seconds, 406464 images, time remaining=98 minutes
6352: loss=3.004, avg loss=2.981, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=841.8 milliseconds, train=4.6 seconds, 406528 images, time remaining=97.9 minutes
6353: loss=3.253, avg loss=3.008, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=993.9 milliseconds, train=4.6 seconds, 406592 images, time remaining=97.9 minutes
6354: loss=3.445, avg loss=3.052, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=799.1 milliseconds, train=4.6 seconds, 406656 images, time remaining=97.8 minutes
6355: loss=2.790, avg loss=3.026, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 406720 images, time remaining=97.8 minutes
6356: loss=3.231, avg loss=3.046, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=4.6 seconds, 406784 images, time remaining=97.7 minutes
6357: loss=2.929, avg loss=3.034, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 406848 images, time remaining=97.7 minutes
6358: loss=2.592, avg loss=2.990, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.6 seconds, train=4.6 seconds, 406912 images, time remaining=97.6 minutes
6359: loss=3.817, avg loss=3.073, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.5 seconds, train=4.6 seconds, 406976 images, time remaining=97.6 minutes
6360: loss=3.059, avg loss=3.072, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.9 seconds, train=4.6 seconds, 407040 images, time remaining=97.5 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6361: loss=3.438, avg loss=3.108, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.0 seconds, train=4.6 seconds, 407104 images, time remaining=97.4 minutes
6362: loss=3.522, avg loss=3.150, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 407168 images, time remaining=97.4 minutes
6363: loss=3.742, avg loss=3.209, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.6 seconds, train=4.6 seconds, 407232 images, time remaining=97.3 minutes
6364: loss=3.163, avg loss=3.204, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.6 seconds, train=4.6 seconds, 407296 images, time remaining=97.3 minutes
6365: loss=2.932, avg loss=3.177, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.5 seconds, train=4.6 seconds, 407360 images, time remaining=97.2 minutes
6366: loss=3.760, avg loss=3.235, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.1 seconds, train=4.6 seconds, 407424 images, time remaining=97.2 minutes
6367: loss=3.393, avg loss=3.251, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=702.0 milliseconds, train=4.6 seconds, 407488 images, time remaining=97.1 minutes
6368: loss=3.326, avg loss=3.259, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=4.6 seconds, 407552 images, time remaining=97.1 minutes
6369: loss=3.182, avg loss=3.251, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=956.6 milliseconds, train=4.6 seconds, 407616 images, time remaining=97 minutes
6370: loss=2.854, avg loss=3.211, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.1 seconds, train=4.6 seconds, 407680 images, time remaining=96.9 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6371: loss=2.953, avg loss=3.185, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.5 seconds, train=3.9 seconds, 407744 images, time remaining=96.9 minutes
6372: loss=3.368, avg loss=3.204, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.4 seconds, train=3.9 seconds, 407808 images, time remaining=96.8 minutes
6373: loss=2.986, avg loss=3.182, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.0 seconds, train=3.9 seconds, 407872 images, time remaining=96.8 minutes
6374: loss=3.213, avg loss=3.185, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.6 seconds, train=3.9 seconds, 407936 images, time remaining=96.7 minutes
6375: loss=2.286, avg loss=3.095, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=916.3 milliseconds, train=3.9 seconds, 408000 images, time remaining=96.7 minutes
6376: loss=2.663, avg loss=3.052, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.5 seconds, train=3.9 seconds, 408064 images, time remaining=96.6 minutes
6377: loss=3.108, avg loss=3.058, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=862.0 milliseconds, train=3.9 seconds, 408128 images, time remaining=96.6 minutes
6378: loss=3.085, avg loss=3.060, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=3.9 seconds, 408192 images, time remaining=96.5 minutes
6379: loss=2.935, avg loss=3.048, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.1 seconds, train=3.9 seconds, 408256 images, time remaining=96.5 minutes
6380: loss=2.818, avg loss=3.025, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=899.2 milliseconds, train=3.9 seconds, 408320 images, time remaining=96.4 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446d6000000
6381: loss=3.264, avg loss=3.049, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.7 seconds, train=5.1 seconds, 408384 images, time remaining=96.3 minutes
6382: loss=3.074, avg loss=3.051, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=819.1 milliseconds, train=5.1 seconds, 408448 images, time remaining=96.3 minutes
6383: loss=3.479, avg loss=3.094, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=791.8 milliseconds, train=5.1 seconds, 408512 images, time remaining=96.2 minutes
6384: loss=3.322, avg loss=3.117, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.3 seconds, train=5.1 seconds, 408576 images, time remaining=96.2 minutes
6385: loss=3.086, avg loss=3.114, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.2 seconds, train=5.1 seconds, 408640 images, time remaining=96.1 minutes
6386: loss=3.922, avg loss=3.194, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.7 seconds, train=5.1 seconds, 408704 images, time remaining=96.1 minutes
6387: loss=3.779, avg loss=3.253, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.8 seconds, train=5.1 seconds, 408768 images, time remaining=96 minutes
6388: loss=3.476, avg loss=3.275, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 408832 images, time remaining=96 minutes
6389: loss=3.087, avg loss=3.256, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.4 seconds, train=5.1 seconds, 408896 images, time remaining=95.9 minutes
6390: loss=5.060, avg loss=3.437, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.1 seconds, train=5.1 seconds, 408960 images, time remaining=95.9 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446d6000000
6391: loss=3.476, avg loss=3.441, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.7 seconds, train=5.1 seconds, 409024 images, time remaining=95.8 minutes
6392: loss=2.985, avg loss=3.395, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=1.2 seconds, train=5.0 seconds, 409088 images, time remaining=95.8 minutes
6393: loss=3.502, avg loss=3.406, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=613.4 milliseconds, train=5.1 seconds, 409152 images, time remaining=95.7 minutes
6394: loss=2.795, avg loss=3.345, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=758.0 milliseconds, train=5.1 seconds, 409216 images, time remaining=95.7 minutes
6395: loss=4.354, avg loss=3.446, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=759.6 milliseconds, train=5.1 seconds, 409280 images, time remaining=95.6 minutes
6396: loss=3.490, avg loss=3.450, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.9 seconds, train=5.1 seconds, 409344 images, time remaining=95.6 minutes
6397: loss=4.718, avg loss=3.577, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=3.2 seconds, train=5.1 seconds, 409408 images, time remaining=95.5 minutes
6398: loss=3.400, avg loss=3.559, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.4 seconds, train=5.1 seconds, 409472 images, time remaining=95.5 minutes
6399: loss=4.097, avg loss=3.613, last=96.27%, best=96.27%, next=6684, rate=0.00130000, load 64=2.4 seconds, train=5.0 seconds, 409536 images, time remaining=95.4 minutes
6400: loss=3.426, avg loss=3.594, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.1 seconds, train=5.0 seconds, 409600 images, time remaining=95.3 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x14515a000000
6401: loss=3.355, avg loss=3.570, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=837.5 milliseconds, train=1.1 seconds, 409664 images, time remaining=95.3 minutes
6402: loss=3.343, avg loss=3.548, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=698.6 milliseconds, train=1.1 seconds, 409728 images, time remaining=95.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6403: loss=3.982, avg loss=3.591, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.0 seconds, train=1.1 seconds, 409792 images, time remaining=95.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6404: loss=4.184, avg loss=3.650, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=1.1 seconds, 409856 images, time remaining=95.1 minutes
6405: loss=3.883, avg loss=3.674, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=875.5 milliseconds, train=1.1 seconds, 409920 images, time remaining=95 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6406: loss=3.647, avg loss=3.671, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.1 seconds, 409984 images, time remaining=94.9 minutes
6407: loss=4.266, avg loss=3.730, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=685.8 milliseconds, train=1.1 seconds, 410048 images, time remaining=94.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6408: loss=3.960, avg loss=3.753, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.2 seconds, 410112 images, time remaining=94.8 minutes
6409: loss=3.732, avg loss=3.751, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=843.8 milliseconds, train=1.2 seconds, 410176 images, time remaining=94.8 minutes
6410: loss=3.342, avg loss=3.710, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=742.2 milliseconds, train=1.1 seconds, 410240 images, time remaining=94.7 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446d6000000
6411: loss=2.966, avg loss=3.636, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.9 seconds, train=3.9 seconds, 410304 images, time remaining=94.6 minutes
6412: loss=2.688, avg loss=3.541, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=802.1 milliseconds, train=3.8 seconds, 410368 images, time remaining=94.6 minutes
6413: loss=3.478, avg loss=3.535, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=577.6 milliseconds, train=3.9 seconds, 410432 images, time remaining=94.5 minutes
6414: loss=3.212, avg loss=3.502, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=3.9 seconds, 410496 images, time remaining=94.4 minutes
6415: loss=3.366, avg loss=3.489, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=690.9 milliseconds, train=3.9 seconds, 410560 images, time remaining=94.4 minutes
6416: loss=3.033, avg loss=3.443, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=3.9 seconds, 410624 images, time remaining=94.3 minutes
6417: loss=3.161, avg loss=3.415, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=538.9 milliseconds, train=3.9 seconds, 410688 images, time remaining=94.3 minutes
6418: loss=3.050, avg loss=3.379, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=534.8 milliseconds, train=3.9 seconds, 410752 images, time remaining=94.2 minutes
6419: loss=3.404, avg loss=3.381, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.0 seconds, train=3.9 seconds, 410816 images, time remaining=94.2 minutes
6420: loss=2.684, avg loss=3.311, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=617.1 milliseconds, train=3.9 seconds, 410880 images, time remaining=94.1 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x1446d6000000
6421: loss=3.625, avg loss=3.343, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=614.1 milliseconds, train=4.6 seconds, 410944 images, time remaining=94 minutes
6422: loss=3.172, avg loss=3.326, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.0 seconds, train=4.6 seconds, 411008 images, time remaining=94 minutes
6423: loss=3.012, avg loss=3.294, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.0 seconds, train=4.6 seconds, 411072 images, time remaining=93.9 minutes
6424: loss=3.196, avg loss=3.285, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=854.4 milliseconds, train=4.6 seconds, 411136 images, time remaining=93.9 minutes
6425: loss=3.080, avg loss=3.264, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.0 seconds, train=4.6 seconds, 411200 images, time remaining=93.8 minutes
6426: loss=2.771, avg loss=3.215, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=4.6 seconds, 411264 images, time remaining=93.8 minutes
6427: loss=3.116, avg loss=3.205, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.2 seconds, train=4.6 seconds, 411328 images, time remaining=93.7 minutes
6428: loss=3.118, avg loss=3.196, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.8 seconds, train=4.7 seconds, 411392 images, time remaining=93.7 minutes
6429: loss=3.425, avg loss=3.219, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=4.6 seconds, 411456 images, time remaining=93.6 minutes
6430: loss=2.969, avg loss=3.194, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=892.3 milliseconds, train=4.6 seconds, 411520 images, time remaining=93.6 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146348000000
6431: loss=2.562, avg loss=3.131, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=1.4 seconds, 411584 images, time remaining=93.5 minutes
6432: loss=2.526, avg loss=3.070, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=973.2 milliseconds, train=1.4 seconds, 411648 images, time remaining=93.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6433: loss=3.371, avg loss=3.100, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.5 seconds, train=1.4 seconds, 411712 images, time remaining=93.4 minutes
6434: loss=3.092, avg loss=3.100, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=846.7 milliseconds, train=1.4 seconds, 411776 images, time remaining=93.3 minutes
6435: loss=3.242, avg loss=3.114, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=791.4 milliseconds, train=1.4 seconds, 411840 images, time remaining=93.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6436: loss=2.805, avg loss=3.083, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.0 seconds, train=1.4 seconds, 411904 images, time remaining=93.2 minutes
6437: loss=2.909, avg loss=3.066, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=772.8 milliseconds, train=1.4 seconds, 411968 images, time remaining=93.1 minutes
6438: loss=3.188, avg loss=3.078, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=1.4 seconds, 412032 images, time remaining=93 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6439: loss=2.741, avg loss=3.044, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.7 seconds, train=1.4 seconds, 412096 images, time remaining=93 minutes
6440: loss=2.529, avg loss=2.993, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=763.6 milliseconds, train=1.5 seconds, 412160 images, time remaining=92.9 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1463f4000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6441: loss=2.783, avg loss=2.972, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.6 seconds, train=1.7 seconds, 412224 images, time remaining=92.8 minutes
6442: loss=2.740, avg loss=2.948, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=1.6 seconds, 412288 images, time remaining=92.8 minutes
6443: loss=2.123, avg loss=2.866, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.6 seconds, 412352 images, time remaining=92.7 minutes
6444: loss=2.242, avg loss=2.803, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=752.8 milliseconds, train=1.6 seconds, 412416 images, time remaining=92.6 minutes
6445: loss=2.525, avg loss=2.776, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=1.7 seconds, 412480 images, time remaining=92.6 minutes
6446: loss=2.577, avg loss=2.756, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=477.9 milliseconds, train=1.6 seconds, 412544 images, time remaining=92.5 minutes
6447: loss=2.095, avg loss=2.690, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=597.4 milliseconds, train=1.6 seconds, 412608 images, time remaining=92.4 minutes
6448: loss=2.047, avg loss=2.625, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=739.4 milliseconds, train=1.6 seconds, 412672 images, time remaining=92.4 minutes
6449: loss=1.946, avg loss=2.557, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=910.9 milliseconds, train=1.6 seconds, 412736 images, time remaining=92.3 minutes
6450: loss=2.312, avg loss=2.533, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=1.6 seconds, 412800 images, time remaining=92.2 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6451: loss=2.585, avg loss=2.538, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=952.9 milliseconds, train=4.5 seconds, 412864 images, time remaining=92.2 minutes
6452: loss=2.398, avg loss=2.524, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=4.5 seconds, 412928 images, time remaining=92.1 minutes
6453: loss=3.596, avg loss=2.631, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=689.0 milliseconds, train=4.5 seconds, 412992 images, time remaining=92.1 minutes
6454: loss=3.004, avg loss=2.669, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=4.5 seconds, 413056 images, time remaining=92 minutes
6455: loss=2.232, avg loss=2.625, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=974.6 milliseconds, train=4.5 seconds, 413120 images, time remaining=92 minutes
6456: loss=2.784, avg loss=2.641, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=678.9 milliseconds, train=4.5 seconds, 413184 images, time remaining=91.9 minutes
6457: loss=2.331, avg loss=2.610, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=809.0 milliseconds, train=4.5 seconds, 413248 images, time remaining=91.8 minutes
6458: loss=3.072, avg loss=2.656, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=4.5 seconds, 413312 images, time remaining=91.8 minutes
6459: loss=2.974, avg loss=2.688, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=4.5 seconds, 413376 images, time remaining=91.7 minutes
6460: loss=2.545, avg loss=2.674, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=424.1 milliseconds, train=4.5 seconds, 413440 images, time remaining=91.7 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145b50000000
6461: loss=2.535, avg loss=2.660, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=579.0 milliseconds, train=1.5 seconds, 413504 images, time remaining=91.6 minutes
6462: loss=2.319, avg loss=2.626, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=457.1 milliseconds, train=1.5 seconds, 413568 images, time remaining=91.5 minutes
6463: loss=2.408, avg loss=2.604, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=604.2 milliseconds, train=1.4 seconds, 413632 images, time remaining=91.5 minutes
6464: loss=2.194, avg loss=2.563, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=666.5 milliseconds, train=1.4 seconds, 413696 images, time remaining=91.4 minutes
6465: loss=2.528, avg loss=2.559, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=578.6 milliseconds, train=1.5 seconds, 413760 images, time remaining=91.3 minutes
6466: loss=2.491, avg loss=2.553, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=382.5 milliseconds, train=1.5 seconds, 413824 images, time remaining=91.3 minutes
6467: loss=2.916, avg loss=2.589, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=966.2 milliseconds, train=1.5 seconds, 413888 images, time remaining=91.2 minutes
6468: loss=2.585, avg loss=2.589, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=803.2 milliseconds, train=1.5 seconds, 413952 images, time remaining=91.2 minutes
6469: loss=3.156, avg loss=2.645, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=414.5 milliseconds, train=1.5 seconds, 414016 images, time remaining=91.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6470: loss=2.891, avg loss=2.670, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.7 seconds, train=1.5 seconds, 414080 images, time remaining=91 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6471: loss=2.729, avg loss=2.676, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.4 seconds, train=4.5 seconds, 414144 images, time remaining=91 minutes
6472: loss=2.232, avg loss=2.631, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=845.5 milliseconds, train=4.5 seconds, 414208 images, time remaining=90.9 minutes
6473: loss=3.192, avg loss=2.688, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=807.2 milliseconds, train=4.5 seconds, 414272 images, time remaining=90.9 minutes
6474: loss=3.211, avg loss=2.740, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=948.9 milliseconds, train=4.5 seconds, 414336 images, time remaining=90.8 minutes
6475: loss=3.285, avg loss=2.794, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=4.5 seconds, 414400 images, time remaining=90.7 minutes
6476: loss=3.390, avg loss=2.854, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.0 seconds, train=4.5 seconds, 414464 images, time remaining=90.7 minutes
6477: loss=2.260, avg loss=2.794, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.5 seconds, 414528 images, time remaining=90.6 minutes
6478: loss=2.306, avg loss=2.746, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.5 seconds, train=4.5 seconds, 414592 images, time remaining=90.6 minutes
6479: loss=2.510, avg loss=2.722, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=607.4 milliseconds, train=4.5 seconds, 414656 images, time remaining=90.5 minutes
6480: loss=3.233, avg loss=2.773, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.7 seconds, train=4.5 seconds, 414720 images, time remaining=90.5 minutes
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6481: loss=2.041, avg loss=2.700, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=879.7 milliseconds, train=1.8 seconds, 414784 images, time remaining=90.4 minutes
6482: loss=2.619, avg loss=2.692, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=746.2 milliseconds, train=1.8 seconds, 414848 images, time remaining=90.3 minutes
6483: loss=2.759, avg loss=2.698, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=791.0 milliseconds, train=1.8 seconds, 414912 images, time remaining=90.3 minutes
6484: loss=2.391, avg loss=2.668, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=1.8 seconds, 414976 images, time remaining=90.2 minutes
6485: loss=2.748, avg loss=2.676, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.8 seconds, 415040 images, time remaining=90.2 minutes
6486: loss=2.731, avg loss=2.681, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=849.4 milliseconds, train=1.8 seconds, 415104 images, time remaining=90.1 minutes
6487: loss=2.268, avg loss=2.640, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=1.9 seconds, 415168 images, time remaining=90 minutes
6488: loss=1.887, avg loss=2.565, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=831.6 milliseconds, train=1.8 seconds, 415232 images, time remaining=90 minutes
6489: loss=2.391, avg loss=2.547, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=1.8 seconds, 415296 images, time remaining=89.9 minutes
6490: loss=2.348, avg loss=2.527, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.8 seconds, 415360 images, time remaining=89.8 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6491: loss=2.546, avg loss=2.529, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=777.4 milliseconds, train=3.4 seconds, 415424 images, time remaining=89.8 minutes
6492: loss=2.447, avg loss=2.521, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=815.2 milliseconds, train=3.4 seconds, 415488 images, time remaining=89.7 minutes
6493: loss=2.330, avg loss=2.502, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=642.4 milliseconds, train=3.4 seconds, 415552 images, time remaining=89.6 minutes
6494: loss=2.152, avg loss=2.467, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=939.6 milliseconds, train=3.4 seconds, 415616 images, time remaining=89.6 minutes
6495: loss=2.540, avg loss=2.474, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=576.6 milliseconds, train=3.4 seconds, 415680 images, time remaining=89.5 minutes
6496: loss=1.687, avg loss=2.396, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=3.4 seconds, 415744 images, time remaining=89.5 minutes
6497: loss=2.108, avg loss=2.367, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=753.5 milliseconds, train=3.4 seconds, 415808 images, time remaining=89.4 minutes
6498: loss=2.602, avg loss=2.390, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=672.8 milliseconds, train=3.4 seconds, 415872 images, time remaining=89.3 minutes
6499: loss=2.780, avg loss=2.429, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=3.4 seconds, 415936 images, time remaining=89.3 minutes
6500: loss=2.560, avg loss=2.442, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.5 seconds, train=3.4 seconds, 416000 images, time remaining=89.2 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6501: loss=2.231, avg loss=2.421, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=3.8 seconds, 416064 images, time remaining=89.2 minutes
6502: loss=2.219, avg loss=2.401, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=3.8 seconds, 416128 images, time remaining=89.1 minutes
6503: loss=2.910, avg loss=2.452, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=960.7 milliseconds, train=3.8 seconds, 416192 images, time remaining=89.1 minutes
6504: loss=2.553, avg loss=2.462, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=791.1 milliseconds, train=3.8 seconds, 416256 images, time remaining=89 minutes
6505: loss=2.261, avg loss=2.442, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.0 seconds, train=3.8 seconds, 416320 images, time remaining=88.9 minutes
6506: loss=2.569, avg loss=2.455, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=737.2 milliseconds, train=3.8 seconds, 416384 images, time remaining=88.9 minutes
6507: loss=3.138, avg loss=2.523, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=893.6 milliseconds, train=3.8 seconds, 416448 images, time remaining=88.8 minutes
6508: loss=3.638, avg loss=2.634, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.4 seconds, train=3.8 seconds, 416512 images, time remaining=88.8 minutes
6509: loss=2.213, avg loss=2.592, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=617.9 milliseconds, train=3.8 seconds, 416576 images, time remaining=88.7 minutes
6510: loss=2.473, avg loss=2.580, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.9 seconds, train=3.8 seconds, 416640 images, time remaining=88.6 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1463fa000000
6511: loss=1.994, avg loss=2.522, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=858.2 milliseconds, train=1.6 seconds, 416704 images, time remaining=88.6 minutes
6512: loss=2.406, avg loss=2.510, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=811.5 milliseconds, train=1.6 seconds, 416768 images, time remaining=88.5 minutes
6513: loss=2.772, avg loss=2.536, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=860.4 milliseconds, train=1.6 seconds, 416832 images, time remaining=88.4 minutes
6514: loss=2.521, avg loss=2.535, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=1.6 seconds, 416896 images, time remaining=88.4 minutes
6515: loss=1.984, avg loss=2.480, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=692.6 milliseconds, train=1.6 seconds, 416960 images, time remaining=88.3 minutes
6516: loss=2.345, avg loss=2.466, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=1.6 seconds, 417024 images, time remaining=88.2 minutes
6517: loss=2.708, avg loss=2.490, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=446.8 milliseconds, train=1.6 seconds, 417088 images, time remaining=88.2 minutes
6518: loss=2.689, avg loss=2.510, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=816.3 milliseconds, train=1.7 seconds, 417152 images, time remaining=88.1 minutes
6519: loss=2.359, avg loss=2.495, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=953.3 milliseconds, train=1.6 seconds, 417216 images, time remaining=88.1 minutes
6520: loss=2.602, avg loss=2.506, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=544.4 milliseconds, train=1.6 seconds, 417280 images, time remaining=88 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6521: loss=2.413, avg loss=2.496, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=584.4 milliseconds, train=3.9 seconds, 417344 images, time remaining=87.9 minutes
6522: loss=2.634, avg loss=2.510, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=927.4 milliseconds, train=3.9 seconds, 417408 images, time remaining=87.9 minutes
6523: loss=2.608, avg loss=2.520, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=599.5 milliseconds, train=3.9 seconds, 417472 images, time remaining=87.8 minutes
6524: loss=2.487, avg loss=2.517, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=3.9 seconds, 417536 images, time remaining=87.8 minutes
6525: loss=2.439, avg loss=2.509, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=663.9 milliseconds, train=3.9 seconds, 417600 images, time remaining=87.7 minutes
6526: loss=2.364, avg loss=2.494, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=782.1 milliseconds, train=3.9 seconds, 417664 images, time remaining=87.6 minutes
6527: loss=2.651, avg loss=2.510, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=872.8 milliseconds, train=3.9 seconds, 417728 images, time remaining=87.6 minutes
6528: loss=2.361, avg loss=2.495, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=514.3 milliseconds, train=3.9 seconds, 417792 images, time remaining=87.5 minutes
6529: loss=1.890, avg loss=2.435, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=872.9 milliseconds, train=3.9 seconds, 417856 images, time remaining=87.5 minutes
6530: loss=2.976, avg loss=2.489, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=3.9 seconds, 417920 images, time remaining=87.4 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146498000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6531: loss=3.146, avg loss=2.555, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.8 seconds, train=1.2 seconds, 417984 images, time remaining=87.3 minutes
6532: loss=1.922, avg loss=2.491, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=566.3 milliseconds, train=1.2 seconds, 418048 images, time remaining=87.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6533: loss=2.407, avg loss=2.483, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.3 seconds, train=1.2 seconds, 418112 images, time remaining=87.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6534: loss=3.214, avg loss=2.556, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.2 seconds, 418176 images, time remaining=87.2 minutes
6535: loss=2.275, avg loss=2.528, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=556.4 milliseconds, train=1.2 seconds, 418240 images, time remaining=87.1 minutes
6536: loss=2.379, avg loss=2.513, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 418304 images, time remaining=87 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6537: loss=2.643, avg loss=2.526, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.7 seconds, train=1.2 seconds, 418368 images, time remaining=86.9 minutes
6538: loss=2.683, avg loss=2.542, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=852.0 milliseconds, train=1.2 seconds, 418432 images, time remaining=86.9 minutes
6539: loss=2.519, avg loss=2.539, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=674.3 milliseconds, train=1.2 seconds, 418496 images, time remaining=86.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6540: loss=2.695, avg loss=2.555, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.6 seconds, train=1.2 seconds, 418560 images, time remaining=86.8 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6541: loss=2.505, avg loss=2.550, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=601.7 milliseconds, train=3.9 seconds, 418624 images, time remaining=86.7 minutes
6542: loss=2.503, avg loss=2.545, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=979.1 milliseconds, train=3.9 seconds, 418688 images, time remaining=86.7 minutes
6543: loss=2.533, avg loss=2.544, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=3.9 seconds, 418752 images, time remaining=86.6 minutes
6544: loss=2.694, avg loss=2.559, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=672.7 milliseconds, train=3.9 seconds, 418816 images, time remaining=86.5 minutes
6545: loss=2.208, avg loss=2.524, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=540.2 milliseconds, train=3.9 seconds, 418880 images, time remaining=86.5 minutes
6546: loss=2.683, avg loss=2.540, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=3.9 seconds, 418944 images, time remaining=86.4 minutes
6547: loss=2.073, avg loss=2.493, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=794.8 milliseconds, train=3.9 seconds, 419008 images, time remaining=86.3 minutes
6548: loss=2.420, avg loss=2.486, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=3.9 seconds, 419072 images, time remaining=86.3 minutes
6549: loss=2.147, avg loss=2.452, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=3.9 seconds, 419136 images, time remaining=86.2 minutes
6550: loss=2.421, avg loss=2.449, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=3.9 seconds, 419200 images, time remaining=86.2 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145f5a000000
6551: loss=2.319, avg loss=2.436, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=422.4 milliseconds, train=1.6 seconds, 419264 images, time remaining=86.1 minutes
6552: loss=2.636, avg loss=2.456, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=656.1 milliseconds, train=1.7 seconds, 419328 images, time remaining=86 minutes
6553: loss=2.119, avg loss=2.422, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=609.6 milliseconds, train=1.6 seconds, 419392 images, time remaining=86 minutes
6554: loss=2.021, avg loss=2.382, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=741.1 milliseconds, train=1.6 seconds, 419456 images, time remaining=85.9 minutes
6555: loss=2.145, avg loss=2.358, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=749.1 milliseconds, train=1.6 seconds, 419520 images, time remaining=85.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6556: loss=2.415, avg loss=2.364, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.6 seconds, 419584 images, time remaining=85.8 minutes
6557: loss=2.273, avg loss=2.355, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=836.2 milliseconds, train=1.6 seconds, 419648 images, time remaining=85.7 minutes
6558: loss=2.536, avg loss=2.373, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=576.1 milliseconds, train=1.7 seconds, 419712 images, time remaining=85.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6559: loss=2.173, avg loss=2.353, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.3 seconds, train=1.6 seconds, 419776 images, time remaining=85.6 minutes
6560: loss=2.381, avg loss=2.356, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=668.8 milliseconds, train=1.7 seconds, 419840 images, time remaining=85.5 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6561: loss=2.427, avg loss=2.363, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=1.9 seconds, 419904 images, time remaining=85.5 minutes
6562: loss=1.954, avg loss=2.322, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=680.4 milliseconds, train=1.9 seconds, 419968 images, time remaining=85.4 minutes
6563: loss=2.162, avg loss=2.306, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=943.6 milliseconds, train=1.9 seconds, 420032 images, time remaining=85.3 minutes
6564: loss=2.483, avg loss=2.324, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=493.6 milliseconds, train=1.9 seconds, 420096 images, time remaining=85.3 minutes
6565: loss=3.160, avg loss=2.407, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=674.7 milliseconds, train=1.9 seconds, 420160 images, time remaining=85.2 minutes
6566: loss=2.257, avg loss=2.392, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=1.9 seconds, 420224 images, time remaining=85.1 minutes
6567: loss=1.980, avg loss=2.351, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=888.3 milliseconds, train=1.9 seconds, 420288 images, time remaining=85.1 minutes
6568: loss=2.159, avg loss=2.332, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=728.3 milliseconds, train=1.9 seconds, 420352 images, time remaining=85 minutes
6569: loss=2.322, avg loss=2.331, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=1.9 seconds, 420416 images, time remaining=84.9 minutes
6570: loss=2.462, avg loss=2.344, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=1.9 seconds, 420480 images, time remaining=84.9 minutes
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6571: loss=3.195, avg loss=2.429, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=858.6 milliseconds, train=5.0 seconds, 420544 images, time remaining=84.8 minutes
6572: loss=3.180, avg loss=2.504, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=961.4 milliseconds, train=4.9 seconds, 420608 images, time remaining=84.8 minutes
6573: loss=2.591, avg loss=2.513, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=909.0 milliseconds, train=4.9 seconds, 420672 images, time remaining=84.7 minutes
6574: loss=2.988, avg loss=2.560, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=530.5 milliseconds, train=4.9 seconds, 420736 images, time remaining=84.7 minutes
6575: loss=3.299, avg loss=2.634, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=916.3 milliseconds, train=4.9 seconds, 420800 images, time remaining=84.6 minutes
6576: loss=2.524, avg loss=2.623, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=903.7 milliseconds, train=4.9 seconds, 420864 images, time remaining=84.6 minutes
6577: loss=3.097, avg loss=2.671, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.1 seconds, train=5.0 seconds, 420928 images, time remaining=84.5 minutes
6578: loss=3.354, avg loss=2.739, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=689.6 milliseconds, train=5.0 seconds, 420992 images, time remaining=84.5 minutes
6579: loss=3.240, avg loss=2.789, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.7 seconds, train=4.9 seconds, 421056 images, time remaining=84.4 minutes
6580: loss=2.861, avg loss=2.796, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=729.6 milliseconds, train=5.0 seconds, 421120 images, time remaining=84.3 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6581: loss=2.802, avg loss=2.797, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=4.6 seconds, 421184 images, time remaining=84.3 minutes
6582: loss=2.715, avg loss=2.789, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=4.6 seconds, 421248 images, time remaining=84.2 minutes
6583: loss=2.804, avg loss=2.790, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=672.8 milliseconds, train=4.6 seconds, 421312 images, time remaining=84.2 minutes
6584: loss=2.624, avg loss=2.774, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=843.3 milliseconds, train=4.6 seconds, 421376 images, time remaining=84.1 minutes
6585: loss=2.883, avg loss=2.785, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.6 seconds, 421440 images, time remaining=84.1 minutes
6586: loss=2.816, avg loss=2.788, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=941.9 milliseconds, train=4.6 seconds, 421504 images, time remaining=84 minutes
6587: loss=2.465, avg loss=2.755, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=836.0 milliseconds, train=4.6 seconds, 421568 images, time remaining=84 minutes
6588: loss=2.677, avg loss=2.748, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=4.6 seconds, 421632 images, time remaining=83.9 minutes
6589: loss=2.866, avg loss=2.759, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=942.9 milliseconds, train=4.6 seconds, 421696 images, time remaining=83.8 minutes
6590: loss=2.241, avg loss=2.708, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=510.4 milliseconds, train=4.6 seconds, 421760 images, time remaining=83.8 minutes
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x145f5c000000
6591: loss=2.745, avg loss=2.711, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=621.9 milliseconds, train=1.1 seconds, 421824 images, time remaining=83.7 minutes
6592: loss=2.447, avg loss=2.685, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=593.9 milliseconds, train=1.1 seconds, 421888 images, time remaining=83.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6593: loss=2.862, avg loss=2.703, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=1.1 seconds, 421952 images, time remaining=83.6 minutes
6594: loss=3.314, avg loss=2.764, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=671.7 milliseconds, train=1.1 seconds, 422016 images, time remaining=83.5 minutes
6595: loss=2.783, avg loss=2.766, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=528.6 milliseconds, train=1.1 seconds, 422080 images, time remaining=83.5 minutes
6596: loss=2.244, avg loss=2.713, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=532.0 milliseconds, train=1.1 seconds, 422144 images, time remaining=83.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6597: loss=3.255, avg loss=2.768, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.1 seconds, 422208 images, time remaining=83.3 minutes
6598: loss=2.757, avg loss=2.767, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=830.6 milliseconds, train=1.2 seconds, 422272 images, time remaining=83.3 minutes
6599: loss=1.952, avg loss=2.685, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=599.5 milliseconds, train=1.1 seconds, 422336 images, time remaining=83.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6600: loss=2.097, avg loss=2.626, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.1 seconds, 422400 images, time remaining=83.1 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6601: loss=3.018, avg loss=2.665, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=4.7 seconds, 422464 images, time remaining=83.1 minutes
6602: loss=2.134, avg loss=2.612, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=4.6 seconds, 422528 images, time remaining=83 minutes
6603: loss=3.219, avg loss=2.673, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=503.6 milliseconds, train=4.6 seconds, 422592 images, time remaining=83 minutes
6604: loss=3.096, avg loss=2.715, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=4.6 seconds, 422656 images, time remaining=82.9 minutes
6605: loss=3.542, avg loss=2.798, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=639.0 milliseconds, train=4.6 seconds, 422720 images, time remaining=82.9 minutes
6606: loss=2.638, avg loss=2.782, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=670.3 milliseconds, train=4.6 seconds, 422784 images, time remaining=82.8 minutes
6607: loss=2.565, avg loss=2.760, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.6 seconds, train=4.6 seconds, 422848 images, time remaining=82.8 minutes
6608: loss=3.245, avg loss=2.809, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=4.7 seconds, 422912 images, time remaining=82.7 minutes
6609: loss=3.291, avg loss=2.857, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=810.9 milliseconds, train=4.6 seconds, 422976 images, time remaining=82.7 minutes
6610: loss=3.637, avg loss=2.935, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=728.1 milliseconds, train=4.7 seconds, 423040 images, time remaining=82.6 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6611: loss=2.380, avg loss=2.880, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.5 seconds, 423104 images, time remaining=82.5 minutes
6612: loss=2.653, avg loss=2.857, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=955.9 milliseconds, train=4.5 seconds, 423168 images, time remaining=82.5 minutes
6613: loss=2.176, avg loss=2.789, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=4.5 seconds, 423232 images, time remaining=82.4 minutes
6614: loss=2.768, avg loss=2.787, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=4.5 seconds, 423296 images, time remaining=82.4 minutes
6615: loss=2.291, avg loss=2.737, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.5 seconds, 423360 images, time remaining=82.3 minutes
6616: loss=2.419, avg loss=2.705, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=4.5 seconds, 423424 images, time remaining=82.3 minutes
6617: loss=2.202, avg loss=2.655, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=4.5 seconds, 423488 images, time remaining=82.2 minutes
6618: loss=2.491, avg loss=2.639, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=4.5 seconds, 423552 images, time remaining=82.1 minutes
6619: loss=2.605, avg loss=2.635, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=4.5 seconds, 423616 images, time remaining=82.1 minutes
6620: loss=3.217, avg loss=2.693, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=671.4 milliseconds, train=4.5 seconds, 423680 images, time remaining=82 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14570b000000
6621: loss=3.166, avg loss=2.741, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=616.0 milliseconds, train=1.2 seconds, 423744 images, time remaining=82 minutes
6622: loss=2.567, avg loss=2.723, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=780.7 milliseconds, train=1.2 seconds, 423808 images, time remaining=81.9 minutes
6623: loss=2.233, avg loss=2.674, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=901.5 milliseconds, train=1.2 seconds, 423872 images, time remaining=81.8 minutes
6624: loss=2.226, avg loss=2.629, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=947.7 milliseconds, train=1.2 seconds, 423936 images, time remaining=81.8 minutes
6625: loss=2.276, avg loss=2.594, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=759.4 milliseconds, train=1.2 seconds, 424000 images, time remaining=81.7 minutes
6626: loss=2.367, avg loss=2.571, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=723.5 milliseconds, train=1.2 seconds, 424064 images, time remaining=81.6 minutes
6627: loss=2.106, avg loss=2.525, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=454.1 milliseconds, train=1.2 seconds, 424128 images, time remaining=81.6 minutes
6628: loss=2.930, avg loss=2.565, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=341.0 milliseconds, train=1.2 seconds, 424192 images, time remaining=81.5 minutes
6629: loss=2.298, avg loss=2.539, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=732.5 milliseconds, train=1.2 seconds, 424256 images, time remaining=81.4 minutes
6630: loss=2.250, avg loss=2.510, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=554.1 milliseconds, train=1.2 seconds, 424320 images, time remaining=81.4 minutes
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6631: loss=2.864, avg loss=2.545, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=769.9 milliseconds, train=4.4 seconds, 424384 images, time remaining=81.3 minutes
6632: loss=1.715, avg loss=2.462, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=816.9 milliseconds, train=4.4 seconds, 424448 images, time remaining=81.3 minutes
6633: loss=2.324, avg loss=2.448, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.0 seconds, train=4.4 seconds, 424512 images, time remaining=81.2 minutes
6634: loss=2.345, avg loss=2.438, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=773.1 milliseconds, train=4.4 seconds, 424576 images, time remaining=81.1 minutes
6635: loss=2.610, avg loss=2.455, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.4 seconds, 424640 images, time remaining=81.1 minutes
6636: loss=2.320, avg loss=2.442, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=3.0 seconds, train=4.4 seconds, 424704 images, time remaining=81 minutes
6637: loss=2.521, avg loss=2.450, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=867.6 milliseconds, train=4.4 seconds, 424768 images, time remaining=81 minutes
6638: loss=2.703, avg loss=2.475, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.4 seconds, 424832 images, time remaining=80.9 minutes
6639: loss=1.797, avg loss=2.407, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=4.5 seconds, 424896 images, time remaining=80.9 minutes
6640: loss=2.542, avg loss=2.421, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=4.5 seconds, 424960 images, time remaining=80.8 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6641: loss=2.121, avg loss=2.391, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=1.9 seconds, 425024 images, time remaining=80.8 minutes
6642: loss=2.308, avg loss=2.382, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=780.2 milliseconds, train=1.9 seconds, 425088 images, time remaining=80.7 minutes
6643: loss=2.111, avg loss=2.355, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=775.5 milliseconds, train=1.9 seconds, 425152 images, time remaining=80.6 minutes
6644: loss=2.041, avg loss=2.324, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.9 seconds, 425216 images, time remaining=80.5 minutes
6645: loss=1.998, avg loss=2.291, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=590.4 milliseconds, train=1.9 seconds, 425280 images, time remaining=80.5 minutes
6646: loss=2.003, avg loss=2.262, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=797.6 milliseconds, train=1.9 seconds, 425344 images, time remaining=80.4 minutes
6647: loss=1.927, avg loss=2.229, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=712.8 milliseconds, train=1.9 seconds, 425408 images, time remaining=80.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6648: loss=2.662, avg loss=2.272, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.6 seconds, train=1.9 seconds, 425472 images, time remaining=80.3 minutes
6649: loss=1.977, avg loss=2.243, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.3 seconds, train=1.9 seconds, 425536 images, time remaining=80.2 minutes
6650: loss=2.227, avg loss=2.241, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=843.3 milliseconds, train=1.9 seconds, 425600 images, time remaining=80.2 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463a2000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6651: loss=2.274, avg loss=2.244, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.1 seconds, train=1.5 seconds, 425664 images, time remaining=80.1 minutes
6652: loss=2.525, avg loss=2.272, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.5 seconds, 425728 images, time remaining=80 minutes
6653: loss=2.347, avg loss=2.280, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=965.6 milliseconds, train=1.5 seconds, 425792 images, time remaining=80 minutes
6654: loss=2.265, avg loss=2.278, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=1.5 seconds, 425856 images, time remaining=79.9 minutes
6655: loss=2.277, avg loss=2.278, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=921.2 milliseconds, train=1.5 seconds, 425920 images, time remaining=79.8 minutes
6656: loss=2.323, avg loss=2.283, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=914.9 milliseconds, train=1.5 seconds, 425984 images, time remaining=79.8 minutes
6657: loss=2.070, avg loss=2.262, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=671.4 milliseconds, train=1.5 seconds, 426048 images, time remaining=79.7 minutes
6658: loss=2.067, avg loss=2.242, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=573.5 milliseconds, train=1.5 seconds, 426112 images, time remaining=79.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6659: loss=2.036, avg loss=2.221, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.3 seconds, train=1.5 seconds, 426176 images, time remaining=79.6 minutes
6660: loss=1.957, avg loss=2.195, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=782.0 milliseconds, train=1.5 seconds, 426240 images, time remaining=79.5 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6661: loss=2.323, avg loss=2.208, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 426304 images, time remaining=79.4 minutes
6662: loss=2.459, avg loss=2.233, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=525.5 milliseconds, train=2.1 seconds, 426368 images, time remaining=79.4 minutes
6663: loss=2.234, avg loss=2.233, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 426432 images, time remaining=79.3 minutes
6664: loss=1.886, avg loss=2.198, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=940.9 milliseconds, train=2.1 seconds, 426496 images, time remaining=79.3 minutes
6665: loss=2.572, avg loss=2.236, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 426560 images, time remaining=79.2 minutes
6666: loss=1.791, avg loss=2.191, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=479.9 milliseconds, train=2.1 seconds, 426624 images, time remaining=79.1 minutes
6667: loss=1.918, avg loss=2.164, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 426688 images, time remaining=79.1 minutes
6668: loss=2.182, avg loss=2.166, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=908.9 milliseconds, train=2.1 seconds, 426752 images, time remaining=79 minutes
6669: loss=2.085, avg loss=2.158, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=633.8 milliseconds, train=2.1 seconds, 426816 images, time remaining=78.9 minutes
6670: loss=2.017, avg loss=2.144, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 426880 images, time remaining=78.9 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6671: loss=2.811, avg loss=2.210, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=807.4 milliseconds, train=5.1 seconds, 426944 images, time remaining=78.8 minutes
6672: loss=3.163, avg loss=2.306, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.8 seconds, train=5.0 seconds, 427008 images, time remaining=78.8 minutes
6673: loss=2.861, avg loss=2.361, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=2.2 seconds, train=5.0 seconds, 427072 images, time remaining=78.7 minutes
6674: loss=2.452, avg loss=2.370, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.5 seconds, train=5.0 seconds, 427136 images, time remaining=78.7 minutes
6675: loss=2.817, avg loss=2.415, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=651.4 milliseconds, train=5.0 seconds, 427200 images, time remaining=78.6 minutes
6676: loss=2.815, avg loss=2.455, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.4 seconds, train=5.0 seconds, 427264 images, time remaining=78.6 minutes
6677: loss=2.298, avg loss=2.439, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=465.7 milliseconds, train=5.0 seconds, 427328 images, time remaining=78.5 minutes
6678: loss=2.453, avg loss=2.441, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=660.9 milliseconds, train=5.0 seconds, 427392 images, time remaining=78.4 minutes
6679: loss=2.741, avg loss=2.471, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=768.8 milliseconds, train=5.1 seconds, 427456 images, time remaining=78.4 minutes
6680: loss=3.120, avg loss=2.536, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=621.4 milliseconds, train=5.0 seconds, 427520 images, time remaining=78.3 minutes
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6681: loss=2.902, avg loss=2.572, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=773.5 milliseconds, train=5.0 seconds, 427584 images, time remaining=78.3 minutes
6682: loss=1.902, avg loss=2.505, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=1.2 seconds, train=4.9 seconds, 427648 images, time remaining=78.2 minutes
6683: loss=2.716, avg loss=2.526, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=650.9 milliseconds, train=4.9 seconds, 427712 images, time remaining=78.2 minutes
6684: loss=2.768, avg loss=2.551, last=96.27%, best=96.27%, next=6684, rate=0.00013000, load 64=579.9 milliseconds, train=4.9 seconds, 427776 images, time remaining=78.1 minutes
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=91721, unique_truth_count=57264
rank=0 of ranks=91721rank=100 of ranks=91721rank=200 of ranks=91721rank=300 of ranks=91721rank=400 of ranks=91721rank=500 of ranks=91721rank=600 of ranks=91721rank=700 of ranks=91721rank=800 of ranks=91721rank=900 of ranks=91721rank=1000 of ranks=91721rank=1100 of ranks=91721rank=1200 of ranks=91721rank=1300 of ranks=91721rank=1400 of ranks=91721rank=1500 of ranks=91721rank=1600 of ranks=91721rank=1700 of ranks=91721rank=1800 of ranks=91721rank=1900 of ranks=91721rank=2000 of ranks=91721rank=2100 of ranks=91721rank=2200 of ranks=91721rank=2300 of ranks=91721rank=2400 of ranks=91721rank=2500 of ranks=91721rank=2600 of ranks=91721rank=2700 of ranks=91721rank=2800 of ranks=91721rank=2900 of ranks=91721rank=3000 of ranks=91721rank=3100 of ranks=91721rank=3200 of ranks=91721rank=3300 of ranks=91721rank=3400 of ranks=91721rank=3500 of ranks=91721rank=3600 of ranks=91721rank=3700 of ranks=91721rank=3800 of ranks=91721rank=3900 of ranks=91721rank=4000 of ranks=91721rank=4100 of ranks=91721rank=4200 of ranks=91721rank=4300 of ranks=91721rank=4400 of ranks=91721rank=4500 of ranks=91721rank=4600 of ranks=91721rank=4700 of ranks=91721rank=4800 of ranks=91721rank=4900 of ranks=91721rank=5000 of ranks=91721rank=5100 of ranks=91721rank=5200 of ranks=91721rank=5300 of ranks=91721rank=5400 of ranks=91721rank=5500 of ranks=91721rank=5600 of ranks=91721rank=5700 of ranks=91721rank=5800 of ranks=91721rank=5900 of ranks=91721rank=6000 of ranks=91721rank=6100 of ranks=91721rank=6200 of ranks=91721rank=6300 of ranks=91721rank=6400 of ranks=91721rank=6500 of ranks=91721rank=6600 of ranks=91721rank=6700 of ranks=91721rank=6800 of ranks=91721rank=6900 of ranks=91721rank=7000 of ranks=91721rank=7100 of ranks=91721rank=7200 of ranks=91721rank=7300 of ranks=91721rank=7400 of ranks=91721rank=7500 of ranks=91721rank=7600 of ranks=91721rank=7700 of ranks=91721rank=7800 of ranks=91721rank=7900 of ranks=91721rank=8000 of ranks=91721rank=8100 of ranks=91721rank=8200 of ranks=91721rank=8300 of ranks=91721rank=8400 of ranks=91721rank=8500 of ranks=91721rank=8600 of ranks=91721rank=8700 of ranks=91721rank=8800 of ranks=91721rank=8900 of ranks=91721rank=9000 of ranks=91721rank=9100 of ranks=91721rank=9200 of ranks=91721rank=9300 of ranks=91721rank=9400 of ranks=91721rank=9500 of ranks=91721rank=9600 of ranks=91721rank=9700 of ranks=91721rank=9800 of ranks=91721rank=9900 of ranks=91721rank=10000 of ranks=91721rank=10100 of ranks=91721rank=10200 of ranks=91721rank=10300 of ranks=91721rank=10400 of ranks=91721rank=10500 of ranks=91721rank=10600 of ranks=91721rank=10700 of ranks=91721rank=10800 of ranks=91721rank=10900 of ranks=91721rank=11000 of ranks=91721rank=11100 of ranks=91721rank=11200 of ranks=91721rank=11300 of ranks=91721rank=11400 of ranks=91721rank=11500 of ranks=91721rank=11600 of ranks=91721rank=11700 of ranks=91721rank=11800 of ranks=91721rank=11900 of ranks=91721rank=12000 of ranks=91721rank=12100 of ranks=91721rank=12200 of ranks=91721rank=12300 of ranks=91721rank=12400 of ranks=91721rank=12500 of ranks=91721rank=12600 of ranks=91721rank=12700 of ranks=91721rank=12800 of ranks=91721rank=12900 of ranks=91721rank=13000 of ranks=91721rank=13100 of ranks=91721rank=13200 of ranks=91721rank=13300 of ranks=91721rank=13400 of ranks=91721rank=13500 of ranks=91721rank=13600 of ranks=91721rank=13700 of ranks=91721rank=13800 of ranks=91721rank=13900 of ranks=91721rank=14000 of ranks=91721rank=14100 of ranks=91721rank=14200 of ranks=91721rank=14300 of ranks=91721rank=14400 of ranks=91721rank=14500 of ranks=91721rank=14600 of ranks=91721rank=14700 of ranks=91721rank=14800 of ranks=91721rank=14900 of ranks=91721rank=15000 of ranks=91721rank=15100 of ranks=91721rank=15200 of ranks=91721rank=15300 of ranks=91721rank=15400 of ranks=91721rank=15500 of ranks=91721rank=15600 of ranks=91721rank=15700 of ranks=91721rank=15800 of ranks=91721rank=15900 of ranks=91721rank=16000 of ranks=91721rank=16100 of ranks=91721rank=16200 of ranks=91721rank=16300 of ranks=91721rank=16400 of ranks=91721rank=16500 of ranks=91721rank=16600 of ranks=91721rank=16700 of ranks=91721rank=16800 of ranks=91721rank=16900 of ranks=91721rank=17000 of ranks=91721rank=17100 of ranks=91721rank=17200 of ranks=91721rank=17300 of ranks=91721rank=17400 of ranks=91721rank=17500 of ranks=91721rank=17600 of ranks=91721rank=17700 of ranks=91721rank=17800 of ranks=91721rank=17900 of ranks=91721rank=18000 of ranks=91721rank=18100 of ranks=91721rank=18200 of ranks=91721rank=18300 of ranks=91721rank=18400 of ranks=91721rank=18500 of ranks=91721rank=18600 of ranks=91721rank=18700 of ranks=91721rank=18800 of ranks=91721rank=18900 of ranks=91721rank=19000 of ranks=91721rank=19100 of ranks=91721rank=19200 of ranks=91721rank=19300 of ranks=91721rank=19400 of ranks=91721rank=19500 of ranks=91721rank=19600 of ranks=91721rank=19700 of ranks=91721rank=19800 of ranks=91721rank=19900 of ranks=91721rank=20000 of ranks=91721rank=20100 of ranks=91721rank=20200 of ranks=91721rank=20300 of ranks=91721rank=20400 of ranks=91721rank=20500 of ranks=91721rank=20600 of ranks=91721rank=20700 of ranks=91721rank=20800 of ranks=91721rank=20900 of ranks=91721rank=21000 of ranks=91721rank=21100 of ranks=91721rank=21200 of ranks=91721rank=21300 of ranks=91721rank=21400 of ranks=91721rank=21500 of ranks=91721rank=21600 of ranks=91721rank=21700 of ranks=91721rank=21800 of ranks=91721rank=21900 of ranks=91721rank=22000 of ranks=91721rank=22100 of ranks=91721rank=22200 of ranks=91721rank=22300 of ranks=91721rank=22400 of ranks=91721rank=22500 of ranks=91721rank=22600 of ranks=91721rank=22700 of ranks=91721rank=22800 of ranks=91721rank=22900 of ranks=91721rank=23000 of ranks=91721rank=23100 of ranks=91721rank=23200 of ranks=91721rank=23300 of ranks=91721rank=23400 of ranks=91721rank=23500 of ranks=91721rank=23600 of ranks=91721rank=23700 of ranks=91721rank=23800 of ranks=91721rank=23900 of ranks=91721rank=24000 of ranks=91721rank=24100 of ranks=91721rank=24200 of ranks=91721rank=24300 of ranks=91721rank=24400 of ranks=91721rank=24500 of ranks=91721rank=24600 of ranks=91721rank=24700 of ranks=91721rank=24800 of ranks=91721rank=24900 of ranks=91721rank=25000 of ranks=91721rank=25100 of ranks=91721rank=25200 of ranks=91721rank=25300 of ranks=91721rank=25400 of ranks=91721rank=25500 of ranks=91721rank=25600 of ranks=91721rank=25700 of ranks=91721rank=25800 of ranks=91721rank=25900 of ranks=91721rank=26000 of ranks=91721rank=26100 of ranks=91721rank=26200 of ranks=91721rank=26300 of ranks=91721rank=26400 of ranks=91721rank=26500 of ranks=91721rank=26600 of ranks=91721rank=26700 of ranks=91721rank=26800 of ranks=91721rank=26900 of ranks=91721rank=27000 of ranks=91721rank=27100 of ranks=91721rank=27200 of ranks=91721rank=27300 of ranks=91721rank=27400 of ranks=91721rank=27500 of ranks=91721rank=27600 of ranks=91721rank=27700 of ranks=91721rank=27800 of ranks=91721rank=27900 of ranks=91721rank=28000 of ranks=91721rank=28100 of ranks=91721rank=28200 of ranks=91721rank=28300 of ranks=91721rank=28400 of ranks=91721rank=28500 of ranks=91721rank=28600 of ranks=91721rank=28700 of ranks=91721rank=28800 of ranks=91721rank=28900 of ranks=91721rank=29000 of ranks=91721rank=29100 of ranks=91721rank=29200 of ranks=91721rank=29300 of ranks=91721rank=29400 of ranks=91721rank=29500 of ranks=91721rank=29600 of ranks=91721rank=29700 of ranks=91721rank=29800 of ranks=91721rank=29900 of ranks=91721rank=30000 of ranks=91721rank=30100 of ranks=91721rank=30200 of ranks=91721rank=30300 of ranks=91721rank=30400 of ranks=91721rank=30500 of ranks=91721rank=30600 of ranks=91721rank=30700 of ranks=91721rank=30800 of ranks=91721rank=30900 of ranks=91721rank=31000 of ranks=91721rank=31100 of ranks=91721rank=31200 of ranks=91721rank=31300 of ranks=91721rank=31400 of ranks=91721rank=31500 of ranks=91721rank=31600 of ranks=91721rank=31700 of ranks=91721rank=31800 of ranks=91721rank=31900 of ranks=91721rank=32000 of ranks=91721rank=32100 of ranks=91721rank=32200 of ranks=91721rank=32300 of ranks=91721rank=32400 of ranks=91721rank=32500 of ranks=91721rank=32600 of ranks=91721rank=32700 of ranks=91721rank=32800 of ranks=91721rank=32900 of ranks=91721rank=33000 of ranks=91721rank=33100 of ranks=91721rank=33200 of ranks=91721rank=33300 of ranks=91721rank=33400 of ranks=91721rank=33500 of ranks=91721rank=33600 of ranks=91721rank=33700 of ranks=91721rank=33800 of ranks=91721rank=33900 of ranks=91721rank=34000 of ranks=91721rank=34100 of ranks=91721rank=34200 of ranks=91721rank=34300 of ranks=91721rank=34400 of ranks=91721rank=34500 of ranks=91721rank=34600 of ranks=91721rank=34700 of ranks=91721rank=34800 of ranks=91721rank=34900 of ranks=91721rank=35000 of ranks=91721rank=35100 of ranks=91721rank=35200 of ranks=91721rank=35300 of ranks=91721rank=35400 of ranks=91721rank=35500 of ranks=91721rank=35600 of ranks=91721rank=35700 of ranks=91721rank=35800 of ranks=91721rank=35900 of ranks=91721rank=36000 of ranks=91721rank=36100 of ranks=91721rank=36200 of ranks=91721rank=36300 of ranks=91721rank=36400 of ranks=91721rank=36500 of ranks=91721rank=36600 of ranks=91721rank=36700 of ranks=91721rank=36800 of ranks=91721rank=36900 of ranks=91721rank=37000 of ranks=91721rank=37100 of ranks=91721rank=37200 of ranks=91721rank=37300 of ranks=91721rank=37400 of ranks=91721rank=37500 of ranks=91721rank=37600 of ranks=91721rank=37700 of ranks=91721rank=37800 of ranks=91721rank=37900 of ranks=91721rank=38000 of ranks=91721rank=38100 of ranks=91721rank=38200 of ranks=91721rank=38300 of ranks=91721rank=38400 of ranks=91721rank=38500 of ranks=91721rank=38600 of ranks=91721rank=38700 of ranks=91721rank=38800 of ranks=91721rank=38900 of ranks=91721rank=39000 of ranks=91721rank=39100 of ranks=91721rank=39200 of ranks=91721rank=39300 of ranks=91721rank=39400 of ranks=91721rank=39500 of ranks=91721rank=39600 of ranks=91721rank=39700 of ranks=91721rank=39800 of ranks=91721rank=39900 of ranks=91721rank=40000 of ranks=91721rank=40100 of ranks=91721rank=40200 of ranks=91721rank=40300 of ranks=91721rank=40400 of ranks=91721rank=40500 of ranks=91721rank=40600 of ranks=91721rank=40700 of ranks=91721rank=40800 of ranks=91721rank=40900 of ranks=91721rank=41000 of ranks=91721rank=41100 of ranks=91721rank=41200 of ranks=91721rank=41300 of ranks=91721rank=41400 of ranks=91721rank=41500 of ranks=91721rank=41600 of ranks=91721rank=41700 of ranks=91721rank=41800 of ranks=91721rank=41900 of ranks=91721rank=42000 of ranks=91721rank=42100 of ranks=91721rank=42200 of ranks=91721rank=42300 of ranks=91721rank=42400 of ranks=91721rank=42500 of ranks=91721rank=42600 of ranks=91721rank=42700 of ranks=91721rank=42800 of ranks=91721rank=42900 of ranks=91721rank=43000 of ranks=91721rank=43100 of ranks=91721rank=43200 of ranks=91721rank=43300 of ranks=91721rank=43400 of ranks=91721rank=43500 of ranks=91721rank=43600 of ranks=91721rank=43700 of ranks=91721rank=43800 of ranks=91721rank=43900 of ranks=91721rank=44000 of ranks=91721rank=44100 of ranks=91721rank=44200 of ranks=91721rank=44300 of ranks=91721rank=44400 of ranks=91721rank=44500 of ranks=91721rank=44600 of ranks=91721rank=44700 of ranks=91721rank=44800 of ranks=91721rank=44900 of ranks=91721rank=45000 of ranks=91721rank=45100 of ranks=91721rank=45200 of ranks=91721rank=45300 of ranks=91721rank=45400 of ranks=91721rank=45500 of ranks=91721rank=45600 of ranks=91721rank=45700 of ranks=91721rank=45800 of ranks=91721rank=45900 of ranks=91721rank=46000 of ranks=91721rank=46100 of ranks=91721rank=46200 of ranks=91721rank=46300 of ranks=91721rank=46400 of ranks=91721rank=46500 of ranks=91721rank=46600 of ranks=91721rank=46700 of ranks=91721rank=46800 of ranks=91721rank=46900 of ranks=91721rank=47000 of ranks=91721rank=47100 of ranks=91721rank=47200 of ranks=91721rank=47300 of ranks=91721rank=47400 of ranks=91721rank=47500 of ranks=91721rank=47600 of ranks=91721rank=47700 of ranks=91721rank=47800 of ranks=91721rank=47900 of ranks=91721rank=48000 of ranks=91721rank=48100 of ranks=91721rank=48200 of ranks=91721rank=48300 of ranks=91721rank=48400 of ranks=91721rank=48500 of ranks=91721rank=48600 of ranks=91721rank=48700 of ranks=91721rank=48800 of ranks=91721rank=48900 of ranks=91721rank=49000 of ranks=91721rank=49100 of ranks=91721rank=49200 of ranks=91721rank=49300 of ranks=91721rank=49400 of ranks=91721rank=49500 of ranks=91721rank=49600 of ranks=91721rank=49700 of ranks=91721rank=49800 of ranks=91721rank=49900 of ranks=91721rank=50000 of ranks=91721rank=50100 of ranks=91721rank=50200 of ranks=91721rank=50300 of ranks=91721rank=50400 of ranks=91721rank=50500 of ranks=91721rank=50600 of ranks=91721rank=50700 of ranks=91721rank=50800 of ranks=91721rank=50900 of ranks=91721rank=51000 of ranks=91721rank=51100 of ranks=91721rank=51200 of ranks=91721rank=51300 of ranks=91721rank=51400 of ranks=91721rank=51500 of ranks=91721rank=51600 of ranks=91721rank=51700 of ranks=91721rank=51800 of ranks=91721rank=51900 of ranks=91721rank=52000 of ranks=91721rank=52100 of ranks=91721rank=52200 of ranks=91721rank=52300 of ranks=91721rank=52400 of ranks=91721rank=52500 of ranks=91721rank=52600 of ranks=91721rank=52700 of ranks=91721rank=52800 of ranks=91721rank=52900 of ranks=91721rank=53000 of ranks=91721rank=53100 of ranks=91721rank=53200 of ranks=91721rank=53300 of ranks=91721rank=53400 of ranks=91721rank=53500 of ranks=91721rank=53600 of ranks=91721rank=53700 of ranks=91721rank=53800 of ranks=91721rank=53900 of ranks=91721rank=54000 of ranks=91721rank=54100 of ranks=91721rank=54200 of ranks=91721rank=54300 of ranks=91721rank=54400 of ranks=91721rank=54500 of ranks=91721rank=54600 of ranks=91721rank=54700 of ranks=91721rank=54800 of ranks=91721rank=54900 of ranks=91721rank=55000 of ranks=91721rank=55100 of ranks=91721rank=55200 of ranks=91721rank=55300 of ranks=91721rank=55400 of ranks=91721rank=55500 of ranks=91721rank=55600 of ranks=91721rank=55700 of ranks=91721rank=55800 of ranks=91721rank=55900 of ranks=91721rank=56000 of ranks=91721rank=56100 of ranks=91721rank=56200 of ranks=91721rank=56300 of ranks=91721rank=56400 of ranks=91721rank=56500 of ranks=91721rank=56600 of ranks=91721rank=56700 of ranks=91721rank=56800 of ranks=91721rank=56900 of ranks=91721rank=57000 of ranks=91721rank=57100 of ranks=91721rank=57200 of ranks=91721rank=57300 of ranks=91721rank=57400 of ranks=91721rank=57500 of ranks=91721rank=57600 of ranks=91721rank=57700 of ranks=91721rank=57800 of ranks=91721rank=57900 of ranks=91721rank=58000 of ranks=91721rank=58100 of ranks=91721rank=58200 of ranks=91721rank=58300 of ranks=91721rank=58400 of ranks=91721rank=58500 of ranks=91721rank=58600 of ranks=91721rank=58700 of ranks=91721rank=58800 of ranks=91721rank=58900 of ranks=91721rank=59000 of ranks=91721rank=59100 of ranks=91721rank=59200 of ranks=91721rank=59300 of ranks=91721rank=59400 of ranks=91721rank=59500 of ranks=91721rank=59600 of ranks=91721rank=59700 of ranks=91721rank=59800 of ranks=91721rank=59900 of ranks=91721rank=60000 of ranks=91721rank=60100 of ranks=91721rank=60200 of ranks=91721rank=60300 of ranks=91721rank=60400 of ranks=91721rank=60500 of ranks=91721rank=60600 of ranks=91721rank=60700 of ranks=91721rank=60800 of ranks=91721rank=60900 of ranks=91721rank=61000 of ranks=91721rank=61100 of ranks=91721rank=61200 of ranks=91721rank=61300 of ranks=91721rank=61400 of ranks=91721rank=61500 of ranks=91721rank=61600 of ranks=91721rank=61700 of ranks=91721rank=61800 of ranks=91721rank=61900 of ranks=91721rank=62000 of ranks=91721rank=62100 of ranks=91721rank=62200 of ranks=91721rank=62300 of ranks=91721rank=62400 of ranks=91721rank=62500 of ranks=91721rank=62600 of ranks=91721rank=62700 of ranks=91721rank=62800 of ranks=91721rank=62900 of ranks=91721rank=63000 of ranks=91721rank=63100 of ranks=91721rank=63200 of ranks=91721rank=63300 of ranks=91721rank=63400 of ranks=91721rank=63500 of ranks=91721rank=63600 of ranks=91721rank=63700 of ranks=91721rank=63800 of ranks=91721rank=63900 of ranks=91721rank=64000 of ranks=91721rank=64100 of ranks=91721rank=64200 of ranks=91721rank=64300 of ranks=91721rank=64400 of ranks=91721rank=64500 of ranks=91721rank=64600 of ranks=91721rank=64700 of ranks=91721rank=64800 of ranks=91721rank=64900 of ranks=91721rank=65000 of ranks=91721rank=65100 of ranks=91721rank=65200 of ranks=91721rank=65300 of ranks=91721rank=65400 of ranks=91721rank=65500 of ranks=91721rank=65600 of ranks=91721rank=65700 of ranks=91721rank=65800 of ranks=91721rank=65900 of ranks=91721rank=66000 of ranks=91721rank=66100 of ranks=91721rank=66200 of ranks=91721rank=66300 of ranks=91721rank=66400 of ranks=91721rank=66500 of ranks=91721rank=66600 of ranks=91721rank=66700 of ranks=91721rank=66800 of ranks=91721rank=66900 of ranks=91721rank=67000 of ranks=91721rank=67100 of ranks=91721rank=67200 of ranks=91721rank=67300 of ranks=91721rank=67400 of ranks=91721rank=67500 of ranks=91721rank=67600 of ranks=91721rank=67700 of ranks=91721rank=67800 of ranks=91721rank=67900 of ranks=91721rank=68000 of ranks=91721rank=68100 of ranks=91721rank=68200 of ranks=91721rank=68300 of ranks=91721rank=68400 of ranks=91721rank=68500 of ranks=91721rank=68600 of ranks=91721rank=68700 of ranks=91721rank=68800 of ranks=91721rank=68900 of ranks=91721rank=69000 of ranks=91721rank=69100 of ranks=91721rank=69200 of ranks=91721rank=69300 of ranks=91721rank=69400 of ranks=91721rank=69500 of ranks=91721rank=69600 of ranks=91721rank=69700 of ranks=91721rank=69800 of ranks=91721rank=69900 of ranks=91721rank=70000 of ranks=91721rank=70100 of ranks=91721rank=70200 of ranks=91721rank=70300 of ranks=91721rank=70400 of ranks=91721rank=70500 of ranks=91721rank=70600 of ranks=91721rank=70700 of ranks=91721rank=70800 of ranks=91721rank=70900 of ranks=91721rank=71000 of ranks=91721rank=71100 of ranks=91721rank=71200 of ranks=91721rank=71300 of ranks=91721rank=71400 of ranks=91721rank=71500 of ranks=91721rank=71600 of ranks=91721rank=71700 of ranks=91721rank=71800 of ranks=91721rank=71900 of ranks=91721rank=72000 of ranks=91721rank=72100 of ranks=91721rank=72200 of ranks=91721rank=72300 of ranks=91721rank=72400 of ranks=91721rank=72500 of ranks=91721rank=72600 of ranks=91721rank=72700 of ranks=91721rank=72800 of ranks=91721rank=72900 of ranks=91721rank=73000 of ranks=91721rank=73100 of ranks=91721rank=73200 of ranks=91721rank=73300 of ranks=91721rank=73400 of ranks=91721rank=73500 of ranks=91721rank=73600 of ranks=91721rank=73700 of ranks=91721rank=73800 of ranks=91721rank=73900 of ranks=91721rank=74000 of ranks=91721rank=74100 of ranks=91721rank=74200 of ranks=91721rank=74300 of ranks=91721rank=74400 of ranks=91721rank=74500 of ranks=91721rank=74600 of ranks=91721rank=74700 of ranks=91721rank=74800 of ranks=91721rank=74900 of ranks=91721rank=75000 of ranks=91721rank=75100 of ranks=91721rank=75200 of ranks=91721rank=75300 of ranks=91721rank=75400 of ranks=91721rank=75500 of ranks=91721rank=75600 of ranks=91721rank=75700 of ranks=91721rank=75800 of ranks=91721rank=75900 of ranks=91721rank=76000 of ranks=91721rank=76100 of ranks=91721rank=76200 of ranks=91721rank=76300 of ranks=91721rank=76400 of ranks=91721rank=76500 of ranks=91721rank=76600 of ranks=91721rank=76700 of ranks=91721rank=76800 of ranks=91721rank=76900 of ranks=91721rank=77000 of ranks=91721rank=77100 of ranks=91721rank=77200 of ranks=91721rank=77300 of ranks=91721rank=77400 of ranks=91721rank=77500 of ranks=91721rank=77600 of ranks=91721rank=77700 of ranks=91721rank=77800 of ranks=91721rank=77900 of ranks=91721rank=78000 of ranks=91721rank=78100 of ranks=91721rank=78200 of ranks=91721rank=78300 of ranks=91721rank=78400 of ranks=91721rank=78500 of ranks=91721rank=78600 of ranks=91721rank=78700 of ranks=91721rank=78800 of ranks=91721rank=78900 of ranks=91721rank=79000 of ranks=91721rank=79100 of ranks=91721rank=79200 of ranks=91721rank=79300 of ranks=91721rank=79400 of ranks=91721rank=79500 of ranks=91721rank=79600 of ranks=91721rank=79700 of ranks=91721rank=79800 of ranks=91721rank=79900 of ranks=91721rank=80000 of ranks=91721rank=80100 of ranks=91721rank=80200 of ranks=91721rank=80300 of ranks=91721rank=80400 of ranks=91721rank=80500 of ranks=91721rank=80600 of ranks=91721rank=80700 of ranks=91721rank=80800 of ranks=91721rank=80900 of ranks=91721rank=81000 of ranks=91721rank=81100 of ranks=91721rank=81200 of ranks=91721rank=81300 of ranks=91721rank=81400 of ranks=91721rank=81500 of ranks=91721rank=81600 of ranks=91721rank=81700 of ranks=91721rank=81800 of ranks=91721rank=81900 of ranks=91721rank=82000 of ranks=91721rank=82100 of ranks=91721rank=82200 of ranks=91721rank=82300 of ranks=91721rank=82400 of ranks=91721rank=82500 of ranks=91721rank=82600 of ranks=91721rank=82700 of ranks=91721rank=82800 of ranks=91721rank=82900 of ranks=91721rank=83000 of ranks=91721rank=83100 of ranks=91721rank=83200 of ranks=91721rank=83300 of ranks=91721rank=83400 of ranks=91721rank=83500 of ranks=91721rank=83600 of ranks=91721rank=83700 of ranks=91721rank=83800 of ranks=91721rank=83900 of ranks=91721rank=84000 of ranks=91721rank=84100 of ranks=91721rank=84200 of ranks=91721rank=84300 of ranks=91721rank=84400 of ranks=91721rank=84500 of ranks=91721rank=84600 of ranks=91721rank=84700 of ranks=91721rank=84800 of ranks=91721rank=84900 of ranks=91721rank=85000 of ranks=91721rank=85100 of ranks=91721rank=85200 of ranks=91721rank=85300 of ranks=91721rank=85400 of ranks=91721rank=85500 of ranks=91721rank=85600 of ranks=91721rank=85700 of ranks=91721rank=85800 of ranks=91721rank=85900 of ranks=91721rank=86000 of ranks=91721rank=86100 of ranks=91721rank=86200 of ranks=91721rank=86300 of ranks=91721rank=86400 of ranks=91721rank=86500 of ranks=91721rank=86600 of ranks=91721rank=86700 of ranks=91721rank=86800 of ranks=91721rank=86900 of ranks=91721rank=87000 of ranks=91721rank=87100 of ranks=91721rank=87200 of ranks=91721rank=87300 of ranks=91721rank=87400 of ranks=91721rank=87500 of ranks=91721rank=87600 of ranks=91721rank=87700 of ranks=91721rank=87800 of ranks=91721rank=87900 of ranks=91721rank=88000 of ranks=91721rank=88100 of ranks=91721rank=88200 of ranks=91721rank=88300 of ranks=91721rank=88400 of ranks=91721rank=88500 of ranks=91721rank=88600 of ranks=91721rank=88700 of ranks=91721rank=88800 of ranks=91721rank=88900 of ranks=91721rank=89000 of ranks=91721rank=89100 of ranks=91721rank=89200 of ranks=91721rank=89300 of ranks=91721rank=89400 of ranks=91721rank=89500 of ranks=91721rank=89600 of ranks=91721rank=89700 of ranks=91721rank=89800 of ranks=91721rank=89900 of ranks=91721rank=90000 of ranks=91721rank=90100 of ranks=91721rank=90200 of ranks=91721rank=90300 of ranks=91721rank=90400 of ranks=91721rank=90500 of ranks=91721rank=90600 of ranks=91721rank=90700 of ranks=91721rank=90800 of ranks=91721rank=90900 of ranks=91721rank=91000 of ranks=91721rank=91100 of ranks=91721rank=91200 of ranks=91721rank=91300 of ranks=91721rank=91400 of ranks=91721rank=91500 of ranks=91721rank=91600 of ranks=91721rank=91700 of ranks=91721

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              97.6157    490   1639      8    498           76.0766
   1 car                    98.7196  50022  23337    294  50316           83.9058
   2 truck                  98.1590   1810   2918     15   1825           76.4002
   3 bus                    96.4240    361   2123      5    366           72.7230
   4 pedestrian             96.7864   4160   4861     99   4259           75.6823

for conf_thresh=0.25, precision=0.91, recall=0.96, F1 score=0.94
for conf_thresh=0.25, TP=55256, FP=5219, FN=2008, average IoU=82.91%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=97.54%
Total detection time: 133 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
6685: loss=2.265, avg loss=2.522, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=651.8 milliseconds, train=2.1 seconds, 427840 images, time remaining=78.5 minutes
6686: loss=2.312, avg loss=2.501, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=743.6 milliseconds, train=2.1 seconds, 427904 images, time remaining=78.4 minutes
6687: loss=2.995, avg loss=2.550, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=550.1 milliseconds, train=2.1 seconds, 427968 images, time remaining=78.4 minutes
6688: loss=1.878, avg loss=2.483, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=732.2 milliseconds, train=2.1 seconds, 428032 images, time remaining=78.3 minutes
6689: loss=2.831, avg loss=2.518, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=754.8 milliseconds, train=2.1 seconds, 428096 images, time remaining=78.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6690: loss=2.038, avg loss=2.470, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=2.1 seconds, 428160 images, time remaining=78.2 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6691: loss=2.357, avg loss=2.459, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=3.9 seconds, 428224 images, time remaining=78.1 minutes
6692: loss=2.114, avg loss=2.424, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=714.2 milliseconds, train=3.9 seconds, 428288 images, time remaining=78.1 minutes
6693: loss=2.132, avg loss=2.395, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=667.7 milliseconds, train=3.9 seconds, 428352 images, time remaining=78 minutes
6694: loss=2.430, avg loss=2.398, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=3.9 seconds, 428416 images, time remaining=78 minutes
6695: loss=2.601, avg loss=2.419, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=3.9 seconds, 428480 images, time remaining=77.9 minutes
6696: loss=2.470, avg loss=2.424, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=518.9 milliseconds, train=3.9 seconds, 428544 images, time remaining=77.8 minutes
6697: loss=2.550, avg loss=2.436, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=634.8 milliseconds, train=3.9 seconds, 428608 images, time remaining=77.8 minutes
6698: loss=2.424, avg loss=2.435, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=690.3 milliseconds, train=3.9 seconds, 428672 images, time remaining=77.7 minutes
6699: loss=2.712, avg loss=2.463, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=824.7 milliseconds, train=3.9 seconds, 428736 images, time remaining=77.7 minutes
6700: loss=2.874, avg loss=2.504, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.3 seconds, train=3.9 seconds, 428800 images, time remaining=77.6 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6701: loss=1.978, avg loss=2.451, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.4 seconds, train=1.9 seconds, 428864 images, time remaining=77.5 minutes
6702: loss=2.331, avg loss=2.439, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=1.9 seconds, 428928 images, time remaining=77.5 minutes
6703: loss=1.988, avg loss=2.394, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=871.0 milliseconds, train=1.9 seconds, 428992 images, time remaining=77.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6704: loss=2.465, avg loss=2.401, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=1.9 seconds, 429056 images, time remaining=77.3 minutes
6705: loss=2.161, avg loss=2.377, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=759.0 milliseconds, train=1.9 seconds, 429120 images, time remaining=77.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6706: loss=2.033, avg loss=2.343, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=1.9 seconds, 429184 images, time remaining=77.2 minutes
6707: loss=3.134, avg loss=2.422, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.9 seconds, 429248 images, time remaining=77.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6708: loss=1.904, avg loss=2.370, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.7 seconds, train=1.9 seconds, 429312 images, time remaining=77.1 minutes
6709: loss=1.718, avg loss=2.305, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.9 seconds, 429376 images, time remaining=77 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6710: loss=1.964, avg loss=2.271, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.6 seconds, train=1.9 seconds, 429440 images, time remaining=77 minutes
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6711: loss=2.053, avg loss=2.249, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=965.2 milliseconds, train=2.0 seconds, 429504 images, time remaining=76.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6712: loss=1.742, avg loss=2.198, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.3 seconds, train=1.9 seconds, 429568 images, time remaining=76.9 minutes
6713: loss=2.024, avg loss=2.181, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=2.0 seconds, 429632 images, time remaining=76.8 minutes
6714: loss=2.430, avg loss=2.206, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=610.9 milliseconds, train=2.0 seconds, 429696 images, time remaining=76.7 minutes
6715: loss=2.810, avg loss=2.266, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=886.2 milliseconds, train=2.0 seconds, 429760 images, time remaining=76.6 minutes
6716: loss=1.886, avg loss=2.228, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=2.0 seconds, 429824 images, time remaining=76.6 minutes
6717: loss=2.500, avg loss=2.255, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=561.6 milliseconds, train=2.0 seconds, 429888 images, time remaining=76.5 minutes
6718: loss=2.351, avg loss=2.265, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.8 seconds, train=2.0 seconds, 429952 images, time remaining=76.4 minutes
6719: loss=2.268, avg loss=2.265, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=2.0 seconds, 430016 images, time remaining=76.4 minutes
6720: loss=1.947, avg loss=2.233, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=2.0 seconds, 430080 images, time remaining=76.3 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6721: loss=2.560, avg loss=2.266, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=3.4 seconds, 430144 images, time remaining=76.3 minutes
6722: loss=1.951, avg loss=2.235, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.8 seconds, train=3.4 seconds, 430208 images, time remaining=76.2 minutes
6723: loss=2.372, avg loss=2.248, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=900.4 milliseconds, train=3.4 seconds, 430272 images, time remaining=76.2 minutes
6724: loss=2.888, avg loss=2.312, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=3.4 seconds, 430336 images, time remaining=76.1 minutes
6725: loss=2.214, avg loss=2.302, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=695.0 milliseconds, train=3.4 seconds, 430400 images, time remaining=76 minutes
6726: loss=2.117, avg loss=2.284, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=810.6 milliseconds, train=3.4 seconds, 430464 images, time remaining=76 minutes
6727: loss=2.208, avg loss=2.276, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=944.3 milliseconds, train=3.4 seconds, 430528 images, time remaining=75.9 minutes
6728: loss=2.704, avg loss=2.319, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=880.2 milliseconds, train=3.4 seconds, 430592 images, time remaining=75.8 minutes
6729: loss=2.486, avg loss=2.336, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=997.2 milliseconds, train=3.4 seconds, 430656 images, time remaining=75.8 minutes
6730: loss=2.635, avg loss=2.366, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=857.6 milliseconds, train=3.4 seconds, 430720 images, time remaining=75.7 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146402000000
6731: loss=1.975, avg loss=2.327, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=772.6 milliseconds, train=1.4 seconds, 430784 images, time remaining=75.7 minutes
6732: loss=2.201, avg loss=2.314, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=650.2 milliseconds, train=1.4 seconds, 430848 images, time remaining=75.6 minutes
6733: loss=2.029, avg loss=2.286, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.4 seconds, 430912 images, time remaining=75.5 minutes
6734: loss=1.905, avg loss=2.248, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=770.9 milliseconds, train=1.4 seconds, 430976 images, time remaining=75.5 minutes
6735: loss=1.856, avg loss=2.208, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=683.3 milliseconds, train=1.4 seconds, 431040 images, time remaining=75.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6736: loss=2.153, avg loss=2.203, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.4 seconds, train=1.4 seconds, 431104 images, time remaining=75.3 minutes
6737: loss=2.358, avg loss=2.218, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=795.5 milliseconds, train=1.4 seconds, 431168 images, time remaining=75.3 minutes
6738: loss=2.730, avg loss=2.270, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=923.7 milliseconds, train=1.4 seconds, 431232 images, time remaining=75.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6739: loss=2.136, avg loss=2.256, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=1.4 seconds, 431296 images, time remaining=75.2 minutes
6740: loss=2.107, avg loss=2.241, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=1.4 seconds, 431360 images, time remaining=75.1 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6741: loss=2.622, avg loss=2.279, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=5.1 seconds, 431424 images, time remaining=75 minutes
6742: loss=2.945, avg loss=2.346, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=5.1 seconds, 431488 images, time remaining=75 minutes
6743: loss=2.745, avg loss=2.386, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=5.0 seconds, 431552 images, time remaining=74.9 minutes
6744: loss=2.422, avg loss=2.389, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=5.0 seconds, 431616 images, time remaining=74.9 minutes
6745: loss=2.982, avg loss=2.449, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=5.0 seconds, 431680 images, time remaining=74.8 minutes
6746: loss=2.884, avg loss=2.492, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.3 seconds, train=5.0 seconds, 431744 images, time remaining=74.8 minutes
6747: loss=2.811, avg loss=2.524, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.9 seconds, train=5.1 seconds, 431808 images, time remaining=74.7 minutes
6748: loss=2.817, avg loss=2.553, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=5.0 seconds, 431872 images, time remaining=74.7 minutes
6749: loss=2.938, avg loss=2.592, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=5.0 seconds, 431936 images, time remaining=74.6 minutes
6750: loss=2.458, avg loss=2.579, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=5.0 seconds, 432000 images, time remaining=74.5 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1458fa000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6751: loss=2.573, avg loss=2.578, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.8 seconds, train=1.4 seconds, 432064 images, time remaining=74.5 minutes
6752: loss=2.627, avg loss=2.583, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=1.5 seconds, 432128 images, time remaining=74.4 minutes
6753: loss=2.326, avg loss=2.557, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=753.9 milliseconds, train=1.4 seconds, 432192 images, time remaining=74.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6754: loss=2.024, avg loss=2.504, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=3.2 seconds, train=1.4 seconds, 432256 images, time remaining=74.3 minutes
6755: loss=2.522, avg loss=2.506, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=978.9 milliseconds, train=1.5 seconds, 432320 images, time remaining=74.2 minutes
6756: loss=2.835, avg loss=2.539, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=725.1 milliseconds, train=1.5 seconds, 432384 images, time remaining=74.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6757: loss=2.447, avg loss=2.529, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=1.4 seconds, 432448 images, time remaining=74.1 minutes
6758: loss=2.558, avg loss=2.532, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.4 seconds, 432512 images, time remaining=74 minutes
6759: loss=1.834, avg loss=2.462, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.4 seconds, 432576 images, time remaining=74 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6760: loss=1.952, avg loss=2.411, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=1.4 seconds, 432640 images, time remaining=73.9 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6761: loss=2.165, avg loss=2.387, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=666.7 milliseconds, train=2.1 seconds, 432704 images, time remaining=73.8 minutes
6762: loss=1.846, avg loss=2.333, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=672.3 milliseconds, train=2.1 seconds, 432768 images, time remaining=73.8 minutes
6763: loss=2.276, avg loss=2.327, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=780.1 milliseconds, train=2.1 seconds, 432832 images, time remaining=73.7 minutes
6764: loss=2.368, avg loss=2.331, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=631.4 milliseconds, train=2.1 seconds, 432896 images, time remaining=73.7 minutes
6765: loss=1.994, avg loss=2.297, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=2.1 seconds, 432960 images, time remaining=73.6 minutes
6766: loss=2.329, avg loss=2.301, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=2.1 seconds, 433024 images, time remaining=73.5 minutes
6767: loss=2.721, avg loss=2.343, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=841.1 milliseconds, train=2.1 seconds, 433088 images, time remaining=73.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6768: loss=2.085, avg loss=2.317, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.6 seconds, train=2.1 seconds, 433152 images, time remaining=73.4 minutes
6769: loss=2.086, avg loss=2.294, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=939.3 milliseconds, train=2.1 seconds, 433216 images, time remaining=73.3 minutes
6770: loss=2.143, avg loss=2.279, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=2.1 seconds, 433280 images, time remaining=73.3 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6771: loss=2.784, avg loss=2.329, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=4.6 seconds, 433344 images, time remaining=73.2 minutes
6772: loss=2.063, avg loss=2.303, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.7 seconds, train=4.6 seconds, 433408 images, time remaining=73.2 minutes
6773: loss=3.157, avg loss=2.388, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=930.5 milliseconds, train=4.6 seconds, 433472 images, time remaining=73.1 minutes
6774: loss=2.063, avg loss=2.356, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=566.6 milliseconds, train=4.6 seconds, 433536 images, time remaining=73 minutes
6775: loss=2.191, avg loss=2.339, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=624.1 milliseconds, train=4.6 seconds, 433600 images, time remaining=73 minutes
6776: loss=2.515, avg loss=2.357, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=488.4 milliseconds, train=4.6 seconds, 433664 images, time remaining=72.9 minutes
6777: loss=3.081, avg loss=2.429, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=566.3 milliseconds, train=4.6 seconds, 433728 images, time remaining=72.9 minutes
6778: loss=2.910, avg loss=2.477, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=673.2 milliseconds, train=4.7 seconds, 433792 images, time remaining=72.8 minutes
6779: loss=2.532, avg loss=2.483, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=851.3 milliseconds, train=4.6 seconds, 433856 images, time remaining=72.8 minutes
6780: loss=2.493, avg loss=2.484, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=985.6 milliseconds, train=4.6 seconds, 433920 images, time remaining=72.7 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463b2000000
6781: loss=1.888, avg loss=2.424, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=839.3 milliseconds, train=1.2 seconds, 433984 images, time remaining=72.6 minutes
6782: loss=2.402, avg loss=2.422, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=906.6 milliseconds, train=1.2 seconds, 434048 images, time remaining=72.6 minutes
6783: loss=2.803, avg loss=2.460, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=1.2 seconds, 434112 images, time remaining=72.5 minutes
6784: loss=2.361, avg loss=2.450, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 434176 images, time remaining=72.4 minutes
6785: loss=2.409, avg loss=2.446, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=884.8 milliseconds, train=1.2 seconds, 434240 images, time remaining=72.4 minutes
6786: loss=2.139, avg loss=2.415, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=881.2 milliseconds, train=1.2 seconds, 434304 images, time remaining=72.3 minutes
6787: loss=3.143, avg loss=2.488, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=561.2 milliseconds, train=1.2 seconds, 434368 images, time remaining=72.2 minutes
6788: loss=1.803, avg loss=2.420, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=620.4 milliseconds, train=1.2 seconds, 434432 images, time remaining=72.2 minutes
6789: loss=2.393, avg loss=2.417, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=747.1 milliseconds, train=1.2 seconds, 434496 images, time remaining=72.1 minutes
6790: loss=2.575, avg loss=2.433, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=575.4 milliseconds, train=1.2 seconds, 434560 images, time remaining=72 minutes
Resizing, random_coef=1.40, batch=4, 1056x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6791: loss=1.577, avg loss=2.347, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=505.4 milliseconds, train=3.3 seconds, 434624 images, time remaining=72 minutes
6792: loss=1.288, avg loss=2.241, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=746.2 milliseconds, train=3.3 seconds, 434688 images, time remaining=71.9 minutes
6793: loss=1.984, avg loss=2.216, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=926.3 milliseconds, train=3.3 seconds, 434752 images, time remaining=71.9 minutes
6794: loss=2.068, avg loss=2.201, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=787.9 milliseconds, train=3.3 seconds, 434816 images, time remaining=71.8 minutes
6795: loss=1.697, avg loss=2.150, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=954.4 milliseconds, train=3.3 seconds, 434880 images, time remaining=71.8 minutes
6796: loss=2.410, avg loss=2.176, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=934.5 milliseconds, train=3.4 seconds, 434944 images, time remaining=71.7 minutes
6797: loss=1.915, avg loss=2.150, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=766.3 milliseconds, train=3.3 seconds, 435008 images, time remaining=71.6 minutes
6798: loss=2.122, avg loss=2.147, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=694.2 milliseconds, train=3.3 seconds, 435072 images, time remaining=71.6 minutes
6799: loss=2.645, avg loss=2.197, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=960.9 milliseconds, train=3.3 seconds, 435136 images, time remaining=71.5 minutes
6800: loss=2.141, avg loss=2.191, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=697.8 milliseconds, train=3.3 seconds, 435200 images, time remaining=71.4 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6801: loss=2.045, avg loss=2.177, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=715.0 milliseconds, train=1.9 seconds, 435264 images, time remaining=71.4 minutes
6802: loss=2.759, avg loss=2.235, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=535.5 milliseconds, train=1.9 seconds, 435328 images, time remaining=71.3 minutes
6803: loss=2.073, avg loss=2.219, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.9 seconds, 435392 images, time remaining=71.3 minutes
6804: loss=2.647, avg loss=2.262, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.9 seconds, train=1.9 seconds, 435456 images, time remaining=71.2 minutes
6805: loss=2.507, avg loss=2.286, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=952.3 milliseconds, train=1.9 seconds, 435520 images, time remaining=71.1 minutes
6806: loss=1.963, avg loss=2.254, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=699.7 milliseconds, train=1.9 seconds, 435584 images, time remaining=71.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6807: loss=1.525, avg loss=2.181, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=1.9 seconds, 435648 images, time remaining=71 minutes
6808: loss=2.041, avg loss=2.167, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=1.9 seconds, 435712 images, time remaining=70.9 minutes
6809: loss=2.222, avg loss=2.173, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=744.5 milliseconds, train=1.9 seconds, 435776 images, time remaining=70.9 minutes
6810: loss=1.964, avg loss=2.152, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=819.9 milliseconds, train=1.9 seconds, 435840 images, time remaining=70.8 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6811: loss=2.694, avg loss=2.206, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.9 seconds, train=5.1 seconds, 435904 images, time remaining=70.8 minutes
6812: loss=2.445, avg loss=2.230, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=728.6 milliseconds, train=5.0 seconds, 435968 images, time remaining=70.7 minutes
6813: loss=2.451, avg loss=2.252, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=826.4 milliseconds, train=5.0 seconds, 436032 images, time remaining=70.7 minutes
6814: loss=2.644, avg loss=2.291, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=5.0 seconds, 436096 images, time remaining=70.6 minutes
6815: loss=3.114, avg loss=2.373, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=5.0 seconds, 436160 images, time remaining=70.6 minutes
6816: loss=3.071, avg loss=2.443, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=5.1 seconds, 436224 images, time remaining=70.5 minutes
6817: loss=2.457, avg loss=2.445, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=622.8 milliseconds, train=5.0 seconds, 436288 images, time remaining=70.4 minutes
6818: loss=3.259, avg loss=2.526, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=5.1 seconds, 436352 images, time remaining=70.4 minutes
6819: loss=2.223, avg loss=2.496, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=5.1 seconds, 436416 images, time remaining=70.3 minutes
6820: loss=2.811, avg loss=2.527, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=997.8 milliseconds, train=5.1 seconds, 436480 images, time remaining=70.3 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6821: loss=2.608, avg loss=2.535, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=3.6 seconds, 436544 images, time remaining=70.2 minutes
6822: loss=1.972, avg loss=2.479, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=3.6 seconds, 436608 images, time remaining=70.2 minutes
6823: loss=2.214, avg loss=2.453, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=3.6 seconds, 436672 images, time remaining=70.1 minutes
6824: loss=2.587, avg loss=2.466, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=3.6 seconds, 436736 images, time remaining=70.1 minutes
6825: loss=1.999, avg loss=2.419, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=3.6 seconds, 436800 images, time remaining=70 minutes
6826: loss=2.026, avg loss=2.380, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=3.6 seconds, 436864 images, time remaining=69.9 minutes
6827: loss=2.128, avg loss=2.355, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=3.6 seconds, 436928 images, time remaining=69.9 minutes
6828: loss=2.587, avg loss=2.378, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=3.6 seconds, 436992 images, time remaining=69.8 minutes
6829: loss=2.109, avg loss=2.351, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=3.0 seconds, train=3.6 seconds, 437056 images, time remaining=69.8 minutes
6830: loss=2.197, avg loss=2.336, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=3.6 seconds, 437120 images, time remaining=69.7 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1450dea00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6831: loss=3.156, avg loss=2.418, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=1.2 seconds, 437184 images, time remaining=69.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6832: loss=2.034, avg loss=2.379, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=1.2 seconds, 437248 images, time remaining=69.6 minutes
6833: loss=2.091, avg loss=2.351, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 437312 images, time remaining=69.5 minutes
6834: loss=2.417, avg loss=2.357, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=873.9 milliseconds, train=1.2 seconds, 437376 images, time remaining=69.4 minutes
6835: loss=2.622, avg loss=2.384, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.2 seconds, 437440 images, time remaining=69.3 minutes
6836: loss=3.044, avg loss=2.450, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=866.3 milliseconds, train=1.2 seconds, 437504 images, time remaining=69.3 minutes
6837: loss=2.589, avg loss=2.464, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=1.2 seconds, 437568 images, time remaining=69.2 minutes
6838: loss=2.492, avg loss=2.467, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=769.9 milliseconds, train=1.2 seconds, 437632 images, time remaining=69.2 minutes
6839: loss=2.150, avg loss=2.435, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=555.8 milliseconds, train=1.2 seconds, 437696 images, time remaining=69.1 minutes
6840: loss=2.220, avg loss=2.413, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.2 seconds, 437760 images, time remaining=69 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6841: loss=2.388, avg loss=2.411, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=986.3 milliseconds, train=3.9 seconds, 437824 images, time remaining=69 minutes
6842: loss=2.717, avg loss=2.441, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=998.6 milliseconds, train=3.9 seconds, 437888 images, time remaining=68.9 minutes
6843: loss=2.921, avg loss=2.489, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=799.9 milliseconds, train=3.9 seconds, 437952 images, time remaining=68.9 minutes
6844: loss=2.090, avg loss=2.450, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=912.1 milliseconds, train=3.9 seconds, 438016 images, time remaining=68.8 minutes
6845: loss=3.116, avg loss=2.516, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=3.9 seconds, 438080 images, time remaining=68.8 minutes
6846: loss=2.862, avg loss=2.551, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=718.7 milliseconds, train=3.9 seconds, 438144 images, time remaining=68.7 minutes
6847: loss=2.360, avg loss=2.532, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=871.3 milliseconds, train=3.9 seconds, 438208 images, time remaining=68.6 minutes
6848: loss=2.829, avg loss=2.561, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=694.0 milliseconds, train=3.9 seconds, 438272 images, time remaining=68.6 minutes
6849: loss=1.988, avg loss=2.504, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=670.0 milliseconds, train=3.9 seconds, 438336 images, time remaining=68.5 minutes
6850: loss=2.741, avg loss=2.528, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=626.4 milliseconds, train=3.9 seconds, 438400 images, time remaining=68.4 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145b60e00000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6851: loss=2.569, avg loss=2.532, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=1.2 seconds, 438464 images, time remaining=68.4 minutes
6852: loss=2.276, avg loss=2.506, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=554.7 milliseconds, train=1.2 seconds, 438528 images, time remaining=68.3 minutes
6853: loss=2.503, avg loss=2.506, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=533.2 milliseconds, train=1.2 seconds, 438592 images, time remaining=68.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6854: loss=2.476, avg loss=2.503, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=3.1 seconds, train=1.2 seconds, 438656 images, time remaining=68.2 minutes
6855: loss=2.198, avg loss=2.473, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=463.4 milliseconds, train=1.2 seconds, 438720 images, time remaining=68.1 minutes
6856: loss=2.917, avg loss=2.517, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=630.9 milliseconds, train=1.2 seconds, 438784 images, time remaining=68.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6857: loss=2.215, avg loss=2.487, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=1.2 seconds, 438848 images, time remaining=68 minutes
6858: loss=2.758, avg loss=2.514, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=610.8 milliseconds, train=1.2 seconds, 438912 images, time remaining=67.9 minutes
6859: loss=2.323, avg loss=2.495, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=1.2 seconds, 438976 images, time remaining=67.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6860: loss=2.393, avg loss=2.485, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.3 seconds, train=1.2 seconds, 439040 images, time remaining=67.8 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6861: loss=2.052, avg loss=2.441, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=610.2 milliseconds, train=3.4 seconds, 439104 images, time remaining=67.8 minutes
6862: loss=2.384, avg loss=2.436, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=626.1 milliseconds, train=3.4 seconds, 439168 images, time remaining=67.7 minutes
6863: loss=2.354, avg loss=2.427, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=881.6 milliseconds, train=3.4 seconds, 439232 images, time remaining=67.6 minutes
6864: loss=2.356, avg loss=2.420, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=658.0 milliseconds, train=3.4 seconds, 439296 images, time remaining=67.6 minutes
6865: loss=2.897, avg loss=2.468, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=3.4 seconds, 439360 images, time remaining=67.5 minutes
6866: loss=1.864, avg loss=2.408, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=3.4 seconds, 439424 images, time remaining=67.4 minutes
6867: loss=2.075, avg loss=2.374, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.6 seconds, train=3.4 seconds, 439488 images, time remaining=67.4 minutes
6868: loss=2.368, avg loss=2.374, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=671.8 milliseconds, train=3.4 seconds, 439552 images, time remaining=67.3 minutes
6869: loss=2.277, avg loss=2.364, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=3.4 seconds, 439616 images, time remaining=67.3 minutes
6870: loss=2.049, avg loss=2.333, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=872.3 milliseconds, train=3.4 seconds, 439680 images, time remaining=67.2 minutes
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6871: loss=2.232, avg loss=2.322, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.1 seconds, 439744 images, time remaining=67.1 minutes
6872: loss=2.086, avg loss=2.299, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.1 seconds, 439808 images, time remaining=67.1 minutes
6873: loss=2.032, avg loss=2.272, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=4.1 seconds, 439872 images, time remaining=67 minutes
6874: loss=2.547, avg loss=2.300, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=731.7 milliseconds, train=4.1 seconds, 439936 images, time remaining=67 minutes
6875: loss=2.827, avg loss=2.352, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=4.1 seconds, 440000 images, time remaining=66.9 minutes
6876: loss=2.515, avg loss=2.369, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=911.2 milliseconds, train=4.1 seconds, 440064 images, time remaining=66.9 minutes
6877: loss=2.345, avg loss=2.366, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=4.1 seconds, 440128 images, time remaining=66.8 minutes
6878: loss=1.720, avg loss=2.302, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=4.1 seconds, 440192 images, time remaining=66.8 minutes
6879: loss=2.224, avg loss=2.294, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.1 seconds, 440256 images, time remaining=66.7 minutes
6880: loss=2.805, avg loss=2.345, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=4.1 seconds, 440320 images, time remaining=66.6 minutes
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6881: loss=2.830, avg loss=2.393, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=3.2 seconds, train=5.0 seconds, 440384 images, time remaining=66.6 minutes
6882: loss=2.589, avg loss=2.413, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=5.0 seconds, 440448 images, time remaining=66.5 minutes
6883: loss=2.115, avg loss=2.383, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.9 seconds, 440512 images, time remaining=66.5 minutes
6884: loss=2.715, avg loss=2.416, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.7 seconds, train=4.9 seconds, 440576 images, time remaining=66.4 minutes
6885: loss=2.569, avg loss=2.432, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.9 seconds, 440640 images, time remaining=66.4 minutes
6886: loss=2.515, avg loss=2.440, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.7 seconds, train=4.9 seconds, 440704 images, time remaining=66.3 minutes
6887: loss=2.268, avg loss=2.423, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=4.9 seconds, 440768 images, time remaining=66.2 minutes
6888: loss=2.426, avg loss=2.423, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=684.6 milliseconds, train=4.9 seconds, 440832 images, time remaining=66.2 minutes
6889: loss=2.867, avg loss=2.467, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=819.2 milliseconds, train=4.9 seconds, 440896 images, time remaining=66.1 minutes
6890: loss=3.156, avg loss=2.536, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=879.0 milliseconds, train=4.9 seconds, 440960 images, time remaining=66.1 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145754000000
6891: loss=2.005, avg loss=2.483, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=987.5 milliseconds, train=1.2 seconds, 441024 images, time remaining=66 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6892: loss=3.142, avg loss=2.549, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.7 seconds, train=1.2 seconds, 441088 images, time remaining=66 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6893: loss=2.303, avg loss=2.524, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=1.2 seconds, 441152 images, time remaining=65.9 minutes
6894: loss=2.229, avg loss=2.495, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=902.8 milliseconds, train=1.2 seconds, 441216 images, time remaining=65.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6895: loss=2.082, avg loss=2.454, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=1.2 seconds, 441280 images, time remaining=65.8 minutes
6896: loss=2.227, avg loss=2.431, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 441344 images, time remaining=65.7 minutes
6897: loss=1.967, avg loss=2.385, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=702.7 milliseconds, train=1.2 seconds, 441408 images, time remaining=65.6 minutes
6898: loss=2.644, avg loss=2.410, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 441472 images, time remaining=65.6 minutes
6899: loss=2.676, avg loss=2.437, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=645.6 milliseconds, train=1.2 seconds, 441536 images, time remaining=65.5 minutes
6900: loss=2.657, avg loss=2.459, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=727.6 milliseconds, train=1.2 seconds, 441600 images, time remaining=65.4 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145766200000
6901: loss=2.512, avg loss=2.464, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=941.3 milliseconds, train=1.5 seconds, 441664 images, time remaining=65.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6902: loss=2.073, avg loss=2.425, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.9 seconds, train=1.5 seconds, 441728 images, time remaining=65.3 minutes
6903: loss=2.418, avg loss=2.424, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=749.9 milliseconds, train=1.5 seconds, 441792 images, time remaining=65.2 minutes
6904: loss=2.292, avg loss=2.411, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=1.5 seconds, 441856 images, time remaining=65.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6905: loss=1.835, avg loss=2.354, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.9 seconds, train=1.5 seconds, 441920 images, time remaining=65.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6906: loss=2.093, avg loss=2.328, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.3 seconds, train=1.5 seconds, 441984 images, time remaining=65 minutes
6907: loss=2.309, avg loss=2.326, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=972.2 milliseconds, train=1.5 seconds, 442048 images, time remaining=65 minutes
6908: loss=2.343, avg loss=2.327, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=862.3 milliseconds, train=1.5 seconds, 442112 images, time remaining=64.9 minutes
6909: loss=1.960, avg loss=2.291, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=1.5 seconds, 442176 images, time remaining=64.9 minutes
6910: loss=2.045, avg loss=2.266, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=960.7 milliseconds, train=1.5 seconds, 442240 images, time remaining=64.8 minutes
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6911: loss=2.609, avg loss=2.300, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=767.7 milliseconds, train=4.1 seconds, 442304 images, time remaining=64.8 minutes
6912: loss=2.256, avg loss=2.296, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=4.1 seconds, 442368 images, time remaining=64.7 minutes
6913: loss=1.703, avg loss=2.237, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=4.1 seconds, 442432 images, time remaining=64.6 minutes
6914: loss=2.383, avg loss=2.251, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=866.3 milliseconds, train=4.1 seconds, 442496 images, time remaining=64.6 minutes
6915: loss=2.382, avg loss=2.264, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=896.8 milliseconds, train=4.1 seconds, 442560 images, time remaining=64.5 minutes
6916: loss=2.246, avg loss=2.263, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=587.7 milliseconds, train=4.1 seconds, 442624 images, time remaining=64.4 minutes
6917: loss=2.315, avg loss=2.268, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=774.0 milliseconds, train=4.1 seconds, 442688 images, time remaining=64.4 minutes
6918: loss=2.091, avg loss=2.250, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=756.7 milliseconds, train=4.1 seconds, 442752 images, time remaining=64.3 minutes
6919: loss=2.471, avg loss=2.272, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.1 seconds, 442816 images, time remaining=64.3 minutes
6920: loss=2.230, avg loss=2.268, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=735.0 milliseconds, train=4.1 seconds, 442880 images, time remaining=64.2 minutes
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6921: loss=1.973, avg loss=2.238, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=616.7 milliseconds, train=2.0 seconds, 442944 images, time remaining=64.2 minutes
6922: loss=2.098, avg loss=2.224, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=642.7 milliseconds, train=2.0 seconds, 443008 images, time remaining=64.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6923: loss=2.588, avg loss=2.261, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.6 seconds, train=2.0 seconds, 443072 images, time remaining=64 minutes
6924: loss=2.353, avg loss=2.270, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=590.6 milliseconds, train=2.0 seconds, 443136 images, time remaining=64 minutes
6925: loss=2.443, avg loss=2.287, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=877.1 milliseconds, train=2.0 seconds, 443200 images, time remaining=63.9 minutes
6926: loss=2.445, avg loss=2.303, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.0 seconds, train=2.0 seconds, 443264 images, time remaining=63.8 minutes
6927: loss=2.316, avg loss=2.304, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=490.2 milliseconds, train=2.0 seconds, 443328 images, time remaining=63.8 minutes
6928: loss=1.788, avg loss=2.253, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=973.9 milliseconds, train=2.0 seconds, 443392 images, time remaining=63.7 minutes
6929: loss=1.968, avg loss=2.224, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=2.0 seconds, 443456 images, time remaining=63.6 minutes
6930: loss=1.906, avg loss=2.192, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=592.5 milliseconds, train=2.0 seconds, 443520 images, time remaining=63.6 minutes
Resizing, random_coef=1.40, batch=4, 768x576
GPU #0: allocating workspace: 4.2 GiB begins at 0x144a4e000000
6931: loss=2.318, avg loss=2.205, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=785.5 milliseconds, train=1.5 seconds, 443584 images, time remaining=63.5 minutes
6932: loss=2.456, avg loss=2.230, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=600.5 milliseconds, train=1.5 seconds, 443648 images, time remaining=63.5 minutes
6933: loss=2.159, avg loss=2.223, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=463.9 milliseconds, train=1.5 seconds, 443712 images, time remaining=63.4 minutes
6934: loss=1.763, avg loss=2.177, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.5 seconds, 443776 images, time remaining=63.3 minutes
6935: loss=2.356, avg loss=2.195, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=661.1 milliseconds, train=1.6 seconds, 443840 images, time remaining=63.3 minutes
6936: loss=2.179, avg loss=2.193, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=804.0 milliseconds, train=1.5 seconds, 443904 images, time remaining=63.2 minutes
6937: loss=2.080, avg loss=2.182, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=945.5 milliseconds, train=1.5 seconds, 443968 images, time remaining=63.1 minutes
6938: loss=2.110, avg loss=2.175, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=960.3 milliseconds, train=1.5 seconds, 444032 images, time remaining=63.1 minutes
6939: loss=2.060, avg loss=2.163, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=899.6 milliseconds, train=1.6 seconds, 444096 images, time remaining=63 minutes
6940: loss=2.406, avg loss=2.188, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=627.4 milliseconds, train=1.5 seconds, 444160 images, time remaining=63 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145c04000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6941: loss=2.433, avg loss=2.212, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=1.2 seconds, 444224 images, time remaining=62.9 minutes
6942: loss=1.862, avg loss=2.177, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=945.7 milliseconds, train=1.2 seconds, 444288 images, time remaining=62.8 minutes
6943: loss=2.131, avg loss=2.173, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=966.0 milliseconds, train=1.2 seconds, 444352 images, time remaining=62.7 minutes
6944: loss=2.199, avg loss=2.175, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=811.6 milliseconds, train=1.2 seconds, 444416 images, time remaining=62.7 minutes
6945: loss=2.334, avg loss=2.191, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=733.5 milliseconds, train=1.2 seconds, 444480 images, time remaining=62.6 minutes
6946: loss=2.482, avg loss=2.220, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=754.3 milliseconds, train=1.2 seconds, 444544 images, time remaining=62.6 minutes
6947: loss=2.261, avg loss=2.224, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=852.4 milliseconds, train=1.2 seconds, 444608 images, time remaining=62.5 minutes
6948: loss=2.380, avg loss=2.240, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=952.8 milliseconds, train=1.2 seconds, 444672 images, time remaining=62.4 minutes
6949: loss=2.582, avg loss=2.274, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=572.6 milliseconds, train=1.2 seconds, 444736 images, time remaining=62.4 minutes
6950: loss=3.062, avg loss=2.353, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=786.7 milliseconds, train=1.2 seconds, 444800 images, time remaining=62.3 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6951: loss=2.195, avg loss=2.337, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=698.2 milliseconds, train=2.1 seconds, 444864 images, time remaining=62.2 minutes
6952: loss=2.181, avg loss=2.321, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=692.0 milliseconds, train=2.1 seconds, 444928 images, time remaining=62.2 minutes
6953: loss=2.115, avg loss=2.301, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=2.1 seconds, 444992 images, time remaining=62.1 minutes
6954: loss=2.367, avg loss=2.307, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=612.8 milliseconds, train=2.1 seconds, 445056 images, time remaining=62 minutes
6955: loss=2.119, avg loss=2.289, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=924.1 milliseconds, train=2.1 seconds, 445120 images, time remaining=62 minutes
6956: loss=1.871, avg loss=2.247, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=920.0 milliseconds, train=2.1 seconds, 445184 images, time remaining=61.9 minutes
6957: loss=1.686, avg loss=2.191, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=682.0 milliseconds, train=2.1 seconds, 445248 images, time remaining=61.9 minutes
6958: loss=2.239, avg loss=2.196, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=2.1 seconds, 445312 images, time remaining=61.8 minutes
6959: loss=2.066, avg loss=2.183, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=642.9 milliseconds, train=2.1 seconds, 445376 images, time remaining=61.7 minutes
6960: loss=3.016, avg loss=2.266, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.8 seconds, train=2.1 seconds, 445440 images, time remaining=61.7 minutes
Resizing, random_coef=1.40, batch=4, 1216x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6961: loss=2.397, avg loss=2.279, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=4.5 seconds, 445504 images, time remaining=61.6 minutes
6962: loss=2.333, avg loss=2.284, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.7 seconds, train=4.4 seconds, 445568 images, time remaining=61.6 minutes
6963: loss=2.115, avg loss=2.267, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=780.8 milliseconds, train=4.4 seconds, 445632 images, time remaining=61.5 minutes
6964: loss=2.967, avg loss=2.337, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=4.4 seconds, 445696 images, time remaining=61.5 minutes
6965: loss=1.832, avg loss=2.287, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=982.2 milliseconds, train=4.4 seconds, 445760 images, time remaining=61.4 minutes
6966: loss=3.242, avg loss=2.382, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=4.4 seconds, 445824 images, time remaining=61.3 minutes
6967: loss=2.318, avg loss=2.376, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=509.4 milliseconds, train=4.4 seconds, 445888 images, time remaining=61.3 minutes
6968: loss=2.437, avg loss=2.382, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=710.3 milliseconds, train=4.4 seconds, 445952 images, time remaining=61.2 minutes
6969: loss=2.308, avg loss=2.375, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=4.4 seconds, 446016 images, time remaining=61.2 minutes
6970: loss=3.042, avg loss=2.441, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=802.4 milliseconds, train=4.4 seconds, 446080 images, time remaining=61.1 minutes
Resizing, random_coef=1.40, batch=4, 896x672
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
6971: loss=1.918, avg loss=2.389, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=1.8 seconds, 446144 images, time remaining=61 minutes
6972: loss=2.408, avg loss=2.391, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=651.3 milliseconds, train=1.8 seconds, 446208 images, time remaining=61 minutes
6973: loss=1.935, avg loss=2.345, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=609.7 milliseconds, train=1.8 seconds, 446272 images, time remaining=60.9 minutes
6974: loss=2.223, avg loss=2.333, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=1.8 seconds, 446336 images, time remaining=60.9 minutes
6975: loss=1.883, avg loss=2.288, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=1.8 seconds, 446400 images, time remaining=60.8 minutes
6976: loss=1.882, avg loss=2.247, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=653.9 milliseconds, train=1.8 seconds, 446464 images, time remaining=60.7 minutes
6977: loss=1.565, avg loss=2.179, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=790.0 milliseconds, train=1.8 seconds, 446528 images, time remaining=60.7 minutes
6978: loss=2.031, avg loss=2.164, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=812.4 milliseconds, train=1.8 seconds, 446592 images, time remaining=60.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6979: loss=1.932, avg loss=2.141, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=1.8 seconds, 446656 images, time remaining=60.5 minutes
6980: loss=2.407, avg loss=2.168, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=769.7 milliseconds, train=1.8 seconds, 446720 images, time remaining=60.5 minutes
Resizing, random_coef=1.40, batch=4, 1024x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
6981: loss=2.202, avg loss=2.171, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=3.2 seconds, 446784 images, time remaining=60.4 minutes
6982: loss=2.031, avg loss=2.157, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=956.0 milliseconds, train=3.3 seconds, 446848 images, time remaining=60.4 minutes
6983: loss=2.466, avg loss=2.188, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=738.6 milliseconds, train=3.3 seconds, 446912 images, time remaining=60.3 minutes
6984: loss=1.675, avg loss=2.137, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=856.4 milliseconds, train=3.3 seconds, 446976 images, time remaining=60.2 minutes
6985: loss=1.937, avg loss=2.117, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=801.1 milliseconds, train=3.3 seconds, 447040 images, time remaining=60.2 minutes
6986: loss=2.238, avg loss=2.129, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.4 seconds, train=3.3 seconds, 447104 images, time remaining=60.1 minutes
6987: loss=2.245, avg loss=2.141, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=702.3 milliseconds, train=3.2 seconds, 447168 images, time remaining=60.1 minutes
6988: loss=2.468, avg loss=2.173, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=973.8 milliseconds, train=3.3 seconds, 447232 images, time remaining=60 minutes
6989: loss=1.980, avg loss=2.154, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=902.7 milliseconds, train=3.3 seconds, 447296 images, time remaining=59.9 minutes
6990: loss=2.514, avg loss=2.190, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=3.3 seconds, 447360 images, time remaining=59.9 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14600e000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
6991: loss=2.069, avg loss=2.178, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=1.2 seconds, 447424 images, time remaining=59.8 minutes
6992: loss=2.372, avg loss=2.197, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=806.4 milliseconds, train=1.2 seconds, 447488 images, time remaining=59.8 minutes
6993: loss=2.806, avg loss=2.258, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=653.4 milliseconds, train=1.2 seconds, 447552 images, time remaining=59.7 minutes
6994: loss=1.705, avg loss=2.203, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=705.1 milliseconds, train=1.2 seconds, 447616 images, time remaining=59.6 minutes
6995: loss=2.161, avg loss=2.199, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=880.5 milliseconds, train=1.2 seconds, 447680 images, time remaining=59.6 minutes
6996: loss=1.861, avg loss=2.165, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=1.2 seconds, 447744 images, time remaining=59.5 minutes
6997: loss=2.091, avg loss=2.157, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=773.0 milliseconds, train=1.2 seconds, 447808 images, time remaining=59.4 minutes
6998: loss=2.610, avg loss=2.203, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=916.8 milliseconds, train=1.2 seconds, 447872 images, time remaining=59.4 minutes
6999: loss=2.379, avg loss=2.220, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=877.4 milliseconds, train=1.2 seconds, 447936 images, time remaining=59.3 minutes
7000: loss=2.303, avg loss=2.229, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=893.0 milliseconds, train=1.2 seconds, 448000 images, time remaining=59.2 minutes
Saving weights to /workspace/.cache/splits/combined_7000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7001: loss=2.642, avg loss=2.270, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=895.4 milliseconds, train=2.1 seconds, 448064 images, time remaining=59.2 minutes
7002: loss=2.445, avg loss=2.287, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=701.2 milliseconds, train=2.1 seconds, 448128 images, time remaining=59.1 minutes
7003: loss=2.302, avg loss=2.289, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=893.1 milliseconds, train=2.1 seconds, 448192 images, time remaining=59 minutes
7004: loss=2.034, avg loss=2.263, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=2.1 seconds, 448256 images, time remaining=59 minutes
7005: loss=2.259, avg loss=2.263, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=2.1 seconds, 448320 images, time remaining=58.9 minutes
7006: loss=1.757, avg loss=2.212, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=560.2 milliseconds, train=2.1 seconds, 448384 images, time remaining=58.9 minutes
7007: loss=2.362, avg loss=2.227, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=797.6 milliseconds, train=2.1 seconds, 448448 images, time remaining=58.8 minutes
7008: loss=2.022, avg loss=2.207, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.7 seconds, train=2.1 seconds, 448512 images, time remaining=58.8 minutes
7009: loss=2.343, avg loss=2.220, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=835.2 milliseconds, train=2.1 seconds, 448576 images, time remaining=58.7 minutes
7010: loss=2.462, avg loss=2.245, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=473.9 milliseconds, train=2.1 seconds, 448640 images, time remaining=58.6 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1463ac000000
7011: loss=2.405, avg loss=2.261, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=654.1 milliseconds, train=1.5 seconds, 448704 images, time remaining=58.5 minutes
7012: loss=1.776, avg loss=2.212, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=666.1 milliseconds, train=1.5 seconds, 448768 images, time remaining=58.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7013: loss=1.950, avg loss=2.186, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=1.5 seconds, 448832 images, time remaining=58.4 minutes
7014: loss=2.277, avg loss=2.195, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=783.2 milliseconds, train=1.5 seconds, 448896 images, time remaining=58.4 minutes
7015: loss=2.473, avg loss=2.223, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=748.8 milliseconds, train=1.5 seconds, 448960 images, time remaining=58.3 minutes
7016: loss=1.669, avg loss=2.168, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=631.1 milliseconds, train=1.5 seconds, 449024 images, time remaining=58.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7017: loss=2.346, avg loss=2.185, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=1.5 seconds, 449088 images, time remaining=58.2 minutes
7018: loss=2.223, avg loss=2.189, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=632.6 milliseconds, train=1.5 seconds, 449152 images, time remaining=58.1 minutes
7019: loss=2.322, avg loss=2.202, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=494.0 milliseconds, train=1.5 seconds, 449216 images, time remaining=58 minutes
7020: loss=2.044, avg loss=2.187, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=1.5 seconds, 449280 images, time remaining=58 minutes
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7021: loss=2.850, avg loss=2.253, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=797.9 milliseconds, train=4.9 seconds, 449344 images, time remaining=57.9 minutes
7022: loss=2.624, avg loss=2.290, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=4.9 seconds, 449408 images, time remaining=57.9 minutes
7023: loss=3.175, avg loss=2.379, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=4.8 seconds, 449472 images, time remaining=57.8 minutes
7024: loss=3.064, avg loss=2.447, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=4.9 seconds, 449536 images, time remaining=57.8 minutes
7025: loss=2.605, avg loss=2.463, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.5 seconds, train=4.8 seconds, 449600 images, time remaining=57.7 minutes
7026: loss=2.542, avg loss=2.471, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=4.8 seconds, 449664 images, time remaining=57.6 minutes
7027: loss=2.758, avg loss=2.499, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=4.8 seconds, 449728 images, time remaining=57.6 minutes
7028: loss=2.682, avg loss=2.518, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=694.1 milliseconds, train=4.8 seconds, 449792 images, time remaining=57.5 minutes
7029: loss=2.307, avg loss=2.497, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.6 seconds, train=4.8 seconds, 449856 images, time remaining=57.5 minutes
7030: loss=3.176, avg loss=2.565, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=965.6 milliseconds, train=4.9 seconds, 449920 images, time remaining=57.4 minutes
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7031: loss=2.785, avg loss=2.587, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=3.8 seconds, 449984 images, time remaining=57.4 minutes
7032: loss=2.683, avg loss=2.596, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=665.0 milliseconds, train=3.8 seconds, 450048 images, time remaining=57.3 minutes
7033: loss=2.292, avg loss=2.566, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=3.8 seconds, 450112 images, time remaining=57.3 minutes
7034: loss=2.356, avg loss=2.545, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.7 seconds, train=3.8 seconds, 450176 images, time remaining=57.2 minutes
7035: loss=3.098, avg loss=2.600, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=615.6 milliseconds, train=3.8 seconds, 450240 images, time remaining=57.1 minutes
7036: loss=2.297, avg loss=2.570, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=795.6 milliseconds, train=3.8 seconds, 450304 images, time remaining=57.1 minutes
7037: loss=2.414, avg loss=2.554, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=876.3 milliseconds, train=3.8 seconds, 450368 images, time remaining=57 minutes
7038: loss=2.725, avg loss=2.571, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=3.8 seconds, 450432 images, time remaining=57 minutes
7039: loss=1.906, avg loss=2.505, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.8 seconds, train=3.8 seconds, 450496 images, time remaining=56.9 minutes
7040: loss=2.319, avg loss=2.486, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=926.0 milliseconds, train=3.8 seconds, 450560 images, time remaining=56.8 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7041: loss=1.783, avg loss=2.416, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.9 seconds, train=1.9 seconds, 450624 images, time remaining=56.8 minutes
7042: loss=1.723, avg loss=2.347, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=881.9 milliseconds, train=1.9 seconds, 450688 images, time remaining=56.7 minutes
7043: loss=1.842, avg loss=2.296, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=924.3 milliseconds, train=1.9 seconds, 450752 images, time remaining=56.7 minutes
7044: loss=1.680, avg loss=2.235, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=854.7 milliseconds, train=1.9 seconds, 450816 images, time remaining=56.6 minutes
7045: loss=2.042, avg loss=2.215, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=591.3 milliseconds, train=1.9 seconds, 450880 images, time remaining=56.5 minutes
7046: loss=2.343, avg loss=2.228, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=729.7 milliseconds, train=1.9 seconds, 450944 images, time remaining=56.5 minutes
7047: loss=2.228, avg loss=2.228, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=593.9 milliseconds, train=1.9 seconds, 451008 images, time remaining=56.4 minutes
7048: loss=2.033, avg loss=2.209, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=473.7 milliseconds, train=1.9 seconds, 451072 images, time remaining=56.3 minutes
7049: loss=1.936, avg loss=2.181, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.6 seconds, train=1.9 seconds, 451136 images, time remaining=56.3 minutes
7050: loss=2.419, avg loss=2.205, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=433.6 milliseconds, train=1.9 seconds, 451200 images, time remaining=56.2 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7051: loss=3.135, avg loss=2.298, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=689.9 milliseconds, train=3.6 seconds, 451264 images, time remaining=56.2 minutes
7052: loss=2.433, avg loss=2.312, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=3.6 seconds, 451328 images, time remaining=56.1 minutes
7053: loss=2.174, avg loss=2.298, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.0 seconds, train=3.6 seconds, 451392 images, time remaining=56 minutes
7054: loss=2.509, avg loss=2.319, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=3.6 seconds, 451456 images, time remaining=56 minutes
7055: loss=2.139, avg loss=2.301, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.2 seconds, train=3.6 seconds, 451520 images, time remaining=55.9 minutes
7056: loss=1.694, avg loss=2.240, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=604.3 milliseconds, train=3.6 seconds, 451584 images, time remaining=55.9 minutes
7057: loss=1.913, avg loss=2.207, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=708.6 milliseconds, train=3.6 seconds, 451648 images, time remaining=55.8 minutes
7058: loss=2.386, avg loss=2.225, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.9 seconds, train=3.6 seconds, 451712 images, time remaining=55.8 minutes
7059: loss=2.422, avg loss=2.245, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=667.9 milliseconds, train=3.6 seconds, 451776 images, time remaining=55.7 minutes
7060: loss=2.778, avg loss=2.298, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.5 seconds, train=3.6 seconds, 451840 images, time remaining=55.6 minutes
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7061: loss=2.622, avg loss=2.331, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.9 seconds, train=4.1 seconds, 451904 images, time remaining=55.6 minutes
7062: loss=2.923, avg loss=2.390, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=766.1 milliseconds, train=4.1 seconds, 451968 images, time remaining=55.5 minutes
7063: loss=2.003, avg loss=2.351, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=805.9 milliseconds, train=4.1 seconds, 452032 images, time remaining=55.5 minutes
7064: loss=2.160, avg loss=2.332, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=653.3 milliseconds, train=4.1 seconds, 452096 images, time remaining=55.4 minutes
7065: loss=2.615, avg loss=2.360, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=4.1 seconds, 452160 images, time remaining=55.4 minutes
7066: loss=2.281, avg loss=2.352, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.7 seconds, train=4.1 seconds, 452224 images, time remaining=55.3 minutes
7067: loss=2.696, avg loss=2.387, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=4.1 seconds, 452288 images, time remaining=55.2 minutes
7068: loss=2.939, avg loss=2.442, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=540.0 milliseconds, train=4.1 seconds, 452352 images, time remaining=55.2 minutes
7069: loss=2.109, avg loss=2.409, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=797.3 milliseconds, train=4.1 seconds, 452416 images, time remaining=55.1 minutes
7070: loss=2.000, avg loss=2.368, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=664.4 milliseconds, train=4.1 seconds, 452480 images, time remaining=55.1 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146456000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7071: loss=2.106, avg loss=2.342, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.1 seconds, train=1.2 seconds, 452544 images, time remaining=55 minutes
7072: loss=2.228, avg loss=2.330, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=514.2 milliseconds, train=1.2 seconds, 452608 images, time remaining=54.9 minutes
7073: loss=2.686, avg loss=2.366, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=856.5 milliseconds, train=1.2 seconds, 452672 images, time remaining=54.9 minutes
7074: loss=2.499, avg loss=2.379, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=654.7 milliseconds, train=1.2 seconds, 452736 images, time remaining=54.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7075: loss=2.301, avg loss=2.371, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=1.2 seconds, 452800 images, time remaining=54.8 minutes
7076: loss=2.237, avg loss=2.358, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=630.8 milliseconds, train=1.2 seconds, 452864 images, time remaining=54.7 minutes
7077: loss=2.077, avg loss=2.330, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=799.0 milliseconds, train=1.2 seconds, 452928 images, time remaining=54.6 minutes
7078: loss=2.310, avg loss=2.328, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=497.4 milliseconds, train=1.2 seconds, 452992 images, time remaining=54.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7079: loss=2.646, avg loss=2.360, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.7 seconds, train=1.2 seconds, 453056 images, time remaining=54.5 minutes
7080: loss=2.345, avg loss=2.358, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=620.9 milliseconds, train=1.2 seconds, 453120 images, time remaining=54.4 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7081: loss=2.439, avg loss=2.366, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.9 seconds, train=4.5 seconds, 453184 images, time remaining=54.4 minutes
7082: loss=2.155, avg loss=2.345, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=951.6 milliseconds, train=4.5 seconds, 453248 images, time remaining=54.3 minutes
7083: loss=2.349, avg loss=2.346, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=554.0 milliseconds, train=4.5 seconds, 453312 images, time remaining=54.3 minutes
7084: loss=2.543, avg loss=2.365, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=2.2 seconds, train=4.5 seconds, 453376 images, time remaining=54.2 minutes
7085: loss=2.870, avg loss=2.416, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=595.7 milliseconds, train=4.5 seconds, 453440 images, time remaining=54.1 minutes
7086: loss=2.502, avg loss=2.424, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=4.5 seconds, 453504 images, time remaining=54.1 minutes
7087: loss=2.859, avg loss=2.468, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=759.9 milliseconds, train=4.5 seconds, 453568 images, time remaining=54 minutes
7088: loss=2.571, avg loss=2.478, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.1 seconds, train=4.5 seconds, 453632 images, time remaining=54 minutes
7089: loss=2.334, avg loss=2.464, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=1.3 seconds, train=4.5 seconds, 453696 images, time remaining=53.9 minutes
7090: loss=2.359, avg loss=2.453, last=97.54%, best=97.54%, next=7090, rate=0.00013000, load 64=927.3 milliseconds, train=4.5 seconds, 453760 images, time remaining=53.9 minutes
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=90357, unique_truth_count=57264
rank=0 of ranks=90357rank=100 of ranks=90357rank=200 of ranks=90357rank=300 of ranks=90357rank=400 of ranks=90357rank=500 of ranks=90357rank=600 of ranks=90357rank=700 of ranks=90357rank=800 of ranks=90357rank=900 of ranks=90357rank=1000 of ranks=90357rank=1100 of ranks=90357rank=1200 of ranks=90357rank=1300 of ranks=90357rank=1400 of ranks=90357rank=1500 of ranks=90357rank=1600 of ranks=90357rank=1700 of ranks=90357rank=1800 of ranks=90357rank=1900 of ranks=90357rank=2000 of ranks=90357rank=2100 of ranks=90357rank=2200 of ranks=90357rank=2300 of ranks=90357rank=2400 of ranks=90357rank=2500 of ranks=90357rank=2600 of ranks=90357rank=2700 of ranks=90357rank=2800 of ranks=90357rank=2900 of ranks=90357rank=3000 of ranks=90357rank=3100 of ranks=90357rank=3200 of ranks=90357rank=3300 of ranks=90357rank=3400 of ranks=90357rank=3500 of ranks=90357rank=3600 of ranks=90357rank=3700 of ranks=90357rank=3800 of ranks=90357rank=3900 of ranks=90357rank=4000 of ranks=90357rank=4100 of ranks=90357rank=4200 of ranks=90357rank=4300 of ranks=90357rank=4400 of ranks=90357rank=4500 of ranks=90357rank=4600 of ranks=90357rank=4700 of ranks=90357rank=4800 of ranks=90357rank=4900 of ranks=90357rank=5000 of ranks=90357rank=5100 of ranks=90357rank=5200 of ranks=90357rank=5300 of ranks=90357rank=5400 of ranks=90357rank=5500 of ranks=90357rank=5600 of ranks=90357rank=5700 of ranks=90357rank=5800 of ranks=90357rank=5900 of ranks=90357rank=6000 of ranks=90357rank=6100 of ranks=90357rank=6200 of ranks=90357rank=6300 of ranks=90357rank=6400 of ranks=90357rank=6500 of ranks=90357rank=6600 of ranks=90357rank=6700 of ranks=90357rank=6800 of ranks=90357rank=6900 of ranks=90357rank=7000 of ranks=90357rank=7100 of ranks=90357rank=7200 of ranks=90357rank=7300 of ranks=90357rank=7400 of ranks=90357rank=7500 of ranks=90357rank=7600 of ranks=90357rank=7700 of ranks=90357rank=7800 of ranks=90357rank=7900 of ranks=90357rank=8000 of ranks=90357rank=8100 of ranks=90357rank=8200 of ranks=90357rank=8300 of ranks=90357rank=8400 of ranks=90357rank=8500 of ranks=90357rank=8600 of ranks=90357rank=8700 of ranks=90357rank=8800 of ranks=90357rank=8900 of ranks=90357rank=9000 of ranks=90357rank=9100 of ranks=90357rank=9200 of ranks=90357rank=9300 of ranks=90357rank=9400 of ranks=90357rank=9500 of ranks=90357rank=9600 of ranks=90357rank=9700 of ranks=90357rank=9800 of ranks=90357rank=9900 of ranks=90357rank=10000 of ranks=90357rank=10100 of ranks=90357rank=10200 of ranks=90357rank=10300 of ranks=90357rank=10400 of ranks=90357rank=10500 of ranks=90357rank=10600 of ranks=90357rank=10700 of ranks=90357rank=10800 of ranks=90357rank=10900 of ranks=90357rank=11000 of ranks=90357rank=11100 of ranks=90357rank=11200 of ranks=90357rank=11300 of ranks=90357rank=11400 of ranks=90357rank=11500 of ranks=90357rank=11600 of ranks=90357rank=11700 of ranks=90357rank=11800 of ranks=90357rank=11900 of ranks=90357rank=12000 of ranks=90357rank=12100 of ranks=90357rank=12200 of ranks=90357rank=12300 of ranks=90357rank=12400 of ranks=90357rank=12500 of ranks=90357rank=12600 of ranks=90357rank=12700 of ranks=90357rank=12800 of ranks=90357rank=12900 of ranks=90357rank=13000 of ranks=90357rank=13100 of ranks=90357rank=13200 of ranks=90357rank=13300 of ranks=90357rank=13400 of ranks=90357rank=13500 of ranks=90357rank=13600 of ranks=90357rank=13700 of ranks=90357rank=13800 of ranks=90357rank=13900 of ranks=90357rank=14000 of ranks=90357rank=14100 of ranks=90357rank=14200 of ranks=90357rank=14300 of ranks=90357rank=14400 of ranks=90357rank=14500 of ranks=90357rank=14600 of ranks=90357rank=14700 of ranks=90357rank=14800 of ranks=90357rank=14900 of ranks=90357rank=15000 of ranks=90357rank=15100 of ranks=90357rank=15200 of ranks=90357rank=15300 of ranks=90357rank=15400 of ranks=90357rank=15500 of ranks=90357rank=15600 of ranks=90357rank=15700 of ranks=90357rank=15800 of ranks=90357rank=15900 of ranks=90357rank=16000 of ranks=90357rank=16100 of ranks=90357rank=16200 of ranks=90357rank=16300 of ranks=90357rank=16400 of ranks=90357rank=16500 of ranks=90357rank=16600 of ranks=90357rank=16700 of ranks=90357rank=16800 of ranks=90357rank=16900 of ranks=90357rank=17000 of ranks=90357rank=17100 of ranks=90357rank=17200 of ranks=90357rank=17300 of ranks=90357rank=17400 of ranks=90357rank=17500 of ranks=90357rank=17600 of ranks=90357rank=17700 of ranks=90357rank=17800 of ranks=90357rank=17900 of ranks=90357rank=18000 of ranks=90357rank=18100 of ranks=90357rank=18200 of ranks=90357rank=18300 of ranks=90357rank=18400 of ranks=90357rank=18500 of ranks=90357rank=18600 of ranks=90357rank=18700 of ranks=90357rank=18800 of ranks=90357rank=18900 of ranks=90357rank=19000 of ranks=90357rank=19100 of ranks=90357rank=19200 of ranks=90357rank=19300 of ranks=90357rank=19400 of ranks=90357rank=19500 of ranks=90357rank=19600 of ranks=90357rank=19700 of ranks=90357rank=19800 of ranks=90357rank=19900 of ranks=90357rank=20000 of ranks=90357rank=20100 of ranks=90357rank=20200 of ranks=90357rank=20300 of ranks=90357rank=20400 of ranks=90357rank=20500 of ranks=90357rank=20600 of ranks=90357rank=20700 of ranks=90357rank=20800 of ranks=90357rank=20900 of ranks=90357rank=21000 of ranks=90357rank=21100 of ranks=90357rank=21200 of ranks=90357rank=21300 of ranks=90357rank=21400 of ranks=90357rank=21500 of ranks=90357rank=21600 of ranks=90357rank=21700 of ranks=90357rank=21800 of ranks=90357rank=21900 of ranks=90357rank=22000 of ranks=90357rank=22100 of ranks=90357rank=22200 of ranks=90357rank=22300 of ranks=90357rank=22400 of ranks=90357rank=22500 of ranks=90357rank=22600 of ranks=90357rank=22700 of ranks=90357rank=22800 of ranks=90357rank=22900 of ranks=90357rank=23000 of ranks=90357rank=23100 of ranks=90357rank=23200 of ranks=90357rank=23300 of ranks=90357rank=23400 of ranks=90357rank=23500 of ranks=90357rank=23600 of ranks=90357rank=23700 of ranks=90357rank=23800 of ranks=90357rank=23900 of ranks=90357rank=24000 of ranks=90357rank=24100 of ranks=90357rank=24200 of ranks=90357rank=24300 of ranks=90357rank=24400 of ranks=90357rank=24500 of ranks=90357rank=24600 of ranks=90357rank=24700 of ranks=90357rank=24800 of ranks=90357rank=24900 of ranks=90357rank=25000 of ranks=90357rank=25100 of ranks=90357rank=25200 of ranks=90357rank=25300 of ranks=90357rank=25400 of ranks=90357rank=25500 of ranks=90357rank=25600 of ranks=90357rank=25700 of ranks=90357rank=25800 of ranks=90357rank=25900 of ranks=90357rank=26000 of ranks=90357rank=26100 of ranks=90357rank=26200 of ranks=90357rank=26300 of ranks=90357rank=26400 of ranks=90357rank=26500 of ranks=90357rank=26600 of ranks=90357rank=26700 of ranks=90357rank=26800 of ranks=90357rank=26900 of ranks=90357rank=27000 of ranks=90357rank=27100 of ranks=90357rank=27200 of ranks=90357rank=27300 of ranks=90357rank=27400 of ranks=90357rank=27500 of ranks=90357rank=27600 of ranks=90357rank=27700 of ranks=90357rank=27800 of ranks=90357rank=27900 of ranks=90357rank=28000 of ranks=90357rank=28100 of ranks=90357rank=28200 of ranks=90357rank=28300 of ranks=90357rank=28400 of ranks=90357rank=28500 of ranks=90357rank=28600 of ranks=90357rank=28700 of ranks=90357rank=28800 of ranks=90357rank=28900 of ranks=90357rank=29000 of ranks=90357rank=29100 of ranks=90357rank=29200 of ranks=90357rank=29300 of ranks=90357rank=29400 of ranks=90357rank=29500 of ranks=90357rank=29600 of ranks=90357rank=29700 of ranks=90357rank=29800 of ranks=90357rank=29900 of ranks=90357rank=30000 of ranks=90357rank=30100 of ranks=90357rank=30200 of ranks=90357rank=30300 of ranks=90357rank=30400 of ranks=90357rank=30500 of ranks=90357rank=30600 of ranks=90357rank=30700 of ranks=90357rank=30800 of ranks=90357rank=30900 of ranks=90357rank=31000 of ranks=90357rank=31100 of ranks=90357rank=31200 of ranks=90357rank=31300 of ranks=90357rank=31400 of ranks=90357rank=31500 of ranks=90357rank=31600 of ranks=90357rank=31700 of ranks=90357rank=31800 of ranks=90357rank=31900 of ranks=90357rank=32000 of ranks=90357rank=32100 of ranks=90357rank=32200 of ranks=90357rank=32300 of ranks=90357rank=32400 of ranks=90357rank=32500 of ranks=90357rank=32600 of ranks=90357rank=32700 of ranks=90357rank=32800 of ranks=90357rank=32900 of ranks=90357rank=33000 of ranks=90357rank=33100 of ranks=90357rank=33200 of ranks=90357rank=33300 of ranks=90357rank=33400 of ranks=90357rank=33500 of ranks=90357rank=33600 of ranks=90357rank=33700 of ranks=90357rank=33800 of ranks=90357rank=33900 of ranks=90357rank=34000 of ranks=90357rank=34100 of ranks=90357rank=34200 of ranks=90357rank=34300 of ranks=90357rank=34400 of ranks=90357rank=34500 of ranks=90357rank=34600 of ranks=90357rank=34700 of ranks=90357rank=34800 of ranks=90357rank=34900 of ranks=90357rank=35000 of ranks=90357rank=35100 of ranks=90357rank=35200 of ranks=90357rank=35300 of ranks=90357rank=35400 of ranks=90357rank=35500 of ranks=90357rank=35600 of ranks=90357rank=35700 of ranks=90357rank=35800 of ranks=90357rank=35900 of ranks=90357rank=36000 of ranks=90357rank=36100 of ranks=90357rank=36200 of ranks=90357rank=36300 of ranks=90357rank=36400 of ranks=90357rank=36500 of ranks=90357rank=36600 of ranks=90357rank=36700 of ranks=90357rank=36800 of ranks=90357rank=36900 of ranks=90357rank=37000 of ranks=90357rank=37100 of ranks=90357rank=37200 of ranks=90357rank=37300 of ranks=90357rank=37400 of ranks=90357rank=37500 of ranks=90357rank=37600 of ranks=90357rank=37700 of ranks=90357rank=37800 of ranks=90357rank=37900 of ranks=90357rank=38000 of ranks=90357rank=38100 of ranks=90357rank=38200 of ranks=90357rank=38300 of ranks=90357rank=38400 of ranks=90357rank=38500 of ranks=90357rank=38600 of ranks=90357rank=38700 of ranks=90357rank=38800 of ranks=90357rank=38900 of ranks=90357rank=39000 of ranks=90357rank=39100 of ranks=90357rank=39200 of ranks=90357rank=39300 of ranks=90357rank=39400 of ranks=90357rank=39500 of ranks=90357rank=39600 of ranks=90357rank=39700 of ranks=90357rank=39800 of ranks=90357rank=39900 of ranks=90357rank=40000 of ranks=90357rank=40100 of ranks=90357rank=40200 of ranks=90357rank=40300 of ranks=90357rank=40400 of ranks=90357rank=40500 of ranks=90357rank=40600 of ranks=90357rank=40700 of ranks=90357rank=40800 of ranks=90357rank=40900 of ranks=90357rank=41000 of ranks=90357rank=41100 of ranks=90357rank=41200 of ranks=90357rank=41300 of ranks=90357rank=41400 of ranks=90357rank=41500 of ranks=90357rank=41600 of ranks=90357rank=41700 of ranks=90357rank=41800 of ranks=90357rank=41900 of ranks=90357rank=42000 of ranks=90357rank=42100 of ranks=90357rank=42200 of ranks=90357rank=42300 of ranks=90357rank=42400 of ranks=90357rank=42500 of ranks=90357rank=42600 of ranks=90357rank=42700 of ranks=90357rank=42800 of ranks=90357rank=42900 of ranks=90357rank=43000 of ranks=90357rank=43100 of ranks=90357rank=43200 of ranks=90357rank=43300 of ranks=90357rank=43400 of ranks=90357rank=43500 of ranks=90357rank=43600 of ranks=90357rank=43700 of ranks=90357rank=43800 of ranks=90357rank=43900 of ranks=90357rank=44000 of ranks=90357rank=44100 of ranks=90357rank=44200 of ranks=90357rank=44300 of ranks=90357rank=44400 of ranks=90357rank=44500 of ranks=90357rank=44600 of ranks=90357rank=44700 of ranks=90357rank=44800 of ranks=90357rank=44900 of ranks=90357rank=45000 of ranks=90357rank=45100 of ranks=90357rank=45200 of ranks=90357rank=45300 of ranks=90357rank=45400 of ranks=90357rank=45500 of ranks=90357rank=45600 of ranks=90357rank=45700 of ranks=90357rank=45800 of ranks=90357rank=45900 of ranks=90357rank=46000 of ranks=90357rank=46100 of ranks=90357rank=46200 of ranks=90357rank=46300 of ranks=90357rank=46400 of ranks=90357rank=46500 of ranks=90357rank=46600 of ranks=90357rank=46700 of ranks=90357rank=46800 of ranks=90357rank=46900 of ranks=90357rank=47000 of ranks=90357rank=47100 of ranks=90357rank=47200 of ranks=90357rank=47300 of ranks=90357rank=47400 of ranks=90357rank=47500 of ranks=90357rank=47600 of ranks=90357rank=47700 of ranks=90357rank=47800 of ranks=90357rank=47900 of ranks=90357rank=48000 of ranks=90357rank=48100 of ranks=90357rank=48200 of ranks=90357rank=48300 of ranks=90357rank=48400 of ranks=90357rank=48500 of ranks=90357rank=48600 of ranks=90357rank=48700 of ranks=90357rank=48800 of ranks=90357rank=48900 of ranks=90357rank=49000 of ranks=90357rank=49100 of ranks=90357rank=49200 of ranks=90357rank=49300 of ranks=90357rank=49400 of ranks=90357rank=49500 of ranks=90357rank=49600 of ranks=90357rank=49700 of ranks=90357rank=49800 of ranks=90357rank=49900 of ranks=90357rank=50000 of ranks=90357rank=50100 of ranks=90357rank=50200 of ranks=90357rank=50300 of ranks=90357rank=50400 of ranks=90357rank=50500 of ranks=90357rank=50600 of ranks=90357rank=50700 of ranks=90357rank=50800 of ranks=90357rank=50900 of ranks=90357rank=51000 of ranks=90357rank=51100 of ranks=90357rank=51200 of ranks=90357rank=51300 of ranks=90357rank=51400 of ranks=90357rank=51500 of ranks=90357rank=51600 of ranks=90357rank=51700 of ranks=90357rank=51800 of ranks=90357rank=51900 of ranks=90357rank=52000 of ranks=90357rank=52100 of ranks=90357rank=52200 of ranks=90357rank=52300 of ranks=90357rank=52400 of ranks=90357rank=52500 of ranks=90357rank=52600 of ranks=90357rank=52700 of ranks=90357rank=52800 of ranks=90357rank=52900 of ranks=90357rank=53000 of ranks=90357rank=53100 of ranks=90357rank=53200 of ranks=90357rank=53300 of ranks=90357rank=53400 of ranks=90357rank=53500 of ranks=90357rank=53600 of ranks=90357rank=53700 of ranks=90357rank=53800 of ranks=90357rank=53900 of ranks=90357rank=54000 of ranks=90357rank=54100 of ranks=90357rank=54200 of ranks=90357rank=54300 of ranks=90357rank=54400 of ranks=90357rank=54500 of ranks=90357rank=54600 of ranks=90357rank=54700 of ranks=90357rank=54800 of ranks=90357rank=54900 of ranks=90357rank=55000 of ranks=90357rank=55100 of ranks=90357rank=55200 of ranks=90357rank=55300 of ranks=90357rank=55400 of ranks=90357rank=55500 of ranks=90357rank=55600 of ranks=90357rank=55700 of ranks=90357rank=55800 of ranks=90357rank=55900 of ranks=90357rank=56000 of ranks=90357rank=56100 of ranks=90357rank=56200 of ranks=90357rank=56300 of ranks=90357rank=56400 of ranks=90357rank=56500 of ranks=90357rank=56600 of ranks=90357rank=56700 of ranks=90357rank=56800 of ranks=90357rank=56900 of ranks=90357rank=57000 of ranks=90357rank=57100 of ranks=90357rank=57200 of ranks=90357rank=57300 of ranks=90357rank=57400 of ranks=90357rank=57500 of ranks=90357rank=57600 of ranks=90357rank=57700 of ranks=90357rank=57800 of ranks=90357rank=57900 of ranks=90357rank=58000 of ranks=90357rank=58100 of ranks=90357rank=58200 of ranks=90357rank=58300 of ranks=90357rank=58400 of ranks=90357rank=58500 of ranks=90357rank=58600 of ranks=90357rank=58700 of ranks=90357rank=58800 of ranks=90357rank=58900 of ranks=90357rank=59000 of ranks=90357rank=59100 of ranks=90357rank=59200 of ranks=90357rank=59300 of ranks=90357rank=59400 of ranks=90357rank=59500 of ranks=90357rank=59600 of ranks=90357rank=59700 of ranks=90357rank=59800 of ranks=90357rank=59900 of ranks=90357rank=60000 of ranks=90357rank=60100 of ranks=90357rank=60200 of ranks=90357rank=60300 of ranks=90357rank=60400 of ranks=90357rank=60500 of ranks=90357rank=60600 of ranks=90357rank=60700 of ranks=90357rank=60800 of ranks=90357rank=60900 of ranks=90357rank=61000 of ranks=90357rank=61100 of ranks=90357rank=61200 of ranks=90357rank=61300 of ranks=90357rank=61400 of ranks=90357rank=61500 of ranks=90357rank=61600 of ranks=90357rank=61700 of ranks=90357rank=61800 of ranks=90357rank=61900 of ranks=90357rank=62000 of ranks=90357rank=62100 of ranks=90357rank=62200 of ranks=90357rank=62300 of ranks=90357rank=62400 of ranks=90357rank=62500 of ranks=90357rank=62600 of ranks=90357rank=62700 of ranks=90357rank=62800 of ranks=90357rank=62900 of ranks=90357rank=63000 of ranks=90357rank=63100 of ranks=90357rank=63200 of ranks=90357rank=63300 of ranks=90357rank=63400 of ranks=90357rank=63500 of ranks=90357rank=63600 of ranks=90357rank=63700 of ranks=90357rank=63800 of ranks=90357rank=63900 of ranks=90357rank=64000 of ranks=90357rank=64100 of ranks=90357rank=64200 of ranks=90357rank=64300 of ranks=90357rank=64400 of ranks=90357rank=64500 of ranks=90357rank=64600 of ranks=90357rank=64700 of ranks=90357rank=64800 of ranks=90357rank=64900 of ranks=90357rank=65000 of ranks=90357rank=65100 of ranks=90357rank=65200 of ranks=90357rank=65300 of ranks=90357rank=65400 of ranks=90357rank=65500 of ranks=90357rank=65600 of ranks=90357rank=65700 of ranks=90357rank=65800 of ranks=90357rank=65900 of ranks=90357rank=66000 of ranks=90357rank=66100 of ranks=90357rank=66200 of ranks=90357rank=66300 of ranks=90357rank=66400 of ranks=90357rank=66500 of ranks=90357rank=66600 of ranks=90357rank=66700 of ranks=90357rank=66800 of ranks=90357rank=66900 of ranks=90357rank=67000 of ranks=90357rank=67100 of ranks=90357rank=67200 of ranks=90357rank=67300 of ranks=90357rank=67400 of ranks=90357rank=67500 of ranks=90357rank=67600 of ranks=90357rank=67700 of ranks=90357rank=67800 of ranks=90357rank=67900 of ranks=90357rank=68000 of ranks=90357rank=68100 of ranks=90357rank=68200 of ranks=90357rank=68300 of ranks=90357rank=68400 of ranks=90357rank=68500 of ranks=90357rank=68600 of ranks=90357rank=68700 of ranks=90357rank=68800 of ranks=90357rank=68900 of ranks=90357rank=69000 of ranks=90357rank=69100 of ranks=90357rank=69200 of ranks=90357rank=69300 of ranks=90357rank=69400 of ranks=90357rank=69500 of ranks=90357rank=69600 of ranks=90357rank=69700 of ranks=90357rank=69800 of ranks=90357rank=69900 of ranks=90357rank=70000 of ranks=90357rank=70100 of ranks=90357rank=70200 of ranks=90357rank=70300 of ranks=90357rank=70400 of ranks=90357rank=70500 of ranks=90357rank=70600 of ranks=90357rank=70700 of ranks=90357rank=70800 of ranks=90357rank=70900 of ranks=90357rank=71000 of ranks=90357rank=71100 of ranks=90357rank=71200 of ranks=90357rank=71300 of ranks=90357rank=71400 of ranks=90357rank=71500 of ranks=90357rank=71600 of ranks=90357rank=71700 of ranks=90357rank=71800 of ranks=90357rank=71900 of ranks=90357rank=72000 of ranks=90357rank=72100 of ranks=90357rank=72200 of ranks=90357rank=72300 of ranks=90357rank=72400 of ranks=90357rank=72500 of ranks=90357rank=72600 of ranks=90357rank=72700 of ranks=90357rank=72800 of ranks=90357rank=72900 of ranks=90357rank=73000 of ranks=90357rank=73100 of ranks=90357rank=73200 of ranks=90357rank=73300 of ranks=90357rank=73400 of ranks=90357rank=73500 of ranks=90357rank=73600 of ranks=90357rank=73700 of ranks=90357rank=73800 of ranks=90357rank=73900 of ranks=90357rank=74000 of ranks=90357rank=74100 of ranks=90357rank=74200 of ranks=90357rank=74300 of ranks=90357rank=74400 of ranks=90357rank=74500 of ranks=90357rank=74600 of ranks=90357rank=74700 of ranks=90357rank=74800 of ranks=90357rank=74900 of ranks=90357rank=75000 of ranks=90357rank=75100 of ranks=90357rank=75200 of ranks=90357rank=75300 of ranks=90357rank=75400 of ranks=90357rank=75500 of ranks=90357rank=75600 of ranks=90357rank=75700 of ranks=90357rank=75800 of ranks=90357rank=75900 of ranks=90357rank=76000 of ranks=90357rank=76100 of ranks=90357rank=76200 of ranks=90357rank=76300 of ranks=90357rank=76400 of ranks=90357rank=76500 of ranks=90357rank=76600 of ranks=90357rank=76700 of ranks=90357rank=76800 of ranks=90357rank=76900 of ranks=90357rank=77000 of ranks=90357rank=77100 of ranks=90357rank=77200 of ranks=90357rank=77300 of ranks=90357rank=77400 of ranks=90357rank=77500 of ranks=90357rank=77600 of ranks=90357rank=77700 of ranks=90357rank=77800 of ranks=90357rank=77900 of ranks=90357rank=78000 of ranks=90357rank=78100 of ranks=90357rank=78200 of ranks=90357rank=78300 of ranks=90357rank=78400 of ranks=90357rank=78500 of ranks=90357rank=78600 of ranks=90357rank=78700 of ranks=90357rank=78800 of ranks=90357rank=78900 of ranks=90357rank=79000 of ranks=90357rank=79100 of ranks=90357rank=79200 of ranks=90357rank=79300 of ranks=90357rank=79400 of ranks=90357rank=79500 of ranks=90357rank=79600 of ranks=90357rank=79700 of ranks=90357rank=79800 of ranks=90357rank=79900 of ranks=90357rank=80000 of ranks=90357rank=80100 of ranks=90357rank=80200 of ranks=90357rank=80300 of ranks=90357rank=80400 of ranks=90357rank=80500 of ranks=90357rank=80600 of ranks=90357rank=80700 of ranks=90357rank=80800 of ranks=90357rank=80900 of ranks=90357rank=81000 of ranks=90357rank=81100 of ranks=90357rank=81200 of ranks=90357rank=81300 of ranks=90357rank=81400 of ranks=90357rank=81500 of ranks=90357rank=81600 of ranks=90357rank=81700 of ranks=90357rank=81800 of ranks=90357rank=81900 of ranks=90357rank=82000 of ranks=90357rank=82100 of ranks=90357rank=82200 of ranks=90357rank=82300 of ranks=90357rank=82400 of ranks=90357rank=82500 of ranks=90357rank=82600 of ranks=90357rank=82700 of ranks=90357rank=82800 of ranks=90357rank=82900 of ranks=90357rank=83000 of ranks=90357rank=83100 of ranks=90357rank=83200 of ranks=90357rank=83300 of ranks=90357rank=83400 of ranks=90357rank=83500 of ranks=90357rank=83600 of ranks=90357rank=83700 of ranks=90357rank=83800 of ranks=90357rank=83900 of ranks=90357rank=84000 of ranks=90357rank=84100 of ranks=90357rank=84200 of ranks=90357rank=84300 of ranks=90357rank=84400 of ranks=90357rank=84500 of ranks=90357rank=84600 of ranks=90357rank=84700 of ranks=90357rank=84800 of ranks=90357rank=84900 of ranks=90357rank=85000 of ranks=90357rank=85100 of ranks=90357rank=85200 of ranks=90357rank=85300 of ranks=90357rank=85400 of ranks=90357rank=85500 of ranks=90357rank=85600 of ranks=90357rank=85700 of ranks=90357rank=85800 of ranks=90357rank=85900 of ranks=90357rank=86000 of ranks=90357rank=86100 of ranks=90357rank=86200 of ranks=90357rank=86300 of ranks=90357rank=86400 of ranks=90357rank=86500 of ranks=90357rank=86600 of ranks=90357rank=86700 of ranks=90357rank=86800 of ranks=90357rank=86900 of ranks=90357rank=87000 of ranks=90357rank=87100 of ranks=90357rank=87200 of ranks=90357rank=87300 of ranks=90357rank=87400 of ranks=90357rank=87500 of ranks=90357rank=87600 of ranks=90357rank=87700 of ranks=90357rank=87800 of ranks=90357rank=87900 of ranks=90357rank=88000 of ranks=90357rank=88100 of ranks=90357rank=88200 of ranks=90357rank=88300 of ranks=90357rank=88400 of ranks=90357rank=88500 of ranks=90357rank=88600 of ranks=90357rank=88700 of ranks=90357rank=88800 of ranks=90357rank=88900 of ranks=90357rank=89000 of ranks=90357rank=89100 of ranks=90357rank=89200 of ranks=90357rank=89300 of ranks=90357rank=89400 of ranks=90357rank=89500 of ranks=90357rank=89600 of ranks=90357rank=89700 of ranks=90357rank=89800 of ranks=90357rank=89900 of ranks=90357rank=90000 of ranks=90357rank=90100 of ranks=90357rank=90200 of ranks=90357rank=90300 of ranks=90357

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              97.8373    491    866      7    498           79.2539
   1 car                    98.7926  50036  23861    280  50316           83.7238
   2 truck                  98.5403   1813   2456     12   1825           78.2111
   3 bus                    97.1426    362   1713      4    366           73.6291
   4 pedestrian             97.2189   4177   4582     82   4259           76.9094

for conf_thresh=0.25, precision=0.91, recall=0.97, F1 score=0.94
for conf_thresh=0.25, TP=55394, FP=5364, FN=1870, average IoU=82.94%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=97.91%
Total detection time: 125 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7091: loss=1.999, avg loss=2.408, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=606.5 milliseconds, train=3.4 seconds, 453824 images, time remaining=54.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7092: loss=2.040, avg loss=2.371, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=3.4 seconds, train=3.4 seconds, 453888 images, time remaining=54 minutes
7093: loss=2.284, avg loss=2.362, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=671.3 milliseconds, train=3.4 seconds, 453952 images, time remaining=54 minutes
7094: loss=2.626, avg loss=2.389, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.5 seconds, train=3.4 seconds, 454016 images, time remaining=53.9 minutes
7095: loss=2.200, avg loss=2.370, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=979.8 milliseconds, train=3.4 seconds, 454080 images, time remaining=53.8 minutes
7096: loss=2.400, avg loss=2.373, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=875.1 milliseconds, train=3.4 seconds, 454144 images, time remaining=53.8 minutes
7097: loss=1.495, avg loss=2.285, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.2 seconds, train=3.4 seconds, 454208 images, time remaining=53.7 minutes
7098: loss=2.013, avg loss=2.258, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=866.8 milliseconds, train=3.4 seconds, 454272 images, time remaining=53.7 minutes
7099: loss=2.352, avg loss=2.267, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=507.4 milliseconds, train=3.4 seconds, 454336 images, time remaining=53.6 minutes
7100: loss=2.334, avg loss=2.274, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=3.4 seconds, 454400 images, time remaining=53.5 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7101: loss=2.029, avg loss=2.250, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.3 seconds, train=3.9 seconds, 454464 images, time remaining=53.5 minutes
7102: loss=2.161, avg loss=2.241, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.7 seconds, train=3.9 seconds, 454528 images, time remaining=53.4 minutes
7103: loss=2.647, avg loss=2.281, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.6 seconds, train=3.9 seconds, 454592 images, time remaining=53.4 minutes
7104: loss=1.947, avg loss=2.248, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.6 seconds, train=3.9 seconds, 454656 images, time remaining=53.3 minutes
7105: loss=2.317, avg loss=2.255, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=3.9 seconds, 454720 images, time remaining=53.2 minutes
7106: loss=2.458, avg loss=2.275, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=3.9 seconds, 454784 images, time remaining=53.2 minutes
7107: loss=2.083, avg loss=2.256, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=975.8 milliseconds, train=3.8 seconds, 454848 images, time remaining=53.1 minutes
7108: loss=2.522, avg loss=2.282, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=3.9 seconds, 454912 images, time remaining=53.1 minutes
7109: loss=2.654, avg loss=2.320, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=908.3 milliseconds, train=3.9 seconds, 454976 images, time remaining=53 minutes
7110: loss=2.429, avg loss=2.331, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=923.8 milliseconds, train=3.9 seconds, 455040 images, time remaining=53 minutes
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7111: loss=2.196, avg loss=2.317, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=2.1 seconds, 455104 images, time remaining=52.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7112: loss=1.984, avg loss=2.284, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.8 seconds, train=2.0 seconds, 455168 images, time remaining=52.8 minutes
7113: loss=1.826, avg loss=2.238, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=728.5 milliseconds, train=2.0 seconds, 455232 images, time remaining=52.8 minutes
7114: loss=2.222, avg loss=2.236, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=535.1 milliseconds, train=2.0 seconds, 455296 images, time remaining=52.7 minutes
7115: loss=1.558, avg loss=2.168, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=780.6 milliseconds, train=2.0 seconds, 455360 images, time remaining=52.7 minutes
7116: loss=2.421, avg loss=2.194, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=678.8 milliseconds, train=2.0 seconds, 455424 images, time remaining=52.6 minutes
7117: loss=2.263, avg loss=2.201, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.5 seconds, train=2.0 seconds, 455488 images, time remaining=52.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7118: loss=2.107, avg loss=2.191, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=2.0 seconds, 455552 images, time remaining=52.5 minutes
7119: loss=2.169, avg loss=2.189, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=901.1 milliseconds, train=2.0 seconds, 455616 images, time remaining=52.4 minutes
7120: loss=2.710, avg loss=2.241, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.2 seconds, train=2.0 seconds, 455680 images, time remaining=52.3 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7121: loss=2.334, avg loss=2.250, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=767.6 milliseconds, train=4.5 seconds, 455744 images, time remaining=52.3 minutes
7122: loss=2.246, avg loss=2.250, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=891.9 milliseconds, train=4.5 seconds, 455808 images, time remaining=52.2 minutes
7123: loss=2.256, avg loss=2.251, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=4.5 seconds, 455872 images, time remaining=52.2 minutes
7124: loss=2.581, avg loss=2.284, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=4.5 seconds, 455936 images, time remaining=52.1 minutes
7125: loss=2.151, avg loss=2.270, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.6 seconds, train=4.5 seconds, 456000 images, time remaining=52 minutes
7126: loss=2.004, avg loss=2.244, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=901.2 milliseconds, train=4.5 seconds, 456064 images, time remaining=52 minutes
7127: loss=2.128, avg loss=2.232, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=671.0 milliseconds, train=4.5 seconds, 456128 images, time remaining=51.9 minutes
7128: loss=2.707, avg loss=2.280, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.7 seconds, train=4.5 seconds, 456192 images, time remaining=51.9 minutes
7129: loss=2.809, avg loss=2.332, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=654.2 milliseconds, train=4.5 seconds, 456256 images, time remaining=51.8 minutes
7130: loss=2.453, avg loss=2.345, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=873.4 milliseconds, train=4.5 seconds, 456320 images, time remaining=51.8 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146368000000
7131: loss=1.943, avg loss=2.304, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=669.9 milliseconds, train=1.6 seconds, 456384 images, time remaining=51.7 minutes
7132: loss=1.827, avg loss=2.257, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=773.3 milliseconds, train=1.6 seconds, 456448 images, time remaining=51.7 minutes
7133: loss=2.344, avg loss=2.265, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=1.6 seconds, 456512 images, time remaining=51.6 minutes
7134: loss=1.729, avg loss=2.212, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=830.4 milliseconds, train=1.6 seconds, 456576 images, time remaining=51.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7135: loss=1.945, avg loss=2.185, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=1.6 seconds, 456640 images, time remaining=51.4 minutes
7136: loss=1.999, avg loss=2.166, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=932.8 milliseconds, train=1.6 seconds, 456704 images, time remaining=51.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7137: loss=2.632, avg loss=2.213, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.9 seconds, train=1.6 seconds, 456768 images, time remaining=51.3 minutes
7138: loss=1.746, avg loss=2.166, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=993.4 milliseconds, train=1.6 seconds, 456832 images, time remaining=51.3 minutes
7139: loss=2.063, avg loss=2.156, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=854.0 milliseconds, train=1.6 seconds, 456896 images, time remaining=51.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7140: loss=2.719, avg loss=2.212, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.7 seconds, train=1.6 seconds, 456960 images, time remaining=51.1 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146368000000
7141: loss=1.677, avg loss=2.159, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=852.7 milliseconds, train=1.6 seconds, 457024 images, time remaining=51.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7142: loss=2.046, avg loss=2.148, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=3.3 seconds, train=1.6 seconds, 457088 images, time remaining=51 minutes
7143: loss=1.893, avg loss=2.122, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=1.6 seconds, 457152 images, time remaining=51 minutes
7144: loss=2.086, avg loss=2.119, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.0 seconds, train=1.6 seconds, 457216 images, time remaining=50.9 minutes
7145: loss=1.838, avg loss=2.091, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=1.6 seconds, 457280 images, time remaining=50.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7146: loss=2.408, avg loss=2.122, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=1.6 seconds, 457344 images, time remaining=50.8 minutes
7147: loss=1.882, avg loss=2.098, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=663.8 milliseconds, train=1.6 seconds, 457408 images, time remaining=50.7 minutes
7148: loss=2.493, avg loss=2.138, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.5 seconds, train=1.6 seconds, 457472 images, time remaining=50.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7149: loss=2.232, avg loss=2.147, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.8 seconds, train=1.6 seconds, 457536 images, time remaining=50.6 minutes
7150: loss=2.156, avg loss=2.148, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.3 seconds, train=1.6 seconds, 457600 images, time remaining=50.5 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146368000000
7151: loss=2.082, avg loss=2.141, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=643.9 milliseconds, train=1.6 seconds, 457664 images, time remaining=50.5 minutes
7152: loss=2.520, avg loss=2.179, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=580.8 milliseconds, train=1.7 seconds, 457728 images, time remaining=50.4 minutes
7153: loss=2.592, avg loss=2.220, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=808.8 milliseconds, train=1.6 seconds, 457792 images, time remaining=50.3 minutes
7154: loss=1.852, avg loss=2.184, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=770.4 milliseconds, train=1.6 seconds, 457856 images, time remaining=50.3 minutes
7155: loss=2.511, avg loss=2.216, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=1.6 seconds, 457920 images, time remaining=50.2 minutes
7156: loss=1.876, avg loss=2.182, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=914.4 milliseconds, train=1.7 seconds, 457984 images, time remaining=50.1 minutes
7157: loss=2.097, avg loss=2.174, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=955.3 milliseconds, train=1.6 seconds, 458048 images, time remaining=50.1 minutes
7158: loss=2.712, avg loss=2.228, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=719.9 milliseconds, train=1.6 seconds, 458112 images, time remaining=50 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7159: loss=2.017, avg loss=2.207, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.8 seconds, train=1.6 seconds, 458176 images, time remaining=50 minutes
7160: loss=2.198, avg loss=2.206, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.3 seconds, train=1.6 seconds, 458240 images, time remaining=49.9 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7161: loss=2.383, avg loss=2.223, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=910.8 milliseconds, train=3.4 seconds, 458304 images, time remaining=49.8 minutes
7162: loss=2.390, avg loss=2.240, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=777.8 milliseconds, train=3.4 seconds, 458368 images, time remaining=49.8 minutes
7163: loss=2.646, avg loss=2.281, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=970.7 milliseconds, train=3.4 seconds, 458432 images, time remaining=49.7 minutes
7164: loss=2.629, avg loss=2.315, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=532.2 milliseconds, train=3.4 seconds, 458496 images, time remaining=49.6 minutes
7165: loss=2.140, avg loss=2.298, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.0 seconds, train=3.4 seconds, 458560 images, time remaining=49.6 minutes
7166: loss=2.191, avg loss=2.287, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.9 seconds, train=3.4 seconds, 458624 images, time remaining=49.5 minutes
7167: loss=2.480, avg loss=2.307, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=3.4 seconds, 458688 images, time remaining=49.5 minutes
7168: loss=2.697, avg loss=2.346, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=766.4 milliseconds, train=3.4 seconds, 458752 images, time remaining=49.4 minutes
7169: loss=2.169, avg loss=2.328, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.8 seconds, train=3.4 seconds, 458816 images, time remaining=49.4 minutes
7170: loss=1.809, avg loss=2.276, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=838.1 milliseconds, train=3.4 seconds, 458880 images, time remaining=49.3 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x145921e00000
7171: loss=2.190, avg loss=2.267, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.4 seconds, train=1.5 seconds, 458944 images, time remaining=49.2 minutes
7172: loss=2.154, avg loss=2.256, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.5 seconds, train=1.5 seconds, 459008 images, time remaining=49.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7173: loss=2.482, avg loss=2.279, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=4.3 seconds, train=1.5 seconds, 459072 images, time remaining=49.1 minutes
7174: loss=2.162, avg loss=2.267, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=570.6 milliseconds, train=1.5 seconds, 459136 images, time remaining=49 minutes
7175: loss=1.843, avg loss=2.225, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=647.8 milliseconds, train=1.5 seconds, 459200 images, time remaining=49 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7176: loss=2.180, avg loss=2.220, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.3 seconds, train=1.5 seconds, 459264 images, time remaining=48.9 minutes
7177: loss=1.900, avg loss=2.188, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=707.2 milliseconds, train=1.5 seconds, 459328 images, time remaining=48.9 minutes
7178: loss=1.636, avg loss=2.133, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=751.3 milliseconds, train=1.5 seconds, 459392 images, time remaining=48.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7179: loss=1.743, avg loss=2.094, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.5 seconds, train=1.5 seconds, 459456 images, time remaining=48.7 minutes
7180: loss=1.893, avg loss=2.074, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=798.9 milliseconds, train=1.5 seconds, 459520 images, time remaining=48.7 minutes
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7181: loss=2.177, avg loss=2.084, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=649.4 milliseconds, train=3.2 seconds, 459584 images, time remaining=48.6 minutes
7182: loss=2.886, avg loss=2.164, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=940.2 milliseconds, train=3.2 seconds, 459648 images, time remaining=48.5 minutes
7183: loss=2.238, avg loss=2.172, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.8 seconds, train=3.2 seconds, 459712 images, time remaining=48.5 minutes
7184: loss=2.305, avg loss=2.185, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=3.2 seconds, 459776 images, time remaining=48.4 minutes
7185: loss=1.989, avg loss=2.165, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=959.4 milliseconds, train=3.2 seconds, 459840 images, time remaining=48.4 minutes
7186: loss=1.933, avg loss=2.142, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.0 seconds, train=3.2 seconds, 459904 images, time remaining=48.3 minutes
7187: loss=2.144, avg loss=2.142, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=617.3 milliseconds, train=3.2 seconds, 459968 images, time remaining=48.2 minutes
7188: loss=1.913, avg loss=2.120, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.1 seconds, train=3.2 seconds, 460032 images, time remaining=48.2 minutes
7189: loss=2.032, avg loss=2.111, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=706.7 milliseconds, train=3.2 seconds, 460096 images, time remaining=48.1 minutes
7190: loss=1.985, avg loss=2.098, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=1.2 seconds, train=3.2 seconds, 460160 images, time remaining=48.1 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1463aa000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7191: loss=2.195, avg loss=2.108, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.1 seconds, train=1.7 seconds, 460224 images, time remaining=48 minutes
7192: loss=2.257, avg loss=2.123, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=861.1 milliseconds, train=1.7 seconds, 460288 images, time remaining=47.9 minutes
7193: loss=2.047, avg loss=2.115, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=694.5 milliseconds, train=1.7 seconds, 460352 images, time remaining=47.9 minutes
7194: loss=2.456, avg loss=2.149, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=790.3 milliseconds, train=1.6 seconds, 460416 images, time remaining=47.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7195: loss=1.795, avg loss=2.114, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=2.8 seconds, train=1.6 seconds, 460480 images, time remaining=47.8 minutes
7196: loss=1.927, avg loss=2.095, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=892.4 milliseconds, train=1.7 seconds, 460544 images, time remaining=47.7 minutes
7197: loss=2.295, avg loss=2.115, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=649.6 milliseconds, train=1.6 seconds, 460608 images, time remaining=47.6 minutes
7198: loss=2.194, avg loss=2.123, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=494.8 milliseconds, train=1.6 seconds, 460672 images, time remaining=47.6 minutes
7199: loss=1.504, avg loss=2.061, last=97.91%, best=97.91%, next=7496, rate=0.00013000, load 64=809.4 milliseconds, train=1.6 seconds, 460736 images, time remaining=47.5 minutes
7200: loss=2.062, avg loss=2.061, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=843.3 milliseconds, train=1.6 seconds, 460800 images, time remaining=47.4 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x1459d3a00000
7201: loss=2.025, avg loss=2.058, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=696.1 milliseconds, train=1.5 seconds, 460864 images, time remaining=47.4 minutes
7202: loss=2.537, avg loss=2.106, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=683.2 milliseconds, train=1.5 seconds, 460928 images, time remaining=47.3 minutes
7203: loss=2.128, avg loss=2.108, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=754.0 milliseconds, train=1.5 seconds, 460992 images, time remaining=47.3 minutes
7204: loss=2.409, avg loss=2.138, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=396.2 milliseconds, train=1.5 seconds, 461056 images, time remaining=47.2 minutes
7205: loss=1.892, avg loss=2.113, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=375.3 milliseconds, train=1.5 seconds, 461120 images, time remaining=47.1 minutes
7206: loss=2.129, avg loss=2.115, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=992.0 milliseconds, train=1.5 seconds, 461184 images, time remaining=47.1 minutes
7207: loss=2.040, avg loss=2.107, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=390.4 milliseconds, train=1.5 seconds, 461248 images, time remaining=47 minutes
7208: loss=2.149, avg loss=2.112, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=646.5 milliseconds, train=1.5 seconds, 461312 images, time remaining=46.9 minutes
7209: loss=2.133, avg loss=2.114, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=462.4 milliseconds, train=1.5 seconds, 461376 images, time remaining=46.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7210: loss=2.025, avg loss=2.105, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=1.5 seconds, 461440 images, time remaining=46.8 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7211: loss=2.567, avg loss=2.151, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=674.3 milliseconds, train=4.7 seconds, 461504 images, time remaining=46.8 minutes
7212: loss=3.173, avg loss=2.253, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=561.0 milliseconds, train=4.6 seconds, 461568 images, time remaining=46.7 minutes
7213: loss=2.828, avg loss=2.311, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=949.0 milliseconds, train=4.6 seconds, 461632 images, time remaining=46.7 minutes
7214: loss=2.986, avg loss=2.378, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=4.6 seconds, 461696 images, time remaining=46.6 minutes
7215: loss=2.547, avg loss=2.395, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=633.8 milliseconds, train=4.6 seconds, 461760 images, time remaining=46.5 minutes
7216: loss=2.504, avg loss=2.406, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=583.9 milliseconds, train=4.6 seconds, 461824 images, time remaining=46.5 minutes
7217: loss=2.605, avg loss=2.426, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.6 seconds, train=4.6 seconds, 461888 images, time remaining=46.4 minutes
7218: loss=2.810, avg loss=2.464, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=491.1 milliseconds, train=4.6 seconds, 461952 images, time remaining=46.4 minutes
7219: loss=2.327, avg loss=2.451, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=617.1 milliseconds, train=4.6 seconds, 462016 images, time remaining=46.3 minutes
7220: loss=2.613, avg loss=2.467, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=623.0 milliseconds, train=4.6 seconds, 462080 images, time remaining=46.2 minutes
Resizing, random_coef=1.40, batch=4, 1024x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7221: loss=2.363, avg loss=2.456, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=512.8 milliseconds, train=3.2 seconds, 462144 images, time remaining=46.2 minutes
7222: loss=2.809, avg loss=2.492, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=742.4 milliseconds, train=3.2 seconds, 462208 images, time remaining=46.1 minutes
7223: loss=1.779, avg loss=2.420, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=3.2 seconds, 462272 images, time remaining=46.1 minutes
7224: loss=2.128, avg loss=2.391, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=3.2 seconds, 462336 images, time remaining=46 minutes
7225: loss=2.005, avg loss=2.353, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.3 seconds, train=3.2 seconds, 462400 images, time remaining=45.9 minutes
7226: loss=1.878, avg loss=2.305, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=780.4 milliseconds, train=3.2 seconds, 462464 images, time remaining=45.9 minutes
7227: loss=2.028, avg loss=2.277, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=755.1 milliseconds, train=3.2 seconds, 462528 images, time remaining=45.8 minutes
7228: loss=2.837, avg loss=2.333, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=3.2 seconds, 462592 images, time remaining=45.8 minutes
7229: loss=1.950, avg loss=2.295, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=3.2 seconds, 462656 images, time remaining=45.7 minutes
7230: loss=1.893, avg loss=2.255, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=730.0 milliseconds, train=3.2 seconds, 462720 images, time remaining=45.7 minutes
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1463b0000000
7231: loss=2.396, avg loss=2.269, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=940.7 milliseconds, train=1.4 seconds, 462784 images, time remaining=45.6 minutes
7232: loss=1.792, avg loss=2.221, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=713.1 milliseconds, train=1.4 seconds, 462848 images, time remaining=45.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7233: loss=2.217, avg loss=2.221, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=1.4 seconds, 462912 images, time remaining=45.5 minutes
7234: loss=2.016, avg loss=2.200, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=514.0 milliseconds, train=1.4 seconds, 462976 images, time remaining=45.4 minutes
7235: loss=2.068, avg loss=2.187, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.4 seconds, 463040 images, time remaining=45.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7236: loss=1.685, avg loss=2.137, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.5 seconds, train=1.4 seconds, 463104 images, time remaining=45.3 minutes
7237: loss=2.351, avg loss=2.158, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=606.5 milliseconds, train=1.4 seconds, 463168 images, time remaining=45.2 minutes
7238: loss=2.129, avg loss=2.155, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=658.3 milliseconds, train=1.4 seconds, 463232 images, time remaining=45.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7239: loss=1.951, avg loss=2.135, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=1.4 seconds, 463296 images, time remaining=45.1 minutes
7240: loss=2.177, avg loss=2.139, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=927.2 milliseconds, train=1.4 seconds, 463360 images, time remaining=45 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7241: loss=2.635, avg loss=2.189, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=619.5 milliseconds, train=3.9 seconds, 463424 images, time remaining=45 minutes
7242: loss=2.311, avg loss=2.201, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=3.9 seconds, 463488 images, time remaining=44.9 minutes
7243: loss=2.369, avg loss=2.218, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=601.4 milliseconds, train=3.9 seconds, 463552 images, time remaining=44.9 minutes
7244: loss=2.427, avg loss=2.239, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=818.3 milliseconds, train=3.9 seconds, 463616 images, time remaining=44.8 minutes
7245: loss=2.054, avg loss=2.220, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.4 seconds, train=3.9 seconds, 463680 images, time remaining=44.8 minutes
7246: loss=2.832, avg loss=2.281, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=3.9 seconds, 463744 images, time remaining=44.7 minutes
7247: loss=2.175, avg loss=2.271, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=808.3 milliseconds, train=3.9 seconds, 463808 images, time remaining=44.6 minutes
7248: loss=2.420, avg loss=2.286, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=3.9 seconds, 463872 images, time remaining=44.6 minutes
7249: loss=2.809, avg loss=2.338, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=3.9 seconds, 463936 images, time remaining=44.5 minutes
7250: loss=2.539, avg loss=2.358, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=525.2 milliseconds, train=3.9 seconds, 464000 images, time remaining=44.5 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x1463b2000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7251: loss=2.512, avg loss=2.374, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=1.2 seconds, 464064 images, time remaining=44.4 minutes
7252: loss=1.724, avg loss=2.309, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=766.1 milliseconds, train=1.2 seconds, 464128 images, time remaining=44.3 minutes
7253: loss=2.671, avg loss=2.345, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=794.2 milliseconds, train=1.2 seconds, 464192 images, time remaining=44.3 minutes
7254: loss=2.031, avg loss=2.313, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=712.1 milliseconds, train=1.2 seconds, 464256 images, time remaining=44.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7255: loss=2.292, avg loss=2.311, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=1.2 seconds, 464320 images, time remaining=44.1 minutes
7256: loss=2.054, avg loss=2.286, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=886.1 milliseconds, train=1.2 seconds, 464384 images, time remaining=44.1 minutes
7257: loss=2.214, avg loss=2.278, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=518.7 milliseconds, train=1.2 seconds, 464448 images, time remaining=44 minutes
7258: loss=2.053, avg loss=2.256, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=760.2 milliseconds, train=1.2 seconds, 464512 images, time remaining=44 minutes
7259: loss=1.784, avg loss=2.209, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.2 seconds, 464576 images, time remaining=43.9 minutes
7260: loss=2.289, avg loss=2.217, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=483.2 milliseconds, train=1.2 seconds, 464640 images, time remaining=43.8 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x14575c000000
7261: loss=1.495, avg loss=2.144, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=601.6 milliseconds, train=1.9 seconds, 464704 images, time remaining=43.8 minutes
7262: loss=1.826, avg loss=2.113, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=685.4 milliseconds, train=1.9 seconds, 464768 images, time remaining=43.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7263: loss=2.001, avg loss=2.101, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=1.9 seconds, 464832 images, time remaining=43.6 minutes
7264: loss=2.209, avg loss=2.112, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=873.4 milliseconds, train=1.9 seconds, 464896 images, time remaining=43.6 minutes
7265: loss=1.894, avg loss=2.090, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=647.0 milliseconds, train=1.9 seconds, 464960 images, time remaining=43.5 minutes
7266: loss=1.864, avg loss=2.068, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.7 seconds, train=1.9 seconds, 465024 images, time remaining=43.5 minutes
7267: loss=2.111, avg loss=2.072, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=704.3 milliseconds, train=1.9 seconds, 465088 images, time remaining=43.4 minutes
7268: loss=2.097, avg loss=2.075, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=1.9 seconds, 465152 images, time remaining=43.3 minutes
7269: loss=2.111, avg loss=2.078, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=659.0 milliseconds, train=1.9 seconds, 465216 images, time remaining=43.3 minutes
7270: loss=1.863, avg loss=2.057, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=605.3 milliseconds, train=1.9 seconds, 465280 images, time remaining=43.2 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7271: loss=2.000, avg loss=2.051, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=496.2 milliseconds, train=3.6 seconds, 465344 images, time remaining=43.1 minutes
7272: loss=2.704, avg loss=2.116, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=658.7 milliseconds, train=3.6 seconds, 465408 images, time remaining=43.1 minutes
7273: loss=1.917, avg loss=2.096, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=3.6 seconds, 465472 images, time remaining=43 minutes
7274: loss=2.297, avg loss=2.116, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=828.8 milliseconds, train=3.6 seconds, 465536 images, time remaining=43 minutes
7275: loss=2.121, avg loss=2.117, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=800.3 milliseconds, train=3.6 seconds, 465600 images, time remaining=42.9 minutes
7276: loss=2.096, avg loss=2.115, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=870.2 milliseconds, train=3.6 seconds, 465664 images, time remaining=42.9 minutes
7277: loss=2.692, avg loss=2.173, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=3.6 seconds, 465728 images, time remaining=42.8 minutes
7278: loss=2.407, avg loss=2.196, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=3.6 seconds, 465792 images, time remaining=42.7 minutes
7279: loss=2.600, avg loss=2.236, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=511.5 milliseconds, train=3.6 seconds, 465856 images, time remaining=42.7 minutes
7280: loss=2.372, avg loss=2.250, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=515.6 milliseconds, train=3.6 seconds, 465920 images, time remaining=42.6 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145fc6000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7281: loss=2.623, avg loss=2.287, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.2 seconds, train=1.4 seconds, 465984 images, time remaining=42.6 minutes
7282: loss=1.776, avg loss=2.236, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=595.8 milliseconds, train=1.4 seconds, 466048 images, time remaining=42.5 minutes
7283: loss=2.245, avg loss=2.237, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=950.6 milliseconds, train=1.4 seconds, 466112 images, time remaining=42.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7284: loss=1.928, avg loss=2.206, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=1.4 seconds, 466176 images, time remaining=42.4 minutes
7285: loss=2.714, avg loss=2.257, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=853.0 milliseconds, train=1.4 seconds, 466240 images, time remaining=42.3 minutes
7286: loss=2.629, avg loss=2.294, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=604.5 milliseconds, train=1.4 seconds, 466304 images, time remaining=42.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7287: loss=2.083, avg loss=2.273, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=1.4 seconds, 466368 images, time remaining=42.2 minutes
7288: loss=1.932, avg loss=2.239, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=458.4 milliseconds, train=1.5 seconds, 466432 images, time remaining=42.1 minutes
7289: loss=1.903, avg loss=2.205, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=421.3 milliseconds, train=1.4 seconds, 466496 images, time remaining=42.1 minutes
7290: loss=1.966, avg loss=2.181, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.4 seconds, 466560 images, time remaining=42 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x14580e000000
7291: loss=1.551, avg loss=2.118, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=482.9 milliseconds, train=2.1 seconds, 466624 images, time remaining=41.9 minutes
7292: loss=2.235, avg loss=2.130, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=615.3 milliseconds, train=2.1 seconds, 466688 images, time remaining=41.9 minutes
7293: loss=1.840, avg loss=2.101, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=715.7 milliseconds, train=2.1 seconds, 466752 images, time remaining=41.8 minutes
7294: loss=2.019, avg loss=2.093, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=2.1 seconds, 466816 images, time remaining=41.8 minutes
7295: loss=1.875, avg loss=2.071, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=529.5 milliseconds, train=2.1 seconds, 466880 images, time remaining=41.7 minutes
7296: loss=1.550, avg loss=2.019, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=2.1 seconds, 466944 images, time remaining=41.6 minutes
7297: loss=1.769, avg loss=1.994, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=2.1 seconds, 467008 images, time remaining=41.6 minutes
7298: loss=2.252, avg loss=2.020, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=696.0 milliseconds, train=2.1 seconds, 467072 images, time remaining=41.5 minutes
7299: loss=1.862, avg loss=2.004, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=473.0 milliseconds, train=2.1 seconds, 467136 images, time remaining=41.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7300: loss=1.820, avg loss=1.986, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.2 seconds, train=2.1 seconds, 467200 images, time remaining=41.4 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7301: loss=2.462, avg loss=2.033, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.2 seconds, train=4.9 seconds, 467264 images, time remaining=41.3 minutes
7302: loss=2.206, avg loss=2.050, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.7 seconds, train=4.8 seconds, 467328 images, time remaining=41.3 minutes
7303: loss=3.155, avg loss=2.161, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=740.1 milliseconds, train=4.8 seconds, 467392 images, time remaining=41.2 minutes
7304: loss=2.461, avg loss=2.191, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=454.5 milliseconds, train=4.9 seconds, 467456 images, time remaining=41.2 minutes
7305: loss=2.512, avg loss=2.223, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=680.0 milliseconds, train=4.8 seconds, 467520 images, time remaining=41.1 minutes
7306: loss=2.582, avg loss=2.259, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=4.8 seconds, 467584 images, time remaining=41 minutes
7307: loss=2.712, avg loss=2.304, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=843.8 milliseconds, train=4.8 seconds, 467648 images, time remaining=41 minutes
7308: loss=2.559, avg loss=2.330, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=835.5 milliseconds, train=4.8 seconds, 467712 images, time remaining=40.9 minutes
7309: loss=2.317, avg loss=2.328, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=567.0 milliseconds, train=4.9 seconds, 467776 images, time remaining=40.9 minutes
7310: loss=2.424, avg loss=2.338, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=657.4 milliseconds, train=4.8 seconds, 467840 images, time remaining=40.8 minutes
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7311: loss=2.042, avg loss=2.308, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=2.0 seconds, 467904 images, time remaining=40.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7312: loss=2.167, avg loss=2.294, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=2.0 seconds, 467968 images, time remaining=40.7 minutes
7313: loss=2.025, avg loss=2.267, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=782.2 milliseconds, train=2.0 seconds, 468032 images, time remaining=40.6 minutes
7314: loss=2.912, avg loss=2.332, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=2.0 seconds, 468096 images, time remaining=40.6 minutes
7315: loss=1.785, avg loss=2.277, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=2.0 seconds, 468160 images, time remaining=40.5 minutes
7316: loss=1.759, avg loss=2.225, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=595.4 milliseconds, train=2.0 seconds, 468224 images, time remaining=40.4 minutes
7317: loss=2.184, avg loss=2.221, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=981.0 milliseconds, train=2.0 seconds, 468288 images, time remaining=40.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7318: loss=1.830, avg loss=2.182, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=2.0 seconds, 468352 images, time remaining=40.3 minutes
7319: loss=1.750, avg loss=2.139, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=948.7 milliseconds, train=2.0 seconds, 468416 images, time remaining=40.3 minutes
7320: loss=2.909, avg loss=2.216, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=570.5 milliseconds, train=2.0 seconds, 468480 images, time remaining=40.2 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7321: loss=2.775, avg loss=2.272, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=554.3 milliseconds, train=5.1 seconds, 468544 images, time remaining=40.2 minutes
7322: loss=3.172, avg loss=2.362, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=775.8 milliseconds, train=5.1 seconds, 468608 images, time remaining=40.1 minutes
7323: loss=3.291, avg loss=2.455, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=5.1 seconds, 468672 images, time remaining=40.1 minutes
7324: loss=2.684, avg loss=2.478, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=952.2 milliseconds, train=5.1 seconds, 468736 images, time remaining=40 minutes
7325: loss=1.959, avg loss=2.426, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.1 seconds, train=5.1 seconds, 468800 images, time remaining=39.9 minutes
7326: loss=1.865, avg loss=2.370, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=877.6 milliseconds, train=5.1 seconds, 468864 images, time remaining=39.9 minutes
7327: loss=2.711, avg loss=2.404, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=5.1 seconds, 468928 images, time remaining=39.8 minutes
7328: loss=2.971, avg loss=2.460, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=996.2 milliseconds, train=5.1 seconds, 468992 images, time remaining=39.8 minutes
7329: loss=2.958, avg loss=2.510, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=725.7 milliseconds, train=5.1 seconds, 469056 images, time remaining=39.7 minutes
7330: loss=2.274, avg loss=2.487, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=5.1 seconds, 469120 images, time remaining=39.7 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x1457f7a00000
7331: loss=1.775, avg loss=2.415, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.4 seconds, 469184 images, time remaining=39.6 minutes
7332: loss=2.018, avg loss=2.376, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=817.7 milliseconds, train=1.4 seconds, 469248 images, time remaining=39.5 minutes
7333: loss=1.956, avg loss=2.334, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=611.8 milliseconds, train=1.4 seconds, 469312 images, time remaining=39.5 minutes
7334: loss=1.746, avg loss=2.275, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=787.3 milliseconds, train=1.4 seconds, 469376 images, time remaining=39.4 minutes
7335: loss=2.320, avg loss=2.280, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.4 seconds, 469440 images, time remaining=39.3 minutes
7336: loss=1.775, avg loss=2.229, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=544.7 milliseconds, train=1.5 seconds, 469504 images, time remaining=39.3 minutes
7337: loss=1.922, avg loss=2.198, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=643.1 milliseconds, train=1.4 seconds, 469568 images, time remaining=39.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7338: loss=1.915, avg loss=2.170, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=1.4 seconds, 469632 images, time remaining=39.2 minutes
7339: loss=2.181, avg loss=2.171, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.4 seconds, 469696 images, time remaining=39.1 minutes
7340: loss=2.623, avg loss=2.216, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=1.4 seconds, 469760 images, time remaining=39 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146426000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7341: loss=2.028, avg loss=2.198, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=1.6 seconds, 469824 images, time remaining=39 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7342: loss=2.170, avg loss=2.195, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=4.1 seconds, train=1.6 seconds, 469888 images, time remaining=38.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7343: loss=2.092, avg loss=2.185, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.5 seconds, train=1.6 seconds, 469952 images, time remaining=38.8 minutes
7344: loss=2.366, avg loss=2.203, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=728.7 milliseconds, train=1.6 seconds, 470016 images, time remaining=38.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7345: loss=2.240, avg loss=2.206, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.8 seconds, train=1.6 seconds, 470080 images, time remaining=38.7 minutes
7346: loss=1.936, avg loss=2.179, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=799.0 milliseconds, train=1.6 seconds, 470144 images, time remaining=38.7 minutes
7347: loss=1.904, avg loss=2.152, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=447.1 milliseconds, train=1.6 seconds, 470208 images, time remaining=38.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7348: loss=1.622, avg loss=2.099, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=4.1 seconds, train=1.6 seconds, 470272 images, time remaining=38.5 minutes
7349: loss=2.250, avg loss=2.114, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=836.3 milliseconds, train=1.7 seconds, 470336 images, time remaining=38.5 minutes
7350: loss=2.202, avg loss=2.123, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=888.4 milliseconds, train=1.6 seconds, 470400 images, time remaining=38.4 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x14590c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7351: loss=2.544, avg loss=2.165, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.2 seconds, train=1.2 seconds, 470464 images, time remaining=38.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7352: loss=2.744, avg loss=2.223, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.2 seconds, 470528 images, time remaining=38.3 minutes
7353: loss=1.970, avg loss=2.197, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=994.6 milliseconds, train=1.2 seconds, 470592 images, time remaining=38.2 minutes
7354: loss=2.156, avg loss=2.193, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=867.3 milliseconds, train=1.2 seconds, 470656 images, time remaining=38.2 minutes
7355: loss=2.164, avg loss=2.190, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=532.6 milliseconds, train=1.2 seconds, 470720 images, time remaining=38.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7356: loss=1.923, avg loss=2.164, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=1.2 seconds, 470784 images, time remaining=38 minutes
7357: loss=2.594, avg loss=2.207, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.2 seconds, 470848 images, time remaining=38 minutes
7358: loss=2.300, avg loss=2.216, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=591.6 milliseconds, train=1.2 seconds, 470912 images, time remaining=37.9 minutes
7359: loss=1.989, avg loss=2.193, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=709.6 milliseconds, train=1.2 seconds, 470976 images, time remaining=37.9 minutes
7360: loss=1.929, avg loss=2.167, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.2 seconds, 471040 images, time remaining=37.8 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x146428000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7361: loss=2.179, avg loss=2.168, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=1.5 seconds, 471104 images, time remaining=37.7 minutes
7362: loss=2.116, avg loss=2.163, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=717.5 milliseconds, train=1.5 seconds, 471168 images, time remaining=37.7 minutes
7363: loss=2.093, avg loss=2.156, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=730.1 milliseconds, train=1.5 seconds, 471232 images, time remaining=37.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7364: loss=1.850, avg loss=2.125, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.4 seconds, train=1.5 seconds, 471296 images, time remaining=37.6 minutes
7365: loss=2.176, avg loss=2.130, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=1.5 seconds, 471360 images, time remaining=37.5 minutes
7366: loss=2.278, avg loss=2.145, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=699.7 milliseconds, train=1.6 seconds, 471424 images, time remaining=37.4 minutes
7367: loss=2.259, avg loss=2.156, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.5 seconds, 471488 images, time remaining=37.4 minutes
7368: loss=1.931, avg loss=2.134, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=722.1 milliseconds, train=1.5 seconds, 471552 images, time remaining=37.3 minutes
7369: loss=2.252, avg loss=2.146, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.5 seconds, 471616 images, time remaining=37.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7370: loss=1.799, avg loss=2.111, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=1.5 seconds, 471680 images, time remaining=37.2 minutes
Resizing, random_coef=1.40, batch=4, 704x544
GPU #0: allocating workspace: 289.6 MiB begins at 0x14648c000000
7371: loss=2.777, avg loss=2.178, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=910.2 milliseconds, train=1.1 seconds, 471744 images, time remaining=37.1 minutes
7372: loss=2.389, avg loss=2.199, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.1 seconds, 471808 images, time remaining=37.1 minutes
7373: loss=2.272, avg loss=2.206, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=963.5 milliseconds, train=1.1 seconds, 471872 images, time remaining=37 minutes
7374: loss=2.084, avg loss=2.194, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=825.5 milliseconds, train=1.1 seconds, 471936 images, time remaining=37 minutes
7375: loss=2.446, avg loss=2.219, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.1 seconds, 472000 images, time remaining=36.9 minutes
7376: loss=2.761, avg loss=2.273, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=784.3 milliseconds, train=1.1 seconds, 472064 images, time remaining=36.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7377: loss=1.982, avg loss=2.244, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.1 seconds, 472128 images, time remaining=36.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7378: loss=1.986, avg loss=2.218, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.8 seconds, train=1.1 seconds, 472192 images, time remaining=36.7 minutes
7379: loss=2.118, avg loss=2.208, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=983.1 milliseconds, train=1.2 seconds, 472256 images, time remaining=36.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7380: loss=2.209, avg loss=2.208, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=1.1 seconds, 472320 images, time remaining=36.6 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x1457fee00000
7381: loss=2.014, avg loss=2.189, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.6 seconds, 472384 images, time remaining=36.5 minutes
7382: loss=2.240, avg loss=2.194, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=570.8 milliseconds, train=1.6 seconds, 472448 images, time remaining=36.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7383: loss=1.963, avg loss=2.171, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.3 seconds, train=1.6 seconds, 472512 images, time remaining=36.4 minutes
7384: loss=1.665, avg loss=2.120, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=745.1 milliseconds, train=1.6 seconds, 472576 images, time remaining=36.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7385: loss=1.883, avg loss=2.097, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=1.6 seconds, 472640 images, time remaining=36.3 minutes
7386: loss=2.692, avg loss=2.156, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.6 seconds, 472704 images, time remaining=36.2 minutes
7387: loss=2.236, avg loss=2.164, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=733.1 milliseconds, train=1.6 seconds, 472768 images, time remaining=36.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7388: loss=1.910, avg loss=2.139, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.2 seconds, train=1.6 seconds, 472832 images, time remaining=36.1 minutes
7389: loss=1.542, avg loss=2.079, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=1.6 seconds, 472896 images, time remaining=36 minutes
7390: loss=2.454, avg loss=2.117, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=970.5 milliseconds, train=1.6 seconds, 472960 images, time remaining=36 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146334000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7391: loss=2.377, avg loss=2.143, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=1.6 seconds, 473024 images, time remaining=35.9 minutes
7392: loss=2.013, avg loss=2.130, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=1.6 seconds, 473088 images, time remaining=35.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7393: loss=1.726, avg loss=2.089, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.9 seconds, train=1.6 seconds, 473152 images, time remaining=35.8 minutes
7394: loss=2.080, avg loss=2.088, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.6 seconds, 473216 images, time remaining=35.7 minutes
7395: loss=2.142, avg loss=2.094, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.6 seconds, 473280 images, time remaining=35.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7396: loss=1.988, avg loss=2.083, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.6 seconds, train=1.6 seconds, 473344 images, time remaining=35.6 minutes
7397: loss=2.343, avg loss=2.109, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.6 seconds, 473408 images, time remaining=35.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7398: loss=2.005, avg loss=2.099, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.2 seconds, train=1.6 seconds, 473472 images, time remaining=35.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7399: loss=1.842, avg loss=2.073, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=1.6 seconds, 473536 images, time remaining=35.4 minutes
7400: loss=1.691, avg loss=2.035, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=1.6 seconds, 473600 images, time remaining=35.3 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146334000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7401: loss=1.778, avg loss=2.009, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=1.6 seconds, 473664 images, time remaining=35.3 minutes
7402: loss=1.984, avg loss=2.007, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.6 seconds, 473728 images, time remaining=35.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7403: loss=2.586, avg loss=2.065, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.6 seconds, train=1.6 seconds, 473792 images, time remaining=35.2 minutes
7404: loss=1.944, avg loss=2.053, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=610.8 milliseconds, train=1.6 seconds, 473856 images, time remaining=35.1 minutes
7405: loss=2.770, avg loss=2.124, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=991.6 milliseconds, train=1.6 seconds, 473920 images, time remaining=35 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7406: loss=1.923, avg loss=2.104, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=1.6 seconds, 473984 images, time remaining=35 minutes
7407: loss=1.946, avg loss=2.088, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.6 seconds, 474048 images, time remaining=34.9 minutes
7408: loss=2.590, avg loss=2.138, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=979.3 milliseconds, train=1.7 seconds, 474112 images, time remaining=34.9 minutes
7409: loss=1.853, avg loss=2.110, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.6 seconds, 474176 images, time remaining=34.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7410: loss=2.257, avg loss=2.125, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.6 seconds, train=1.6 seconds, 474240 images, time remaining=34.8 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x146336000000
7411: loss=2.271, avg loss=2.139, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=994.7 milliseconds, train=1.5 seconds, 474304 images, time remaining=34.7 minutes
7412: loss=2.038, avg loss=2.129, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=656.0 milliseconds, train=1.5 seconds, 474368 images, time remaining=34.6 minutes
7413: loss=2.457, avg loss=2.162, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=919.4 milliseconds, train=1.5 seconds, 474432 images, time remaining=34.6 minutes
7414: loss=1.877, avg loss=2.133, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=1.5 seconds, 474496 images, time remaining=34.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7415: loss=2.189, avg loss=2.139, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=1.5 seconds, 474560 images, time remaining=34.4 minutes
7416: loss=2.301, avg loss=2.155, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=1.5 seconds, 474624 images, time remaining=34.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7417: loss=1.953, avg loss=2.135, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=4.0 seconds, train=1.5 seconds, 474688 images, time remaining=34.3 minutes
7418: loss=2.594, avg loss=2.181, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=544.1 milliseconds, train=1.5 seconds, 474752 images, time remaining=34.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7419: loss=1.818, avg loss=2.145, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.9 seconds, train=1.5 seconds, 474816 images, time remaining=34.2 minutes
7420: loss=2.221, avg loss=2.152, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=871.7 milliseconds, train=1.5 seconds, 474880 images, time remaining=34.1 minutes
Resizing, random_coef=1.40, batch=4, 928x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145b68000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7421: loss=1.862, avg loss=2.123, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.1 seconds, train=2.0 seconds, 474944 images, time remaining=34.1 minutes
7422: loss=1.747, avg loss=2.086, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=2.0 seconds, 475008 images, time remaining=34 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7423: loss=1.849, avg loss=2.062, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.2 seconds, train=2.0 seconds, 475072 images, time remaining=34 minutes
7424: loss=1.790, avg loss=2.035, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=860.2 milliseconds, train=2.0 seconds, 475136 images, time remaining=33.9 minutes
7425: loss=2.322, avg loss=2.064, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=2.1 seconds, 475200 images, time remaining=33.8 minutes
7426: loss=2.249, avg loss=2.082, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=657.9 milliseconds, train=2.0 seconds, 475264 images, time remaining=33.8 minutes
7427: loss=2.178, avg loss=2.092, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=2.0 seconds, 475328 images, time remaining=33.7 minutes
7428: loss=1.686, avg loss=2.051, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=966.0 milliseconds, train=2.0 seconds, 475392 images, time remaining=33.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7429: loss=2.238, avg loss=2.070, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.9 seconds, train=2.0 seconds, 475456 images, time remaining=33.6 minutes
7430: loss=2.643, avg loss=2.127, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=903.2 milliseconds, train=2.0 seconds, 475520 images, time remaining=33.5 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7431: loss=2.630, avg loss=2.177, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.8 seconds, train=3.9 seconds, 475584 images, time remaining=33.5 minutes
7432: loss=2.714, avg loss=2.231, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=3.9 seconds, 475648 images, time remaining=33.4 minutes
7433: loss=2.185, avg loss=2.227, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.6 seconds, train=3.9 seconds, 475712 images, time remaining=33.4 minutes
7434: loss=3.208, avg loss=2.325, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.3 seconds, train=3.9 seconds, 475776 images, time remaining=33.3 minutes
7435: loss=2.267, avg loss=2.319, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=715.4 milliseconds, train=3.9 seconds, 475840 images, time remaining=33.2 minutes
7436: loss=2.872, avg loss=2.374, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=3.9 seconds, 475904 images, time remaining=33.2 minutes
7437: loss=2.443, avg loss=2.381, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=3.9 seconds, 475968 images, time remaining=33.1 minutes
7438: loss=2.866, avg loss=2.430, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.1 seconds, train=3.9 seconds, 476032 images, time remaining=33.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7439: loss=2.502, avg loss=2.437, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=4.1 seconds, train=3.9 seconds, 476096 images, time remaining=33 minutes
7440: loss=2.833, avg loss=2.476, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=978.4 milliseconds, train=3.9 seconds, 476160 images, time remaining=33 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145c94000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7441: loss=2.079, avg loss=2.437, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.0 seconds, train=1.6 seconds, 476224 images, time remaining=32.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7442: loss=1.955, avg loss=2.389, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=1.6 seconds, 476288 images, time remaining=32.8 minutes
7443: loss=2.238, avg loss=2.374, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=914.2 milliseconds, train=1.6 seconds, 476352 images, time remaining=32.8 minutes
7444: loss=2.512, avg loss=2.387, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=1.6 seconds, 476416 images, time remaining=32.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7445: loss=1.795, avg loss=2.328, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.9 seconds, train=1.6 seconds, 476480 images, time remaining=32.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7446: loss=1.747, avg loss=2.270, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.7 seconds, train=1.6 seconds, 476544 images, time remaining=32.6 minutes
7447: loss=1.746, avg loss=2.218, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.2 seconds, train=1.6 seconds, 476608 images, time remaining=32.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7448: loss=1.985, avg loss=2.194, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.3 seconds, train=1.6 seconds, 476672 images, time remaining=32.5 minutes
7449: loss=1.794, avg loss=2.154, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.6 seconds, train=1.6 seconds, 476736 images, time remaining=32.4 minutes
7450: loss=1.818, avg loss=2.121, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=1.6 seconds, 476800 images, time remaining=32.3 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x145c96000000
7451: loss=1.994, avg loss=2.108, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=594.7 milliseconds, train=1.5 seconds, 476864 images, time remaining=32.3 minutes
7452: loss=1.799, avg loss=2.077, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=1.5 seconds, 476928 images, time remaining=32.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7453: loss=1.777, avg loss=2.047, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.7 seconds, train=1.5 seconds, 476992 images, time remaining=32.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7454: loss=2.247, avg loss=2.067, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.6 seconds, train=1.5 seconds, 477056 images, time remaining=32.1 minutes
7455: loss=2.121, avg loss=2.072, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=957.0 milliseconds, train=1.5 seconds, 477120 images, time remaining=32 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7456: loss=2.027, avg loss=2.068, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.5 seconds, train=1.5 seconds, 477184 images, time remaining=32 minutes
7457: loss=1.986, avg loss=2.060, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=1.5 seconds, 477248 images, time remaining=31.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7458: loss=1.940, avg loss=2.048, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=3.5 seconds, train=1.5 seconds, 477312 images, time remaining=31.9 minutes
7459: loss=1.722, avg loss=2.015, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=845.2 milliseconds, train=1.5 seconds, 477376 images, time remaining=31.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7460: loss=1.838, avg loss=1.997, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.9 seconds, train=1.5 seconds, 477440 images, time remaining=31.8 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145822000000
7461: loss=2.052, avg loss=2.003, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 second, train=2.1 seconds, 477504 images, time remaining=31.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7462: loss=2.326, avg loss=2.035, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.6 seconds, train=2.1 seconds, 477568 images, time remaining=31.6 minutes
7463: loss=2.683, avg loss=2.100, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=2.1 seconds, 477632 images, time remaining=31.6 minutes
7464: loss=1.862, avg loss=2.076, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=948.0 milliseconds, train=2.1 seconds, 477696 images, time remaining=31.5 minutes
7465: loss=2.139, avg loss=2.082, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=688.3 milliseconds, train=2.1 seconds, 477760 images, time remaining=31.4 minutes
7466: loss=2.022, avg loss=2.076, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=2.1 seconds, 477824 images, time remaining=31.4 minutes
7467: loss=2.377, avg loss=2.106, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=2.1 seconds, 477888 images, time remaining=31.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7468: loss=1.972, avg loss=2.093, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.9 seconds, train=2.1 seconds, 477952 images, time remaining=31.3 minutes
7469: loss=1.705, avg loss=2.054, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=743.7 milliseconds, train=2.1 seconds, 478016 images, time remaining=31.2 minutes
7470: loss=2.194, avg loss=2.068, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=2.1 seconds, 478080 images, time remaining=31.1 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145822000000
7471: loss=2.363, avg loss=2.098, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=2.1 seconds, 478144 images, time remaining=31.1 minutes
7472: loss=1.901, avg loss=2.078, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=932.0 milliseconds, train=2.1 seconds, 478208 images, time remaining=31 minutes
7473: loss=1.754, avg loss=2.046, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=2.1 seconds, 478272 images, time remaining=31 minutes
7474: loss=1.739, avg loss=2.015, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=847.0 milliseconds, train=2.1 seconds, 478336 images, time remaining=30.9 minutes
7475: loss=2.158, avg loss=2.029, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=756.3 milliseconds, train=2.1 seconds, 478400 images, time remaining=30.8 minutes
7476: loss=1.982, avg loss=2.024, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.4 seconds, train=2.1 seconds, 478464 images, time remaining=30.8 minutes
7477: loss=1.650, avg loss=1.987, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.7 seconds, train=2.1 seconds, 478528 images, time remaining=30.7 minutes
7478: loss=2.380, avg loss=2.026, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=851.4 milliseconds, train=2.1 seconds, 478592 images, time remaining=30.7 minutes
7479: loss=2.266, avg loss=2.050, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.3 seconds, train=2.1 seconds, 478656 images, time remaining=30.6 minutes
7480: loss=2.008, avg loss=2.046, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=740.4 milliseconds, train=2.1 seconds, 478720 images, time remaining=30.5 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7481: loss=2.541, avg loss=2.096, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=870.4 milliseconds, train=3.9 seconds, 478784 images, time remaining=30.5 minutes
7482: loss=1.898, avg loss=2.076, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.0 seconds, train=3.9 seconds, 478848 images, time remaining=30.4 minutes
7483: loss=2.357, avg loss=2.104, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=937.9 milliseconds, train=3.9 seconds, 478912 images, time remaining=30.4 minutes
7484: loss=2.179, avg loss=2.111, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.0 seconds, train=3.9 seconds, 478976 images, time remaining=30.3 minutes
7485: loss=2.378, avg loss=2.138, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=719.6 milliseconds, train=3.9 seconds, 479040 images, time remaining=30.3 minutes
7486: loss=1.852, avg loss=2.109, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=873.1 milliseconds, train=3.9 seconds, 479104 images, time remaining=30.2 minutes
7487: loss=2.049, avg loss=2.103, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=3.9 seconds, 479168 images, time remaining=30.1 minutes
7488: loss=2.664, avg loss=2.159, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.5 seconds, train=3.9 seconds, 479232 images, time remaining=30.1 minutes
7489: loss=3.059, avg loss=2.249, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.6 seconds, train=3.9 seconds, 479296 images, time remaining=30 minutes
7490: loss=2.402, avg loss=2.265, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.1 seconds, train=3.9 seconds, 479360 images, time remaining=30 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7491: loss=2.644, avg loss=2.303, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.5 seconds, train=5.1 seconds, 479424 images, time remaining=29.9 minutes
7492: loss=2.555, avg loss=2.328, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=803.8 milliseconds, train=5.0 seconds, 479488 images, time remaining=29.9 minutes
7493: loss=2.821, avg loss=2.377, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=1.8 seconds, train=5.0 seconds, 479552 images, time remaining=29.8 minutes
7494: loss=2.739, avg loss=2.413, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=506.8 milliseconds, train=5.1 seconds, 479616 images, time remaining=29.7 minutes
7495: loss=2.836, avg loss=2.455, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.3 seconds, train=5.0 seconds, 479680 images, time remaining=29.7 minutes
7496: loss=2.815, avg loss=2.491, last=97.91%, best=97.91%, next=7496, rate=0.00001300, load 64=2.4 seconds, train=5.0 seconds, 479744 images, time remaining=29.6 minutes
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=87742, unique_truth_count=57264
rank=0 of ranks=87742rank=100 of ranks=87742rank=200 of ranks=87742rank=300 of ranks=87742rank=400 of ranks=87742rank=500 of ranks=87742rank=600 of ranks=87742rank=700 of ranks=87742rank=800 of ranks=87742rank=900 of ranks=87742rank=1000 of ranks=87742rank=1100 of ranks=87742rank=1200 of ranks=87742rank=1300 of ranks=87742rank=1400 of ranks=87742rank=1500 of ranks=87742rank=1600 of ranks=87742rank=1700 of ranks=87742rank=1800 of ranks=87742rank=1900 of ranks=87742rank=2000 of ranks=87742rank=2100 of ranks=87742rank=2200 of ranks=87742rank=2300 of ranks=87742rank=2400 of ranks=87742rank=2500 of ranks=87742rank=2600 of ranks=87742rank=2700 of ranks=87742rank=2800 of ranks=87742rank=2900 of ranks=87742rank=3000 of ranks=87742rank=3100 of ranks=87742rank=3200 of ranks=87742rank=3300 of ranks=87742rank=3400 of ranks=87742rank=3500 of ranks=87742rank=3600 of ranks=87742rank=3700 of ranks=87742rank=3800 of ranks=87742rank=3900 of ranks=87742rank=4000 of ranks=87742rank=4100 of ranks=87742rank=4200 of ranks=87742rank=4300 of ranks=87742rank=4400 of ranks=87742rank=4500 of ranks=87742rank=4600 of ranks=87742rank=4700 of ranks=87742rank=4800 of ranks=87742rank=4900 of ranks=87742rank=5000 of ranks=87742rank=5100 of ranks=87742rank=5200 of ranks=87742rank=5300 of ranks=87742rank=5400 of ranks=87742rank=5500 of ranks=87742rank=5600 of ranks=87742rank=5700 of ranks=87742rank=5800 of ranks=87742rank=5900 of ranks=87742rank=6000 of ranks=87742rank=6100 of ranks=87742rank=6200 of ranks=87742rank=6300 of ranks=87742rank=6400 of ranks=87742rank=6500 of ranks=87742rank=6600 of ranks=87742rank=6700 of ranks=87742rank=6800 of ranks=87742rank=6900 of ranks=87742rank=7000 of ranks=87742rank=7100 of ranks=87742rank=7200 of ranks=87742rank=7300 of ranks=87742rank=7400 of ranks=87742rank=7500 of ranks=87742rank=7600 of ranks=87742rank=7700 of ranks=87742rank=7800 of ranks=87742rank=7900 of ranks=87742rank=8000 of ranks=87742rank=8100 of ranks=87742rank=8200 of ranks=87742rank=8300 of ranks=87742rank=8400 of ranks=87742rank=8500 of ranks=87742rank=8600 of ranks=87742rank=8700 of ranks=87742rank=8800 of ranks=87742rank=8900 of ranks=87742rank=9000 of ranks=87742rank=9100 of ranks=87742rank=9200 of ranks=87742rank=9300 of ranks=87742rank=9400 of ranks=87742rank=9500 of ranks=87742rank=9600 of ranks=87742rank=9700 of ranks=87742rank=9800 of ranks=87742rank=9900 of ranks=87742rank=10000 of ranks=87742rank=10100 of ranks=87742rank=10200 of ranks=87742rank=10300 of ranks=87742rank=10400 of ranks=87742rank=10500 of ranks=87742rank=10600 of ranks=87742rank=10700 of ranks=87742rank=10800 of ranks=87742rank=10900 of ranks=87742rank=11000 of ranks=87742rank=11100 of ranks=87742rank=11200 of ranks=87742rank=11300 of ranks=87742rank=11400 of ranks=87742rank=11500 of ranks=87742rank=11600 of ranks=87742rank=11700 of ranks=87742rank=11800 of ranks=87742rank=11900 of ranks=87742rank=12000 of ranks=87742rank=12100 of ranks=87742rank=12200 of ranks=87742rank=12300 of ranks=87742rank=12400 of ranks=87742rank=12500 of ranks=87742rank=12600 of ranks=87742rank=12700 of ranks=87742rank=12800 of ranks=87742rank=12900 of ranks=87742rank=13000 of ranks=87742rank=13100 of ranks=87742rank=13200 of ranks=87742rank=13300 of ranks=87742rank=13400 of ranks=87742rank=13500 of ranks=87742rank=13600 of ranks=87742rank=13700 of ranks=87742rank=13800 of ranks=87742rank=13900 of ranks=87742rank=14000 of ranks=87742rank=14100 of ranks=87742rank=14200 of ranks=87742rank=14300 of ranks=87742rank=14400 of ranks=87742rank=14500 of ranks=87742rank=14600 of ranks=87742rank=14700 of ranks=87742rank=14800 of ranks=87742rank=14900 of ranks=87742rank=15000 of ranks=87742rank=15100 of ranks=87742rank=15200 of ranks=87742rank=15300 of ranks=87742rank=15400 of ranks=87742rank=15500 of ranks=87742rank=15600 of ranks=87742rank=15700 of ranks=87742rank=15800 of ranks=87742rank=15900 of ranks=87742rank=16000 of ranks=87742rank=16100 of ranks=87742rank=16200 of ranks=87742rank=16300 of ranks=87742rank=16400 of ranks=87742rank=16500 of ranks=87742rank=16600 of ranks=87742rank=16700 of ranks=87742rank=16800 of ranks=87742rank=16900 of ranks=87742rank=17000 of ranks=87742rank=17100 of ranks=87742rank=17200 of ranks=87742rank=17300 of ranks=87742rank=17400 of ranks=87742rank=17500 of ranks=87742rank=17600 of ranks=87742rank=17700 of ranks=87742rank=17800 of ranks=87742rank=17900 of ranks=87742rank=18000 of ranks=87742rank=18100 of ranks=87742rank=18200 of ranks=87742rank=18300 of ranks=87742rank=18400 of ranks=87742rank=18500 of ranks=87742rank=18600 of ranks=87742rank=18700 of ranks=87742rank=18800 of ranks=87742rank=18900 of ranks=87742rank=19000 of ranks=87742rank=19100 of ranks=87742rank=19200 of ranks=87742rank=19300 of ranks=87742rank=19400 of ranks=87742rank=19500 of ranks=87742rank=19600 of ranks=87742rank=19700 of ranks=87742rank=19800 of ranks=87742rank=19900 of ranks=87742rank=20000 of ranks=87742rank=20100 of ranks=87742rank=20200 of ranks=87742rank=20300 of ranks=87742rank=20400 of ranks=87742rank=20500 of ranks=87742rank=20600 of ranks=87742rank=20700 of ranks=87742rank=20800 of ranks=87742rank=20900 of ranks=87742rank=21000 of ranks=87742rank=21100 of ranks=87742rank=21200 of ranks=87742rank=21300 of ranks=87742rank=21400 of ranks=87742rank=21500 of ranks=87742rank=21600 of ranks=87742rank=21700 of ranks=87742rank=21800 of ranks=87742rank=21900 of ranks=87742rank=22000 of ranks=87742rank=22100 of ranks=87742rank=22200 of ranks=87742rank=22300 of ranks=87742rank=22400 of ranks=87742rank=22500 of ranks=87742rank=22600 of ranks=87742rank=22700 of ranks=87742rank=22800 of ranks=87742rank=22900 of ranks=87742rank=23000 of ranks=87742rank=23100 of ranks=87742rank=23200 of ranks=87742rank=23300 of ranks=87742rank=23400 of ranks=87742rank=23500 of ranks=87742rank=23600 of ranks=87742rank=23700 of ranks=87742rank=23800 of ranks=87742rank=23900 of ranks=87742rank=24000 of ranks=87742rank=24100 of ranks=87742rank=24200 of ranks=87742rank=24300 of ranks=87742rank=24400 of ranks=87742rank=24500 of ranks=87742rank=24600 of ranks=87742rank=24700 of ranks=87742rank=24800 of ranks=87742rank=24900 of ranks=87742rank=25000 of ranks=87742rank=25100 of ranks=87742rank=25200 of ranks=87742rank=25300 of ranks=87742rank=25400 of ranks=87742rank=25500 of ranks=87742rank=25600 of ranks=87742rank=25700 of ranks=87742rank=25800 of ranks=87742rank=25900 of ranks=87742rank=26000 of ranks=87742rank=26100 of ranks=87742rank=26200 of ranks=87742rank=26300 of ranks=87742rank=26400 of ranks=87742rank=26500 of ranks=87742rank=26600 of ranks=87742rank=26700 of ranks=87742rank=26800 of ranks=87742rank=26900 of ranks=87742rank=27000 of ranks=87742rank=27100 of ranks=87742rank=27200 of ranks=87742rank=27300 of ranks=87742rank=27400 of ranks=87742rank=27500 of ranks=87742rank=27600 of ranks=87742rank=27700 of ranks=87742rank=27800 of ranks=87742rank=27900 of ranks=87742rank=28000 of ranks=87742rank=28100 of ranks=87742rank=28200 of ranks=87742rank=28300 of ranks=87742rank=28400 of ranks=87742rank=28500 of ranks=87742rank=28600 of ranks=87742rank=28700 of ranks=87742rank=28800 of ranks=87742rank=28900 of ranks=87742rank=29000 of ranks=87742rank=29100 of ranks=87742rank=29200 of ranks=87742rank=29300 of ranks=87742rank=29400 of ranks=87742rank=29500 of ranks=87742rank=29600 of ranks=87742rank=29700 of ranks=87742rank=29800 of ranks=87742rank=29900 of ranks=87742rank=30000 of ranks=87742rank=30100 of ranks=87742rank=30200 of ranks=87742rank=30300 of ranks=87742rank=30400 of ranks=87742rank=30500 of ranks=87742rank=30600 of ranks=87742rank=30700 of ranks=87742rank=30800 of ranks=87742rank=30900 of ranks=87742rank=31000 of ranks=87742rank=31100 of ranks=87742rank=31200 of ranks=87742rank=31300 of ranks=87742rank=31400 of ranks=87742rank=31500 of ranks=87742rank=31600 of ranks=87742rank=31700 of ranks=87742rank=31800 of ranks=87742rank=31900 of ranks=87742rank=32000 of ranks=87742rank=32100 of ranks=87742rank=32200 of ranks=87742rank=32300 of ranks=87742rank=32400 of ranks=87742rank=32500 of ranks=87742rank=32600 of ranks=87742rank=32700 of ranks=87742rank=32800 of ranks=87742rank=32900 of ranks=87742rank=33000 of ranks=87742rank=33100 of ranks=87742rank=33200 of ranks=87742rank=33300 of ranks=87742rank=33400 of ranks=87742rank=33500 of ranks=87742rank=33600 of ranks=87742rank=33700 of ranks=87742rank=33800 of ranks=87742rank=33900 of ranks=87742rank=34000 of ranks=87742rank=34100 of ranks=87742rank=34200 of ranks=87742rank=34300 of ranks=87742rank=34400 of ranks=87742rank=34500 of ranks=87742rank=34600 of ranks=87742rank=34700 of ranks=87742rank=34800 of ranks=87742rank=34900 of ranks=87742rank=35000 of ranks=87742rank=35100 of ranks=87742rank=35200 of ranks=87742rank=35300 of ranks=87742rank=35400 of ranks=87742rank=35500 of ranks=87742rank=35600 of ranks=87742rank=35700 of ranks=87742rank=35800 of ranks=87742rank=35900 of ranks=87742rank=36000 of ranks=87742rank=36100 of ranks=87742rank=36200 of ranks=87742rank=36300 of ranks=87742rank=36400 of ranks=87742rank=36500 of ranks=87742rank=36600 of ranks=87742rank=36700 of ranks=87742rank=36800 of ranks=87742rank=36900 of ranks=87742rank=37000 of ranks=87742rank=37100 of ranks=87742rank=37200 of ranks=87742rank=37300 of ranks=87742rank=37400 of ranks=87742rank=37500 of ranks=87742rank=37600 of ranks=87742rank=37700 of ranks=87742rank=37800 of ranks=87742rank=37900 of ranks=87742rank=38000 of ranks=87742rank=38100 of ranks=87742rank=38200 of ranks=87742rank=38300 of ranks=87742rank=38400 of ranks=87742rank=38500 of ranks=87742rank=38600 of ranks=87742rank=38700 of ranks=87742rank=38800 of ranks=87742rank=38900 of ranks=87742rank=39000 of ranks=87742rank=39100 of ranks=87742rank=39200 of ranks=87742rank=39300 of ranks=87742rank=39400 of ranks=87742rank=39500 of ranks=87742rank=39600 of ranks=87742rank=39700 of ranks=87742rank=39800 of ranks=87742rank=39900 of ranks=87742rank=40000 of ranks=87742rank=40100 of ranks=87742rank=40200 of ranks=87742rank=40300 of ranks=87742rank=40400 of ranks=87742rank=40500 of ranks=87742rank=40600 of ranks=87742rank=40700 of ranks=87742rank=40800 of ranks=87742rank=40900 of ranks=87742rank=41000 of ranks=87742rank=41100 of ranks=87742rank=41200 of ranks=87742rank=41300 of ranks=87742rank=41400 of ranks=87742rank=41500 of ranks=87742rank=41600 of ranks=87742rank=41700 of ranks=87742rank=41800 of ranks=87742rank=41900 of ranks=87742rank=42000 of ranks=87742rank=42100 of ranks=87742rank=42200 of ranks=87742rank=42300 of ranks=87742rank=42400 of ranks=87742rank=42500 of ranks=87742rank=42600 of ranks=87742rank=42700 of ranks=87742rank=42800 of ranks=87742rank=42900 of ranks=87742rank=43000 of ranks=87742rank=43100 of ranks=87742rank=43200 of ranks=87742rank=43300 of ranks=87742rank=43400 of ranks=87742rank=43500 of ranks=87742rank=43600 of ranks=87742rank=43700 of ranks=87742rank=43800 of ranks=87742rank=43900 of ranks=87742rank=44000 of ranks=87742rank=44100 of ranks=87742rank=44200 of ranks=87742rank=44300 of ranks=87742rank=44400 of ranks=87742rank=44500 of ranks=87742rank=44600 of ranks=87742rank=44700 of ranks=87742rank=44800 of ranks=87742rank=44900 of ranks=87742rank=45000 of ranks=87742rank=45100 of ranks=87742rank=45200 of ranks=87742rank=45300 of ranks=87742rank=45400 of ranks=87742rank=45500 of ranks=87742rank=45600 of ranks=87742rank=45700 of ranks=87742rank=45800 of ranks=87742rank=45900 of ranks=87742rank=46000 of ranks=87742rank=46100 of ranks=87742rank=46200 of ranks=87742rank=46300 of ranks=87742rank=46400 of ranks=87742rank=46500 of ranks=87742rank=46600 of ranks=87742rank=46700 of ranks=87742rank=46800 of ranks=87742rank=46900 of ranks=87742rank=47000 of ranks=87742rank=47100 of ranks=87742rank=47200 of ranks=87742rank=47300 of ranks=87742rank=47400 of ranks=87742rank=47500 of ranks=87742rank=47600 of ranks=87742rank=47700 of ranks=87742rank=47800 of ranks=87742rank=47900 of ranks=87742rank=48000 of ranks=87742rank=48100 of ranks=87742rank=48200 of ranks=87742rank=48300 of ranks=87742rank=48400 of ranks=87742rank=48500 of ranks=87742rank=48600 of ranks=87742rank=48700 of ranks=87742rank=48800 of ranks=87742rank=48900 of ranks=87742rank=49000 of ranks=87742rank=49100 of ranks=87742rank=49200 of ranks=87742rank=49300 of ranks=87742rank=49400 of ranks=87742rank=49500 of ranks=87742rank=49600 of ranks=87742rank=49700 of ranks=87742rank=49800 of ranks=87742rank=49900 of ranks=87742rank=50000 of ranks=87742rank=50100 of ranks=87742rank=50200 of ranks=87742rank=50300 of ranks=87742rank=50400 of ranks=87742rank=50500 of ranks=87742rank=50600 of ranks=87742rank=50700 of ranks=87742rank=50800 of ranks=87742rank=50900 of ranks=87742rank=51000 of ranks=87742rank=51100 of ranks=87742rank=51200 of ranks=87742rank=51300 of ranks=87742rank=51400 of ranks=87742rank=51500 of ranks=87742rank=51600 of ranks=87742rank=51700 of ranks=87742rank=51800 of ranks=87742rank=51900 of ranks=87742rank=52000 of ranks=87742rank=52100 of ranks=87742rank=52200 of ranks=87742rank=52300 of ranks=87742rank=52400 of ranks=87742rank=52500 of ranks=87742rank=52600 of ranks=87742rank=52700 of ranks=87742rank=52800 of ranks=87742rank=52900 of ranks=87742rank=53000 of ranks=87742rank=53100 of ranks=87742rank=53200 of ranks=87742rank=53300 of ranks=87742rank=53400 of ranks=87742rank=53500 of ranks=87742rank=53600 of ranks=87742rank=53700 of ranks=87742rank=53800 of ranks=87742rank=53900 of ranks=87742rank=54000 of ranks=87742rank=54100 of ranks=87742rank=54200 of ranks=87742rank=54300 of ranks=87742rank=54400 of ranks=87742rank=54500 of ranks=87742rank=54600 of ranks=87742rank=54700 of ranks=87742rank=54800 of ranks=87742rank=54900 of ranks=87742rank=55000 of ranks=87742rank=55100 of ranks=87742rank=55200 of ranks=87742rank=55300 of ranks=87742rank=55400 of ranks=87742rank=55500 of ranks=87742rank=55600 of ranks=87742rank=55700 of ranks=87742rank=55800 of ranks=87742rank=55900 of ranks=87742rank=56000 of ranks=87742rank=56100 of ranks=87742rank=56200 of ranks=87742rank=56300 of ranks=87742rank=56400 of ranks=87742rank=56500 of ranks=87742rank=56600 of ranks=87742rank=56700 of ranks=87742rank=56800 of ranks=87742rank=56900 of ranks=87742rank=57000 of ranks=87742rank=57100 of ranks=87742rank=57200 of ranks=87742rank=57300 of ranks=87742rank=57400 of ranks=87742rank=57500 of ranks=87742rank=57600 of ranks=87742rank=57700 of ranks=87742rank=57800 of ranks=87742rank=57900 of ranks=87742rank=58000 of ranks=87742rank=58100 of ranks=87742rank=58200 of ranks=87742rank=58300 of ranks=87742rank=58400 of ranks=87742rank=58500 of ranks=87742rank=58600 of ranks=87742rank=58700 of ranks=87742rank=58800 of ranks=87742rank=58900 of ranks=87742rank=59000 of ranks=87742rank=59100 of ranks=87742rank=59200 of ranks=87742rank=59300 of ranks=87742rank=59400 of ranks=87742rank=59500 of ranks=87742rank=59600 of ranks=87742rank=59700 of ranks=87742rank=59800 of ranks=87742rank=59900 of ranks=87742rank=60000 of ranks=87742rank=60100 of ranks=87742rank=60200 of ranks=87742rank=60300 of ranks=87742rank=60400 of ranks=87742rank=60500 of ranks=87742rank=60600 of ranks=87742rank=60700 of ranks=87742rank=60800 of ranks=87742rank=60900 of ranks=87742rank=61000 of ranks=87742rank=61100 of ranks=87742rank=61200 of ranks=87742rank=61300 of ranks=87742rank=61400 of ranks=87742rank=61500 of ranks=87742rank=61600 of ranks=87742rank=61700 of ranks=87742rank=61800 of ranks=87742rank=61900 of ranks=87742rank=62000 of ranks=87742rank=62100 of ranks=87742rank=62200 of ranks=87742rank=62300 of ranks=87742rank=62400 of ranks=87742rank=62500 of ranks=87742rank=62600 of ranks=87742rank=62700 of ranks=87742rank=62800 of ranks=87742rank=62900 of ranks=87742rank=63000 of ranks=87742rank=63100 of ranks=87742rank=63200 of ranks=87742rank=63300 of ranks=87742rank=63400 of ranks=87742rank=63500 of ranks=87742rank=63600 of ranks=87742rank=63700 of ranks=87742rank=63800 of ranks=87742rank=63900 of ranks=87742rank=64000 of ranks=87742rank=64100 of ranks=87742rank=64200 of ranks=87742rank=64300 of ranks=87742rank=64400 of ranks=87742rank=64500 of ranks=87742rank=64600 of ranks=87742rank=64700 of ranks=87742rank=64800 of ranks=87742rank=64900 of ranks=87742rank=65000 of ranks=87742rank=65100 of ranks=87742rank=65200 of ranks=87742rank=65300 of ranks=87742rank=65400 of ranks=87742rank=65500 of ranks=87742rank=65600 of ranks=87742rank=65700 of ranks=87742rank=65800 of ranks=87742rank=65900 of ranks=87742rank=66000 of ranks=87742rank=66100 of ranks=87742rank=66200 of ranks=87742rank=66300 of ranks=87742rank=66400 of ranks=87742rank=66500 of ranks=87742rank=66600 of ranks=87742rank=66700 of ranks=87742rank=66800 of ranks=87742rank=66900 of ranks=87742rank=67000 of ranks=87742rank=67100 of ranks=87742rank=67200 of ranks=87742rank=67300 of ranks=87742rank=67400 of ranks=87742rank=67500 of ranks=87742rank=67600 of ranks=87742rank=67700 of ranks=87742rank=67800 of ranks=87742rank=67900 of ranks=87742rank=68000 of ranks=87742rank=68100 of ranks=87742rank=68200 of ranks=87742rank=68300 of ranks=87742rank=68400 of ranks=87742rank=68500 of ranks=87742rank=68600 of ranks=87742rank=68700 of ranks=87742rank=68800 of ranks=87742rank=68900 of ranks=87742rank=69000 of ranks=87742rank=69100 of ranks=87742rank=69200 of ranks=87742rank=69300 of ranks=87742rank=69400 of ranks=87742rank=69500 of ranks=87742rank=69600 of ranks=87742rank=69700 of ranks=87742rank=69800 of ranks=87742rank=69900 of ranks=87742rank=70000 of ranks=87742rank=70100 of ranks=87742rank=70200 of ranks=87742rank=70300 of ranks=87742rank=70400 of ranks=87742rank=70500 of ranks=87742rank=70600 of ranks=87742rank=70700 of ranks=87742rank=70800 of ranks=87742rank=70900 of ranks=87742rank=71000 of ranks=87742rank=71100 of ranks=87742rank=71200 of ranks=87742rank=71300 of ranks=87742rank=71400 of ranks=87742rank=71500 of ranks=87742rank=71600 of ranks=87742rank=71700 of ranks=87742rank=71800 of ranks=87742rank=71900 of ranks=87742rank=72000 of ranks=87742rank=72100 of ranks=87742rank=72200 of ranks=87742rank=72300 of ranks=87742rank=72400 of ranks=87742rank=72500 of ranks=87742rank=72600 of ranks=87742rank=72700 of ranks=87742rank=72800 of ranks=87742rank=72900 of ranks=87742rank=73000 of ranks=87742rank=73100 of ranks=87742rank=73200 of ranks=87742rank=73300 of ranks=87742rank=73400 of ranks=87742rank=73500 of ranks=87742rank=73600 of ranks=87742rank=73700 of ranks=87742rank=73800 of ranks=87742rank=73900 of ranks=87742rank=74000 of ranks=87742rank=74100 of ranks=87742rank=74200 of ranks=87742rank=74300 of ranks=87742rank=74400 of ranks=87742rank=74500 of ranks=87742rank=74600 of ranks=87742rank=74700 of ranks=87742rank=74800 of ranks=87742rank=74900 of ranks=87742rank=75000 of ranks=87742rank=75100 of ranks=87742rank=75200 of ranks=87742rank=75300 of ranks=87742rank=75400 of ranks=87742rank=75500 of ranks=87742rank=75600 of ranks=87742rank=75700 of ranks=87742rank=75800 of ranks=87742rank=75900 of ranks=87742rank=76000 of ranks=87742rank=76100 of ranks=87742rank=76200 of ranks=87742rank=76300 of ranks=87742rank=76400 of ranks=87742rank=76500 of ranks=87742rank=76600 of ranks=87742rank=76700 of ranks=87742rank=76800 of ranks=87742rank=76900 of ranks=87742rank=77000 of ranks=87742rank=77100 of ranks=87742rank=77200 of ranks=87742rank=77300 of ranks=87742rank=77400 of ranks=87742rank=77500 of ranks=87742rank=77600 of ranks=87742rank=77700 of ranks=87742rank=77800 of ranks=87742rank=77900 of ranks=87742rank=78000 of ranks=87742rank=78100 of ranks=87742rank=78200 of ranks=87742rank=78300 of ranks=87742rank=78400 of ranks=87742rank=78500 of ranks=87742rank=78600 of ranks=87742rank=78700 of ranks=87742rank=78800 of ranks=87742rank=78900 of ranks=87742rank=79000 of ranks=87742rank=79100 of ranks=87742rank=79200 of ranks=87742rank=79300 of ranks=87742rank=79400 of ranks=87742rank=79500 of ranks=87742rank=79600 of ranks=87742rank=79700 of ranks=87742rank=79800 of ranks=87742rank=79900 of ranks=87742rank=80000 of ranks=87742rank=80100 of ranks=87742rank=80200 of ranks=87742rank=80300 of ranks=87742rank=80400 of ranks=87742rank=80500 of ranks=87742rank=80600 of ranks=87742rank=80700 of ranks=87742rank=80800 of ranks=87742rank=80900 of ranks=87742rank=81000 of ranks=87742rank=81100 of ranks=87742rank=81200 of ranks=87742rank=81300 of ranks=87742rank=81400 of ranks=87742rank=81500 of ranks=87742rank=81600 of ranks=87742rank=81700 of ranks=87742rank=81800 of ranks=87742rank=81900 of ranks=87742rank=82000 of ranks=87742rank=82100 of ranks=87742rank=82200 of ranks=87742rank=82300 of ranks=87742rank=82400 of ranks=87742rank=82500 of ranks=87742rank=82600 of ranks=87742rank=82700 of ranks=87742rank=82800 of ranks=87742rank=82900 of ranks=87742rank=83000 of ranks=87742rank=83100 of ranks=87742rank=83200 of ranks=87742rank=83300 of ranks=87742rank=83400 of ranks=87742rank=83500 of ranks=87742rank=83600 of ranks=87742rank=83700 of ranks=87742rank=83800 of ranks=87742rank=83900 of ranks=87742rank=84000 of ranks=87742rank=84100 of ranks=87742rank=84200 of ranks=87742rank=84300 of ranks=87742rank=84400 of ranks=87742rank=84500 of ranks=87742rank=84600 of ranks=87742rank=84700 of ranks=87742rank=84800 of ranks=87742rank=84900 of ranks=87742rank=85000 of ranks=87742rank=85100 of ranks=87742rank=85200 of ranks=87742rank=85300 of ranks=87742rank=85400 of ranks=87742rank=85500 of ranks=87742rank=85600 of ranks=87742rank=85700 of ranks=87742rank=85800 of ranks=87742rank=85900 of ranks=87742rank=86000 of ranks=87742rank=86100 of ranks=87742rank=86200 of ranks=87742rank=86300 of ranks=87742rank=86400 of ranks=87742rank=86500 of ranks=87742rank=86600 of ranks=87742rank=86700 of ranks=87742rank=86800 of ranks=87742rank=86900 of ranks=87742rank=87000 of ranks=87742rank=87100 of ranks=87742rank=87200 of ranks=87742rank=87300 of ranks=87742rank=87400 of ranks=87742rank=87500 of ranks=87742rank=87600 of ranks=87742rank=87700 of ranks=87742

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              98.3244    495    833      3    498           78.8964
   1 car                    98.7754  50021  22465    295  50316           84.1445
   2 truck                  98.5554   1812   2078     13   1825           78.9928
   3 bus                    97.1550    362   1615      4    366           75.6671
   4 pedestrian             97.3131   4177   3884     82   4259           78.0686

for conf_thresh=0.25, precision=0.92, recall=0.97, F1 score=0.94
for conf_thresh=0.25, TP=55448, FP=5131, FN=1816, average IoU=83.44%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=98.02%
Total detection time: 171 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
7497: loss=1.652, avg loss=2.408, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=657.1 milliseconds, train=2.1 seconds, 479808 images, time remaining=29.8 minutes
7498: loss=1.949, avg loss=2.362, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=2.1 seconds, 479872 images, time remaining=29.7 minutes
7499: loss=2.354, avg loss=2.361, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=928.3 milliseconds, train=2.1 seconds, 479936 images, time remaining=29.6 minutes
7500: loss=1.793, avg loss=2.304, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=938.0 milliseconds, train=2.1 seconds, 480000 images, time remaining=29.6 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7501: loss=1.647, avg loss=2.238, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=2.1 seconds, 480064 images, time remaining=29.5 minutes
7502: loss=1.625, avg loss=2.177, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 480128 images, time remaining=29.5 minutes
7503: loss=1.721, avg loss=2.131, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 second, train=2.1 seconds, 480192 images, time remaining=29.4 minutes
7504: loss=1.814, avg loss=2.100, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=930.8 milliseconds, train=2.1 seconds, 480256 images, time remaining=29.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7505: loss=2.331, avg loss=2.123, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=2.1 seconds, 480320 images, time remaining=29.3 minutes
7506: loss=2.318, avg loss=2.142, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=2.1 seconds, 480384 images, time remaining=29.2 minutes
7507: loss=2.184, avg loss=2.146, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 480448 images, time remaining=29.2 minutes
7508: loss=2.258, avg loss=2.158, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 480512 images, time remaining=29.1 minutes
7509: loss=1.567, avg loss=2.099, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=2.1 seconds, 480576 images, time remaining=29 minutes
7510: loss=2.408, avg loss=2.130, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=2.1 seconds, 480640 images, time remaining=29 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145fdc000000
7511: loss=2.395, avg loss=2.156, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=811.7 milliseconds, train=1.2 seconds, 480704 images, time remaining=28.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7512: loss=2.160, avg loss=2.157, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.2 seconds, 480768 images, time remaining=28.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7513: loss=2.744, avg loss=2.215, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.5 seconds, train=1.2 seconds, 480832 images, time remaining=28.8 minutes
7514: loss=2.107, avg loss=2.204, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=954.0 milliseconds, train=1.2 seconds, 480896 images, time remaining=28.7 minutes
7515: loss=2.251, avg loss=2.209, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=1.2 seconds, 480960 images, time remaining=28.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7516: loss=2.178, avg loss=2.206, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=1.2 seconds, 481024 images, time remaining=28.6 minutes
7517: loss=1.684, avg loss=2.154, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=997.0 milliseconds, train=1.2 seconds, 481088 images, time remaining=28.5 minutes
7518: loss=2.451, avg loss=2.183, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=996.3 milliseconds, train=1.2 seconds, 481152 images, time remaining=28.5 minutes
7519: loss=2.401, avg loss=2.205, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=916.1 milliseconds, train=1.2 seconds, 481216 images, time remaining=28.4 minutes
7520: loss=2.200, avg loss=2.205, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=762.3 milliseconds, train=1.2 seconds, 481280 images, time remaining=28.4 minutes
Resizing, random_coef=1.40, batch=4, 1280x992
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7521: loss=2.351, avg loss=2.219, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=4.7 seconds, 481344 images, time remaining=28.3 minutes
7522: loss=2.424, avg loss=2.240, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=4.6 seconds, 481408 images, time remaining=28.2 minutes
7523: loss=2.240, avg loss=2.240, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=4.6 seconds, 481472 images, time remaining=28.2 minutes
7524: loss=2.905, avg loss=2.306, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.9 seconds, train=4.6 seconds, 481536 images, time remaining=28.1 minutes
7525: loss=2.718, avg loss=2.348, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=4.6 seconds, 481600 images, time remaining=28.1 minutes
7526: loss=2.328, avg loss=2.346, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.7 seconds, train=4.6 seconds, 481664 images, time remaining=28 minutes
7527: loss=2.637, avg loss=2.375, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=4.6 seconds, 481728 images, time remaining=28 minutes
7528: loss=2.208, avg loss=2.358, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=4.6 seconds, 481792 images, time remaining=27.9 minutes
7529: loss=2.450, avg loss=2.367, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.3 seconds, train=4.6 seconds, 481856 images, time remaining=27.8 minutes
7530: loss=2.727, avg loss=2.403, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=4.6 seconds, 481920 images, time remaining=27.8 minutes
Resizing, random_coef=1.40, batch=4, 1248x960
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7531: loss=2.428, avg loss=2.406, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=4.5 seconds, 481984 images, time remaining=27.7 minutes
7532: loss=2.166, avg loss=2.382, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=4.5 seconds, 482048 images, time remaining=27.7 minutes
7533: loss=1.997, avg loss=2.343, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=989.0 milliseconds, train=4.5 seconds, 482112 images, time remaining=27.6 minutes
7534: loss=2.649, avg loss=2.374, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=745.0 milliseconds, train=4.5 seconds, 482176 images, time remaining=27.6 minutes
7535: loss=3.286, avg loss=2.465, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=699.0 milliseconds, train=4.5 seconds, 482240 images, time remaining=27.5 minutes
7536: loss=2.566, avg loss=2.475, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=4.5 seconds, 482304 images, time remaining=27.5 minutes
7537: loss=2.681, avg loss=2.496, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=4.5 seconds, 482368 images, time remaining=27.4 minutes
7538: loss=2.066, avg loss=2.453, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=4.5 seconds, 482432 images, time remaining=27.3 minutes
7539: loss=2.924, avg loss=2.500, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=863.4 milliseconds, train=4.5 seconds, 482496 images, time remaining=27.3 minutes
7540: loss=2.173, avg loss=2.467, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=4.5 seconds, 482560 images, time remaining=27.2 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x145f84000000
7541: loss=1.682, avg loss=2.389, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=823.5 milliseconds, train=1.6 seconds, 482624 images, time remaining=27.1 minutes
7542: loss=1.947, avg loss=2.345, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=926.9 milliseconds, train=1.6 seconds, 482688 images, time remaining=27.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7543: loss=2.210, avg loss=2.331, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=1.6 seconds, 482752 images, time remaining=27 minutes
7544: loss=1.631, avg loss=2.261, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.6 seconds, 482816 images, time remaining=27 minutes
7545: loss=2.220, avg loss=2.257, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=911.3 milliseconds, train=1.6 seconds, 482880 images, time remaining=26.9 minutes
7546: loss=2.064, avg loss=2.238, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=966.5 milliseconds, train=1.6 seconds, 482944 images, time remaining=26.9 minutes
7547: loss=2.327, avg loss=2.247, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=921.5 milliseconds, train=1.6 seconds, 483008 images, time remaining=26.8 minutes
7548: loss=2.167, avg loss=2.239, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=637.6 milliseconds, train=1.6 seconds, 483072 images, time remaining=26.7 minutes
7549: loss=2.210, avg loss=2.236, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.6 seconds, 483136 images, time remaining=26.7 minutes
7550: loss=2.185, avg loss=2.231, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=740.3 milliseconds, train=1.6 seconds, 483200 images, time remaining=26.6 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145f8c000000
7551: loss=1.907, avg loss=2.198, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=699.2 milliseconds, train=1.2 seconds, 483264 images, time remaining=26.5 minutes
7552: loss=1.786, avg loss=2.157, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=687.2 milliseconds, train=1.2 seconds, 483328 images, time remaining=26.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7553: loss=2.051, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.2 seconds, 483392 images, time remaining=26.4 minutes
7554: loss=2.208, avg loss=2.153, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=946.7 milliseconds, train=1.2 seconds, 483456 images, time remaining=26.4 minutes
7555: loss=1.608, avg loss=2.098, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=560.8 milliseconds, train=1.2 seconds, 483520 images, time remaining=26.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7556: loss=2.271, avg loss=2.115, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.3 seconds, train=1.2 seconds, 483584 images, time remaining=26.2 minutes
7557: loss=2.534, avg loss=2.157, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=533.7 milliseconds, train=1.2 seconds, 483648 images, time remaining=26.2 minutes
7558: loss=1.621, avg loss=2.104, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.2 seconds, 483712 images, time remaining=26.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7559: loss=2.268, avg loss=2.120, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.0 seconds, train=1.2 seconds, 483776 images, time remaining=26.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7560: loss=2.387, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.2 seconds, 483840 images, time remaining=26 minutes
Resizing, random_coef=1.40, batch=4, 1120x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7561: loss=2.341, avg loss=2.166, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=581.0 milliseconds, train=3.6 seconds, 483904 images, time remaining=25.9 minutes
7562: loss=2.192, avg loss=2.169, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.6 seconds, 483968 images, time remaining=25.9 minutes
7563: loss=1.888, avg loss=2.141, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=832.3 milliseconds, train=3.6 seconds, 484032 images, time remaining=25.8 minutes
7564: loss=1.998, avg loss=2.126, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=3.6 seconds, 484096 images, time remaining=25.8 minutes
7565: loss=2.156, avg loss=2.129, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=742.8 milliseconds, train=3.6 seconds, 484160 images, time remaining=25.7 minutes
7566: loss=2.374, avg loss=2.154, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=515.4 milliseconds, train=3.6 seconds, 484224 images, time remaining=25.6 minutes
7567: loss=2.059, avg loss=2.144, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.2 seconds, train=3.6 seconds, 484288 images, time remaining=25.6 minutes
7568: loss=1.874, avg loss=2.117, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=643.8 milliseconds, train=3.6 seconds, 484352 images, time remaining=25.5 minutes
7569: loss=2.677, avg loss=2.173, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=3.6 seconds, 484416 images, time remaining=25.5 minutes
7570: loss=1.961, avg loss=2.152, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=3.6 seconds, 484480 images, time remaining=25.4 minutes
Resizing, random_coef=1.40, batch=4, 832x640
GPU #0: allocating workspace: 399.1 MiB begins at 0x14642e000000
7571: loss=2.455, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=634.3 milliseconds, train=1.5 seconds, 484544 images, time remaining=25.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7572: loss=2.407, avg loss=2.205, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.0 seconds, train=1.5 seconds, 484608 images, time remaining=25.3 minutes
7573: loss=2.222, avg loss=2.207, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=488.3 milliseconds, train=1.5 seconds, 484672 images, time remaining=25.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7574: loss=1.906, avg loss=2.177, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=4.6 seconds, train=1.5 seconds, 484736 images, time remaining=25.2 minutes
7575: loss=2.017, avg loss=2.161, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.5 seconds, 484800 images, time remaining=25.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7576: loss=1.938, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.5 seconds, 484864 images, time remaining=25 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7577: loss=2.004, avg loss=2.125, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.5 seconds, 484928 images, time remaining=25 minutes
7578: loss=2.176, avg loss=2.130, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=983.0 milliseconds, train=1.5 seconds, 484992 images, time remaining=24.9 minutes
7579: loss=1.767, avg loss=2.094, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=554.3 milliseconds, train=1.5 seconds, 485056 images, time remaining=24.9 minutes
7580: loss=2.166, avg loss=2.101, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=620.1 milliseconds, train=1.5 seconds, 485120 images, time remaining=24.8 minutes
Resizing, random_coef=1.40, batch=4, 1344x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7581: loss=2.882, avg loss=2.179, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=773.7 milliseconds, train=5.0 seconds, 485184 images, time remaining=24.8 minutes
7582: loss=2.321, avg loss=2.193, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=882.7 milliseconds, train=5.0 seconds, 485248 images, time remaining=24.7 minutes
7583: loss=2.942, avg loss=2.268, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=804.2 milliseconds, train=5.1 seconds, 485312 images, time remaining=24.6 minutes
7584: loss=2.513, avg loss=2.293, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=749.9 milliseconds, train=5.0 seconds, 485376 images, time remaining=24.6 minutes
7585: loss=2.734, avg loss=2.337, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=5.0 seconds, 485440 images, time remaining=24.5 minutes
7586: loss=2.461, avg loss=2.349, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=5.0 seconds, 485504 images, time remaining=24.5 minutes
7587: loss=2.817, avg loss=2.396, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=5.0 seconds, 485568 images, time remaining=24.4 minutes
7588: loss=2.481, avg loss=2.404, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=621.5 milliseconds, train=5.0 seconds, 485632 images, time remaining=24.3 minutes
7589: loss=3.237, avg loss=2.488, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=5.0 seconds, 485696 images, time remaining=24.3 minutes
7590: loss=2.355, avg loss=2.474, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=816.6 milliseconds, train=5.0 seconds, 485760 images, time remaining=24.2 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x145cc8000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7591: loss=2.049, avg loss=2.432, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.2 seconds, 485824 images, time remaining=24.2 minutes
7592: loss=2.707, avg loss=2.459, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=736.6 milliseconds, train=1.2 seconds, 485888 images, time remaining=24.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7593: loss=2.153, avg loss=2.429, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=1.2 seconds, 485952 images, time remaining=24 minutes
7594: loss=2.452, avg loss=2.431, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=767.3 milliseconds, train=1.2 seconds, 486016 images, time remaining=24 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7595: loss=2.368, avg loss=2.425, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=1.2 seconds, 486080 images, time remaining=23.9 minutes
7596: loss=2.178, avg loss=2.400, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=805.7 milliseconds, train=1.2 seconds, 486144 images, time remaining=23.9 minutes
7597: loss=2.171, avg loss=2.377, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=630.0 milliseconds, train=1.2 seconds, 486208 images, time remaining=23.8 minutes
7598: loss=2.603, avg loss=2.400, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=763.0 milliseconds, train=1.2 seconds, 486272 images, time remaining=23.7 minutes
7599: loss=2.260, avg loss=2.386, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=685.7 milliseconds, train=1.2 seconds, 486336 images, time remaining=23.7 minutes
7600: loss=1.918, avg loss=2.339, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=401.5 milliseconds, train=1.2 seconds, 486400 images, time remaining=23.6 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7601: loss=2.265, avg loss=2.332, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=2.1 seconds, 486464 images, time remaining=23.6 minutes
7602: loss=1.812, avg loss=2.280, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=620.0 milliseconds, train=2.1 seconds, 486528 images, time remaining=23.5 minutes
7603: loss=2.505, avg loss=2.302, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=719.4 milliseconds, train=2.1 seconds, 486592 images, time remaining=23.4 minutes
7604: loss=2.127, avg loss=2.285, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=837.3 milliseconds, train=2.1 seconds, 486656 images, time remaining=23.4 minutes
7605: loss=1.822, avg loss=2.238, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=748.9 milliseconds, train=2.1 seconds, 486720 images, time remaining=23.3 minutes
7606: loss=1.886, avg loss=2.203, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=611.1 milliseconds, train=2.1 seconds, 486784 images, time remaining=23.3 minutes
7607: loss=1.897, avg loss=2.173, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=805.8 milliseconds, train=2.1 seconds, 486848 images, time remaining=23.2 minutes
7608: loss=2.437, avg loss=2.199, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=2.1 seconds, 486912 images, time remaining=23.1 minutes
7609: loss=1.820, avg loss=2.161, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=621.2 milliseconds, train=2.1 seconds, 486976 images, time remaining=23.1 minutes
7610: loss=2.299, avg loss=2.175, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=887.3 milliseconds, train=2.1 seconds, 487040 images, time remaining=23 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x146388000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7611: loss=2.017, avg loss=2.159, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.4 seconds, 487104 images, time remaining=23 minutes
7612: loss=2.424, avg loss=2.186, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=594.9 milliseconds, train=1.4 seconds, 487168 images, time remaining=22.9 minutes
7613: loss=2.392, avg loss=2.206, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=697.2 milliseconds, train=1.4 seconds, 487232 images, time remaining=22.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7614: loss=1.852, avg loss=2.171, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=1.4 seconds, 487296 images, time remaining=22.8 minutes
7615: loss=1.789, avg loss=2.133, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=493.4 milliseconds, train=1.4 seconds, 487360 images, time remaining=22.7 minutes
7616: loss=2.299, avg loss=2.149, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=656.1 milliseconds, train=1.4 seconds, 487424 images, time remaining=22.6 minutes
7617: loss=2.009, avg loss=2.135, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=636.5 milliseconds, train=1.4 seconds, 487488 images, time remaining=22.6 minutes
7618: loss=2.087, avg loss=2.130, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=483.5 milliseconds, train=1.5 seconds, 487552 images, time remaining=22.5 minutes
7619: loss=2.480, avg loss=2.165, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=644.3 milliseconds, train=1.5 seconds, 487616 images, time remaining=22.5 minutes
7620: loss=1.886, avg loss=2.137, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=1.5 seconds, 487680 images, time remaining=22.4 minutes
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7621: loss=1.824, avg loss=2.106, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=3.2 seconds, 487744 images, time remaining=22.3 minutes
7622: loss=2.150, avg loss=2.110, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=644.2 milliseconds, train=3.2 seconds, 487808 images, time remaining=22.3 minutes
7623: loss=1.904, avg loss=2.090, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=979.4 milliseconds, train=3.2 seconds, 487872 images, time remaining=22.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7624: loss=1.988, avg loss=2.080, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.5 seconds, train=3.2 seconds, 487936 images, time remaining=22.2 minutes
7625: loss=1.828, avg loss=2.054, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=653.3 milliseconds, train=3.2 seconds, 488000 images, time remaining=22.1 minutes
7626: loss=2.459, avg loss=2.095, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=3.2 seconds, 488064 images, time remaining=22.1 minutes
7627: loss=2.448, avg loss=2.130, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=631.9 milliseconds, train=3.2 seconds, 488128 images, time remaining=22 minutes
7628: loss=1.974, avg loss=2.115, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=791.4 milliseconds, train=3.2 seconds, 488192 images, time remaining=21.9 minutes
7629: loss=1.874, avg loss=2.090, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=3.2 seconds, 488256 images, time remaining=21.9 minutes
7630: loss=2.128, avg loss=2.094, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=3.2 seconds, 488320 images, time remaining=21.8 minutes
Resizing, random_coef=1.40, batch=4, 1184x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7631: loss=2.283, avg loss=2.113, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=485.4 milliseconds, train=3.9 seconds, 488384 images, time remaining=21.8 minutes
7632: loss=2.559, avg loss=2.158, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=3.9 seconds, 488448 images, time remaining=21.7 minutes
7633: loss=2.373, avg loss=2.179, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=931.6 milliseconds, train=3.9 seconds, 488512 images, time remaining=21.6 minutes
7634: loss=1.822, avg loss=2.143, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.0 seconds, train=3.9 seconds, 488576 images, time remaining=21.6 minutes
7635: loss=2.430, avg loss=2.172, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.6 seconds, train=3.9 seconds, 488640 images, time remaining=21.5 minutes
7636: loss=2.177, avg loss=2.173, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=3.9 seconds, 488704 images, time remaining=21.5 minutes
7637: loss=2.082, avg loss=2.164, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.9 seconds, 488768 images, time remaining=21.4 minutes
7638: loss=1.920, avg loss=2.139, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=3.9 seconds, 488832 images, time remaining=21.3 minutes
7639: loss=2.716, avg loss=2.197, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=3.9 seconds, 488896 images, time remaining=21.3 minutes
7640: loss=2.388, avg loss=2.216, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=3.8 seconds, 488960 images, time remaining=21.2 minutes
Resizing, random_coef=1.40, batch=4, 928x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145882000000
7641: loss=1.851, avg loss=2.180, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=906.3 milliseconds, train=2.0 seconds, 489024 images, time remaining=21.2 minutes
7642: loss=1.897, avg loss=2.151, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=796.0 milliseconds, train=2.0 seconds, 489088 images, time remaining=21.1 minutes
7643: loss=2.016, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=927.9 milliseconds, train=2.0 seconds, 489152 images, time remaining=21 minutes
7644: loss=2.160, avg loss=2.140, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=931.6 milliseconds, train=2.0 seconds, 489216 images, time remaining=21 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7645: loss=1.861, avg loss=2.112, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.4 seconds, train=2.0 seconds, 489280 images, time remaining=20.9 minutes
7646: loss=2.240, avg loss=2.125, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=2.0 seconds, 489344 images, time remaining=20.9 minutes
7647: loss=1.982, avg loss=2.111, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=809.5 milliseconds, train=2.0 seconds, 489408 images, time remaining=20.8 minutes
7648: loss=2.226, avg loss=2.122, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=937.2 milliseconds, train=2.0 seconds, 489472 images, time remaining=20.8 minutes
7649: loss=2.238, avg loss=2.134, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=922.6 milliseconds, train=2.0 seconds, 489536 images, time remaining=20.7 minutes
7650: loss=2.486, avg loss=2.169, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=2.0 seconds, 489600 images, time remaining=20.6 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x145882000000
7651: loss=1.781, avg loss=2.130, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=1.9 seconds, 489664 images, time remaining=20.6 minutes
7652: loss=1.487, avg loss=2.066, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=799.5 milliseconds, train=1.9 seconds, 489728 images, time remaining=20.5 minutes
7653: loss=2.334, avg loss=2.093, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.9 seconds, 489792 images, time remaining=20.4 minutes
7654: loss=2.083, avg loss=2.092, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=738.7 milliseconds, train=1.9 seconds, 489856 images, time remaining=20.4 minutes
7655: loss=2.178, avg loss=2.100, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=713.8 milliseconds, train=1.9 seconds, 489920 images, time remaining=20.3 minutes
7656: loss=2.264, avg loss=2.117, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.9 seconds, 489984 images, time remaining=20.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7657: loss=1.895, avg loss=2.094, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=1.9 seconds, 490048 images, time remaining=20.2 minutes
7658: loss=2.066, avg loss=2.092, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=746.4 milliseconds, train=1.9 seconds, 490112 images, time remaining=20.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7659: loss=2.151, avg loss=2.098, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.9 seconds, 490176 images, time remaining=20.1 minutes
7660: loss=2.340, avg loss=2.122, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.9 seconds, 490240 images, time remaining=20 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x145a0a000000
7661: loss=2.177, avg loss=2.127, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=1.4 seconds, 490304 images, time remaining=20 minutes
7662: loss=1.928, avg loss=2.107, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.4 seconds, 490368 images, time remaining=19.9 minutes
7663: loss=2.208, avg loss=2.117, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=881.4 milliseconds, train=1.4 seconds, 490432 images, time remaining=19.9 minutes
7664: loss=2.516, avg loss=2.157, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.4 seconds, 490496 images, time remaining=19.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7665: loss=1.875, avg loss=2.129, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=1.4 seconds, 490560 images, time remaining=19.7 minutes
7666: loss=2.056, avg loss=2.122, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=825.3 milliseconds, train=1.4 seconds, 490624 images, time remaining=19.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7667: loss=2.179, avg loss=2.128, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=1.4 seconds, 490688 images, time remaining=19.6 minutes
7668: loss=2.028, avg loss=2.118, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=1.5 seconds, 490752 images, time remaining=19.5 minutes
7669: loss=2.262, avg loss=2.132, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.4 seconds, 490816 images, time remaining=19.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7670: loss=2.356, avg loss=2.155, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.8 seconds, train=1.4 seconds, 490880 images, time remaining=19.4 minutes
Resizing, random_coef=1.40, batch=4, 1216x928
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7671: loss=2.521, avg loss=2.191, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=622.2 milliseconds, train=3.9 seconds, 490944 images, time remaining=19.4 minutes
7672: loss=2.084, avg loss=2.181, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.9 seconds, 491008 images, time remaining=19.3 minutes
7673: loss=2.673, avg loss=2.230, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=876.1 milliseconds, train=3.9 seconds, 491072 images, time remaining=19.3 minutes
7674: loss=1.748, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=3.9 seconds, 491136 images, time remaining=19.2 minutes
7675: loss=1.993, avg loss=2.163, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.3 seconds, train=3.9 seconds, 491200 images, time remaining=19.1 minutes
7676: loss=2.134, avg loss=2.160, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=818.5 milliseconds, train=3.9 seconds, 491264 images, time remaining=19.1 minutes
7677: loss=2.182, avg loss=2.162, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=665.3 milliseconds, train=3.9 seconds, 491328 images, time remaining=19 minutes
7678: loss=1.878, avg loss=2.134, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=3.9 seconds, 491392 images, time remaining=19 minutes
7679: loss=2.424, avg loss=2.163, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=887.4 milliseconds, train=3.9 seconds, 491456 images, time remaining=18.9 minutes
7680: loss=2.609, avg loss=2.207, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=948.3 milliseconds, train=3.9 seconds, 491520 images, time remaining=18.8 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x14534a000000
7681: loss=1.958, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=975.1 milliseconds, train=1.5 seconds, 491584 images, time remaining=18.8 minutes
7682: loss=1.742, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=635.3 milliseconds, train=1.5 seconds, 491648 images, time remaining=18.7 minutes
7683: loss=2.272, avg loss=2.152, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=869.1 milliseconds, train=1.5 seconds, 491712 images, time remaining=18.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7684: loss=1.830, avg loss=2.119, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.5 seconds, 491776 images, time remaining=18.6 minutes
7685: loss=2.082, avg loss=2.116, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.5 seconds, 491840 images, time remaining=18.5 minutes
7686: loss=2.063, avg loss=2.110, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=933.2 milliseconds, train=1.5 seconds, 491904 images, time remaining=18.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7687: loss=2.243, avg loss=2.124, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=1.5 seconds, 491968 images, time remaining=18.4 minutes
7688: loss=2.348, avg loss=2.146, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=641.3 milliseconds, train=1.5 seconds, 492032 images, time remaining=18.4 minutes
7689: loss=2.152, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=1.5 seconds, 492096 images, time remaining=18.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7690: loss=1.648, avg loss=2.097, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=1.5 seconds, 492160 images, time remaining=18.2 minutes
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7691: loss=2.463, avg loss=2.133, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=4.9 seconds, 492224 images, time remaining=18.2 minutes
7692: loss=2.485, avg loss=2.169, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=620.5 milliseconds, train=4.8 seconds, 492288 images, time remaining=18.1 minutes
7693: loss=2.519, avg loss=2.204, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=496.6 milliseconds, train=4.8 seconds, 492352 images, time remaining=18.1 minutes
7694: loss=3.121, avg loss=2.295, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=494.2 milliseconds, train=4.9 seconds, 492416 images, time remaining=18 minutes
7695: loss=2.376, avg loss=2.303, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=901.8 milliseconds, train=4.8 seconds, 492480 images, time remaining=18 minutes
7696: loss=2.472, avg loss=2.320, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=889.6 milliseconds, train=4.8 seconds, 492544 images, time remaining=17.9 minutes
7697: loss=2.760, avg loss=2.364, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=607.2 milliseconds, train=4.8 seconds, 492608 images, time remaining=17.9 minutes
7698: loss=2.214, avg loss=2.349, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=558.1 milliseconds, train=4.8 seconds, 492672 images, time remaining=17.8 minutes
7699: loss=2.771, avg loss=2.391, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=673.7 milliseconds, train=4.8 seconds, 492736 images, time remaining=17.7 minutes
7700: loss=2.060, avg loss=2.358, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=4.8 seconds, 492800 images, time remaining=17.7 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 1088x864
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7701: loss=1.605, avg loss=2.283, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=3.5 seconds, 492864 images, time remaining=17.6 minutes
7702: loss=2.311, avg loss=2.286, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=842.1 milliseconds, train=3.5 seconds, 492928 images, time remaining=17.5 minutes
7703: loss=2.857, avg loss=2.343, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=847.4 milliseconds, train=3.5 seconds, 492992 images, time remaining=17.5 minutes
7704: loss=2.197, avg loss=2.328, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=3.5 seconds, 493056 images, time remaining=17.4 minutes
7705: loss=2.024, avg loss=2.298, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=3.5 seconds, 493120 images, time remaining=17.4 minutes
7706: loss=1.914, avg loss=2.259, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=836.5 milliseconds, train=3.5 seconds, 493184 images, time remaining=17.3 minutes
7707: loss=2.203, avg loss=2.254, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.5 seconds, 493248 images, time remaining=17.3 minutes
7708: loss=2.505, avg loss=2.279, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=586.1 milliseconds, train=3.5 seconds, 493312 images, time remaining=17.2 minutes
7709: loss=1.863, avg loss=2.237, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=705.2 milliseconds, train=3.6 seconds, 493376 images, time remaining=17.1 minutes
7710: loss=2.424, avg loss=2.256, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=3.6 seconds, 493440 images, time remaining=17.1 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7711: loss=2.736, avg loss=2.304, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=818.3 milliseconds, train=1.9 seconds, 493504 images, time remaining=17 minutes
7712: loss=2.205, avg loss=2.294, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=503.6 milliseconds, train=1.9 seconds, 493568 images, time remaining=17 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7713: loss=2.169, avg loss=2.282, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.9 seconds, 493632 images, time remaining=16.9 minutes
7714: loss=1.892, avg loss=2.243, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=599.6 milliseconds, train=1.9 seconds, 493696 images, time remaining=16.9 minutes
7715: loss=2.060, avg loss=2.224, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=855.7 milliseconds, train=1.9 seconds, 493760 images, time remaining=16.8 minutes
7716: loss=1.801, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=765.4 milliseconds, train=1.9 seconds, 493824 images, time remaining=16.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7717: loss=2.154, avg loss=2.179, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=1.9 seconds, 493888 images, time remaining=16.7 minutes
7718: loss=2.132, avg loss=2.175, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=1.9 seconds, 493952 images, time remaining=16.6 minutes
7719: loss=1.907, avg loss=2.148, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=935.9 milliseconds, train=1.9 seconds, 494016 images, time remaining=16.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7720: loss=2.297, avg loss=2.163, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.4 seconds, train=1.9 seconds, 494080 images, time remaining=16.5 minutes
Resizing, random_coef=1.40, batch=4, 992x768
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7721: loss=1.920, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=760.0 milliseconds, train=3.2 seconds, 494144 images, time remaining=16.4 minutes
7722: loss=2.231, avg loss=2.148, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.7 seconds, train=3.2 seconds, 494208 images, time remaining=16.4 minutes
7723: loss=2.047, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=873.7 milliseconds, train=3.2 seconds, 494272 images, time remaining=16.3 minutes
7724: loss=1.978, avg loss=2.122, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=3.2 seconds, 494336 images, time remaining=16.2 minutes
7725: loss=2.382, avg loss=2.148, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=792.5 milliseconds, train=3.2 seconds, 494400 images, time remaining=16.2 minutes
7726: loss=2.236, avg loss=2.156, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=3.2 seconds, 494464 images, time remaining=16.1 minutes
7727: loss=2.029, avg loss=2.144, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=3.2 seconds, 494528 images, time remaining=16.1 minutes
7728: loss=2.163, avg loss=2.146, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=3.2 seconds, 494592 images, time remaining=16 minutes
7729: loss=2.133, avg loss=2.144, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=3.2 seconds, 494656 images, time remaining=15.9 minutes
7730: loss=2.083, avg loss=2.138, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=795.4 milliseconds, train=3.2 seconds, 494720 images, time remaining=15.9 minutes
Resizing, random_coef=1.40, batch=4, 864x672
GPU #0: allocating workspace: 434.4 MiB begins at 0x146496000000
7731: loss=1.795, avg loss=2.104, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=845.9 milliseconds, train=1.6 seconds, 494784 images, time remaining=15.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7732: loss=2.307, avg loss=2.124, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.9 seconds, train=1.6 seconds, 494848 images, time remaining=15.8 minutes
7733: loss=1.767, avg loss=2.089, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=852.6 milliseconds, train=1.6 seconds, 494912 images, time remaining=15.7 minutes
7734: loss=2.241, avg loss=2.104, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=703.7 milliseconds, train=1.6 seconds, 494976 images, time remaining=15.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7735: loss=2.353, avg loss=2.129, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.9 seconds, train=1.6 seconds, 495040 images, time remaining=15.6 minutes
7736: loss=2.216, avg loss=2.137, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=882.7 milliseconds, train=1.6 seconds, 495104 images, time remaining=15.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7737: loss=1.782, avg loss=2.102, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=1.6 seconds, 495168 images, time remaining=15.5 minutes
7738: loss=2.162, avg loss=2.108, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=1.6 seconds, 495232 images, time remaining=15.4 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7739: loss=1.995, avg loss=2.097, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=1.6 seconds, 495296 images, time remaining=15.4 minutes
7740: loss=1.750, avg loss=2.062, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=693.7 milliseconds, train=1.6 seconds, 495360 images, time remaining=15.3 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7741: loss=2.437, avg loss=2.099, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.4 seconds, train=1.9 seconds, 495424 images, time remaining=15.2 minutes
7742: loss=1.995, avg loss=2.089, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=718.6 milliseconds, train=1.9 seconds, 495488 images, time remaining=15.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7743: loss=2.065, avg loss=2.087, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=1.9 seconds, 495552 images, time remaining=15.1 minutes
7744: loss=2.124, avg loss=2.090, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=907.2 milliseconds, train=1.9 seconds, 495616 images, time remaining=15.1 minutes
7745: loss=1.750, avg loss=2.056, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=601.7 milliseconds, train=1.9 seconds, 495680 images, time remaining=15 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7746: loss=1.924, avg loss=2.043, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=1.9 seconds, 495744 images, time remaining=14.9 minutes
7747: loss=1.783, avg loss=2.017, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=810.7 milliseconds, train=1.9 seconds, 495808 images, time remaining=14.9 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7748: loss=2.070, avg loss=2.022, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.9 seconds, 495872 images, time remaining=14.8 minutes
7749: loss=2.309, avg loss=2.051, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=1.9 seconds, 495936 images, time remaining=14.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7750: loss=2.275, avg loss=2.073, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.3 seconds, train=1.9 seconds, 496000 images, time remaining=14.7 minutes
Resizing, random_coef=1.40, batch=4, 1152x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7751: loss=2.197, avg loss=2.086, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=579.5 milliseconds, train=4.1 seconds, 496064 images, time remaining=14.7 minutes
7752: loss=2.341, avg loss=2.111, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.3 seconds, train=4.1 seconds, 496128 images, time remaining=14.6 minutes
7753: loss=2.250, avg loss=2.125, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=4.1 seconds, 496192 images, time remaining=14.5 minutes
7754: loss=1.908, avg loss=2.103, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=4.1 seconds, 496256 images, time remaining=14.5 minutes
7755: loss=2.317, avg loss=2.125, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.9 seconds, train=4.1 seconds, 496320 images, time remaining=14.4 minutes
7756: loss=2.549, avg loss=2.167, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=4.1 seconds, 496384 images, time remaining=14.3 minutes
7757: loss=2.321, avg loss=2.183, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=4.1 seconds, 496448 images, time remaining=14.3 minutes
7758: loss=2.393, avg loss=2.204, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=834.0 milliseconds, train=4.1 seconds, 496512 images, time remaining=14.2 minutes
7759: loss=2.563, avg loss=2.240, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=4.1 seconds, 496576 images, time remaining=14.2 minutes
7760: loss=2.364, avg loss=2.252, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=633.2 milliseconds, train=4.1 seconds, 496640 images, time remaining=14.1 minutes
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1463b8000000
7761: loss=2.099, avg loss=2.237, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=1.4 seconds, 496704 images, time remaining=14.1 minutes
7762: loss=2.879, avg loss=2.301, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=819.8 milliseconds, train=1.4 seconds, 496768 images, time remaining=14 minutes
7763: loss=2.232, avg loss=2.294, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.4 seconds, 496832 images, time remaining=13.9 minutes
7764: loss=2.206, avg loss=2.285, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=1.4 seconds, 496896 images, time remaining=13.9 minutes
7765: loss=1.865, avg loss=2.243, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=883.2 milliseconds, train=1.4 seconds, 496960 images, time remaining=13.8 minutes
7766: loss=2.093, avg loss=2.228, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=915.8 milliseconds, train=1.4 seconds, 497024 images, time remaining=13.8 minutes
7767: loss=2.421, avg loss=2.248, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=989.2 milliseconds, train=1.4 seconds, 497088 images, time remaining=13.7 minutes
7768: loss=1.886, avg loss=2.211, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=856.2 milliseconds, train=1.4 seconds, 497152 images, time remaining=13.6 minutes
7769: loss=1.632, avg loss=2.153, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=610.8 milliseconds, train=1.4 seconds, 497216 images, time remaining=13.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7770: loss=2.005, avg loss=2.139, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=1.4 seconds, 497280 images, time remaining=13.5 minutes
Resizing, random_coef=1.40, batch=4, 1344x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7771: loss=2.917, avg loss=2.216, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.9 seconds, train=5.0 seconds, 497344 images, time remaining=13.5 minutes
7772: loss=2.741, avg loss=2.269, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=728.3 milliseconds, train=4.9 seconds, 497408 images, time remaining=13.4 minutes
7773: loss=2.548, avg loss=2.297, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=884.8 milliseconds, train=5.0 seconds, 497472 images, time remaining=13.3 minutes
7774: loss=3.827, avg loss=2.450, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=5.0 seconds, 497536 images, time remaining=13.3 minutes
7775: loss=2.744, avg loss=2.479, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=4.9 seconds, 497600 images, time remaining=13.2 minutes
7776: loss=2.500, avg loss=2.481, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=729.8 milliseconds, train=4.9 seconds, 497664 images, time remaining=13.2 minutes
7777: loss=3.467, avg loss=2.580, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=893.1 milliseconds, train=4.9 seconds, 497728 images, time remaining=13.1 minutes
7778: loss=4.065, avg loss=2.728, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=5.0 seconds, 497792 images, time remaining=13.1 minutes
7779: loss=2.364, avg loss=2.692, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=4.9 seconds, 497856 images, time remaining=13 minutes
7780: loss=2.804, avg loss=2.703, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=960.3 milliseconds, train=4.9 seconds, 497920 images, time remaining=12.9 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7781: loss=2.120, avg loss=2.645, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=2.1 seconds, 497984 images, time remaining=12.9 minutes
7782: loss=1.987, avg loss=2.579, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=769.1 milliseconds, train=2.1 seconds, 498048 images, time remaining=12.8 minutes
7783: loss=2.033, avg loss=2.524, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=990.2 milliseconds, train=2.1 seconds, 498112 images, time remaining=12.8 minutes
7784: loss=1.883, avg loss=2.460, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=845.9 milliseconds, train=2.1 seconds, 498176 images, time remaining=12.7 minutes
7785: loss=1.914, avg loss=2.406, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 498240 images, time remaining=12.6 minutes
7786: loss=2.262, avg loss=2.391, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=858.2 milliseconds, train=2.1 seconds, 498304 images, time remaining=12.6 minutes
7787: loss=1.869, avg loss=2.339, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 498368 images, time remaining=12.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7788: loss=1.985, avg loss=2.304, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.7 seconds, train=2.1 seconds, 498432 images, time remaining=12.5 minutes
7789: loss=2.131, avg loss=2.286, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 498496 images, time remaining=12.4 minutes
7790: loss=2.059, avg loss=2.264, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=949.3 milliseconds, train=2.1 seconds, 498560 images, time remaining=12.4 minutes
Resizing, random_coef=1.40, batch=4, 1056x800
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7791: loss=2.631, avg loss=2.300, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=716.1 milliseconds, train=3.3 seconds, 498624 images, time remaining=12.3 minutes
7792: loss=1.746, avg loss=2.245, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=3.3 seconds, 498688 images, time remaining=12.2 minutes
7793: loss=1.573, avg loss=2.178, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=876.4 milliseconds, train=3.3 seconds, 498752 images, time remaining=12.2 minutes
7794: loss=2.030, avg loss=2.163, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=866.8 milliseconds, train=3.3 seconds, 498816 images, time remaining=12.1 minutes
7795: loss=1.827, avg loss=2.129, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.0 seconds, train=3.3 seconds, 498880 images, time remaining=12 minutes
7796: loss=1.957, avg loss=2.112, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=841.2 milliseconds, train=3.3 seconds, 498944 images, time remaining=12 minutes
7797: loss=2.257, avg loss=2.127, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=3.3 seconds, 499008 images, time remaining=11.9 minutes
7798: loss=2.368, avg loss=2.151, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.2 seconds, train=3.3 seconds, 499072 images, time remaining=11.9 minutes
7799: loss=1.973, avg loss=2.133, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=3.3 seconds, 499136 images, time remaining=11.8 minutes
7800: loss=1.957, avg loss=2.115, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=649.3 milliseconds, train=3.3 seconds, 499200 images, time remaining=11.8 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7801: loss=2.579, avg loss=2.162, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=2.1 seconds, 499264 images, time remaining=11.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7802: loss=2.367, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.4 seconds, train=2.1 seconds, 499328 images, time remaining=11.7 minutes
7803: loss=1.860, avg loss=2.150, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=2.1 seconds, 499392 images, time remaining=11.6 minutes
7804: loss=2.297, avg loss=2.165, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=2.1 seconds, 499456 images, time remaining=11.5 minutes
7805: loss=2.356, avg loss=2.184, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=2.1 seconds, 499520 images, time remaining=11.5 minutes
7806: loss=2.151, avg loss=2.181, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=634.4 milliseconds, train=2.1 seconds, 499584 images, time remaining=11.4 minutes
7807: loss=2.121, avg loss=2.175, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=894.0 milliseconds, train=2.1 seconds, 499648 images, time remaining=11.3 minutes
7808: loss=1.658, avg loss=2.123, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=2.1 seconds, 499712 images, time remaining=11.3 minutes
7809: loss=2.308, avg loss=2.141, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=2.1 seconds, 499776 images, time remaining=11.2 minutes
7810: loss=1.942, avg loss=2.121, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=761.6 milliseconds, train=2.1 seconds, 499840 images, time remaining=11.2 minutes
Resizing, random_coef=1.40, batch=4, 736x576
GPU #0: allocating workspace: 319.6 MiB begins at 0x146372000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7811: loss=1.639, avg loss=2.073, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=1.2 seconds, 499904 images, time remaining=11.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7812: loss=1.968, avg loss=2.063, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.2 seconds, 499968 images, time remaining=11 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7813: loss=2.171, avg loss=2.073, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.2 seconds, 500032 images, time remaining=11 minutes
7814: loss=1.968, avg loss=2.063, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=693.9 milliseconds, train=1.2 seconds, 500096 images, time remaining=10.9 minutes
7815: loss=2.105, avg loss=2.067, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=738.5 milliseconds, train=1.2 seconds, 500160 images, time remaining=10.9 minutes
7816: loss=2.314, avg loss=2.092, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=696.3 milliseconds, train=1.2 seconds, 500224 images, time remaining=10.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7817: loss=2.016, avg loss=2.084, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=1.2 seconds, 500288 images, time remaining=10.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7818: loss=2.702, avg loss=2.146, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=1.2 seconds, 500352 images, time remaining=10.7 minutes
7819: loss=2.103, avg loss=2.142, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=647.7 milliseconds, train=1.2 seconds, 500416 images, time remaining=10.6 minutes
7820: loss=2.366, avg loss=2.164, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=737.6 milliseconds, train=1.2 seconds, 500480 images, time remaining=10.6 minutes
Resizing, random_coef=1.40, batch=4, 960x736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
7821: loss=2.488, avg loss=2.197, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=2.1 seconds, 500544 images, time remaining=10.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7822: loss=1.587, avg loss=2.136, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=2.1 seconds, 500608 images, time remaining=10.4 minutes
7823: loss=2.437, avg loss=2.166, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=2.1 seconds, 500672 images, time remaining=10.4 minutes
7824: loss=2.332, avg loss=2.182, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=942.0 milliseconds, train=2.1 seconds, 500736 images, time remaining=10.3 minutes
7825: loss=2.068, avg loss=2.171, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=2.1 seconds, 500800 images, time remaining=10.3 minutes
7826: loss=1.640, avg loss=2.118, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=753.3 milliseconds, train=2.1 seconds, 500864 images, time remaining=10.2 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7827: loss=1.832, avg loss=2.089, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.0 seconds, train=2.1 seconds, 500928 images, time remaining=10.2 minutes
7828: loss=2.171, avg loss=2.097, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=741.9 milliseconds, train=2.1 seconds, 500992 images, time remaining=10.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7829: loss=1.936, avg loss=2.081, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.2 seconds, train=2.1 seconds, 501056 images, time remaining=10 minutes
7830: loss=1.974, avg loss=2.071, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=2.1 seconds, 501120 images, time remaining=10 minutes
Resizing, random_coef=1.40, batch=4, 800x608
GPU #0: allocating workspace: 365.4 MiB begins at 0x14643c000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7831: loss=2.085, avg loss=2.072, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.4 seconds, 501184 images, time remaining=9.9 minutes
7832: loss=2.332, avg loss=2.098, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.3 seconds, train=1.4 seconds, 501248 images, time remaining=9.9 minutes
7833: loss=2.040, avg loss=2.092, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=689.3 milliseconds, train=1.5 seconds, 501312 images, time remaining=9.8 minutes
7834: loss=2.910, avg loss=2.174, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=1.4 seconds, 501376 images, time remaining=9.8 minutes
7835: loss=2.122, avg loss=2.169, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=962.6 milliseconds, train=1.4 seconds, 501440 images, time remaining=9.7 minutes
7836: loss=1.872, avg loss=2.139, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=629.7 milliseconds, train=1.4 seconds, 501504 images, time remaining=9.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7837: loss=2.238, avg loss=2.149, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=1.4 seconds, 501568 images, time remaining=9.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7838: loss=2.216, avg loss=2.156, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=1.4 seconds, 501632 images, time remaining=9.5 minutes
7839: loss=2.535, avg loss=2.194, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=482.2 milliseconds, train=1.5 seconds, 501696 images, time remaining=9.4 minutes
7840: loss=1.990, avg loss=2.173, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=718.7 milliseconds, train=1.5 seconds, 501760 images, time remaining=9.4 minutes
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1464ba000000
7841: loss=2.044, avg loss=2.160, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=565.5 milliseconds, train=1.4 seconds, 501824 images, time remaining=9.3 minutes
7842: loss=2.024, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=865.8 milliseconds, train=1.4 seconds, 501888 images, time remaining=9.3 minutes
7843: loss=2.268, avg loss=2.159, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.4 seconds, 501952 images, time remaining=9.2 minutes
7844: loss=2.050, avg loss=2.148, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=794.5 milliseconds, train=1.4 seconds, 502016 images, time remaining=9.1 minutes
7845: loss=1.734, avg loss=2.107, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=600.5 milliseconds, train=1.4 seconds, 502080 images, time remaining=9.1 minutes
7846: loss=1.974, avg loss=2.093, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=993.1 milliseconds, train=1.4 seconds, 502144 images, time remaining=9 minutes
7847: loss=2.306, avg loss=2.115, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=658.2 milliseconds, train=1.4 seconds, 502208 images, time remaining=9 minutes
7848: loss=2.175, avg loss=2.121, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=462.4 milliseconds, train=1.4 seconds, 502272 images, time remaining=8.9 minutes
7849: loss=1.880, avg loss=2.097, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=1.4 seconds, 502336 images, time remaining=8.8 minutes
7850: loss=2.290, avg loss=2.116, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=899.8 milliseconds, train=1.4 seconds, 502400 images, time remaining=8.8 minutes
Resizing, random_coef=1.40, batch=4, 1184x896
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7851: loss=2.393, avg loss=2.144, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.9 seconds, train=3.8 seconds, 502464 images, time remaining=8.7 minutes
7852: loss=2.794, avg loss=2.209, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.6 seconds, train=3.8 seconds, 502528 images, time remaining=8.7 minutes
7853: loss=2.242, avg loss=2.212, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=769.3 milliseconds, train=3.8 seconds, 502592 images, time remaining=8.6 minutes
7854: loss=2.201, avg loss=2.211, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.1 seconds, train=3.8 seconds, 502656 images, time remaining=8.6 minutes
7855: loss=2.274, avg loss=2.217, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.8 seconds, 502720 images, time remaining=8.5 minutes
7856: loss=2.428, avg loss=2.238, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=3.8 seconds, 502784 images, time remaining=8.4 minutes
7857: loss=2.267, avg loss=2.241, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=930.7 milliseconds, train=3.8 seconds, 502848 images, time remaining=8.4 minutes
7858: loss=1.993, avg loss=2.216, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.4 seconds, train=3.8 seconds, 502912 images, time remaining=8.3 minutes
7859: loss=2.616, avg loss=2.256, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=937.0 milliseconds, train=3.8 seconds, 502976 images, time remaining=8.3 minutes
7860: loss=2.162, avg loss=2.247, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.3 seconds, train=3.8 seconds, 503040 images, time remaining=8.2 minutes
Resizing, random_coef=1.40, batch=4, 1088x832
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7861: loss=2.027, avg loss=2.225, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=3.4 seconds, 503104 images, time remaining=8.1 minutes
7862: loss=2.073, avg loss=2.210, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=3.4 seconds, 503168 images, time remaining=8.1 minutes
7863: loss=2.070, avg loss=2.196, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=3.4 seconds, 503232 images, time remaining=8 minutes
7864: loss=2.469, avg loss=2.223, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.7 seconds, train=3.4 seconds, 503296 images, time remaining=8 minutes
7865: loss=2.285, avg loss=2.229, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=3.4 seconds, 503360 images, time remaining=7.9 minutes
7866: loss=2.567, avg loss=2.263, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=948.3 milliseconds, train=3.4 seconds, 503424 images, time remaining=7.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7867: loss=1.478, avg loss=2.184, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.8 seconds, train=3.4 seconds, 503488 images, time remaining=7.8 minutes
7868: loss=1.969, avg loss=2.163, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=948.6 milliseconds, train=3.4 seconds, 503552 images, time remaining=7.7 minutes
7869: loss=1.818, avg loss=2.128, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.0 seconds, train=3.4 seconds, 503616 images, time remaining=7.7 minutes
7870: loss=1.983, avg loss=2.114, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=3.4 seconds, train=3.4 seconds, 503680 images, time remaining=7.6 minutes
Resizing, random_coef=1.40, batch=4, 896x704
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7871: loss=2.237, avg loss=2.126, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.9 seconds, 503744 images, time remaining=7.6 minutes
7872: loss=1.950, avg loss=2.109, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=702.7 milliseconds, train=1.9 seconds, 503808 images, time remaining=7.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7873: loss=1.844, avg loss=2.082, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.9 seconds, 503872 images, time remaining=7.4 minutes
7874: loss=2.267, avg loss=2.101, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=700.4 milliseconds, train=1.9 seconds, 503936 images, time remaining=7.4 minutes
7875: loss=1.897, avg loss=2.080, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=882.5 milliseconds, train=1.9 seconds, 504000 images, time remaining=7.3 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7876: loss=2.363, avg loss=2.108, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=1.9 seconds, 504064 images, time remaining=7.3 minutes
7877: loss=1.917, avg loss=2.089, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.0 seconds, train=1.9 seconds, 504128 images, time remaining=7.2 minutes
7878: loss=2.228, avg loss=2.103, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.9 seconds, 504192 images, time remaining=7.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7879: loss=2.525, avg loss=2.145, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.9 seconds, train=1.9 seconds, 504256 images, time remaining=7.1 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7880: loss=2.160, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=4.1 seconds, train=1.9 seconds, 504320 images, time remaining=7 minutes
Resizing, random_coef=1.40, batch=4, 800x640
GPU #0: allocating workspace: 384.1 MiB begins at 0x146428000000
7881: loss=1.967, avg loss=2.129, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=1.5 seconds, 504384 images, time remaining=7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7882: loss=1.989, avg loss=2.115, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.4 seconds, train=1.5 seconds, 504448 images, time remaining=6.9 minutes
7883: loss=1.971, avg loss=2.100, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=946.7 milliseconds, train=1.5 seconds, 504512 images, time remaining=6.8 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7884: loss=1.647, avg loss=2.055, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.8 seconds, train=1.5 seconds, 504576 images, time remaining=6.8 minutes
7885: loss=2.295, avg loss=2.079, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.5 seconds, train=1.5 seconds, 504640 images, time remaining=6.7 minutes
7886: loss=2.108, avg loss=2.082, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=771.3 milliseconds, train=1.5 seconds, 504704 images, time remaining=6.7 minutes
7887: loss=2.403, avg loss=2.114, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.2 seconds, train=1.5 seconds, 504768 images, time remaining=6.6 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7888: loss=2.175, avg loss=2.120, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.6 seconds, train=1.5 seconds, 504832 images, time remaining=6.6 minutes
7889: loss=1.724, avg loss=2.081, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=860.7 milliseconds, train=1.5 seconds, 504896 images, time remaining=6.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7890: loss=2.266, avg loss=2.099, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=4.3 seconds, train=1.5 seconds, 504960 images, time remaining=6.4 minutes
Resizing, random_coef=1.40, batch=4, 1312x1024
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7891: loss=2.279, avg loss=2.117, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=739.4 milliseconds, train=4.9 seconds, 505024 images, time remaining=6.4 minutes
7892: loss=2.421, avg loss=2.147, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=989.3 milliseconds, train=4.9 seconds, 505088 images, time remaining=6.3 minutes
7893: loss=2.641, avg loss=2.197, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=2.5 seconds, train=4.8 seconds, 505152 images, time remaining=6.3 minutes
7894: loss=2.723, avg loss=2.249, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.4 seconds, train=4.9 seconds, 505216 images, time remaining=6.2 minutes
7895: loss=3.308, avg loss=2.355, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.8 seconds, train=4.9 seconds, 505280 images, time remaining=6.1 minutes
7896: loss=2.080, avg loss=2.328, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.9 seconds, train=4.8 seconds, 505344 images, time remaining=6.1 minutes
7897: loss=2.753, avg loss=2.370, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.6 seconds, train=4.8 seconds, 505408 images, time remaining=6 minutes
7898: loss=2.847, avg loss=2.418, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=4.8 seconds, 505472 images, time remaining=6 minutes
7899: loss=2.049, avg loss=2.381, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=1.1 seconds, train=4.8 seconds, 505536 images, time remaining=5.9 minutes
7900: loss=2.900, avg loss=2.433, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=785.3 milliseconds, train=4.8 seconds, 505600 images, time remaining=5.9 minutes
Saving weights to /workspace/.cache/splits/combined_last.weights
Resizing, random_coef=1.40, batch=4, 768x608
GPU #0: allocating workspace: 351.1 MiB begins at 0x1458ec000000
7901: loss=2.831, avg loss=2.473, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=772.9 milliseconds, train=1.4 seconds, 505664 images, time remaining=5.8 minutes
7902: loss=2.026, avg loss=2.428, last=98.02%, best=98.02%, next=7902, rate=0.00001300, load 64=618.1 milliseconds, train=1.4 seconds, 505728 images, time remaining=5.7 minutes
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x145b40000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=87412, unique_truth_count=57264
rank=0 of ranks=87412rank=100 of ranks=87412rank=200 of ranks=87412rank=300 of ranks=87412rank=400 of ranks=87412rank=500 of ranks=87412rank=600 of ranks=87412rank=700 of ranks=87412rank=800 of ranks=87412rank=900 of ranks=87412rank=1000 of ranks=87412rank=1100 of ranks=87412rank=1200 of ranks=87412rank=1300 of ranks=87412rank=1400 of ranks=87412rank=1500 of ranks=87412rank=1600 of ranks=87412rank=1700 of ranks=87412rank=1800 of ranks=87412rank=1900 of ranks=87412rank=2000 of ranks=87412rank=2100 of ranks=87412rank=2200 of ranks=87412rank=2300 of ranks=87412rank=2400 of ranks=87412rank=2500 of ranks=87412rank=2600 of ranks=87412rank=2700 of ranks=87412rank=2800 of ranks=87412rank=2900 of ranks=87412rank=3000 of ranks=87412rank=3100 of ranks=87412rank=3200 of ranks=87412rank=3300 of ranks=87412rank=3400 of ranks=87412rank=3500 of ranks=87412rank=3600 of ranks=87412rank=3700 of ranks=87412rank=3800 of ranks=87412rank=3900 of ranks=87412rank=4000 of ranks=87412rank=4100 of ranks=87412rank=4200 of ranks=87412rank=4300 of ranks=87412rank=4400 of ranks=87412rank=4500 of ranks=87412rank=4600 of ranks=87412rank=4700 of ranks=87412rank=4800 of ranks=87412rank=4900 of ranks=87412rank=5000 of ranks=87412rank=5100 of ranks=87412rank=5200 of ranks=87412rank=5300 of ranks=87412rank=5400 of ranks=87412rank=5500 of ranks=87412rank=5600 of ranks=87412rank=5700 of ranks=87412rank=5800 of ranks=87412rank=5900 of ranks=87412rank=6000 of ranks=87412rank=6100 of ranks=87412rank=6200 of ranks=87412rank=6300 of ranks=87412rank=6400 of ranks=87412rank=6500 of ranks=87412rank=6600 of ranks=87412rank=6700 of ranks=87412rank=6800 of ranks=87412rank=6900 of ranks=87412rank=7000 of ranks=87412rank=7100 of ranks=87412rank=7200 of ranks=87412rank=7300 of ranks=87412rank=7400 of ranks=87412rank=7500 of ranks=87412rank=7600 of ranks=87412rank=7700 of ranks=87412rank=7800 of ranks=87412rank=7900 of ranks=87412rank=8000 of ranks=87412rank=8100 of ranks=87412rank=8200 of ranks=87412rank=8300 of ranks=87412rank=8400 of ranks=87412rank=8500 of ranks=87412rank=8600 of ranks=87412rank=8700 of ranks=87412rank=8800 of ranks=87412rank=8900 of ranks=87412rank=9000 of ranks=87412rank=9100 of ranks=87412rank=9200 of ranks=87412rank=9300 of ranks=87412rank=9400 of ranks=87412rank=9500 of ranks=87412rank=9600 of ranks=87412rank=9700 of ranks=87412rank=9800 of ranks=87412rank=9900 of ranks=87412rank=10000 of ranks=87412rank=10100 of ranks=87412rank=10200 of ranks=87412rank=10300 of ranks=87412rank=10400 of ranks=87412rank=10500 of ranks=87412rank=10600 of ranks=87412rank=10700 of ranks=87412rank=10800 of ranks=87412rank=10900 of ranks=87412rank=11000 of ranks=87412rank=11100 of ranks=87412rank=11200 of ranks=87412rank=11300 of ranks=87412rank=11400 of ranks=87412rank=11500 of ranks=87412rank=11600 of ranks=87412rank=11700 of ranks=87412rank=11800 of ranks=87412rank=11900 of ranks=87412rank=12000 of ranks=87412rank=12100 of ranks=87412rank=12200 of ranks=87412rank=12300 of ranks=87412rank=12400 of ranks=87412rank=12500 of ranks=87412rank=12600 of ranks=87412rank=12700 of ranks=87412rank=12800 of ranks=87412rank=12900 of ranks=87412rank=13000 of ranks=87412rank=13100 of ranks=87412rank=13200 of ranks=87412rank=13300 of ranks=87412rank=13400 of ranks=87412rank=13500 of ranks=87412rank=13600 of ranks=87412rank=13700 of ranks=87412rank=13800 of ranks=87412rank=13900 of ranks=87412rank=14000 of ranks=87412rank=14100 of ranks=87412rank=14200 of ranks=87412rank=14300 of ranks=87412rank=14400 of ranks=87412rank=14500 of ranks=87412rank=14600 of ranks=87412rank=14700 of ranks=87412rank=14800 of ranks=87412rank=14900 of ranks=87412rank=15000 of ranks=87412rank=15100 of ranks=87412rank=15200 of ranks=87412rank=15300 of ranks=87412rank=15400 of ranks=87412rank=15500 of ranks=87412rank=15600 of ranks=87412rank=15700 of ranks=87412rank=15800 of ranks=87412rank=15900 of ranks=87412rank=16000 of ranks=87412rank=16100 of ranks=87412rank=16200 of ranks=87412rank=16300 of ranks=87412rank=16400 of ranks=87412rank=16500 of ranks=87412rank=16600 of ranks=87412rank=16700 of ranks=87412rank=16800 of ranks=87412rank=16900 of ranks=87412rank=17000 of ranks=87412rank=17100 of ranks=87412rank=17200 of ranks=87412rank=17300 of ranks=87412rank=17400 of ranks=87412rank=17500 of ranks=87412rank=17600 of ranks=87412rank=17700 of ranks=87412rank=17800 of ranks=87412rank=17900 of ranks=87412rank=18000 of ranks=87412rank=18100 of ranks=87412rank=18200 of ranks=87412rank=18300 of ranks=87412rank=18400 of ranks=87412rank=18500 of ranks=87412rank=18600 of ranks=87412rank=18700 of ranks=87412rank=18800 of ranks=87412rank=18900 of ranks=87412rank=19000 of ranks=87412rank=19100 of ranks=87412rank=19200 of ranks=87412rank=19300 of ranks=87412rank=19400 of ranks=87412rank=19500 of ranks=87412rank=19600 of ranks=87412rank=19700 of ranks=87412rank=19800 of ranks=87412rank=19900 of ranks=87412rank=20000 of ranks=87412rank=20100 of ranks=87412rank=20200 of ranks=87412rank=20300 of ranks=87412rank=20400 of ranks=87412rank=20500 of ranks=87412rank=20600 of ranks=87412rank=20700 of ranks=87412rank=20800 of ranks=87412rank=20900 of ranks=87412rank=21000 of ranks=87412rank=21100 of ranks=87412rank=21200 of ranks=87412rank=21300 of ranks=87412rank=21400 of ranks=87412rank=21500 of ranks=87412rank=21600 of ranks=87412rank=21700 of ranks=87412rank=21800 of ranks=87412rank=21900 of ranks=87412rank=22000 of ranks=87412rank=22100 of ranks=87412rank=22200 of ranks=87412rank=22300 of ranks=87412rank=22400 of ranks=87412rank=22500 of ranks=87412rank=22600 of ranks=87412rank=22700 of ranks=87412rank=22800 of ranks=87412rank=22900 of ranks=87412rank=23000 of ranks=87412rank=23100 of ranks=87412rank=23200 of ranks=87412rank=23300 of ranks=87412rank=23400 of ranks=87412rank=23500 of ranks=87412rank=23600 of ranks=87412rank=23700 of ranks=87412rank=23800 of ranks=87412rank=23900 of ranks=87412rank=24000 of ranks=87412rank=24100 of ranks=87412rank=24200 of ranks=87412rank=24300 of ranks=87412rank=24400 of ranks=87412rank=24500 of ranks=87412rank=24600 of ranks=87412rank=24700 of ranks=87412rank=24800 of ranks=87412rank=24900 of ranks=87412rank=25000 of ranks=87412rank=25100 of ranks=87412rank=25200 of ranks=87412rank=25300 of ranks=87412rank=25400 of ranks=87412rank=25500 of ranks=87412rank=25600 of ranks=87412rank=25700 of ranks=87412rank=25800 of ranks=87412rank=25900 of ranks=87412rank=26000 of ranks=87412rank=26100 of ranks=87412rank=26200 of ranks=87412rank=26300 of ranks=87412rank=26400 of ranks=87412rank=26500 of ranks=87412rank=26600 of ranks=87412rank=26700 of ranks=87412rank=26800 of ranks=87412rank=26900 of ranks=87412rank=27000 of ranks=87412rank=27100 of ranks=87412rank=27200 of ranks=87412rank=27300 of ranks=87412rank=27400 of ranks=87412rank=27500 of ranks=87412rank=27600 of ranks=87412rank=27700 of ranks=87412rank=27800 of ranks=87412rank=27900 of ranks=87412rank=28000 of ranks=87412rank=28100 of ranks=87412rank=28200 of ranks=87412rank=28300 of ranks=87412rank=28400 of ranks=87412rank=28500 of ranks=87412rank=28600 of ranks=87412rank=28700 of ranks=87412rank=28800 of ranks=87412rank=28900 of ranks=87412rank=29000 of ranks=87412rank=29100 of ranks=87412rank=29200 of ranks=87412rank=29300 of ranks=87412rank=29400 of ranks=87412rank=29500 of ranks=87412rank=29600 of ranks=87412rank=29700 of ranks=87412rank=29800 of ranks=87412rank=29900 of ranks=87412rank=30000 of ranks=87412rank=30100 of ranks=87412rank=30200 of ranks=87412rank=30300 of ranks=87412rank=30400 of ranks=87412rank=30500 of ranks=87412rank=30600 of ranks=87412rank=30700 of ranks=87412rank=30800 of ranks=87412rank=30900 of ranks=87412rank=31000 of ranks=87412rank=31100 of ranks=87412rank=31200 of ranks=87412rank=31300 of ranks=87412rank=31400 of ranks=87412rank=31500 of ranks=87412rank=31600 of ranks=87412rank=31700 of ranks=87412rank=31800 of ranks=87412rank=31900 of ranks=87412rank=32000 of ranks=87412rank=32100 of ranks=87412rank=32200 of ranks=87412rank=32300 of ranks=87412rank=32400 of ranks=87412rank=32500 of ranks=87412rank=32600 of ranks=87412rank=32700 of ranks=87412rank=32800 of ranks=87412rank=32900 of ranks=87412rank=33000 of ranks=87412rank=33100 of ranks=87412rank=33200 of ranks=87412rank=33300 of ranks=87412rank=33400 of ranks=87412rank=33500 of ranks=87412rank=33600 of ranks=87412rank=33700 of ranks=87412rank=33800 of ranks=87412rank=33900 of ranks=87412rank=34000 of ranks=87412rank=34100 of ranks=87412rank=34200 of ranks=87412rank=34300 of ranks=87412rank=34400 of ranks=87412rank=34500 of ranks=87412rank=34600 of ranks=87412rank=34700 of ranks=87412rank=34800 of ranks=87412rank=34900 of ranks=87412rank=35000 of ranks=87412rank=35100 of ranks=87412rank=35200 of ranks=87412rank=35300 of ranks=87412rank=35400 of ranks=87412rank=35500 of ranks=87412rank=35600 of ranks=87412rank=35700 of ranks=87412rank=35800 of ranks=87412rank=35900 of ranks=87412rank=36000 of ranks=87412rank=36100 of ranks=87412rank=36200 of ranks=87412rank=36300 of ranks=87412rank=36400 of ranks=87412rank=36500 of ranks=87412rank=36600 of ranks=87412rank=36700 of ranks=87412rank=36800 of ranks=87412rank=36900 of ranks=87412rank=37000 of ranks=87412rank=37100 of ranks=87412rank=37200 of ranks=87412rank=37300 of ranks=87412rank=37400 of ranks=87412rank=37500 of ranks=87412rank=37600 of ranks=87412rank=37700 of ranks=87412rank=37800 of ranks=87412rank=37900 of ranks=87412rank=38000 of ranks=87412rank=38100 of ranks=87412rank=38200 of ranks=87412rank=38300 of ranks=87412rank=38400 of ranks=87412rank=38500 of ranks=87412rank=38600 of ranks=87412rank=38700 of ranks=87412rank=38800 of ranks=87412rank=38900 of ranks=87412rank=39000 of ranks=87412rank=39100 of ranks=87412rank=39200 of ranks=87412rank=39300 of ranks=87412rank=39400 of ranks=87412rank=39500 of ranks=87412rank=39600 of ranks=87412rank=39700 of ranks=87412rank=39800 of ranks=87412rank=39900 of ranks=87412rank=40000 of ranks=87412rank=40100 of ranks=87412rank=40200 of ranks=87412rank=40300 of ranks=87412rank=40400 of ranks=87412rank=40500 of ranks=87412rank=40600 of ranks=87412rank=40700 of ranks=87412rank=40800 of ranks=87412rank=40900 of ranks=87412rank=41000 of ranks=87412rank=41100 of ranks=87412rank=41200 of ranks=87412rank=41300 of ranks=87412rank=41400 of ranks=87412rank=41500 of ranks=87412rank=41600 of ranks=87412rank=41700 of ranks=87412rank=41800 of ranks=87412rank=41900 of ranks=87412rank=42000 of ranks=87412rank=42100 of ranks=87412rank=42200 of ranks=87412rank=42300 of ranks=87412rank=42400 of ranks=87412rank=42500 of ranks=87412rank=42600 of ranks=87412rank=42700 of ranks=87412rank=42800 of ranks=87412rank=42900 of ranks=87412rank=43000 of ranks=87412rank=43100 of ranks=87412rank=43200 of ranks=87412rank=43300 of ranks=87412rank=43400 of ranks=87412rank=43500 of ranks=87412rank=43600 of ranks=87412rank=43700 of ranks=87412rank=43800 of ranks=87412rank=43900 of ranks=87412rank=44000 of ranks=87412rank=44100 of ranks=87412rank=44200 of ranks=87412rank=44300 of ranks=87412rank=44400 of ranks=87412rank=44500 of ranks=87412rank=44600 of ranks=87412rank=44700 of ranks=87412rank=44800 of ranks=87412rank=44900 of ranks=87412rank=45000 of ranks=87412rank=45100 of ranks=87412rank=45200 of ranks=87412rank=45300 of ranks=87412rank=45400 of ranks=87412rank=45500 of ranks=87412rank=45600 of ranks=87412rank=45700 of ranks=87412rank=45800 of ranks=87412rank=45900 of ranks=87412rank=46000 of ranks=87412rank=46100 of ranks=87412rank=46200 of ranks=87412rank=46300 of ranks=87412rank=46400 of ranks=87412rank=46500 of ranks=87412rank=46600 of ranks=87412rank=46700 of ranks=87412rank=46800 of ranks=87412rank=46900 of ranks=87412rank=47000 of ranks=87412rank=47100 of ranks=87412rank=47200 of ranks=87412rank=47300 of ranks=87412rank=47400 of ranks=87412rank=47500 of ranks=87412rank=47600 of ranks=87412rank=47700 of ranks=87412rank=47800 of ranks=87412rank=47900 of ranks=87412rank=48000 of ranks=87412rank=48100 of ranks=87412rank=48200 of ranks=87412rank=48300 of ranks=87412rank=48400 of ranks=87412rank=48500 of ranks=87412rank=48600 of ranks=87412rank=48700 of ranks=87412rank=48800 of ranks=87412rank=48900 of ranks=87412rank=49000 of ranks=87412rank=49100 of ranks=87412rank=49200 of ranks=87412rank=49300 of ranks=87412rank=49400 of ranks=87412rank=49500 of ranks=87412rank=49600 of ranks=87412rank=49700 of ranks=87412rank=49800 of ranks=87412rank=49900 of ranks=87412rank=50000 of ranks=87412rank=50100 of ranks=87412rank=50200 of ranks=87412rank=50300 of ranks=87412rank=50400 of ranks=87412rank=50500 of ranks=87412rank=50600 of ranks=87412rank=50700 of ranks=87412rank=50800 of ranks=87412rank=50900 of ranks=87412rank=51000 of ranks=87412rank=51100 of ranks=87412rank=51200 of ranks=87412rank=51300 of ranks=87412rank=51400 of ranks=87412rank=51500 of ranks=87412rank=51600 of ranks=87412rank=51700 of ranks=87412rank=51800 of ranks=87412rank=51900 of ranks=87412rank=52000 of ranks=87412rank=52100 of ranks=87412rank=52200 of ranks=87412rank=52300 of ranks=87412rank=52400 of ranks=87412rank=52500 of ranks=87412rank=52600 of ranks=87412rank=52700 of ranks=87412rank=52800 of ranks=87412rank=52900 of ranks=87412rank=53000 of ranks=87412rank=53100 of ranks=87412rank=53200 of ranks=87412rank=53300 of ranks=87412rank=53400 of ranks=87412rank=53500 of ranks=87412rank=53600 of ranks=87412rank=53700 of ranks=87412rank=53800 of ranks=87412rank=53900 of ranks=87412rank=54000 of ranks=87412rank=54100 of ranks=87412rank=54200 of ranks=87412rank=54300 of ranks=87412rank=54400 of ranks=87412rank=54500 of ranks=87412rank=54600 of ranks=87412rank=54700 of ranks=87412rank=54800 of ranks=87412rank=54900 of ranks=87412rank=55000 of ranks=87412rank=55100 of ranks=87412rank=55200 of ranks=87412rank=55300 of ranks=87412rank=55400 of ranks=87412rank=55500 of ranks=87412rank=55600 of ranks=87412rank=55700 of ranks=87412rank=55800 of ranks=87412rank=55900 of ranks=87412rank=56000 of ranks=87412rank=56100 of ranks=87412rank=56200 of ranks=87412rank=56300 of ranks=87412rank=56400 of ranks=87412rank=56500 of ranks=87412rank=56600 of ranks=87412rank=56700 of ranks=87412rank=56800 of ranks=87412rank=56900 of ranks=87412rank=57000 of ranks=87412rank=57100 of ranks=87412rank=57200 of ranks=87412rank=57300 of ranks=87412rank=57400 of ranks=87412rank=57500 of ranks=87412rank=57600 of ranks=87412rank=57700 of ranks=87412rank=57800 of ranks=87412rank=57900 of ranks=87412rank=58000 of ranks=87412rank=58100 of ranks=87412rank=58200 of ranks=87412rank=58300 of ranks=87412rank=58400 of ranks=87412rank=58500 of ranks=87412rank=58600 of ranks=87412rank=58700 of ranks=87412rank=58800 of ranks=87412rank=58900 of ranks=87412rank=59000 of ranks=87412rank=59100 of ranks=87412rank=59200 of ranks=87412rank=59300 of ranks=87412rank=59400 of ranks=87412rank=59500 of ranks=87412rank=59600 of ranks=87412rank=59700 of ranks=87412rank=59800 of ranks=87412rank=59900 of ranks=87412rank=60000 of ranks=87412rank=60100 of ranks=87412rank=60200 of ranks=87412rank=60300 of ranks=87412rank=60400 of ranks=87412rank=60500 of ranks=87412rank=60600 of ranks=87412rank=60700 of ranks=87412rank=60800 of ranks=87412rank=60900 of ranks=87412rank=61000 of ranks=87412rank=61100 of ranks=87412rank=61200 of ranks=87412rank=61300 of ranks=87412rank=61400 of ranks=87412rank=61500 of ranks=87412rank=61600 of ranks=87412rank=61700 of ranks=87412rank=61800 of ranks=87412rank=61900 of ranks=87412rank=62000 of ranks=87412rank=62100 of ranks=87412rank=62200 of ranks=87412rank=62300 of ranks=87412rank=62400 of ranks=87412rank=62500 of ranks=87412rank=62600 of ranks=87412rank=62700 of ranks=87412rank=62800 of ranks=87412rank=62900 of ranks=87412rank=63000 of ranks=87412rank=63100 of ranks=87412rank=63200 of ranks=87412rank=63300 of ranks=87412rank=63400 of ranks=87412rank=63500 of ranks=87412rank=63600 of ranks=87412rank=63700 of ranks=87412rank=63800 of ranks=87412rank=63900 of ranks=87412rank=64000 of ranks=87412rank=64100 of ranks=87412rank=64200 of ranks=87412rank=64300 of ranks=87412rank=64400 of ranks=87412rank=64500 of ranks=87412rank=64600 of ranks=87412rank=64700 of ranks=87412rank=64800 of ranks=87412rank=64900 of ranks=87412rank=65000 of ranks=87412rank=65100 of ranks=87412rank=65200 of ranks=87412rank=65300 of ranks=87412rank=65400 of ranks=87412rank=65500 of ranks=87412rank=65600 of ranks=87412rank=65700 of ranks=87412rank=65800 of ranks=87412rank=65900 of ranks=87412rank=66000 of ranks=87412rank=66100 of ranks=87412rank=66200 of ranks=87412rank=66300 of ranks=87412rank=66400 of ranks=87412rank=66500 of ranks=87412rank=66600 of ranks=87412rank=66700 of ranks=87412rank=66800 of ranks=87412rank=66900 of ranks=87412rank=67000 of ranks=87412rank=67100 of ranks=87412rank=67200 of ranks=87412rank=67300 of ranks=87412rank=67400 of ranks=87412rank=67500 of ranks=87412rank=67600 of ranks=87412rank=67700 of ranks=87412rank=67800 of ranks=87412rank=67900 of ranks=87412rank=68000 of ranks=87412rank=68100 of ranks=87412rank=68200 of ranks=87412rank=68300 of ranks=87412rank=68400 of ranks=87412rank=68500 of ranks=87412rank=68600 of ranks=87412rank=68700 of ranks=87412rank=68800 of ranks=87412rank=68900 of ranks=87412rank=69000 of ranks=87412rank=69100 of ranks=87412rank=69200 of ranks=87412rank=69300 of ranks=87412rank=69400 of ranks=87412rank=69500 of ranks=87412rank=69600 of ranks=87412rank=69700 of ranks=87412rank=69800 of ranks=87412rank=69900 of ranks=87412rank=70000 of ranks=87412rank=70100 of ranks=87412rank=70200 of ranks=87412rank=70300 of ranks=87412rank=70400 of ranks=87412rank=70500 of ranks=87412rank=70600 of ranks=87412rank=70700 of ranks=87412rank=70800 of ranks=87412rank=70900 of ranks=87412rank=71000 of ranks=87412rank=71100 of ranks=87412rank=71200 of ranks=87412rank=71300 of ranks=87412rank=71400 of ranks=87412rank=71500 of ranks=87412rank=71600 of ranks=87412rank=71700 of ranks=87412rank=71800 of ranks=87412rank=71900 of ranks=87412rank=72000 of ranks=87412rank=72100 of ranks=87412rank=72200 of ranks=87412rank=72300 of ranks=87412rank=72400 of ranks=87412rank=72500 of ranks=87412rank=72600 of ranks=87412rank=72700 of ranks=87412rank=72800 of ranks=87412rank=72900 of ranks=87412rank=73000 of ranks=87412rank=73100 of ranks=87412rank=73200 of ranks=87412rank=73300 of ranks=87412rank=73400 of ranks=87412rank=73500 of ranks=87412rank=73600 of ranks=87412rank=73700 of ranks=87412rank=73800 of ranks=87412rank=73900 of ranks=87412rank=74000 of ranks=87412rank=74100 of ranks=87412rank=74200 of ranks=87412rank=74300 of ranks=87412rank=74400 of ranks=87412rank=74500 of ranks=87412rank=74600 of ranks=87412rank=74700 of ranks=87412rank=74800 of ranks=87412rank=74900 of ranks=87412rank=75000 of ranks=87412rank=75100 of ranks=87412rank=75200 of ranks=87412rank=75300 of ranks=87412rank=75400 of ranks=87412rank=75500 of ranks=87412rank=75600 of ranks=87412rank=75700 of ranks=87412rank=75800 of ranks=87412rank=75900 of ranks=87412rank=76000 of ranks=87412rank=76100 of ranks=87412rank=76200 of ranks=87412rank=76300 of ranks=87412rank=76400 of ranks=87412rank=76500 of ranks=87412rank=76600 of ranks=87412rank=76700 of ranks=87412rank=76800 of ranks=87412rank=76900 of ranks=87412rank=77000 of ranks=87412rank=77100 of ranks=87412rank=77200 of ranks=87412rank=77300 of ranks=87412rank=77400 of ranks=87412rank=77500 of ranks=87412rank=77600 of ranks=87412rank=77700 of ranks=87412rank=77800 of ranks=87412rank=77900 of ranks=87412rank=78000 of ranks=87412rank=78100 of ranks=87412rank=78200 of ranks=87412rank=78300 of ranks=87412rank=78400 of ranks=87412rank=78500 of ranks=87412rank=78600 of ranks=87412rank=78700 of ranks=87412rank=78800 of ranks=87412rank=78900 of ranks=87412rank=79000 of ranks=87412rank=79100 of ranks=87412rank=79200 of ranks=87412rank=79300 of ranks=87412rank=79400 of ranks=87412rank=79500 of ranks=87412rank=79600 of ranks=87412rank=79700 of ranks=87412rank=79800 of ranks=87412rank=79900 of ranks=87412rank=80000 of ranks=87412rank=80100 of ranks=87412rank=80200 of ranks=87412rank=80300 of ranks=87412rank=80400 of ranks=87412rank=80500 of ranks=87412rank=80600 of ranks=87412rank=80700 of ranks=87412rank=80800 of ranks=87412rank=80900 of ranks=87412rank=81000 of ranks=87412rank=81100 of ranks=87412rank=81200 of ranks=87412rank=81300 of ranks=87412rank=81400 of ranks=87412rank=81500 of ranks=87412rank=81600 of ranks=87412rank=81700 of ranks=87412rank=81800 of ranks=87412rank=81900 of ranks=87412rank=82000 of ranks=87412rank=82100 of ranks=87412rank=82200 of ranks=87412rank=82300 of ranks=87412rank=82400 of ranks=87412rank=82500 of ranks=87412rank=82600 of ranks=87412rank=82700 of ranks=87412rank=82800 of ranks=87412rank=82900 of ranks=87412rank=83000 of ranks=87412rank=83100 of ranks=87412rank=83200 of ranks=87412rank=83300 of ranks=87412rank=83400 of ranks=87412rank=83500 of ranks=87412rank=83600 of ranks=87412rank=83700 of ranks=87412rank=83800 of ranks=87412rank=83900 of ranks=87412rank=84000 of ranks=87412rank=84100 of ranks=87412rank=84200 of ranks=87412rank=84300 of ranks=87412rank=84400 of ranks=87412rank=84500 of ranks=87412rank=84600 of ranks=87412rank=84700 of ranks=87412rank=84800 of ranks=87412rank=84900 of ranks=87412rank=85000 of ranks=87412rank=85100 of ranks=87412rank=85200 of ranks=87412rank=85300 of ranks=87412rank=85400 of ranks=87412rank=85500 of ranks=87412rank=85600 of ranks=87412rank=85700 of ranks=87412rank=85800 of ranks=87412rank=85900 of ranks=87412rank=86000 of ranks=87412rank=86100 of ranks=87412rank=86200 of ranks=87412rank=86300 of ranks=87412rank=86400 of ranks=87412rank=86500 of ranks=87412rank=86600 of ranks=87412rank=86700 of ranks=87412rank=86800 of ranks=87412rank=86900 of ranks=87412rank=87000 of ranks=87412rank=87100 of ranks=87412rank=87200 of ranks=87412rank=87300 of ranks=87412rank=87400 of ranks=87412

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              98.4619    495    781      3    498           79.9739
   1 car                    98.8010  50026  22036    290  50316           84.4507
   2 truck                  98.5854   1812   2154     13   1825           78.9667
   3 bus                    97.2843    363   1616      3    366           74.8302
   4 pedestrian             97.3669   4179   3950     80   4259           78.2990

for conf_thresh=0.25, precision=0.92, recall=0.97, F1 score=0.94
for conf_thresh=0.25, TP=55382, FP=4953, FN=1882, average IoU=83.72%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=98.10%
Total detection time: 106 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
New best mAP, saving weights!
Saving weights to /workspace/.cache/splits/combined_best.weights
7903: loss=2.055, avg loss=2.391, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=729.8 milliseconds, train=2.1 seconds, 505792 images, time remaining=5.7 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7904: loss=2.155, avg loss=2.367, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.7 seconds, train=2.1 seconds, 505856 images, time remaining=5.7 minutes
7905: loss=2.669, avg loss=2.397, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=939.1 milliseconds, train=2.1 seconds, 505920 images, time remaining=5.6 minutes
7906: loss=1.936, avg loss=2.351, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=2.1 seconds, 505984 images, time remaining=5.5 minutes
7907: loss=2.339, avg loss=2.350, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=478.2 milliseconds, train=2.1 seconds, 506048 images, time remaining=5.5 minutes
Performance bottleneck:  loading 64 images took longer than it takes to train.  Slow CPU or hard drive?  Loading images from a network share?
7908: loss=2.173, avg loss=2.332, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.6 seconds, train=2.1 seconds, 506112 images, time remaining=5.4 minutes
7909: loss=1.910, avg loss=2.290, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=620.8 milliseconds, train=2.1 seconds, 506176 images, time remaining=5.3 minutes
7910: loss=2.068, avg loss=2.268, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.8 seconds, train=2.1 seconds, 506240 images, time remaining=5.3 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7911: loss=2.907, avg loss=2.332, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=992.8 milliseconds, train=5.1 seconds, 506304 images, time remaining=5.2 minutes
7912: loss=2.715, avg loss=2.370, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.0 seconds, train=5.1 seconds, 506368 images, time remaining=5.2 minutes
7913: loss=2.982, avg loss=2.431, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=683.6 milliseconds, train=5.1 seconds, 506432 images, time remaining=5.1 minutes
7914: loss=3.224, avg loss=2.510, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=619.3 milliseconds, train=5.1 seconds, 506496 images, time remaining=5 minutes
7915: loss=2.489, avg loss=2.508, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=937.8 milliseconds, train=5.1 seconds, 506560 images, time remaining=5 minutes
7916: loss=2.353, avg loss=2.493, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=455.8 milliseconds, train=5.1 seconds, 506624 images, time remaining=4.9 minutes
7917: loss=2.317, avg loss=2.475, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=907.4 milliseconds, train=5.1 seconds, 506688 images, time remaining=4.9 minutes
7918: loss=2.564, avg loss=2.484, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=3.1 seconds, train=5.1 seconds, 506752 images, time remaining=4.8 minutes
7919: loss=2.303, avg loss=2.466, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=803.0 milliseconds, train=5.1 seconds, 506816 images, time remaining=4.8 minutes
7920: loss=2.578, avg loss=2.477, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=757.0 milliseconds, train=5.1 seconds, 506880 images, time remaining=4.7 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7921: loss=1.971, avg loss=2.427, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=902.3 milliseconds, train=5.1 seconds, 506944 images, time remaining=4.6 minutes
7922: loss=3.056, avg loss=2.490, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=470.0 milliseconds, train=5.1 seconds, 507008 images, time remaining=4.6 minutes
7923: loss=2.571, avg loss=2.498, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=921.9 milliseconds, train=5.1 seconds, 507072 images, time remaining=4.5 minutes
7924: loss=2.474, avg loss=2.495, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.2 seconds, train=5.1 seconds, 507136 images, time remaining=4.5 minutes
7925: loss=2.198, avg loss=2.466, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=940.6 milliseconds, train=5.1 seconds, 507200 images, time remaining=4.4 minutes
7926: loss=2.808, avg loss=2.500, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=5.1 seconds, 507264 images, time remaining=4.3 minutes
7927: loss=2.521, avg loss=2.502, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.1 seconds, train=5.1 seconds, 507328 images, time remaining=4.3 minutes
7928: loss=3.087, avg loss=2.560, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=588.9 milliseconds, train=5.1 seconds, 507392 images, time remaining=4.2 minutes
7929: loss=2.900, avg loss=2.594, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.2 seconds, train=5.1 seconds, 507456 images, time remaining=4.2 minutes
7930: loss=3.166, avg loss=2.652, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=883.8 milliseconds, train=5.1 seconds, 507520 images, time remaining=4.1 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7931: loss=2.632, avg loss=2.650, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=751.4 milliseconds, train=5.1 seconds, 507584 images, time remaining=4.1 minutes
7932: loss=2.115, avg loss=2.596, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=644.4 milliseconds, train=5.1 seconds, 507648 images, time remaining=4 minutes
7933: loss=3.082, avg loss=2.645, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.0 seconds, train=5.1 seconds, 507712 images, time remaining=3.9 minutes
7934: loss=2.607, avg loss=2.641, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=3.4 seconds, train=5.1 seconds, 507776 images, time remaining=3.9 minutes
7935: loss=2.359, avg loss=2.613, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.6 seconds, train=5.1 seconds, 507840 images, time remaining=3.8 minutes
7936: loss=2.527, avg loss=2.604, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=862.7 milliseconds, train=5.1 seconds, 507904 images, time remaining=3.8 minutes
7937: loss=3.046, avg loss=2.648, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=644.1 milliseconds, train=5.1 seconds, 507968 images, time remaining=3.7 minutes
7938: loss=2.657, avg loss=2.649, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=606.6 milliseconds, train=5.1 seconds, 508032 images, time remaining=3.6 minutes
7939: loss=2.825, avg loss=2.667, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=485.0 milliseconds, train=5.1 seconds, 508096 images, time remaining=3.6 minutes
7940: loss=2.851, avg loss=2.685, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=619.9 milliseconds, train=5.1 seconds, 508160 images, time remaining=3.5 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7941: loss=2.573, avg loss=2.674, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=802.8 milliseconds, train=5.1 seconds, 508224 images, time remaining=3.5 minutes
7942: loss=2.788, avg loss=2.685, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=667.3 milliseconds, train=5.1 seconds, 508288 images, time remaining=3.4 minutes
7943: loss=2.147, avg loss=2.631, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=506.6 milliseconds, train=5.1 seconds, 508352 images, time remaining=3.3 minutes
7944: loss=2.602, avg loss=2.628, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=805.9 milliseconds, train=5.1 seconds, 508416 images, time remaining=3.3 minutes
7945: loss=2.939, avg loss=2.660, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=914.9 milliseconds, train=5.1 seconds, 508480 images, time remaining=3.2 minutes
7946: loss=2.598, avg loss=2.653, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=588.9 milliseconds, train=5.1 seconds, 508544 images, time remaining=3.2 minutes
7947: loss=2.192, avg loss=2.607, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=754.5 milliseconds, train=5.1 seconds, 508608 images, time remaining=3.1 minutes
7948: loss=2.689, avg loss=2.615, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=5.1 seconds, 508672 images, time remaining=3.1 minutes
7949: loss=2.290, avg loss=2.583, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=729.5 milliseconds, train=5.1 seconds, 508736 images, time remaining=3 minutes
7950: loss=3.229, avg loss=2.647, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.1 seconds, train=5.1 seconds, 508800 images, time remaining=3 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7951: loss=2.604, avg loss=2.643, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=563.1 milliseconds, train=5.1 seconds, 508864 images, time remaining=2.9 minutes
7952: loss=2.901, avg loss=2.669, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=463.5 milliseconds, train=5.1 seconds, 508928 images, time remaining=2.8 minutes
7953: loss=2.389, avg loss=2.641, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=820.4 milliseconds, train=5.1 seconds, 508992 images, time remaining=2.8 minutes
7954: loss=2.684, avg loss=2.645, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=672.7 milliseconds, train=5.1 seconds, 509056 images, time remaining=2.7 minutes
7955: loss=2.564, avg loss=2.637, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=5.1 seconds, 509120 images, time remaining=2.7 minutes
7956: loss=3.034, avg loss=2.677, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=960.3 milliseconds, train=5.1 seconds, 509184 images, time remaining=2.6 minutes
7957: loss=2.550, avg loss=2.664, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=3.0 seconds, train=5.1 seconds, 509248 images, time remaining=2.5 minutes
7958: loss=2.761, avg loss=2.674, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.5 seconds, train=5.1 seconds, 509312 images, time remaining=2.5 minutes
7959: loss=2.804, avg loss=2.687, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.9 seconds, train=5.1 seconds, 509376 images, time remaining=2.4 minutes
7960: loss=2.614, avg loss=2.680, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.1 seconds, train=5.1 seconds, 509440 images, time remaining=2.3 minutes
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7961: loss=2.420, avg loss=2.654, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.3 seconds, train=5.1 seconds, 509504 images, time remaining=2.3 minutes
7962: loss=2.257, avg loss=2.614, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=968.2 milliseconds, train=5.1 seconds, 509568 images, time remaining=2.2 minutes
7963: loss=1.861, avg loss=2.539, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=5.1 seconds, 509632 images, time remaining=2.2 minutes
7964: loss=3.097, avg loss=2.595, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=818.6 milliseconds, train=5.1 seconds, 509696 images, time remaining=2.1 minutes
7965: loss=2.828, avg loss=2.618, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.7 seconds, train=5.1 seconds, 509760 images, time remaining=2.1 minutes
7966: loss=2.819, avg loss=2.638, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=970.6 milliseconds, train=5.1 seconds, 509824 images, time remaining=120 seconds
7967: loss=2.703, avg loss=2.645, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.2 seconds, train=5.1 seconds, 509888 images, time remaining=116 seconds
7968: loss=2.235, avg loss=2.604, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.0 seconds, train=5.1 seconds, 509952 images, time remaining=113 seconds
7969: loss=2.926, avg loss=2.636, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.1 seconds, train=5.1 seconds, 510016 images, time remaining=109 seconds
7970: loss=2.757, avg loss=2.648, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.1 seconds, train=5.1 seconds, 510080 images, time remaining=106 seconds
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7971: loss=3.090, avg loss=2.692, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=895.5 milliseconds, train=5.1 seconds, 510144 images, time remaining=102 seconds
7972: loss=2.710, avg loss=2.694, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.3 seconds, train=5.1 seconds, 510208 images, time remaining=99 seconds
7973: loss=2.909, avg loss=2.715, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=921.9 milliseconds, train=5.1 seconds, 510272 images, time remaining=95 seconds
7974: loss=1.968, avg loss=2.641, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=868.7 milliseconds, train=5.1 seconds, 510336 images, time remaining=92 seconds
7975: loss=2.670, avg loss=2.644, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=766.4 milliseconds, train=5.1 seconds, 510400 images, time remaining=88 seconds
7976: loss=1.916, avg loss=2.571, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=584.2 milliseconds, train=5.1 seconds, 510464 images, time remaining=85 seconds
7977: loss=3.093, avg loss=2.623, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.2 seconds, train=5.1 seconds, 510528 images, time remaining=81 seconds
7978: loss=2.931, avg loss=2.654, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=697.9 milliseconds, train=5.1 seconds, 510592 images, time remaining=78 seconds
7979: loss=2.646, avg loss=2.653, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=967.0 milliseconds, train=5.1 seconds, 510656 images, time remaining=74 seconds
7980: loss=2.922, avg loss=2.680, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.5 seconds, train=5.1 seconds, 510720 images, time remaining=70 seconds
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7981: loss=2.638, avg loss=2.676, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.5 seconds, train=5.1 seconds, 510784 images, time remaining=67 seconds
7982: loss=2.483, avg loss=2.657, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=744.3 milliseconds, train=5.1 seconds, 510848 images, time remaining=63 seconds
7983: loss=3.219, avg loss=2.713, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.0 seconds, train=5.1 seconds, 510912 images, time remaining=60 seconds
7984: loss=3.309, avg loss=2.772, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.3 seconds, train=5.1 seconds, 510976 images, time remaining=56 seconds
7985: loss=3.173, avg loss=2.813, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.5 seconds, train=5.1 seconds, 511040 images, time remaining=53 seconds
7986: loss=2.420, avg loss=2.773, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.6 seconds, train=5.1 seconds, 511104 images, time remaining=49 seconds
7987: loss=2.933, avg loss=2.789, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.3 seconds, train=5.1 seconds, 511168 images, time remaining=46 seconds
7988: loss=2.912, avg loss=2.802, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.6 seconds, train=5.1 seconds, 511232 images, time remaining=42 seconds
7989: loss=3.168, avg loss=2.838, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=444.0 milliseconds, train=5.1 seconds, 511296 images, time remaining=39 seconds
7990: loss=2.520, avg loss=2.806, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=992.6 milliseconds, train=5.1 seconds, 511360 images, time remaining=35 seconds
Resizing, random_coef=1.40, batch=4, 1376x1056
GPU #0: allocating workspace: 16.6 GiB begins at 0x144734000000
7991: loss=2.929, avg loss=2.819, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.4 seconds, train=5.1 seconds, 511424 images, time remaining=31 seconds
7992: loss=2.784, avg loss=2.815, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=629.5 milliseconds, train=5.1 seconds, 511488 images, time remaining=28 seconds
7993: loss=3.231, avg loss=2.857, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=830.9 milliseconds, train=5.1 seconds, 511552 images, time remaining=24 seconds
7994: loss=3.022, avg loss=2.873, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=620.6 milliseconds, train=5.1 seconds, 511616 images, time remaining=21 seconds
7995: loss=2.937, avg loss=2.880, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.8 seconds, train=5.1 seconds, 511680 images, time remaining=17 seconds
7996: loss=3.030, avg loss=2.895, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.5 seconds, train=5.1 seconds, 511744 images, time remaining=14 seconds
7997: loss=2.485, avg loss=2.854, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=2.4 seconds, train=5.1 seconds, 511808 images, time remaining=10 seconds
7998: loss=3.839, avg loss=2.952, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=1.9 seconds, train=5.1 seconds, 511872 images, time remaining=7 seconds
7999: loss=2.791, avg loss=2.936, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=950.5 milliseconds, train=5.1 seconds, 511936 images, time remaining=3 seconds
8000: loss=2.552, avg loss=2.898, last=98.10%, best=98.10%, next=8308, rate=0.00001300, load 64=680.2 milliseconds, train=5.1 seconds, 512000 images, time remaining=unknown
Resizing to initial size: 960 x 736
GPU #0: allocating workspace: 4.3 GiB begins at 0x144a46000000
Calculating mAP (mean average precision)...
Detection layer #139 is type 17 (yolo)
Detection layer #150 is type 17 (yolo)
Detection layer #161 is type 17 (yolo)
using 4 threads to load 2887 validation images for mAP% calculations
processing #0 (0%) processing #4 (0%) processing #8 (0%) processing #12 (0%) processing #16 (1%) processing #20 (1%) processing #24 (1%) processing #28 (1%) processing #32 (1%) processing #36 (1%) processing #40 (1%) processing #44 (2%) processing #48 (2%) processing #52 (2%) processing #56 (2%) processing #60 (2%) processing #64 (2%) processing #68 (2%) processing #72 (2%) processing #76 (3%) processing #80 (3%) processing #84 (3%) processing #88 (3%) processing #92 (3%) processing #96 (3%) processing #100 (3%) processing #104 (4%) processing #108 (4%) processing #112 (4%) processing #116 (4%) processing #120 (4%) processing #124 (4%) processing #128 (4%) processing #132 (5%) processing #136 (5%) processing #140 (5%) processing #144 (5%) processing #148 (5%) processing #152 (5%) processing #156 (5%) processing #160 (6%) processing #164 (6%) processing #168 (6%) processing #172 (6%) processing #176 (6%) processing #180 (6%) processing #184 (6%) processing #188 (7%) processing #192 (7%) processing #196 (7%) processing #200 (7%) processing #204 (7%) processing #208 (7%) processing #212 (7%) processing #216 (7%) processing #220 (8%) processing #224 (8%) processing #228 (8%) processing #232 (8%) processing #236 (8%) processing #240 (8%) processing #244 (8%) processing #248 (9%) processing #252 (9%) processing #256 (9%) processing #260 (9%) processing #264 (9%) processing #268 (9%) processing #272 (9%) processing #276 (10%) processing #280 (10%) processing #284 (10%) processing #288 (10%) processing #292 (10%) processing #296 (10%) processing #300 (10%) processing #304 (11%) processing #308 (11%) processing #312 (11%) processing #316 (11%) processing #320 (11%) processing #324 (11%) processing #328 (11%) processing #332 (11%) processing #336 (12%) processing #340 (12%) processing #344 (12%) processing #348 (12%) processing #352 (12%) processing #356 (12%) processing #360 (12%) processing #364 (13%) processing #368 (13%) processing #372 (13%) processing #376 (13%) processing #380 (13%) processing #384 (13%) processing #388 (13%) processing #392 (14%) processing #396 (14%) processing #400 (14%) processing #404 (14%) processing #408 (14%) processing #412 (14%) processing #416 (14%) processing #420 (15%) processing #424 (15%) processing #428 (15%) processing #432 (15%) processing #436 (15%) processing #440 (15%) processing #444 (15%) processing #448 (16%) processing #452 (16%) processing #456 (16%) processing #460 (16%) processing #464 (16%) processing #468 (16%) processing #472 (16%) processing #476 (16%) processing #480 (17%) processing #484 (17%) processing #488 (17%) processing #492 (17%) processing #496 (17%) processing #500 (17%) processing #504 (17%) processing #508 (18%) processing #512 (18%) processing #516 (18%) processing #520 (18%) processing #524 (18%) processing #528 (18%) processing #532 (18%) processing #536 (19%) processing #540 (19%) processing #544 (19%) processing #548 (19%) processing #552 (19%) processing #556 (19%) processing #560 (19%) processing #564 (20%) processing #568 (20%) processing #572 (20%) processing #576 (20%) processing #580 (20%) processing #584 (20%) processing #588 (20%) processing #592 (21%) processing #596 (21%) processing #600 (21%) processing #604 (21%) processing #608 (21%) processing #612 (21%) processing #616 (21%) processing #620 (21%) processing #624 (22%) processing #628 (22%) processing #632 (22%) processing #636 (22%) processing #640 (22%) processing #644 (22%) processing #648 (22%) processing #652 (23%) processing #656 (23%) processing #660 (23%) processing #664 (23%) processing #668 (23%) processing #672 (23%) processing #676 (23%) processing #680 (24%) processing #684 (24%) processing #688 (24%) processing #692 (24%) processing #696 (24%) processing #700 (24%) processing #704 (24%) processing #708 (25%) processing #712 (25%) processing #716 (25%) processing #720 (25%) processing #724 (25%) processing #728 (25%) processing #732 (25%) processing #736 (25%) processing #740 (26%) processing #744 (26%) processing #748 (26%) processing #752 (26%) processing #756 (26%) processing #760 (26%) processing #764 (26%) processing #768 (27%) processing #772 (27%) processing #776 (27%) processing #780 (27%) processing #784 (27%) processing #788 (27%) processing #792 (27%) processing #796 (28%) processing #800 (28%) processing #804 (28%) processing #808 (28%) processing #812 (28%) processing #816 (28%) processing #820 (28%) processing #824 (29%) processing #828 (29%) processing #832 (29%) processing #836 (29%) processing #840 (29%) processing #844 (29%) processing #848 (29%) processing #852 (30%) processing #856 (30%) processing #860 (30%) processing #864 (30%) processing #868 (30%) processing #872 (30%) processing #876 (30%) processing #880 (30%) processing #884 (31%) processing #888 (31%) processing #892 (31%) processing #896 (31%) processing #900 (31%) processing #904 (31%) processing #908 (31%) processing #912 (32%) processing #916 (32%) processing #920 (32%) processing #924 (32%) processing #928 (32%) processing #932 (32%) processing #936 (32%) processing #940 (33%) processing #944 (33%) processing #948 (33%) processing #952 (33%) processing #956 (33%) processing #960 (33%) processing #964 (33%) processing #968 (34%) processing #972 (34%) processing #976 (34%) processing #980 (34%) processing #984 (34%) processing #988 (34%) processing #992 (34%) processing #996 (34%) processing #1000 (35%) processing #1004 (35%) processing #1008 (35%) processing #1012 (35%) processing #1016 (35%) processing #1020 (35%) processing #1024 (35%) processing #1028 (36%) processing #1032 (36%) processing #1036 (36%) processing #1040 (36%) processing #1044 (36%) processing #1048 (36%) processing #1052 (36%) processing #1056 (37%) processing #1060 (37%) processing #1064 (37%) processing #1068 (37%) processing #1072 (37%) processing #1076 (37%) processing #1080 (37%) processing #1084 (38%) processing #1088 (38%) processing #1092 (38%) processing #1096 (38%) processing #1100 (38%) processing #1104 (38%) processing #1108 (38%) processing #1112 (39%) processing #1116 (39%) processing #1120 (39%) processing #1124 (39%) processing #1128 (39%) processing #1132 (39%) processing #1136 (39%) processing #1140 (39%) processing #1144 (40%) processing #1148 (40%) processing #1152 (40%) processing #1156 (40%) processing #1160 (40%) processing #1164 (40%) processing #1168 (40%) processing #1172 (41%) processing #1176 (41%) processing #1180 (41%) processing #1184 (41%) processing #1188 (41%) processing #1192 (41%) processing #1196 (41%) processing #1200 (42%) processing #1204 (42%) processing #1208 (42%) processing #1212 (42%) processing #1216 (42%) processing #1220 (42%) processing #1224 (42%) processing #1228 (43%) processing #1232 (43%) processing #1236 (43%) processing #1240 (43%) processing #1244 (43%) processing #1248 (43%) processing #1252 (43%) processing #1256 (44%) processing #1260 (44%) processing #1264 (44%) processing #1268 (44%) processing #1272 (44%) processing #1276 (44%) processing #1280 (44%) processing #1284 (44%) processing #1288 (45%) processing #1292 (45%) processing #1296 (45%) processing #1300 (45%) processing #1304 (45%) processing #1308 (45%) processing #1312 (45%) processing #1316 (46%) processing #1320 (46%) processing #1324 (46%) processing #1328 (46%) processing #1332 (46%) processing #1336 (46%) processing #1340 (46%) processing #1344 (47%) processing #1348 (47%) processing #1352 (47%) processing #1356 (47%) processing #1360 (47%) processing #1364 (47%) processing #1368 (47%) processing #1372 (48%) processing #1376 (48%) processing #1380 (48%) processing #1384 (48%) processing #1388 (48%) processing #1392 (48%) processing #1396 (48%) processing #1400 (48%) processing #1404 (49%) processing #1408 (49%) processing #1412 (49%) processing #1416 (49%) processing #1420 (49%) processing #1424 (49%) processing #1428 (49%) processing #1432 (50%) processing #1436 (50%) processing #1440 (50%) processing #1444 (50%) processing #1448 (50%) processing #1452 (50%) processing #1456 (50%) processing #1460 (51%) processing #1464 (51%) processing #1468 (51%) processing #1472 (51%) processing #1476 (51%) processing #1480 (51%) processing #1484 (51%) processing #1488 (52%) processing #1492 (52%) processing #1496 (52%) processing #1500 (52%) processing #1504 (52%) processing #1508 (52%) processing #1512 (52%) processing #1516 (53%) processing #1520 (53%) processing #1524 (53%) processing #1528 (53%) processing #1532 (53%) processing #1536 (53%) processing #1540 (53%) processing #1544 (53%) processing #1548 (54%) processing #1552 (54%) processing #1556 (54%) processing #1560 (54%) processing #1564 (54%) processing #1568 (54%) processing #1572 (54%) processing #1576 (55%) processing #1580 (55%) processing #1584 (55%) processing #1588 (55%) processing #1592 (55%) processing #1596 (55%) processing #1600 (55%) processing #1604 (56%) processing #1608 (56%) processing #1612 (56%) processing #1616 (56%) processing #1620 (56%) processing #1624 (56%) processing #1628 (56%) processing #1632 (57%) processing #1636 (57%) processing #1640 (57%) processing #1644 (57%) processing #1648 (57%) processing #1652 (57%) processing #1656 (57%) processing #1660 (57%) processing #1664 (58%) processing #1668 (58%) processing #1672 (58%) processing #1676 (58%) processing #1680 (58%) processing #1684 (58%) processing #1688 (58%) processing #1692 (59%) processing #1696 (59%) processing #1700 (59%) processing #1704 (59%) processing #1708 (59%) processing #1712 (59%) processing #1716 (59%) processing #1720 (60%) processing #1724 (60%) processing #1728 (60%) processing #1732 (60%) processing #1736 (60%) processing #1740 (60%) processing #1744 (60%) processing #1748 (61%) processing #1752 (61%) processing #1756 (61%) processing #1760 (61%) processing #1764 (61%) processing #1768 (61%) processing #1772 (61%) processing #1776 (62%) processing #1780 (62%) processing #1784 (62%) processing #1788 (62%) processing #1792 (62%) processing #1796 (62%) processing #1800 (62%) processing #1804 (62%) processing #1808 (63%) processing #1812 (63%) processing #1816 (63%) processing #1820 (63%) processing #1824 (63%) processing #1828 (63%) processing #1832 (63%) processing #1836 (64%) processing #1840 (64%) processing #1844 (64%) processing #1848 (64%) processing #1852 (64%) processing #1856 (64%) processing #1860 (64%) processing #1864 (65%) processing #1868 (65%) processing #1872 (65%) processing #1876 (65%) processing #1880 (65%) processing #1884 (65%) processing #1888 (65%) processing #1892 (66%) processing #1896 (66%) processing #1900 (66%) processing #1904 (66%) processing #1908 (66%) processing #1912 (66%) processing #1916 (66%) processing #1920 (67%) processing #1924 (67%) processing #1928 (67%) processing #1932 (67%) processing #1936 (67%) processing #1940 (67%) processing #1944 (67%) processing #1948 (67%) processing #1952 (68%) processing #1956 (68%) processing #1960 (68%) processing #1964 (68%) processing #1968 (68%) processing #1972 (68%) processing #1976 (68%) processing #1980 (69%) processing #1984 (69%) processing #1988 (69%) processing #1992 (69%) processing #1996 (69%) processing #2000 (69%) processing #2004 (69%) processing #2008 (70%) processing #2012 (70%) processing #2016 (70%) processing #2020 (70%) processing #2024 (70%) processing #2028 (70%) processing #2032 (70%) processing #2036 (71%) processing #2040 (71%) processing #2044 (71%) processing #2048 (71%) processing #2052 (71%) processing #2056 (71%) processing #2060 (71%) processing #2064 (71%) processing #2068 (72%) processing #2072 (72%) processing #2076 (72%) processing #2080 (72%) processing #2084 (72%) processing #2088 (72%) processing #2092 (72%) processing #2096 (73%) processing #2100 (73%) processing #2104 (73%) processing #2108 (73%) processing #2112 (73%) processing #2116 (73%) processing #2120 (73%) processing #2124 (74%) processing #2128 (74%) processing #2132 (74%) processing #2136 (74%) processing #2140 (74%) processing #2144 (74%) processing #2148 (74%) processing #2152 (75%) processing #2156 (75%) processing #2160 (75%) processing #2164 (75%) processing #2168 (75%) processing #2172 (75%) processing #2176 (75%) processing #2180 (76%) processing #2184 (76%) processing #2188 (76%) processing #2192 (76%) processing #2196 (76%) processing #2200 (76%) processing #2204 (76%) processing #2208 (76%) processing #2212 (77%) processing #2216 (77%) processing #2220 (77%) processing #2224 (77%) processing #2228 (77%) processing #2232 (77%) processing #2236 (77%) processing #2240 (78%) processing #2244 (78%) processing #2248 (78%) processing #2252 (78%) processing #2256 (78%) processing #2260 (78%) processing #2264 (78%) processing #2268 (79%) processing #2272 (79%) processing #2276 (79%) processing #2280 (79%) processing #2284 (79%) processing #2288 (79%) processing #2292 (79%) processing #2296 (80%) processing #2300 (80%) processing #2304 (80%) processing #2308 (80%) processing #2312 (80%) processing #2316 (80%) processing #2320 (80%) processing #2324 (80%) processing #2328 (81%) processing #2332 (81%) processing #2336 (81%) processing #2340 (81%) processing #2344 (81%) processing #2348 (81%) processing #2352 (81%) processing #2356 (82%) processing #2360 (82%) processing #2364 (82%) processing #2368 (82%) processing #2372 (82%) processing #2376 (82%) processing #2380 (82%) processing #2384 (83%) processing #2388 (83%) processing #2392 (83%) processing #2396 (83%) processing #2400 (83%) processing #2404 (83%) processing #2408 (83%) processing #2412 (84%) processing #2416 (84%) processing #2420 (84%) processing #2424 (84%) processing #2428 (84%) processing #2432 (84%) processing #2436 (84%) processing #2440 (85%) processing #2444 (85%) processing #2448 (85%) processing #2452 (85%) processing #2456 (85%) processing #2460 (85%) processing #2464 (85%) processing #2468 (85%) processing #2472 (86%) processing #2476 (86%) processing #2480 (86%) processing #2484 (86%) processing #2488 (86%) processing #2492 (86%) processing #2496 (86%) processing #2500 (87%) processing #2504 (87%) processing #2508 (87%) processing #2512 (87%) processing #2516 (87%) processing #2520 (87%) processing #2524 (87%) processing #2528 (88%) processing #2532 (88%) processing #2536 (88%) processing #2540 (88%) processing #2544 (88%) processing #2548 (88%) processing #2552 (88%) processing #2556 (89%) processing #2560 (89%) processing #2564 (89%) processing #2568 (89%) processing #2572 (89%) processing #2576 (89%) processing #2580 (89%) processing #2584 (90%) processing #2588 (90%) processing #2592 (90%) processing #2596 (90%) processing #2600 (90%) processing #2604 (90%) processing #2608 (90%) processing #2612 (90%) processing #2616 (91%) processing #2620 (91%) processing #2624 (91%) processing #2628 (91%) processing #2632 (91%) processing #2636 (91%) processing #2640 (91%) processing #2644 (92%) processing #2648 (92%) processing #2652 (92%) processing #2656 (92%) processing #2660 (92%) processing #2664 (92%) processing #2668 (92%) processing #2672 (93%) processing #2676 (93%) processing #2680 (93%) processing #2684 (93%) processing #2688 (93%) processing #2692 (93%) processing #2696 (93%) processing #2700 (94%) processing #2704 (94%) processing #2708 (94%) processing #2712 (94%) processing #2716 (94%) processing #2720 (94%) processing #2724 (94%) processing #2728 (94%) processing #2732 (95%) processing #2736 (95%) processing #2740 (95%) processing #2744 (95%) processing #2748 (95%) processing #2752 (95%) processing #2756 (95%) processing #2760 (96%) processing #2764 (96%) processing #2768 (96%) processing #2772 (96%) processing #2776 (96%) processing #2780 (96%) processing #2784 (96%) processing #2788 (97%) processing #2792 (97%) processing #2796 (97%) processing #2800 (97%) processing #2804 (97%) processing #2808 (97%) processing #2812 (97%) processing #2816 (98%) processing #2820 (98%) processing #2824 (98%) processing #2828 (98%) processing #2832 (98%) processing #2836 (98%) processing #2840 (98%) processing #2844 (99%) processing #2848 (99%) processing #2852 (99%) processing #2856 (99%) processing #2860 (99%) processing #2864 (99%) processing #2868 (99%) processing #2872 (99%) processing #2876 (100%) processing #2880 (100%) processing #2884 (100%) detections_count=87208, unique_truth_count=57264
rank=0 of ranks=87208rank=100 of ranks=87208rank=200 of ranks=87208rank=300 of ranks=87208rank=400 of ranks=87208rank=500 of ranks=87208rank=600 of ranks=87208rank=700 of ranks=87208rank=800 of ranks=87208rank=900 of ranks=87208rank=1000 of ranks=87208rank=1100 of ranks=87208rank=1200 of ranks=87208rank=1300 of ranks=87208rank=1400 of ranks=87208rank=1500 of ranks=87208rank=1600 of ranks=87208rank=1700 of ranks=87208rank=1800 of ranks=87208rank=1900 of ranks=87208rank=2000 of ranks=87208rank=2100 of ranks=87208rank=2200 of ranks=87208rank=2300 of ranks=87208rank=2400 of ranks=87208rank=2500 of ranks=87208rank=2600 of ranks=87208rank=2700 of ranks=87208rank=2800 of ranks=87208rank=2900 of ranks=87208rank=3000 of ranks=87208rank=3100 of ranks=87208rank=3200 of ranks=87208rank=3300 of ranks=87208rank=3400 of ranks=87208rank=3500 of ranks=87208rank=3600 of ranks=87208rank=3700 of ranks=87208rank=3800 of ranks=87208rank=3900 of ranks=87208rank=4000 of ranks=87208rank=4100 of ranks=87208rank=4200 of ranks=87208rank=4300 of ranks=87208rank=4400 of ranks=87208rank=4500 of ranks=87208rank=4600 of ranks=87208rank=4700 of ranks=87208rank=4800 of ranks=87208rank=4900 of ranks=87208rank=5000 of ranks=87208rank=5100 of ranks=87208rank=5200 of ranks=87208rank=5300 of ranks=87208rank=5400 of ranks=87208rank=5500 of ranks=87208rank=5600 of ranks=87208rank=5700 of ranks=87208rank=5800 of ranks=87208rank=5900 of ranks=87208rank=6000 of ranks=87208rank=6100 of ranks=87208rank=6200 of ranks=87208rank=6300 of ranks=87208rank=6400 of ranks=87208rank=6500 of ranks=87208rank=6600 of ranks=87208rank=6700 of ranks=87208rank=6800 of ranks=87208rank=6900 of ranks=87208rank=7000 of ranks=87208rank=7100 of ranks=87208rank=7200 of ranks=87208rank=7300 of ranks=87208rank=7400 of ranks=87208rank=7500 of ranks=87208rank=7600 of ranks=87208rank=7700 of ranks=87208rank=7800 of ranks=87208rank=7900 of ranks=87208rank=8000 of ranks=87208rank=8100 of ranks=87208rank=8200 of ranks=87208rank=8300 of ranks=87208rank=8400 of ranks=87208rank=8500 of ranks=87208rank=8600 of ranks=87208rank=8700 of ranks=87208rank=8800 of ranks=87208rank=8900 of ranks=87208rank=9000 of ranks=87208rank=9100 of ranks=87208rank=9200 of ranks=87208rank=9300 of ranks=87208rank=9400 of ranks=87208rank=9500 of ranks=87208rank=9600 of ranks=87208rank=9700 of ranks=87208rank=9800 of ranks=87208rank=9900 of ranks=87208rank=10000 of ranks=87208rank=10100 of ranks=87208rank=10200 of ranks=87208rank=10300 of ranks=87208rank=10400 of ranks=87208rank=10500 of ranks=87208rank=10600 of ranks=87208rank=10700 of ranks=87208rank=10800 of ranks=87208rank=10900 of ranks=87208rank=11000 of ranks=87208rank=11100 of ranks=87208rank=11200 of ranks=87208rank=11300 of ranks=87208rank=11400 of ranks=87208rank=11500 of ranks=87208rank=11600 of ranks=87208rank=11700 of ranks=87208rank=11800 of ranks=87208rank=11900 of ranks=87208rank=12000 of ranks=87208rank=12100 of ranks=87208rank=12200 of ranks=87208rank=12300 of ranks=87208rank=12400 of ranks=87208rank=12500 of ranks=87208rank=12600 of ranks=87208rank=12700 of ranks=87208rank=12800 of ranks=87208rank=12900 of ranks=87208rank=13000 of ranks=87208rank=13100 of ranks=87208rank=13200 of ranks=87208rank=13300 of ranks=87208rank=13400 of ranks=87208rank=13500 of ranks=87208rank=13600 of ranks=87208rank=13700 of ranks=87208rank=13800 of ranks=87208rank=13900 of ranks=87208rank=14000 of ranks=87208rank=14100 of ranks=87208rank=14200 of ranks=87208rank=14300 of ranks=87208rank=14400 of ranks=87208rank=14500 of ranks=87208rank=14600 of ranks=87208rank=14700 of ranks=87208rank=14800 of ranks=87208rank=14900 of ranks=87208rank=15000 of ranks=87208rank=15100 of ranks=87208rank=15200 of ranks=87208rank=15300 of ranks=87208rank=15400 of ranks=87208rank=15500 of ranks=87208rank=15600 of ranks=87208rank=15700 of ranks=87208rank=15800 of ranks=87208rank=15900 of ranks=87208rank=16000 of ranks=87208rank=16100 of ranks=87208rank=16200 of ranks=87208rank=16300 of ranks=87208rank=16400 of ranks=87208rank=16500 of ranks=87208rank=16600 of ranks=87208rank=16700 of ranks=87208rank=16800 of ranks=87208rank=16900 of ranks=87208rank=17000 of ranks=87208rank=17100 of ranks=87208rank=17200 of ranks=87208rank=17300 of ranks=87208rank=17400 of ranks=87208rank=17500 of ranks=87208rank=17600 of ranks=87208rank=17700 of ranks=87208rank=17800 of ranks=87208rank=17900 of ranks=87208rank=18000 of ranks=87208rank=18100 of ranks=87208rank=18200 of ranks=87208rank=18300 of ranks=87208rank=18400 of ranks=87208rank=18500 of ranks=87208rank=18600 of ranks=87208rank=18700 of ranks=87208rank=18800 of ranks=87208rank=18900 of ranks=87208rank=19000 of ranks=87208rank=19100 of ranks=87208rank=19200 of ranks=87208rank=19300 of ranks=87208rank=19400 of ranks=87208rank=19500 of ranks=87208rank=19600 of ranks=87208rank=19700 of ranks=87208rank=19800 of ranks=87208rank=19900 of ranks=87208rank=20000 of ranks=87208rank=20100 of ranks=87208rank=20200 of ranks=87208rank=20300 of ranks=87208rank=20400 of ranks=87208rank=20500 of ranks=87208rank=20600 of ranks=87208rank=20700 of ranks=87208rank=20800 of ranks=87208rank=20900 of ranks=87208rank=21000 of ranks=87208rank=21100 of ranks=87208rank=21200 of ranks=87208rank=21300 of ranks=87208rank=21400 of ranks=87208rank=21500 of ranks=87208rank=21600 of ranks=87208rank=21700 of ranks=87208rank=21800 of ranks=87208rank=21900 of ranks=87208rank=22000 of ranks=87208rank=22100 of ranks=87208rank=22200 of ranks=87208rank=22300 of ranks=87208rank=22400 of ranks=87208rank=22500 of ranks=87208rank=22600 of ranks=87208rank=22700 of ranks=87208rank=22800 of ranks=87208rank=22900 of ranks=87208rank=23000 of ranks=87208rank=23100 of ranks=87208rank=23200 of ranks=87208rank=23300 of ranks=87208rank=23400 of ranks=87208rank=23500 of ranks=87208rank=23600 of ranks=87208rank=23700 of ranks=87208rank=23800 of ranks=87208rank=23900 of ranks=87208rank=24000 of ranks=87208rank=24100 of ranks=87208rank=24200 of ranks=87208rank=24300 of ranks=87208rank=24400 of ranks=87208rank=24500 of ranks=87208rank=24600 of ranks=87208rank=24700 of ranks=87208rank=24800 of ranks=87208rank=24900 of ranks=87208rank=25000 of ranks=87208rank=25100 of ranks=87208rank=25200 of ranks=87208rank=25300 of ranks=87208rank=25400 of ranks=87208rank=25500 of ranks=87208rank=25600 of ranks=87208rank=25700 of ranks=87208rank=25800 of ranks=87208rank=25900 of ranks=87208rank=26000 of ranks=87208rank=26100 of ranks=87208rank=26200 of ranks=87208rank=26300 of ranks=87208rank=26400 of ranks=87208rank=26500 of ranks=87208rank=26600 of ranks=87208rank=26700 of ranks=87208rank=26800 of ranks=87208rank=26900 of ranks=87208rank=27000 of ranks=87208rank=27100 of ranks=87208rank=27200 of ranks=87208rank=27300 of ranks=87208rank=27400 of ranks=87208rank=27500 of ranks=87208rank=27600 of ranks=87208rank=27700 of ranks=87208rank=27800 of ranks=87208rank=27900 of ranks=87208rank=28000 of ranks=87208rank=28100 of ranks=87208rank=28200 of ranks=87208rank=28300 of ranks=87208rank=28400 of ranks=87208rank=28500 of ranks=87208rank=28600 of ranks=87208rank=28700 of ranks=87208rank=28800 of ranks=87208rank=28900 of ranks=87208rank=29000 of ranks=87208rank=29100 of ranks=87208rank=29200 of ranks=87208rank=29300 of ranks=87208rank=29400 of ranks=87208rank=29500 of ranks=87208rank=29600 of ranks=87208rank=29700 of ranks=87208rank=29800 of ranks=87208rank=29900 of ranks=87208rank=30000 of ranks=87208rank=30100 of ranks=87208rank=30200 of ranks=87208rank=30300 of ranks=87208rank=30400 of ranks=87208rank=30500 of ranks=87208rank=30600 of ranks=87208rank=30700 of ranks=87208rank=30800 of ranks=87208rank=30900 of ranks=87208rank=31000 of ranks=87208rank=31100 of ranks=87208rank=31200 of ranks=87208rank=31300 of ranks=87208rank=31400 of ranks=87208rank=31500 of ranks=87208rank=31600 of ranks=87208rank=31700 of ranks=87208rank=31800 of ranks=87208rank=31900 of ranks=87208rank=32000 of ranks=87208rank=32100 of ranks=87208rank=32200 of ranks=87208rank=32300 of ranks=87208rank=32400 of ranks=87208rank=32500 of ranks=87208rank=32600 of ranks=87208rank=32700 of ranks=87208rank=32800 of ranks=87208rank=32900 of ranks=87208rank=33000 of ranks=87208rank=33100 of ranks=87208rank=33200 of ranks=87208rank=33300 of ranks=87208rank=33400 of ranks=87208rank=33500 of ranks=87208rank=33600 of ranks=87208rank=33700 of ranks=87208rank=33800 of ranks=87208rank=33900 of ranks=87208rank=34000 of ranks=87208rank=34100 of ranks=87208rank=34200 of ranks=87208rank=34300 of ranks=87208rank=34400 of ranks=87208rank=34500 of ranks=87208rank=34600 of ranks=87208rank=34700 of ranks=87208rank=34800 of ranks=87208rank=34900 of ranks=87208rank=35000 of ranks=87208rank=35100 of ranks=87208rank=35200 of ranks=87208rank=35300 of ranks=87208rank=35400 of ranks=87208rank=35500 of ranks=87208rank=35600 of ranks=87208rank=35700 of ranks=87208rank=35800 of ranks=87208rank=35900 of ranks=87208rank=36000 of ranks=87208rank=36100 of ranks=87208rank=36200 of ranks=87208rank=36300 of ranks=87208rank=36400 of ranks=87208rank=36500 of ranks=87208rank=36600 of ranks=87208rank=36700 of ranks=87208rank=36800 of ranks=87208rank=36900 of ranks=87208rank=37000 of ranks=87208rank=37100 of ranks=87208rank=37200 of ranks=87208rank=37300 of ranks=87208rank=37400 of ranks=87208rank=37500 of ranks=87208rank=37600 of ranks=87208rank=37700 of ranks=87208rank=37800 of ranks=87208rank=37900 of ranks=87208rank=38000 of ranks=87208rank=38100 of ranks=87208rank=38200 of ranks=87208rank=38300 of ranks=87208rank=38400 of ranks=87208rank=38500 of ranks=87208rank=38600 of ranks=87208rank=38700 of ranks=87208rank=38800 of ranks=87208rank=38900 of ranks=87208rank=39000 of ranks=87208rank=39100 of ranks=87208rank=39200 of ranks=87208rank=39300 of ranks=87208rank=39400 of ranks=87208rank=39500 of ranks=87208rank=39600 of ranks=87208rank=39700 of ranks=87208rank=39800 of ranks=87208rank=39900 of ranks=87208rank=40000 of ranks=87208rank=40100 of ranks=87208rank=40200 of ranks=87208rank=40300 of ranks=87208rank=40400 of ranks=87208rank=40500 of ranks=87208rank=40600 of ranks=87208rank=40700 of ranks=87208rank=40800 of ranks=87208rank=40900 of ranks=87208rank=41000 of ranks=87208rank=41100 of ranks=87208rank=41200 of ranks=87208rank=41300 of ranks=87208rank=41400 of ranks=87208rank=41500 of ranks=87208rank=41600 of ranks=87208rank=41700 of ranks=87208rank=41800 of ranks=87208rank=41900 of ranks=87208rank=42000 of ranks=87208rank=42100 of ranks=87208rank=42200 of ranks=87208rank=42300 of ranks=87208rank=42400 of ranks=87208rank=42500 of ranks=87208rank=42600 of ranks=87208rank=42700 of ranks=87208rank=42800 of ranks=87208rank=42900 of ranks=87208rank=43000 of ranks=87208rank=43100 of ranks=87208rank=43200 of ranks=87208rank=43300 of ranks=87208rank=43400 of ranks=87208rank=43500 of ranks=87208rank=43600 of ranks=87208rank=43700 of ranks=87208rank=43800 of ranks=87208rank=43900 of ranks=87208rank=44000 of ranks=87208rank=44100 of ranks=87208rank=44200 of ranks=87208rank=44300 of ranks=87208rank=44400 of ranks=87208rank=44500 of ranks=87208rank=44600 of ranks=87208rank=44700 of ranks=87208rank=44800 of ranks=87208rank=44900 of ranks=87208rank=45000 of ranks=87208rank=45100 of ranks=87208rank=45200 of ranks=87208rank=45300 of ranks=87208rank=45400 of ranks=87208rank=45500 of ranks=87208rank=45600 of ranks=87208rank=45700 of ranks=87208rank=45800 of ranks=87208rank=45900 of ranks=87208rank=46000 of ranks=87208rank=46100 of ranks=87208rank=46200 of ranks=87208rank=46300 of ranks=87208rank=46400 of ranks=87208rank=46500 of ranks=87208rank=46600 of ranks=87208rank=46700 of ranks=87208rank=46800 of ranks=87208rank=46900 of ranks=87208rank=47000 of ranks=87208rank=47100 of ranks=87208rank=47200 of ranks=87208rank=47300 of ranks=87208rank=47400 of ranks=87208rank=47500 of ranks=87208rank=47600 of ranks=87208rank=47700 of ranks=87208rank=47800 of ranks=87208rank=47900 of ranks=87208rank=48000 of ranks=87208rank=48100 of ranks=87208rank=48200 of ranks=87208rank=48300 of ranks=87208rank=48400 of ranks=87208rank=48500 of ranks=87208rank=48600 of ranks=87208rank=48700 of ranks=87208rank=48800 of ranks=87208rank=48900 of ranks=87208rank=49000 of ranks=87208rank=49100 of ranks=87208rank=49200 of ranks=87208rank=49300 of ranks=87208rank=49400 of ranks=87208rank=49500 of ranks=87208rank=49600 of ranks=87208rank=49700 of ranks=87208rank=49800 of ranks=87208rank=49900 of ranks=87208rank=50000 of ranks=87208rank=50100 of ranks=87208rank=50200 of ranks=87208rank=50300 of ranks=87208rank=50400 of ranks=87208rank=50500 of ranks=87208rank=50600 of ranks=87208rank=50700 of ranks=87208rank=50800 of ranks=87208rank=50900 of ranks=87208rank=51000 of ranks=87208rank=51100 of ranks=87208rank=51200 of ranks=87208rank=51300 of ranks=87208rank=51400 of ranks=87208rank=51500 of ranks=87208rank=51600 of ranks=87208rank=51700 of ranks=87208rank=51800 of ranks=87208rank=51900 of ranks=87208rank=52000 of ranks=87208rank=52100 of ranks=87208rank=52200 of ranks=87208rank=52300 of ranks=87208rank=52400 of ranks=87208rank=52500 of ranks=87208rank=52600 of ranks=87208rank=52700 of ranks=87208rank=52800 of ranks=87208rank=52900 of ranks=87208rank=53000 of ranks=87208rank=53100 of ranks=87208rank=53200 of ranks=87208rank=53300 of ranks=87208rank=53400 of ranks=87208rank=53500 of ranks=87208rank=53600 of ranks=87208rank=53700 of ranks=87208rank=53800 of ranks=87208rank=53900 of ranks=87208rank=54000 of ranks=87208rank=54100 of ranks=87208rank=54200 of ranks=87208rank=54300 of ranks=87208rank=54400 of ranks=87208rank=54500 of ranks=87208rank=54600 of ranks=87208rank=54700 of ranks=87208rank=54800 of ranks=87208rank=54900 of ranks=87208rank=55000 of ranks=87208rank=55100 of ranks=87208rank=55200 of ranks=87208rank=55300 of ranks=87208rank=55400 of ranks=87208rank=55500 of ranks=87208rank=55600 of ranks=87208rank=55700 of ranks=87208rank=55800 of ranks=87208rank=55900 of ranks=87208rank=56000 of ranks=87208rank=56100 of ranks=87208rank=56200 of ranks=87208rank=56300 of ranks=87208rank=56400 of ranks=87208rank=56500 of ranks=87208rank=56600 of ranks=87208rank=56700 of ranks=87208rank=56800 of ranks=87208rank=56900 of ranks=87208rank=57000 of ranks=87208rank=57100 of ranks=87208rank=57200 of ranks=87208rank=57300 of ranks=87208rank=57400 of ranks=87208rank=57500 of ranks=87208rank=57600 of ranks=87208rank=57700 of ranks=87208rank=57800 of ranks=87208rank=57900 of ranks=87208rank=58000 of ranks=87208rank=58100 of ranks=87208rank=58200 of ranks=87208rank=58300 of ranks=87208rank=58400 of ranks=87208rank=58500 of ranks=87208rank=58600 of ranks=87208rank=58700 of ranks=87208rank=58800 of ranks=87208rank=58900 of ranks=87208rank=59000 of ranks=87208rank=59100 of ranks=87208rank=59200 of ranks=87208rank=59300 of ranks=87208rank=59400 of ranks=87208rank=59500 of ranks=87208rank=59600 of ranks=87208rank=59700 of ranks=87208rank=59800 of ranks=87208rank=59900 of ranks=87208rank=60000 of ranks=87208rank=60100 of ranks=87208rank=60200 of ranks=87208rank=60300 of ranks=87208rank=60400 of ranks=87208rank=60500 of ranks=87208rank=60600 of ranks=87208rank=60700 of ranks=87208rank=60800 of ranks=87208rank=60900 of ranks=87208rank=61000 of ranks=87208rank=61100 of ranks=87208rank=61200 of ranks=87208rank=61300 of ranks=87208rank=61400 of ranks=87208rank=61500 of ranks=87208rank=61600 of ranks=87208rank=61700 of ranks=87208rank=61800 of ranks=87208rank=61900 of ranks=87208rank=62000 of ranks=87208rank=62100 of ranks=87208rank=62200 of ranks=87208rank=62300 of ranks=87208rank=62400 of ranks=87208rank=62500 of ranks=87208rank=62600 of ranks=87208rank=62700 of ranks=87208rank=62800 of ranks=87208rank=62900 of ranks=87208rank=63000 of ranks=87208rank=63100 of ranks=87208rank=63200 of ranks=87208rank=63300 of ranks=87208rank=63400 of ranks=87208rank=63500 of ranks=87208rank=63600 of ranks=87208rank=63700 of ranks=87208rank=63800 of ranks=87208rank=63900 of ranks=87208rank=64000 of ranks=87208rank=64100 of ranks=87208rank=64200 of ranks=87208rank=64300 of ranks=87208rank=64400 of ranks=87208rank=64500 of ranks=87208rank=64600 of ranks=87208rank=64700 of ranks=87208rank=64800 of ranks=87208rank=64900 of ranks=87208rank=65000 of ranks=87208rank=65100 of ranks=87208rank=65200 of ranks=87208rank=65300 of ranks=87208rank=65400 of ranks=87208rank=65500 of ranks=87208rank=65600 of ranks=87208rank=65700 of ranks=87208rank=65800 of ranks=87208rank=65900 of ranks=87208rank=66000 of ranks=87208rank=66100 of ranks=87208rank=66200 of ranks=87208rank=66300 of ranks=87208rank=66400 of ranks=87208rank=66500 of ranks=87208rank=66600 of ranks=87208rank=66700 of ranks=87208rank=66800 of ranks=87208rank=66900 of ranks=87208rank=67000 of ranks=87208rank=67100 of ranks=87208rank=67200 of ranks=87208rank=67300 of ranks=87208rank=67400 of ranks=87208rank=67500 of ranks=87208rank=67600 of ranks=87208rank=67700 of ranks=87208rank=67800 of ranks=87208rank=67900 of ranks=87208rank=68000 of ranks=87208rank=68100 of ranks=87208rank=68200 of ranks=87208rank=68300 of ranks=87208rank=68400 of ranks=87208rank=68500 of ranks=87208rank=68600 of ranks=87208rank=68700 of ranks=87208rank=68800 of ranks=87208rank=68900 of ranks=87208rank=69000 of ranks=87208rank=69100 of ranks=87208rank=69200 of ranks=87208rank=69300 of ranks=87208rank=69400 of ranks=87208rank=69500 of ranks=87208rank=69600 of ranks=87208rank=69700 of ranks=87208rank=69800 of ranks=87208rank=69900 of ranks=87208rank=70000 of ranks=87208rank=70100 of ranks=87208rank=70200 of ranks=87208rank=70300 of ranks=87208rank=70400 of ranks=87208rank=70500 of ranks=87208rank=70600 of ranks=87208rank=70700 of ranks=87208rank=70800 of ranks=87208rank=70900 of ranks=87208rank=71000 of ranks=87208rank=71100 of ranks=87208rank=71200 of ranks=87208rank=71300 of ranks=87208rank=71400 of ranks=87208rank=71500 of ranks=87208rank=71600 of ranks=87208rank=71700 of ranks=87208rank=71800 of ranks=87208rank=71900 of ranks=87208rank=72000 of ranks=87208rank=72100 of ranks=87208rank=72200 of ranks=87208rank=72300 of ranks=87208rank=72400 of ranks=87208rank=72500 of ranks=87208rank=72600 of ranks=87208rank=72700 of ranks=87208rank=72800 of ranks=87208rank=72900 of ranks=87208rank=73000 of ranks=87208rank=73100 of ranks=87208rank=73200 of ranks=87208rank=73300 of ranks=87208rank=73400 of ranks=87208rank=73500 of ranks=87208rank=73600 of ranks=87208rank=73700 of ranks=87208rank=73800 of ranks=87208rank=73900 of ranks=87208rank=74000 of ranks=87208rank=74100 of ranks=87208rank=74200 of ranks=87208rank=74300 of ranks=87208rank=74400 of ranks=87208rank=74500 of ranks=87208rank=74600 of ranks=87208rank=74700 of ranks=87208rank=74800 of ranks=87208rank=74900 of ranks=87208rank=75000 of ranks=87208rank=75100 of ranks=87208rank=75200 of ranks=87208rank=75300 of ranks=87208rank=75400 of ranks=87208rank=75500 of ranks=87208rank=75600 of ranks=87208rank=75700 of ranks=87208rank=75800 of ranks=87208rank=75900 of ranks=87208rank=76000 of ranks=87208rank=76100 of ranks=87208rank=76200 of ranks=87208rank=76300 of ranks=87208rank=76400 of ranks=87208rank=76500 of ranks=87208rank=76600 of ranks=87208rank=76700 of ranks=87208rank=76800 of ranks=87208rank=76900 of ranks=87208rank=77000 of ranks=87208rank=77100 of ranks=87208rank=77200 of ranks=87208rank=77300 of ranks=87208rank=77400 of ranks=87208rank=77500 of ranks=87208rank=77600 of ranks=87208rank=77700 of ranks=87208rank=77800 of ranks=87208rank=77900 of ranks=87208rank=78000 of ranks=87208rank=78100 of ranks=87208rank=78200 of ranks=87208rank=78300 of ranks=87208rank=78400 of ranks=87208rank=78500 of ranks=87208rank=78600 of ranks=87208rank=78700 of ranks=87208rank=78800 of ranks=87208rank=78900 of ranks=87208rank=79000 of ranks=87208rank=79100 of ranks=87208rank=79200 of ranks=87208rank=79300 of ranks=87208rank=79400 of ranks=87208rank=79500 of ranks=87208rank=79600 of ranks=87208rank=79700 of ranks=87208rank=79800 of ranks=87208rank=79900 of ranks=87208rank=80000 of ranks=87208rank=80100 of ranks=87208rank=80200 of ranks=87208rank=80300 of ranks=87208rank=80400 of ranks=87208rank=80500 of ranks=87208rank=80600 of ranks=87208rank=80700 of ranks=87208rank=80800 of ranks=87208rank=80900 of ranks=87208rank=81000 of ranks=87208rank=81100 of ranks=87208rank=81200 of ranks=87208rank=81300 of ranks=87208rank=81400 of ranks=87208rank=81500 of ranks=87208rank=81600 of ranks=87208rank=81700 of ranks=87208rank=81800 of ranks=87208rank=81900 of ranks=87208rank=82000 of ranks=87208rank=82100 of ranks=87208rank=82200 of ranks=87208rank=82300 of ranks=87208rank=82400 of ranks=87208rank=82500 of ranks=87208rank=82600 of ranks=87208rank=82700 of ranks=87208rank=82800 of ranks=87208rank=82900 of ranks=87208rank=83000 of ranks=87208rank=83100 of ranks=87208rank=83200 of ranks=87208rank=83300 of ranks=87208rank=83400 of ranks=87208rank=83500 of ranks=87208rank=83600 of ranks=87208rank=83700 of ranks=87208rank=83800 of ranks=87208rank=83900 of ranks=87208rank=84000 of ranks=87208rank=84100 of ranks=87208rank=84200 of ranks=87208rank=84300 of ranks=87208rank=84400 of ranks=87208rank=84500 of ranks=87208rank=84600 of ranks=87208rank=84700 of ranks=87208rank=84800 of ranks=87208rank=84900 of ranks=87208rank=85000 of ranks=87208rank=85100 of ranks=87208rank=85200 of ranks=87208rank=85300 of ranks=87208rank=85400 of ranks=87208rank=85500 of ranks=87208rank=85600 of ranks=87208rank=85700 of ranks=87208rank=85800 of ranks=87208rank=85900 of ranks=87208rank=86000 of ranks=87208rank=86100 of ranks=87208rank=86200 of ranks=87208rank=86300 of ranks=87208rank=86400 of ranks=87208rank=86500 of ranks=87208rank=86600 of ranks=87208rank=86700 of ranks=87208rank=86800 of ranks=87208rank=86900 of ranks=87208rank=87000 of ranks=87208rank=87100 of ranks=87208rank=87200 of ranks=87208

  Id  Name                  AP(%)     TP     FP     FN     GT   AvgIoU@conf(%)
  --  --------------------  --------- ------ ------ ------ ------ -----------------
   0 motorbike              98.1284    493    794      5    498           79.4728
   1 car                    98.7919  50026  22222    290  50316           84.3958
   2 truck                  98.5051   1811   1962     14   1825           79.5530
   3 bus                    97.2196    362   1582      4    366           76.4176
   4 pedestrian             97.2328   4173   3783     86   4259           78.1470

for conf_thresh=0.25, precision=0.92, recall=0.97, F1 score=0.94
for conf_thresh=0.25, TP=55383, FP=4931, FN=1881, average IoU=83.69%
IoU threshold=50.00%, used area-under-curve for each unique recall
mean average precision (mAP@0.50)=97.98%
Total detection time: 74 seconds
Set -points flag:
 '-points 101' for MSCOCO
 '-points 11' for PascalVOC 2007 (uncomment 'difficult' in voc.data)
 '-points 0' (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset
Saving weights to /workspace/.cache/splits/combined_8000.weights
Saving weights to /workspace/.cache/splits/combined_last.weights
Saving weights to /workspace/.cache/splits/combined_final.weights

Last accuracy mAP@0.50=97.98%, best=98.10% at iteration #7902.

Training iteration has reached max batch limit of 8000.  If you want
to restart training with these weights, either increase the limit, or
use the "-clear" flag to reset the training images counter to zero.

